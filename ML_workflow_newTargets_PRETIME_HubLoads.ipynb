{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384b0459",
   "metadata": {},
   "source": [
    "# ü§ñ Machine Learning Workflow - Turbinas E√≥licas\n",
    "\n",
    "Notebook completo para el flujo de trabajo de Machine Learning con datos de turbinas e√≥licas.\n",
    "\n",
    "## üìã Objetivo\n",
    "\n",
    "Desarrollar modelos de predicci√≥n para momentos flectores en las palas de turbinas e√≥licas.\n",
    "\n",
    "**Targets**: \n",
    "- Blade root 1 My\n",
    "- Blade root 2 My\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732350f1",
   "metadata": {},
   "source": [
    "## üîß STEP 1: Configuraci√≥n Inicial del Entorno\n",
    "\n",
    "En este paso configuramos el entorno de trabajo completo para el an√°lisis de Machine Learning.\n",
    "\n",
    "### üì¶ Librer√≠as que vamos a importar:\n",
    "\n",
    "1. **Librer√≠as est√°ndar de Python**: os, sys, pathlib para manejo de archivos\n",
    "2. **An√°lisis de datos**: pandas (DataFrames) y numpy (operaciones num√©ricas)\n",
    "3. **Visualizaci√≥n**: matplotlib y seaborn para gr√°ficos\n",
    "4. **Machine Learning**: scikit-learn para modelos y preprocesamiento\n",
    "5. **Algoritmos avanzados**: XGBoost y LightGBM (si est√°n disponibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c621213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as est√°ndar de Python importadas\n",
      "‚úÖ Librer√≠as de an√°lisis de datos importadas\n",
      "‚úÖ Librer√≠as de Machine Learning importadas\n",
      "======================================================================\n",
      "CONFIGURACION BASICA COMPLETADA\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Configuraci√≥n de pandas aplicada\n",
      "\n",
      "======================================================================\n",
      "CONFIGURACION BASICA COMPLETADA\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 1.1: Importar librer√≠as est√°ndar de Python\n",
    "# ============================================================================\n",
    "\n",
    "# Manejo del sistema operativo y archivos\n",
    "import os           # Operaciones del sistema operativo (crear carpetas, paths, etc.)\n",
    "import sys          # Configuraci√≥n del sistema Python (paths, argumentos)\n",
    "from pathlib import Path  # Manejo moderno de rutas de archivos\n",
    "\n",
    "# Manejo de advertencias\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignorar warnings para output m√°s limpio\n",
    "\n",
    "print(\"‚úÖ Librer√≠as est√°ndar de Python importadas\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.2: Importar librer√≠as de an√°lisis de datos\n",
    "# ============================================================================\n",
    "\n",
    "# pandas: Librer√≠a principal para manipulaci√≥n de datos tabulares\n",
    "import pandas as pd\n",
    "\n",
    "# numpy: Operaciones num√©ricas y arrays eficientes\n",
    "import numpy as np\n",
    "\n",
    "# joblib: Guardar y cargar modelos y scalers\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ Librer√≠as de an√°lisis de datos importadas\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.3: Importar librer√≠as de visualizaci√≥n\n",
    "# ============================================================================\n",
    "\n",
    "# matplotlib: Librer√≠a base para gr√°ficos en Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn: Visualizaciones estad√≠sticas de alto nivel\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuraci√≥n de estilo de gr√°ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  # Estilo con grid oscuro\n",
    "sns.set_palette('husl')  # Paleta de colores HUSL (vibrante y distinguible)\n",
    "\n",
    "# Mostrar gr√°ficos directamente en el notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# PASO 1.4: Importar librer√≠as de Machine Learning\n",
    "\n",
    "# ============================================================================\n",
    "# Scikit-learn: Modelos y m√©tricas\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"‚úÖ Librer√≠as de Machine Learning importadas\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.5: Configurar pandas para mejor visualizaci√≥n\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACION BASICA COMPLETADA\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Configuraci√≥n de pandas aplicada\")\n",
    "pd.set_option('display.precision', 4)# Precisi√≥n de 4 decimales para n√∫meros\n",
    "pd.set_option('display.max_rows', 100)# Mostrar hasta 100 filas\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURACION BASICA COMPLETADA\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3536944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Herramientas de ML (sklearn) importadas\n",
      "‚úÖ Modelos de ML (sklearn) importados\n",
      "‚úÖ XGBoost disponible (versi√≥n: 3.1.3)\n",
      "‚ö†Ô∏è  LightGBM NO disponible - instalar con: pip install lightgbm\n",
      "\n",
      "======================================================================\n",
      "üéâ LIBRER√çAS DE MACHINE LEARNING COMPLETADAS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 1.5: Importar herramientas de Machine Learning - Scikit-learn\n",
    "# ============================================================================\n",
    "\n",
    "# --- VALIDACI√ìN Y SEPARACI√ìN DE DATOS ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,    # Dividir datos en train/test\n",
    "    cross_val_score,     # Validaci√≥n cruzada\n",
    "    KFold                # K-Fold para validaci√≥n cruzada robusta\n",
    ")\n",
    "\n",
    "# --- PREPROCESAMIENTO ---\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,      # Normalizaci√≥n Z-score (media=0, std=1)\n",
    "    MinMaxScaler         # Escalado a rango [0,1]\n",
    ")\n",
    "\n",
    "# --- M√âTRICAS DE EVALUACI√ìN ---\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,  # MSE: Error cuadr√°tico medio\n",
    "    r2_score,            # R¬≤: Coeficiente de determinaci√≥n\n",
    "    mean_absolute_error  # MAE: Error absoluto medio\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Herramientas de ML (sklearn) importadas\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.6: Importar modelos de Machine Learning\n",
    "# ============================================================================\n",
    "\n",
    "# --- MODELOS LINEALES ---\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,    # Regresi√≥n lineal simple\n",
    "    Ridge,               # Regresi√≥n Ridge (regularizaci√≥n L2)\n",
    "    Lasso,               # Regresi√≥n Lasso (regularizaci√≥n L1)\n",
    "    ElasticNet           # ElasticNet (combinaci√≥n L1 + L2)\n",
    ")\n",
    "\n",
    "# --- MODELOS BASADOS EN √ÅRBOLES ---\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,      # Random Forest: ensemble de √°rboles\n",
    "    ExtraTreesRegressor,        # Extra Trees: √°rboles con splits aleatorios\n",
    "    GradientBoostingRegressor   # Gradient Boosting: boosting secuencial\n",
    ")\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor  # √Årbol de decisi√≥n simple\n",
    "\n",
    "print(\"‚úÖ Modelos de ML (sklearn) importados\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.7: Importar algoritmos avanzados (XGBoost y LightGBM)\n",
    "# ============================================================================\n",
    "\n",
    "# XGBoost: Extreme Gradient Boosting (muy potente para competiciones)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"‚úÖ XGBoost disponible (versi√≥n: {})\".format(xgb.__version__))\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  XGBoost NO disponible - instalar con: pip install xgboost\")\n",
    "    xgb = None\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "# LightGBM: Light Gradient Boosting Machine (r√°pido y eficiente)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"‚úÖ LightGBM disponible (versi√≥n: {})\".format(lgb.__version__))\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  LightGBM NO disponible - instalar con: pip install lightgbm\")\n",
    "    lgb = None\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ LIBRER√çAS DE MACHINE LEARNING COMPLETADAS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9428642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ESTRUCTURA DE DIRECTORIOS\n",
      "======================================================================\n",
      "Directorio raiz:     C:\\Users\\Bladedgreen\\Desktop\\_GitHub\n",
      "Datos entrenamiento: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train OK\n",
      "Modelos guardados:   C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\03_ML_traditional_models OK\n",
      "Resultados:          C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\_results OK\n",
      "Eda folder:          C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\notebook\\00_EDA_traditional_ML OK\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 1.8: Configurar estructura de directorios del proyecto\n",
    "# ============================================================================\n",
    "\n",
    "# Directorio ra√≠z del proyecto (carpeta padre del notebook)\n",
    "root_dir = Path.cwd().parent\n",
    "\n",
    "# Directorios principales del proyecto\n",
    "data_folder = root_dir / \"data_train\"      # Datos de entrenamiento (CSVs)\n",
    "data_folder_ml = root_dir / \"data_train_traditional_ML\"\n",
    "models_folder = root_dir / \"03_ML_traditional_models\"      # Modelos entrenados guardados\n",
    "results_folder = root_dir / \"_results\"     # Resultados de predicciones\n",
    "eda_folder =  root_dir / 'notebook' /  \"00_EDA_traditional_ML\"\n",
    "complete_dataset_path = data_folder_ml / \"0000_Complete_dataset.csv\"\n",
    "training_folder = root_dir / 'notebook' / '03_Models_training'\n",
    "scaler_folder = root_dir / 'notebook' / '01_Models_scaler'\n",
    "\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "models_folder.mkdir(exist_ok=True)\n",
    "results_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Verificar existencia de carpetas cr√≠ticas\n",
    "data_exists = data_folder.exists()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESTRUCTURA DE DIRECTORIOS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Directorio raiz:     {root_dir}\")\n",
    "print(f\"Datos entrenamiento: {data_folder} {'OK' if data_exists else 'NO EXISTE'}\")\n",
    "print(f\"Modelos guardados:   {models_folder} OK\")\n",
    "print(f\"Resultados:          {results_folder} OK\")\n",
    "print(f\"Eda folder:          {eda_folder} OK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Advertencias si faltan directorios cr√≠ticos\n",
    "if not data_exists:\n",
    "    print(\"\\nADVERTENCIA: Carpeta 'data_train' no encontrada\")\n",
    "    print(\"   -> Usando directorio raiz para buscar datos\")\n",
    "    data_folder = root_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2bb14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Datos guardados (normalizados y no normalizados, de test y train)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos originales\n",
    "X_train = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\X_train.pkl')\n",
    "y_train = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\y_train.pkl')\n",
    "X_test = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\X_test.pkl')\n",
    "y_test = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\y_test.pkl')\n",
    "\n",
    "# Cargar datos normalizados\n",
    "X_train_norm = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\X_train_norm.pkl')\n",
    "y_train_norm = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\y_train_norm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea82baf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ CONFIGURACI√ìN INICIAL COMPLETADA\n",
    "\n",
    "El entorno de trabajo est√° listo para comenzar el an√°lisis de Machine Learning.\n",
    "\n",
    "### üìä Resumen de configuraci√≥n:\n",
    "\n",
    "- ‚úÖ **Librer√≠as b√°sicas**: pandas, numpy, matplotlib, seaborn\n",
    "- ‚úÖ **Librer√≠as ML**: scikit-learn completo\n",
    "- ‚úÖ **Algoritmos avanzados**: XGBoost y LightGBM (si disponibles)\n",
    "- ‚úÖ **Directorios**: data_train, _report, _models, _results\n",
    "- ‚úÖ **Configuraci√≥n**: Pandas optimizado, gr√°ficos configurados\n",
    "\n",
    "### üéØ Variables globales disponibles:\n",
    "\n",
    "```python\n",
    "root_dir         # Directorio ra√≠z del proyecto\n",
    "data_folder      # Carpeta con datos CSV de entrenamiento\n",
    "report_folder    # Carpeta con reportes generados\n",
    "models_folder    # Carpeta para guardar modelos entrenados\n",
    "results_folder   # Carpeta para resultados de predicciones\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Pr√≥ximos Pasos\n",
    "\n",
    "**Indica qu√© quieres hacer a continuaci√≥n:**\n",
    "\n",
    "Por ejemplo:\n",
    "- Cargar datos CSV\n",
    "- Explorar variables y estad√≠sticas\n",
    "- Crear features (lags, ventanas, etc.)\n",
    "- Entrenar modelos\n",
    "- Evaluar y comparar modelos\n",
    "- Hacer predicciones\n",
    "\n",
    "**Todo el c√≥digo se escribir√° aqu√≠ mismo, sin llamadas a funciones externas.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c4c63",
   "metadata": {},
   "source": [
    "## üìä STEP 2: Importar Se√±ales desde Simulaciones Bladed\n",
    "\n",
    "En este paso vamos a importar las se√±ales desde las simulaciones de Bladed para generar los archivos CSV de entrenamiento.\n",
    "\n",
    "### üéØ Objetivo:\n",
    "Generar archivos CSV con las features/se√±ales espec√≠ficas que queremos analizar y usar para el modelo ML.\n",
    "\n",
    "### üì¶ Proceso:\n",
    "1. Importar librer√≠a postprocessbladed\n",
    "2. Configurar rutas y archivos\n",
    "3. Definir variables a extraer\n",
    "4. Cargar datos de simulaciones\n",
    "5. Exportar a CSV\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9926b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - postprocessbladed importado correctamente\n",
      "   Ruta: D:\\Python_modules\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2.1: Importar postprocessbladed (libreria para leer simulaciones Bladed)\n",
    "# ============================================================================\n",
    "\n",
    "# A√±adir ruta de postprocessbladed al path\n",
    "postprocessbladed_parent_path = r\"D:\\Python_modules\"\n",
    "if postprocessbladed_parent_path not in sys.path:\n",
    "    sys.path.append(postprocessbladed_parent_path)\n",
    "\n",
    "# Importar postprocessbladed\n",
    "import postprocessbladed as pp\n",
    "\n",
    "print(\"OK - postprocessbladed importado correctamente\")\n",
    "print(f\"   Ruta: {postprocessbladed_parent_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "851a4e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURACION DE RUTAS\n",
      "======================================================================\n",
      "Simulaciones Bladed: U:\\Studies\\437_lidar_VLOS_IPC\\Outputs\\OPT3_PassiveYaw\\DLC12a\n",
      "Archivos a procesar: 72\n",
      "  1. 0001_DLC12a_030_000\n",
      "  2. 0002_DLC12a_030_000\n",
      "  3. 0003_DLC12a_030_000\n",
      "  4. 0004_DLC12a_030_000\n",
      "  5. 0005_DLC12a_030_000\n",
      "  6. 0006_DLC12a_030_000\n",
      "  7. 0007_DLC12a_050_000\n",
      "  8. 0008_DLC12a_050_000\n",
      "  9. 0009_DLC12a_050_000\n",
      "  10. 0010_DLC12a_050_000\n",
      "  11. 0011_DLC12a_050_000\n",
      "  12. 0012_DLC12a_050_000\n",
      "  13. 0013_DLC12a_070_000\n",
      "  14. 0014_DLC12a_070_000\n",
      "  15. 0015_DLC12a_070_000\n",
      "  16. 0016_DLC12a_070_000\n",
      "  17. 0017_DLC12a_070_000\n",
      "  18. 0018_DLC12a_070_000\n",
      "  19. 0019_DLC12a_090_000\n",
      "  20. 0020_DLC12a_090_000\n",
      "  21. 0021_DLC12a_090_000\n",
      "  22. 0022_DLC12a_090_000\n",
      "  23. 0023_DLC12a_090_000\n",
      "  24. 0024_DLC12a_090_000\n",
      "  25. 0025_DLC12a_110_000\n",
      "  26. 0026_DLC12a_110_000\n",
      "  27. 0027_DLC12a_110_000\n",
      "  28. 0028_DLC12a_110_000\n",
      "  29. 0029_DLC12a_110_000\n",
      "  30. 0030_DLC12a_110_000\n",
      "  31. 0031_DLC12a_130_000\n",
      "  32. 0032_DLC12a_130_000\n",
      "  33. 0033_DLC12a_130_000\n",
      "  34. 0034_DLC12a_130_000\n",
      "  35. 0035_DLC12a_130_000\n",
      "  36. 0036_DLC12a_130_000\n",
      "  37. 0037_DLC12a_150_000\n",
      "  38. 0038_DLC12a_150_000\n",
      "  39. 0039_DLC12a_150_000\n",
      "  40. 0040_DLC12a_150_000\n",
      "  41. 0041_DLC12a_150_000\n",
      "  42. 0042_DLC12a_150_000\n",
      "  43. 0043_DLC12a_170_000\n",
      "  44. 0044_DLC12a_170_000\n",
      "  45. 0045_DLC12a_170_000\n",
      "  46. 0046_DLC12a_170_000\n",
      "  47. 0047_DLC12a_170_000\n",
      "  48. 0048_DLC12a_170_000\n",
      "  49. 0049_DLC12a_190_000\n",
      "  50. 0050_DLC12a_190_000\n",
      "  51. 0051_DLC12a_190_000\n",
      "  52. 0052_DLC12a_190_000\n",
      "  53. 0053_DLC12a_190_000\n",
      "  54. 0054_DLC12a_190_000\n",
      "  55. 0055_DLC12a_210_000\n",
      "  56. 0056_DLC12a_210_000\n",
      "  57. 0057_DLC12a_210_000\n",
      "  58. 0058_DLC12a_210_000\n",
      "  59. 0059_DLC12a_210_000\n",
      "  60. 0060_DLC12a_210_000\n",
      "  61. 0061_DLC12a_230_000\n",
      "  62. 0062_DLC12a_230_000\n",
      "  63. 0063_DLC12a_230_000\n",
      "  64. 0064_DLC12a_230_000\n",
      "  65. 0065_DLC12a_230_000\n",
      "  66. 0066_DLC12a_230_000\n",
      "  67. 0067_DLC12a_250_000\n",
      "  68. 0068_DLC12a_250_000\n",
      "  69. 0069_DLC12a_250_000\n",
      "  70. 0070_DLC12a_250_000\n",
      "  71. 0071_DLC12a_250_000\n",
      "  72. 0072_DLC12a_250_000\n",
      "\n",
      "Guardar CSV en:      C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\n",
      "Anadir unidades:     False\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2.2: Configurar rutas y parametros de carga\n",
    "# ============================================================================\n",
    "\n",
    "# Ruta donde estan las simulaciones de Bladed (archivos .$TE)\n",
    "loadpath = r\"U:\\Studies\\437_lidar_VLOS_IPC\\Outputs\\OPT3_PassiveYaw\\DLC12a\"\n",
    "\n",
    "# Lista de nombres de archivos de simulacion a procesar\n",
    "# PUEDES A√ëADIR VARIOS ARCHIVOS Y SE GENERARA UN CSV POR CADA UNO\n",
    "file_names = [\n",
    "    \"0001_DLC12a_030_000\",\n",
    "    \"0002_DLC12a_030_000\",\n",
    "    \"0003_DLC12a_030_000\",\n",
    "    \"0004_DLC12a_030_000\",\n",
    "    \"0005_DLC12a_030_000\",\n",
    "    \"0006_DLC12a_030_000\",\n",
    "    \"0007_DLC12a_050_000\",\n",
    "    \"0008_DLC12a_050_000\",\n",
    "    \"0009_DLC12a_050_000\",\n",
    "    \"0010_DLC12a_050_000\",\n",
    "    \"0011_DLC12a_050_000\",\n",
    "    \"0012_DLC12a_050_000\",\n",
    "    \"0013_DLC12a_070_000\",\n",
    "    \"0014_DLC12a_070_000\",\n",
    "    \"0015_DLC12a_070_000\",\n",
    "    \"0016_DLC12a_070_000\",\n",
    "    \"0017_DLC12a_070_000\",\n",
    "    \"0018_DLC12a_070_000\",\n",
    "    \"0019_DLC12a_090_000\",\n",
    "    \"0020_DLC12a_090_000\",\n",
    "    \"0021_DLC12a_090_000\",\n",
    "    \"0022_DLC12a_090_000\",\n",
    "    \"0023_DLC12a_090_000\",\n",
    "    \"0024_DLC12a_090_000\",\n",
    "    \"0025_DLC12a_110_000\",\n",
    "    \"0026_DLC12a_110_000\",\n",
    "    \"0027_DLC12a_110_000\",\n",
    "    \"0028_DLC12a_110_000\",\n",
    "    \"0029_DLC12a_110_000\",\n",
    "    \"0030_DLC12a_110_000\",\n",
    "    \"0031_DLC12a_130_000\",\n",
    "    \"0032_DLC12a_130_000\",\n",
    "    \"0033_DLC12a_130_000\",\n",
    "    \"0034_DLC12a_130_000\",\n",
    "    \"0035_DLC12a_130_000\",\n",
    "    \"0036_DLC12a_130_000\",\n",
    "    \"0037_DLC12a_150_000\",\n",
    "    \"0038_DLC12a_150_000\",\n",
    "    \"0039_DLC12a_150_000\",\n",
    "    \"0040_DLC12a_150_000\",\n",
    "    \"0041_DLC12a_150_000\",\n",
    "    \"0042_DLC12a_150_000\",\n",
    "    \"0043_DLC12a_170_000\",\n",
    "    \"0044_DLC12a_170_000\",\n",
    "    \"0045_DLC12a_170_000\",\n",
    "    \"0046_DLC12a_170_000\",\n",
    "    \"0047_DLC12a_170_000\",\n",
    "    \"0048_DLC12a_170_000\",\n",
    "    \"0049_DLC12a_190_000\",\n",
    "    \"0050_DLC12a_190_000\",\n",
    "    \"0051_DLC12a_190_000\",\n",
    "    \"0052_DLC12a_190_000\",\n",
    "    \"0053_DLC12a_190_000\",\n",
    "    \"0054_DLC12a_190_000\",\n",
    "    \"0055_DLC12a_210_000\",\n",
    "    \"0056_DLC12a_210_000\",\n",
    "    \"0057_DLC12a_210_000\",\n",
    "    \"0058_DLC12a_210_000\",\n",
    "    \"0059_DLC12a_210_000\",\n",
    "    \"0060_DLC12a_210_000\",\n",
    "    \"0061_DLC12a_230_000\",\n",
    "    \"0062_DLC12a_230_000\",\n",
    "    \"0063_DLC12a_230_000\",\n",
    "    \"0064_DLC12a_230_000\",\n",
    "    \"0065_DLC12a_230_000\",\n",
    "    \"0066_DLC12a_230_000\",\n",
    "    \"0067_DLC12a_250_000\",\n",
    "    \"0068_DLC12a_250_000\",\n",
    "    \"0069_DLC12a_250_000\",\n",
    "    \"0070_DLC12a_250_000\",\n",
    "    \"0071_DLC12a_250_000\",\n",
    "    \"0072_DLC12a_250_000\"\n",
    "    #\"wind_turb_10ms\"\n",
    "    #\"wind_turb_11ms\",\n",
    "    #\"wind_turb_12ms\",\n",
    "    #\"wind_turb_13ms\",\n",
    "    #\"wind_turb_14ms\",\n",
    "    #\"wind_turb_15ms\",\n",
    "    #\"wind_turb_16ms\",\n",
    "    #\"wind_turb_17ms\",\n",
    "    #\"wind_turb_18ms\",\n",
    "    #\"wind_turb_19ms\",\n",
    "    #\"wind_turb_20ms\"\n",
    "]\n",
    "\n",
    "# Ruta donde guardar los resultados CSV\n",
    "resultspath = str(data_folder)  # Usar data_folder definida anteriormente\n",
    "\n",
    "# Opcion para a√±adir unidades en el CSV (True/False)\n",
    "add_units = False\n",
    "\n",
    "# Crear carpeta de resultados si no existe\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACION DE RUTAS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Simulaciones Bladed: {loadpath}\")\n",
    "print(f\"Archivos a procesar: {len(file_names)}\")\n",
    "for i, fname in enumerate(file_names, 1):\n",
    "    print(f\"  {i}. {fname}\")\n",
    "print(f\"\\nGuardar CSV en:      {resultspath}\")\n",
    "print(f\"Anadir unidades:     {add_units}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b01ad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VARIABLES A EXTRAER\n",
      "======================================================================\n",
      "\n",
      "Hub_fixed: 2 variables\n",
      "  - Stationary hub My\n",
      "  - Stationary hub Mz\n",
      "\n",
      "Hub_rotating: 2 variables\n",
      "  - Blade root 1 My\n",
      "  - Blade root 2 My\n",
      "\n",
      "Pitch_actuator: 2 variables\n",
      "  - Blade 1 pitch angle\n",
      "  - Blade 2 pitch angle\n",
      "\n",
      "Drive_train: 1 variables\n",
      "  - Rotor azimuth angle\n",
      "\n",
      "Summary: 1 variables\n",
      "  - Rotor speed\n",
      "\n",
      "External_controller: 49 variables\n",
      "  - LAC_VLOS_BEAM0_RANGE5\n",
      "  - LAC_VLOS_BEAM1_RANGE5\n",
      "  - LAC_VLOS_BEAM2_RANGE5\n",
      "  - LAC_VLOS_BEAM3_RANGE5\n",
      "  - LAC_VLOS_BEAM4_RANGE5\n",
      "  - LAC_VLOS_BEAM5_RANGE5\n",
      "  - LAC_VLOS_BEAM6_RANGE5\n",
      "  - LAC_VLOS_BEAM7_RANGE5\n",
      "  - LAC_VLOS_BEAM0_RANGE4\n",
      "  - LAC_VLOS_BEAM1_RANGE4\n",
      "  - LAC_VLOS_BEAM2_RANGE4\n",
      "  - LAC_VLOS_BEAM3_RANGE4\n",
      "  - LAC_VLOS_BEAM4_RANGE4\n",
      "  - LAC_VLOS_BEAM5_RANGE4\n",
      "  - LAC_VLOS_BEAM6_RANGE4\n",
      "  - LAC_VLOS_BEAM7_RANGE4\n",
      "  - LAC_VLOS_BEAM0_RANGE3\n",
      "  - LAC_VLOS_BEAM1_RANGE3\n",
      "  - LAC_VLOS_BEAM2_RANGE3\n",
      "  - LAC_VLOS_BEAM3_RANGE3\n",
      "  - LAC_VLOS_BEAM4_RANGE3\n",
      "  - LAC_VLOS_BEAM5_RANGE3\n",
      "  - LAC_VLOS_BEAM6_RANGE3\n",
      "  - LAC_VLOS_BEAM7_RANGE3\n",
      "  - LAC_VLOS_BEAM0_RANGE2\n",
      "  - LAC_VLOS_BEAM1_RANGE2\n",
      "  - LAC_VLOS_BEAM2_RANGE2\n",
      "  - LAC_VLOS_BEAM3_RANGE2\n",
      "  - LAC_VLOS_BEAM4_RANGE2\n",
      "  - LAC_VLOS_BEAM5_RANGE2\n",
      "  - LAC_VLOS_BEAM6_RANGE2\n",
      "  - LAC_VLOS_BEAM7_RANGE2\n",
      "  - LAC_VLOS_BEAM0_RANGE1\n",
      "  - LAC_VLOS_BEAM1_RANGE1\n",
      "  - LAC_VLOS_BEAM2_RANGE1\n",
      "  - LAC_VLOS_BEAM3_RANGE1\n",
      "  - LAC_VLOS_BEAM4_RANGE1\n",
      "  - LAC_VLOS_BEAM5_RANGE1\n",
      "  - LAC_VLOS_BEAM6_RANGE1\n",
      "  - LAC_VLOS_BEAM7_RANGE1\n",
      "  - LAC_VLOS_BEAM0_RANGE0\n",
      "  - LAC_VLOS_BEAM1_RANGE0\n",
      "  - LAC_VLOS_BEAM2_RANGE0\n",
      "  - LAC_VLOS_BEAM3_RANGE0\n",
      "  - LAC_VLOS_BEAM4_RANGE0\n",
      "  - LAC_VLOS_BEAM5_RANGE0\n",
      "  - LAC_VLOS_BEAM6_RANGE0\n",
      "  - LAC_VLOS_BEAM7_RANGE0\n",
      "  - OPER_MEAS_YAWERROR\n",
      "\n",
      "======================================================================\n",
      "TOTAL: 57 variables\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2.3: Definir diccionarios de variables a extraer\n",
    "# ============================================================================\n",
    "# Aqui defines QUE SE√ëALES quieres extraer de las simulaciones\n",
    "# Organizadas por categoria (se√±al de Bladed)\n",
    "\n",
    "# Inicializar diccionario de variables\n",
    "var_dicts = {}\n",
    "\n",
    "# EJEMPLO 1: Variables de velocidad del viento en la pala 1 (Aero_B1)\n",
    "# Estas son las velocidades del viento incidente a diferentes posiciones radiales\n",
    "# var_dicts[\"Aero_B1\"] = {\n",
    "#     \"Aero_B1\": [\n",
    "#         \"Blade 1 Incident axial wind speed at 0m\",\n",
    "#         \"Blade 1 Incident axial wind speed at 6m\",\n",
    "#         \"Blade 1 Incident axial wind speed at 18m\",\n",
    "#         \"Blade 1 Incident axial wind speed at 30m\",\n",
    "#         \"Blade 1 Incident axial wind speed at 46m\",\n",
    "#         \"Blade 1 Incident axial wind speed at 59m\",\n",
    "#         \"Blade 1 Incident axial wind speed at 68.25m\",\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# EJEMPLO 2: Variables de velocidad del viento en la pala 2 (Aero_B2)\n",
    "# var_dicts[\"Aero_B2\"] = {\n",
    "#     \"Aero_B2\": [\n",
    "#         \"Blade 2 Incident axial wind speed at 0m\",\n",
    "#         \"Blade 2 Incident axial wind speed at 6m\",\n",
    "#         \"Blade 2 Incident axial wind speed at 18m\",\n",
    "#         \"Blade 2 Incident axial wind speed at 30m\",\n",
    "#         \"Blade 2 Incident axial wind speed at 46m\",\n",
    "#         \"Blade 2 Incident axial wind speed at 59m\",\n",
    "#         \"Blade 2 Incident axial wind speed at 68.25m\",\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# Posiciones radiales (m) para las se√±ales Aero (deben coincidir con las variables)\n",
    "#AERO_POSITIONS = [0.0, 6.0, 18.0, 30.0, 46.0, 59.0, 68.25]\n",
    "\n",
    "# EJEMPLO 3: Informacion ambiental\n",
    "#var_dicts[\"Environmental_information\"] = {\n",
    "#    \"Environmental_information\": [\"Rotor average longitudinal wind speed\"]\n",
    "#}\n",
    "\n",
    "# EJEMPLO 4: Variables en el hub rotante (TARGETS - lo que queremos predecir)\n",
    "var_dicts[\"Hub_fixed\"] = {\n",
    "    \"Hub_fixed\": [\n",
    "        \"Stationary hub My\",  # Momento flector pala 1 (TARGET)\n",
    "        \"Stationary hub Mz\"   # Momento flector pala 2 (TARGET)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# EJEMPLO 4: Variables en el hub rotante (TARGETS - lo que queremos predecir)\n",
    "var_dicts[\"Hub_rotating\"] = {\n",
    "    \"Hub_rotating\": [\n",
    "        \"Blade root 1 My\",  # Momento flector pala 1 (TARGET)\n",
    "        \"Blade root 2 My\"   # Momento flector pala 2 (TARGET)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# EJEMPLO 5: Actuadores de pitch (control de palas)\n",
    "var_dicts[\"Pitch_actuator\"] = {\n",
    "    \"Pitch_actuator\": [\n",
    "        \"Blade 1 pitch angle\",  # Angulo de pala 1\n",
    "        \"Blade 2 pitch angle\",  # Angulo de pala 2\n",
    "        #\"Blade 1 pitch rate\",   # Velocidad angular pala 1\n",
    "        #\"Blade 2 pitch rate\"    # Velocidad angular pala 2\n",
    "    ]\n",
    "}\n",
    "\n",
    "# EJEMPLO 6: Drivetrain (tren de potencia)\n",
    "var_dicts[\"Drive_train\"] = {\n",
    "    \"Drive_train\": [\"Rotor azimuth angle\"]  # Angulo azimutal del rotor\n",
    "}\n",
    "\n",
    "# EJEMPLO 7: Rotor speed \n",
    "var_dicts[\"Summary\"] = {\n",
    "    \"Summary\": [\"Rotor speed\"]  # Rotor speed\n",
    "}\n",
    "\n",
    "# EJEMPLO 8: VLOS \n",
    "var_dicts[\"External_controller\"] = {\n",
    "    \"External_controller\": [\n",
    "        \"LAC_VLOS_BEAM0_RANGE5\",\n",
    "        \"LAC_VLOS_BEAM1_RANGE5\",\n",
    "        \"LAC_VLOS_BEAM2_RANGE5\",\n",
    "        \"LAC_VLOS_BEAM3_RANGE5\",\n",
    "        \"LAC_VLOS_BEAM4_RANGE5\",\n",
    "        \"LAC_VLOS_BEAM5_RANGE5\",\n",
    "        \"LAC_VLOS_BEAM6_RANGE5\",\n",
    "        \"LAC_VLOS_BEAM7_RANGE5\",\n",
    "        \"LAC_VLOS_BEAM0_RANGE4\",\n",
    "        \"LAC_VLOS_BEAM1_RANGE4\",\n",
    "        \"LAC_VLOS_BEAM2_RANGE4\",\n",
    "        \"LAC_VLOS_BEAM3_RANGE4\",\n",
    "        \"LAC_VLOS_BEAM4_RANGE4\",\n",
    "        \"LAC_VLOS_BEAM5_RANGE4\",\n",
    "        \"LAC_VLOS_BEAM6_RANGE4\",\n",
    "        \"LAC_VLOS_BEAM7_RANGE4\",\n",
    "        \"LAC_VLOS_BEAM0_RANGE3\",\n",
    "        \"LAC_VLOS_BEAM1_RANGE3\",\n",
    "        \"LAC_VLOS_BEAM2_RANGE3\",\n",
    "        \"LAC_VLOS_BEAM3_RANGE3\",\n",
    "        \"LAC_VLOS_BEAM4_RANGE3\",\n",
    "        \"LAC_VLOS_BEAM5_RANGE3\",\n",
    "        \"LAC_VLOS_BEAM6_RANGE3\",\n",
    "        \"LAC_VLOS_BEAM7_RANGE3\",\n",
    "        \"LAC_VLOS_BEAM0_RANGE2\",\n",
    "        \"LAC_VLOS_BEAM1_RANGE2\",\n",
    "        \"LAC_VLOS_BEAM2_RANGE2\",\n",
    "        \"LAC_VLOS_BEAM3_RANGE2\",\n",
    "        \"LAC_VLOS_BEAM4_RANGE2\",\n",
    "        \"LAC_VLOS_BEAM5_RANGE2\",\n",
    "        \"LAC_VLOS_BEAM6_RANGE2\",\n",
    "        \"LAC_VLOS_BEAM7_RANGE2\",\n",
    "        \"LAC_VLOS_BEAM0_RANGE1\",\n",
    "        \"LAC_VLOS_BEAM1_RANGE1\",\n",
    "        \"LAC_VLOS_BEAM2_RANGE1\",\n",
    "        \"LAC_VLOS_BEAM3_RANGE1\",\n",
    "        \"LAC_VLOS_BEAM4_RANGE1\",\n",
    "        \"LAC_VLOS_BEAM5_RANGE1\",\n",
    "        \"LAC_VLOS_BEAM6_RANGE1\",\n",
    "        \"LAC_VLOS_BEAM7_RANGE1\",\n",
    "        \"LAC_VLOS_BEAM0_RANGE0\",\n",
    "        \"LAC_VLOS_BEAM1_RANGE0\",\n",
    "        \"LAC_VLOS_BEAM2_RANGE0\",\n",
    "        \"LAC_VLOS_BEAM3_RANGE0\",\n",
    "        \"LAC_VLOS_BEAM4_RANGE0\",\n",
    "        \"LAC_VLOS_BEAM5_RANGE0\",\n",
    "        \"LAC_VLOS_BEAM6_RANGE0\",\n",
    "        \"LAC_VLOS_BEAM7_RANGE0\",\n",
    "        \"OPER_MEAS_YAWERROR\",\n",
    "        ]  # VLOS\n",
    "}\n",
    "\n",
    "# Resumen de variables definidas\n",
    "print(\"=\"*70)\n",
    "print(\"VARIABLES A EXTRAER\")\n",
    "print(\"=\"*70)\n",
    "total_vars = 0\n",
    "for dict_name, var_dict in var_dicts.items():\n",
    "    for signal, variables in var_dict.items():\n",
    "        print(f\"\\n{signal}: {len(variables)} variables\")\n",
    "        for var in variables:\n",
    "            print(f\"  - {var}\")\n",
    "            total_vars += 1\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOTAL: {total_vars} variables\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af6fd7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - Funcion create_timeseries_csv definida correctamente\n",
      "  Esta funcion replica exactamente timeseries_script.py\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2.4: Funciones de ayuda para procesar archivos Bladed\n",
    "# ============================================================================\n",
    "\n",
    "# FUNCION 1: Crear CSV de series temporales desde Bladed (IGUAL QUE timeseries_script.py)\n",
    "def create_timeseries_csv(bin_series, header_series, filenames, var_dict, output_path, output_filename, add_units=False):\n",
    "    \"\"\"\n",
    "    Crea un CSV con series temporales desde archivos Bladed binarios.\n",
    "    Esta funcion replica exactamente el comportamiento de timeseries_script.py\n",
    "    \n",
    "    Args:\n",
    "        bin_series: Diccionario con datos binarios {archivo: {se√±al: datos}}\n",
    "        header_series: Headers leidos con pp.read_hdr_files()\n",
    "        filenames: Lista de archivos procesados\n",
    "        var_dict: Diccionario con variables a extraer {se√±al: [variables]}\n",
    "        output_path: Ruta donde guardar el CSV\n",
    "        output_filename: Nombre del archivo (sin extension)\n",
    "        add_units: Si True, a√±ade fila de unidades\n",
    "    \n",
    "    Returns:\n",
    "        Ruta del CSV creado\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    import re\n",
    "    \n",
    "    print(f'Creando CSV para variables: {list(var_dict.keys())}')\n",
    "    \n",
    "    # Diccionario de unidades en SI\n",
    "    variable_units = {\n",
    "        \"Time\": \"s\",\n",
    "        \"Rotor average longitudinal wind speed\": \"m/s\",\n",
    "        \"Blade 1 Incident axial wind speed\": \"m/s\",\n",
    "        \"Blade 2 Incident axial wind speed\": \"m/s\",\n",
    "        \"Blade root 1 My\": \"Nm\",\n",
    "        \"Blade root 2 My\": \"Nm\",\n",
    "        \"Stationary hub My\": \"Nm\",\n",
    "        \"Stationary hub Mz\": \"Nm\",\n",
    "        \"Blade 1 pitch angle\": \"deg\",\n",
    "        \"Blade 2 pitch angle\": \"deg\",\n",
    "        \"Blade 1 pitch rate\": \"deg/s\",\n",
    "        \"Blade 2 pitch rate\": \"deg/s\",\n",
    "        \"Rotor azimuth angle\": \"deg\",\n",
    "        \"LAC_VLOS_BEAM0_RANGE5\": \"m/s\",\n",
    "        \"LAC_VLOS_BEAM1_RANGE5\": \"m/s\",\n",
    "        \"LAC_VLOS_BEAM2_RANGE5\": \"m/s\",\n",
    "        \"LAC_VLOS_BEAM3_RANGE5\": \"m/s\",\n",
    "        \"LAC_VLOS_BEAM4_RANGE5\": \"m/s\",\n",
    "        \"LAC_VLOS_BEAM5_RANGE5\": \"m/s\",\n",
    "        \"LAC_VLOS_BEAM6_RANGE5\": \"m/s\",\n",
    "        \"LAC_VLOS_BEAM7_RANGE5\": \"m/s\",\n",
    "        \"LAC_VLOS_BEAM8_RANGE5\": \"m/s\",\n",
    "        \"LAC_VLOS_BEAM9_RANGE5\": \"m/s\"\n",
    "    }\n",
    "    \n",
    "    # Obtener todos los nombres de variables para el header\n",
    "    all_variables = []\n",
    "    for signal in var_dict.keys():\n",
    "        all_variables.extend(var_dict[signal])\n",
    "    \n",
    "    # Crear fila de header\n",
    "    header = ['Time'] + all_variables\n",
    "    \n",
    "    # Inicializar estructura de datos\n",
    "    csv_data = [header]\n",
    "    \n",
    "    # A√±adir fila de unidades si se solicita\n",
    "    if add_units:\n",
    "        units_row = []\n",
    "        for var in header:\n",
    "            if var in variable_units:\n",
    "                units_row.append(variable_units[var])\n",
    "            else:\n",
    "                units_row.append(\"\")\n",
    "        csv_data.append(units_row)\n",
    "    \n",
    "    # Procesar cada archivo\n",
    "    for file in filenames:\n",
    "        print(f'Procesando archivo: {os.path.basename(file)}')\n",
    "        \n",
    "        # Obtener vector de tiempo\n",
    "        time_data = None\n",
    "        first_signal = list(var_dict.keys())[0]\n",
    "        first_variable = var_dict[first_signal][0]\n",
    "        \n",
    "        try:\n",
    "            if first_signal in bin_series[file]:\n",
    "                if isinstance(bin_series[file][first_signal], np.ndarray):\n",
    "                    signal_data = bin_series[file][first_signal]\n",
    "                    \n",
    "                    # Manejo especial para se√±ales Aero\n",
    "                    if first_signal in (\"Aero_B1\", \"Aero_B2\") and hasattr(signal_data.dtype, 'names') and signal_data.dtype.names:\n",
    "                        base_name = signal_data.dtype.names[0]\n",
    "                        arr = signal_data[base_name]\n",
    "                        if arr.ndim == 2:\n",
    "                            time_length = arr.shape[-1] if arr.shape[-1] >= arr.shape[0] else arr.shape[0]\n",
    "                        else:\n",
    "                            time_length = len(arr)\n",
    "                        try:\n",
    "                            dt = header_series[file][first_signal]['dtime']\n",
    "                        except KeyError:\n",
    "                            dt = 0.02\n",
    "                        time_data = [i * dt for i in range(time_length)]\n",
    "                    else:\n",
    "                        # Manejo generico\n",
    "                        if hasattr(signal_data.dtype, 'names') and signal_data.dtype.names:\n",
    "                            if first_variable in signal_data.dtype.names:\n",
    "                                time_length = len(signal_data[first_variable])\n",
    "                                try:\n",
    "                                    dt = header_series[file][first_signal]['dtime']\n",
    "                                except KeyError:\n",
    "                                    dt = 0.02\n",
    "                                time_data = [i * dt for i in range(time_length)]\n",
    "                        else:\n",
    "                            time_length = len(signal_data)\n",
    "                            try:\n",
    "                                dt = header_series[file][first_signal]['dtime']\n",
    "                            except KeyError:\n",
    "                                dt = 0.02\n",
    "                            time_data = [i * dt for i in range(time_length)]\n",
    "        except Exception as e:\n",
    "            print(f'Error obteniendo datos de tiempo: {e}')\n",
    "            continue\n",
    "        \n",
    "        if time_data is None:\n",
    "            print(f\"Advertencia: No se pudo determinar datos de tiempo para {file}\")\n",
    "            continue\n",
    "        \n",
    "        print(f'Procesando {len(time_data)} pasos de tiempo')\n",
    "        \n",
    "        # Procesar cada paso de tiempo\n",
    "        for i in range(len(time_data)):\n",
    "            row = [time_data[i]]\n",
    "            \n",
    "            # A√±adir datos para cada variable\n",
    "            for signal in var_dict.keys():\n",
    "                for variable in var_dict[signal]:\n",
    "                    try:\n",
    "                        if signal in bin_series[file]:\n",
    "                            signal_data = bin_series[file][signal]\n",
    "                            \n",
    "                            if isinstance(signal_data, np.ndarray):\n",
    "                                # Manejo especial para se√±ales Aero\n",
    "                                if signal in (\"Aero_B1\", \"Aero_B2\"):\n",
    "                                    m = re.search(r\"at\\s+([0-9]+(?:\\.[0-9]+)?)m\", variable)\n",
    "                                    if m:\n",
    "                                        try:\n",
    "                                            pos = float(m.group(1))\n",
    "                                            idx = None\n",
    "                                            for j, p in enumerate(AERO_POSITIONS):\n",
    "                                                if abs(p - pos) < 1e-6:\n",
    "                                                    idx = j\n",
    "                                                    break\n",
    "                                            if idx is None:\n",
    "                                                idx = int(np.argmin([abs(p - pos) for p in AERO_POSITIONS]))\n",
    "                                            \n",
    "                                            if hasattr(signal_data.dtype, 'names') and signal_data.dtype.names:\n",
    "                                                base_name = signal_data.dtype.names[0]\n",
    "                                                arr = signal_data[base_name]\n",
    "                                            else:\n",
    "                                                arr = signal_data\n",
    "                                            \n",
    "                                            if arr.ndim == 2:\n",
    "                                                val = arr[idx, i]\n",
    "                                            elif arr.ndim == 1:\n",
    "                                                val = arr[i] if i < arr.shape[0] else np.nan\n",
    "                                            else:\n",
    "                                                val = np.nan\n",
    "                                            row.append(float(val) if np.isfinite(val) else 0.0)\n",
    "                                        except Exception as ex:\n",
    "                                            row.append(0.0)\n",
    "                                    else:\n",
    "                                        row.append(0.0)\n",
    "                                else:\n",
    "                                    # Arrays no-Aero\n",
    "                                    if hasattr(signal_data.dtype, 'names') and signal_data.dtype.names:\n",
    "                                        if variable in signal_data.dtype.names and i < len(signal_data[variable]):\n",
    "                                            row.append(float(signal_data[variable][i]))\n",
    "                                        else:\n",
    "                                            row.append(0.0)\n",
    "                                    else:\n",
    "                                        if signal_data.ndim == 1 and i < signal_data.shape[0]:\n",
    "                                            row.append(float(signal_data[i]))\n",
    "                                        else:\n",
    "                                            row.append(0.0)\n",
    "                            else:\n",
    "                                row.append(0.0)\n",
    "                        else:\n",
    "                            row.append(0.0)\n",
    "                    except Exception as e:\n",
    "                        row.append(0.0)\n",
    "            \n",
    "            csv_data.append(row)\n",
    "    \n",
    "    # Escribir a CSV\n",
    "    output_file = os.path.join(output_path, f\"{output_filename}.csv\")\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_data)\n",
    "    \n",
    "    print(f'Archivo CSV creado: {output_file}')\n",
    "    return output_file\n",
    "\n",
    "print(\"OK - Funcion create_timeseries_csv definida correctamente\")\n",
    "print(\"  Esta funcion replica exactamente timeseries_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfd22b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUSCANDO ARCHIVOS DE SIMULACION\n",
      "======================================================================\n",
      "Directorio: U:\\Studies\\437_lidar_VLOS_IPC\\Outputs\\OPT3_PassiveYaw\\DLC12a\n",
      "\n",
      "72  files found in DLC12a\n",
      "Total stored files: 72\n",
      "Total de archivos .$TE encontrados: 72\n",
      "\n",
      "Patron '0001_DLC12a_030_000':\n",
      "  -> 0001_DLC12a_030_000\n",
      "\n",
      "Patron '0002_DLC12a_030_000':\n",
      "  -> 0002_DLC12a_030_000\n",
      "\n",
      "Patron '0003_DLC12a_030_000':\n",
      "  -> 0003_DLC12a_030_000\n",
      "\n",
      "Patron '0004_DLC12a_030_000':\n",
      "  -> 0004_DLC12a_030_000\n",
      "\n",
      "Patron '0005_DLC12a_030_000':\n",
      "  -> 0005_DLC12a_030_000\n",
      "\n",
      "Patron '0006_DLC12a_030_000':\n",
      "  -> 0006_DLC12a_030_000\n",
      "\n",
      "Patron '0007_DLC12a_050_000':\n",
      "  -> 0007_DLC12a_050_000\n",
      "\n",
      "Patron '0008_DLC12a_050_000':\n",
      "  -> 0008_DLC12a_050_000\n",
      "\n",
      "Patron '0009_DLC12a_050_000':\n",
      "  -> 0009_DLC12a_050_000\n",
      "\n",
      "Patron '0010_DLC12a_050_000':\n",
      "  -> 0010_DLC12a_050_000\n",
      "\n",
      "Patron '0011_DLC12a_050_000':\n",
      "  -> 0011_DLC12a_050_000\n",
      "\n",
      "Patron '0012_DLC12a_050_000':\n",
      "  -> 0012_DLC12a_050_000\n",
      "\n",
      "Patron '0013_DLC12a_070_000':\n",
      "  -> 0013_DLC12a_070_000\n",
      "\n",
      "Patron '0014_DLC12a_070_000':\n",
      "  -> 0014_DLC12a_070_000\n",
      "\n",
      "Patron '0015_DLC12a_070_000':\n",
      "  -> 0015_DLC12a_070_000\n",
      "\n",
      "Patron '0016_DLC12a_070_000':\n",
      "  -> 0016_DLC12a_070_000\n",
      "\n",
      "Patron '0017_DLC12a_070_000':\n",
      "  -> 0017_DLC12a_070_000\n",
      "\n",
      "Patron '0018_DLC12a_070_000':\n",
      "  -> 0018_DLC12a_070_000\n",
      "\n",
      "Patron '0019_DLC12a_090_000':\n",
      "  -> 0019_DLC12a_090_000\n",
      "\n",
      "Patron '0020_DLC12a_090_000':\n",
      "  -> 0020_DLC12a_090_000\n",
      "\n",
      "Patron '0021_DLC12a_090_000':\n",
      "  -> 0021_DLC12a_090_000\n",
      "\n",
      "Patron '0022_DLC12a_090_000':\n",
      "  -> 0022_DLC12a_090_000\n",
      "\n",
      "Patron '0023_DLC12a_090_000':\n",
      "  -> 0023_DLC12a_090_000\n",
      "\n",
      "Patron '0024_DLC12a_090_000':\n",
      "  -> 0024_DLC12a_090_000\n",
      "\n",
      "Patron '0025_DLC12a_110_000':\n",
      "  -> 0025_DLC12a_110_000\n",
      "\n",
      "Patron '0026_DLC12a_110_000':\n",
      "  -> 0026_DLC12a_110_000\n",
      "\n",
      "Patron '0027_DLC12a_110_000':\n",
      "  -> 0027_DLC12a_110_000\n",
      "\n",
      "Patron '0028_DLC12a_110_000':\n",
      "  -> 0028_DLC12a_110_000\n",
      "\n",
      "Patron '0029_DLC12a_110_000':\n",
      "  -> 0029_DLC12a_110_000\n",
      "\n",
      "Patron '0030_DLC12a_110_000':\n",
      "  -> 0030_DLC12a_110_000\n",
      "\n",
      "Patron '0031_DLC12a_130_000':\n",
      "  -> 0031_DLC12a_130_000\n",
      "\n",
      "Patron '0032_DLC12a_130_000':\n",
      "  -> 0032_DLC12a_130_000\n",
      "\n",
      "Patron '0033_DLC12a_130_000':\n",
      "  -> 0033_DLC12a_130_000\n",
      "\n",
      "Patron '0034_DLC12a_130_000':\n",
      "  -> 0034_DLC12a_130_000\n",
      "\n",
      "Patron '0035_DLC12a_130_000':\n",
      "  -> 0035_DLC12a_130_000\n",
      "\n",
      "Patron '0036_DLC12a_130_000':\n",
      "  -> 0036_DLC12a_130_000\n",
      "\n",
      "Patron '0037_DLC12a_150_000':\n",
      "  -> 0037_DLC12a_150_000\n",
      "\n",
      "Patron '0038_DLC12a_150_000':\n",
      "  -> 0038_DLC12a_150_000\n",
      "\n",
      "Patron '0039_DLC12a_150_000':\n",
      "  -> 0039_DLC12a_150_000\n",
      "\n",
      "Patron '0040_DLC12a_150_000':\n",
      "  -> 0040_DLC12a_150_000\n",
      "\n",
      "Patron '0041_DLC12a_150_000':\n",
      "  -> 0041_DLC12a_150_000\n",
      "\n",
      "Patron '0042_DLC12a_150_000':\n",
      "  -> 0042_DLC12a_150_000\n",
      "\n",
      "Patron '0043_DLC12a_170_000':\n",
      "  -> 0043_DLC12a_170_000\n",
      "\n",
      "Patron '0044_DLC12a_170_000':\n",
      "  -> 0044_DLC12a_170_000\n",
      "\n",
      "Patron '0045_DLC12a_170_000':\n",
      "  -> 0045_DLC12a_170_000\n",
      "\n",
      "Patron '0046_DLC12a_170_000':\n",
      "  -> 0046_DLC12a_170_000\n",
      "\n",
      "Patron '0047_DLC12a_170_000':\n",
      "  -> 0047_DLC12a_170_000\n",
      "\n",
      "Patron '0048_DLC12a_170_000':\n",
      "  -> 0048_DLC12a_170_000\n",
      "\n",
      "Patron '0049_DLC12a_190_000':\n",
      "  -> 0049_DLC12a_190_000\n",
      "\n",
      "Patron '0050_DLC12a_190_000':\n",
      "  -> 0050_DLC12a_190_000\n",
      "\n",
      "Patron '0051_DLC12a_190_000':\n",
      "  -> 0051_DLC12a_190_000\n",
      "\n",
      "Patron '0052_DLC12a_190_000':\n",
      "  -> 0052_DLC12a_190_000\n",
      "\n",
      "Patron '0053_DLC12a_190_000':\n",
      "  -> 0053_DLC12a_190_000\n",
      "\n",
      "Patron '0054_DLC12a_190_000':\n",
      "  -> 0054_DLC12a_190_000\n",
      "\n",
      "Patron '0055_DLC12a_210_000':\n",
      "  -> 0055_DLC12a_210_000\n",
      "\n",
      "Patron '0056_DLC12a_210_000':\n",
      "  -> 0056_DLC12a_210_000\n",
      "\n",
      "Patron '0057_DLC12a_210_000':\n",
      "  -> 0057_DLC12a_210_000\n",
      "\n",
      "Patron '0058_DLC12a_210_000':\n",
      "  -> 0058_DLC12a_210_000\n",
      "\n",
      "Patron '0059_DLC12a_210_000':\n",
      "  -> 0059_DLC12a_210_000\n",
      "\n",
      "Patron '0060_DLC12a_210_000':\n",
      "  -> 0060_DLC12a_210_000\n",
      "\n",
      "Patron '0061_DLC12a_230_000':\n",
      "  -> 0061_DLC12a_230_000\n",
      "\n",
      "Patron '0062_DLC12a_230_000':\n",
      "  -> 0062_DLC12a_230_000\n",
      "\n",
      "Patron '0063_DLC12a_230_000':\n",
      "  -> 0063_DLC12a_230_000\n",
      "\n",
      "Patron '0064_DLC12a_230_000':\n",
      "  -> 0064_DLC12a_230_000\n",
      "\n",
      "Patron '0065_DLC12a_230_000':\n",
      "  -> 0065_DLC12a_230_000\n",
      "\n",
      "Patron '0066_DLC12a_230_000':\n",
      "  -> 0066_DLC12a_230_000\n",
      "\n",
      "Patron '0067_DLC12a_250_000':\n",
      "  -> 0067_DLC12a_250_000\n",
      "\n",
      "Patron '0068_DLC12a_250_000':\n",
      "  -> 0068_DLC12a_250_000\n",
      "\n",
      "Patron '0069_DLC12a_250_000':\n",
      "  -> 0069_DLC12a_250_000\n",
      "\n",
      "Patron '0070_DLC12a_250_000':\n",
      "  -> 0070_DLC12a_250_000\n",
      "\n",
      "Patron '0071_DLC12a_250_000':\n",
      "  -> 0071_DLC12a_250_000\n",
      "\n",
      "Patron '0072_DLC12a_250_000':\n",
      "  -> 0072_DLC12a_250_000\n",
      "\n",
      "======================================================================\n",
      "TOTAL: 72 archivos encontrados para 72 patrones\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2.5: Buscar archivos de simulacion usando pp.read_dlcnames\n",
    "# ============================================================================\n",
    "# Usar la misma funcion que timeseries_script.py para buscar archivos\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BUSCANDO ARCHIVOS DE SIMULACION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Directorio: {loadpath}\\n\")\n",
    "\n",
    "# Usar pp.read_dlcnames para buscar todos los archivos .$TE\n",
    "# El filtro {\"\": 1} busca todos los archivos\n",
    "all_dlcnames = pp.read_dlcnames(loadpath, file_filter={\"\": 1})\n",
    "\n",
    "print(f\"Total de archivos .$TE encontrados: {len(all_dlcnames)}\")\n",
    "\n",
    "# Diccionario para almacenar archivos filtrados por cada file_name\n",
    "all_matching_files = {}\n",
    "\n",
    "# Filtrar archivos para cada patron en file_names\n",
    "for file_name in file_names:\n",
    "    # Filtrar archivos que contengan file_name en el nombre\n",
    "    matching = [f for f in all_dlcnames if file_name in os.path.basename(f)]\n",
    "    all_matching_files[file_name] = matching\n",
    "    \n",
    "    print(f\"\\nPatron '{file_name}':\")\n",
    "    if matching:\n",
    "        for file in matching:\n",
    "            print(f\"  -> {os.path.basename(file)}\")\n",
    "    else:\n",
    "        print(f\"  -> ADVERTENCIA: No se encontraron archivos\")\n",
    "\n",
    "# Resumen\n",
    "total_files = sum(len(files) for files in all_matching_files.values())\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOTAL: {total_files} archivos encontrados para {len(file_names)} patrones\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c823252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DICCIONARIO COMBINADO DE VARIABLES\n",
      "======================================================================\n",
      "\n",
      "Total de grupos de se√±ales: 6\n",
      "Signal list: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "\n",
      "Hub_fixed: 2 variables\n",
      "Hub_rotating: 2 variables\n",
      "Pitch_actuator: 2 variables\n",
      "Drive_train: 1 variables\n",
      "Summary: 1 variables\n",
      "External_controller: 49 variables\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2.6: Combinar todos los diccionarios de variables y preparar signal_list\n",
    "# ============================================================================\n",
    "# Unir todos los diccionarios en uno solo para pasarlo a la funcion\n",
    "\n",
    "# Combinar todos los diccionarios de variables en uno\n",
    "combined_var_dict = {}\n",
    "for dict_name, var_dict in var_dicts.items():\n",
    "    combined_var_dict.update(var_dict)\n",
    "\n",
    "# Lista de se√±ales (keys) para pp.read_hdr_files y pp.read_bin_files\n",
    "signal_list = list(combined_var_dict.keys())\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DICCIONARIO COMBINADO DE VARIABLES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal de grupos de se√±ales: {len(combined_var_dict)}\")\n",
    "print(f\"Signal list: {signal_list}\\n\")\n",
    "for signal, variables in combined_var_dict.items():\n",
    "    print(f\"{signal}: {len(variables)} variables\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cfa567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROCESANDO ARCHIVOS POR SEPARADO\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0001_DLC12a_030_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0001_DLC12a_030_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0001_DLC12a_030_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0001_DLC12a_030_000.csv\n",
      "\n",
      "      OK - CSV generado: 0001_DLC12a_030_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0002_DLC12a_030_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0002_DLC12a_030_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0002_DLC12a_030_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0002_DLC12a_030_000.csv\n",
      "\n",
      "      OK - CSV generado: 0002_DLC12a_030_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0003_DLC12a_030_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0003_DLC12a_030_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0003_DLC12a_030_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0003_DLC12a_030_000.csv\n",
      "\n",
      "      OK - CSV generado: 0003_DLC12a_030_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0004_DLC12a_030_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0004_DLC12a_030_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0004_DLC12a_030_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0004_DLC12a_030_000.csv\n",
      "\n",
      "      OK - CSV generado: 0004_DLC12a_030_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0005_DLC12a_030_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0005_DLC12a_030_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0005_DLC12a_030_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0005_DLC12a_030_000.csv\n",
      "\n",
      "      OK - CSV generado: 0005_DLC12a_030_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0006_DLC12a_030_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0006_DLC12a_030_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0006_DLC12a_030_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0006_DLC12a_030_000.csv\n",
      "\n",
      "      OK - CSV generado: 0006_DLC12a_030_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0007_DLC12a_050_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0007_DLC12a_050_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0007_DLC12a_050_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0007_DLC12a_050_000.csv\n",
      "\n",
      "      OK - CSV generado: 0007_DLC12a_050_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0008_DLC12a_050_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0008_DLC12a_050_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0008_DLC12a_050_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0008_DLC12a_050_000.csv\n",
      "\n",
      "      OK - CSV generado: 0008_DLC12a_050_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0009_DLC12a_050_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0009_DLC12a_050_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0009_DLC12a_050_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0009_DLC12a_050_000.csv\n",
      "\n",
      "      OK - CSV generado: 0009_DLC12a_050_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0010_DLC12a_050_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0010_DLC12a_050_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0010_DLC12a_050_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0010_DLC12a_050_000.csv\n",
      "\n",
      "      OK - CSV generado: 0010_DLC12a_050_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0011_DLC12a_050_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0011_DLC12a_050_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0011_DLC12a_050_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0011_DLC12a_050_000.csv\n",
      "\n",
      "      OK - CSV generado: 0011_DLC12a_050_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0012_DLC12a_050_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0012_DLC12a_050_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0012_DLC12a_050_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0012_DLC12a_050_000.csv\n",
      "\n",
      "      OK - CSV generado: 0012_DLC12a_050_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0013_DLC12a_070_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0013_DLC12a_070_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0013_DLC12a_070_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0013_DLC12a_070_000.csv\n",
      "\n",
      "      OK - CSV generado: 0013_DLC12a_070_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0014_DLC12a_070_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0014_DLC12a_070_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0014_DLC12a_070_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0014_DLC12a_070_000.csv\n",
      "\n",
      "      OK - CSV generado: 0014_DLC12a_070_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0015_DLC12a_070_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0015_DLC12a_070_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0015_DLC12a_070_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0015_DLC12a_070_000.csv\n",
      "\n",
      "      OK - CSV generado: 0015_DLC12a_070_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0016_DLC12a_070_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0016_DLC12a_070_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0016_DLC12a_070_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0016_DLC12a_070_000.csv\n",
      "\n",
      "      OK - CSV generado: 0016_DLC12a_070_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0017_DLC12a_070_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0017_DLC12a_070_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0017_DLC12a_070_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0017_DLC12a_070_000.csv\n",
      "\n",
      "      OK - CSV generado: 0017_DLC12a_070_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0018_DLC12a_070_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0018_DLC12a_070_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0018_DLC12a_070_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0018_DLC12a_070_000.csv\n",
      "\n",
      "      OK - CSV generado: 0018_DLC12a_070_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0019_DLC12a_090_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0019_DLC12a_090_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0019_DLC12a_090_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0019_DLC12a_090_000.csv\n",
      "\n",
      "      OK - CSV generado: 0019_DLC12a_090_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0020_DLC12a_090_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0020_DLC12a_090_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0020_DLC12a_090_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0020_DLC12a_090_000.csv\n",
      "\n",
      "      OK - CSV generado: 0020_DLC12a_090_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0021_DLC12a_090_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0021_DLC12a_090_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0021_DLC12a_090_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0021_DLC12a_090_000.csv\n",
      "\n",
      "      OK - CSV generado: 0021_DLC12a_090_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0022_DLC12a_090_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0022_DLC12a_090_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0022_DLC12a_090_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0022_DLC12a_090_000.csv\n",
      "\n",
      "      OK - CSV generado: 0022_DLC12a_090_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0023_DLC12a_090_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0023_DLC12a_090_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0023_DLC12a_090_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0023_DLC12a_090_000.csv\n",
      "\n",
      "      OK - CSV generado: 0023_DLC12a_090_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0024_DLC12a_090_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0024_DLC12a_090_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0024_DLC12a_090_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0024_DLC12a_090_000.csv\n",
      "\n",
      "      OK - CSV generado: 0024_DLC12a_090_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0025_DLC12a_110_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0025_DLC12a_110_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0025_DLC12a_110_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0025_DLC12a_110_000.csv\n",
      "\n",
      "      OK - CSV generado: 0025_DLC12a_110_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0026_DLC12a_110_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0026_DLC12a_110_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0026_DLC12a_110_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0026_DLC12a_110_000.csv\n",
      "\n",
      "      OK - CSV generado: 0026_DLC12a_110_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0027_DLC12a_110_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0027_DLC12a_110_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0027_DLC12a_110_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0027_DLC12a_110_000.csv\n",
      "\n",
      "      OK - CSV generado: 0027_DLC12a_110_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0028_DLC12a_110_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0028_DLC12a_110_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0028_DLC12a_110_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0028_DLC12a_110_000.csv\n",
      "\n",
      "      OK - CSV generado: 0028_DLC12a_110_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0029_DLC12a_110_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0029_DLC12a_110_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0029_DLC12a_110_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0029_DLC12a_110_000.csv\n",
      "\n",
      "      OK - CSV generado: 0029_DLC12a_110_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0030_DLC12a_110_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0030_DLC12a_110_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0030_DLC12a_110_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0030_DLC12a_110_000.csv\n",
      "\n",
      "      OK - CSV generado: 0030_DLC12a_110_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0031_DLC12a_130_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0031_DLC12a_130_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0031_DLC12a_130_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0031_DLC12a_130_000.csv\n",
      "\n",
      "      OK - CSV generado: 0031_DLC12a_130_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0032_DLC12a_130_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0032_DLC12a_130_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0032_DLC12a_130_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0032_DLC12a_130_000.csv\n",
      "\n",
      "      OK - CSV generado: 0032_DLC12a_130_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0033_DLC12a_130_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0033_DLC12a_130_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0033_DLC12a_130_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0033_DLC12a_130_000.csv\n",
      "\n",
      "      OK - CSV generado: 0033_DLC12a_130_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0034_DLC12a_130_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0034_DLC12a_130_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0034_DLC12a_130_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0034_DLC12a_130_000.csv\n",
      "\n",
      "      OK - CSV generado: 0034_DLC12a_130_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0035_DLC12a_130_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0035_DLC12a_130_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0035_DLC12a_130_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0035_DLC12a_130_000.csv\n",
      "\n",
      "      OK - CSV generado: 0035_DLC12a_130_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0036_DLC12a_130_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0036_DLC12a_130_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0036_DLC12a_130_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0036_DLC12a_130_000.csv\n",
      "\n",
      "      OK - CSV generado: 0036_DLC12a_130_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0037_DLC12a_150_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0037_DLC12a_150_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0037_DLC12a_150_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0037_DLC12a_150_000.csv\n",
      "\n",
      "      OK - CSV generado: 0037_DLC12a_150_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0038_DLC12a_150_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0038_DLC12a_150_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0038_DLC12a_150_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0038_DLC12a_150_000.csv\n",
      "\n",
      "      OK - CSV generado: 0038_DLC12a_150_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0039_DLC12a_150_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0039_DLC12a_150_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0039_DLC12a_150_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0039_DLC12a_150_000.csv\n",
      "\n",
      "      OK - CSV generado: 0039_DLC12a_150_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0040_DLC12a_150_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0040_DLC12a_150_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0040_DLC12a_150_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0040_DLC12a_150_000.csv\n",
      "\n",
      "      OK - CSV generado: 0040_DLC12a_150_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0041_DLC12a_150_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0041_DLC12a_150_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0041_DLC12a_150_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0041_DLC12a_150_000.csv\n",
      "\n",
      "      OK - CSV generado: 0041_DLC12a_150_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0042_DLC12a_150_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0042_DLC12a_150_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0042_DLC12a_150_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0042_DLC12a_150_000.csv\n",
      "\n",
      "      OK - CSV generado: 0042_DLC12a_150_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0043_DLC12a_170_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0043_DLC12a_170_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0043_DLC12a_170_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0043_DLC12a_170_000.csv\n",
      "\n",
      "      OK - CSV generado: 0043_DLC12a_170_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0044_DLC12a_170_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0044_DLC12a_170_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0044_DLC12a_170_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0044_DLC12a_170_000.csv\n",
      "\n",
      "      OK - CSV generado: 0044_DLC12a_170_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0045_DLC12a_170_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0045_DLC12a_170_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0045_DLC12a_170_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0045_DLC12a_170_000.csv\n",
      "\n",
      "      OK - CSV generado: 0045_DLC12a_170_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0046_DLC12a_170_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0046_DLC12a_170_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0046_DLC12a_170_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0046_DLC12a_170_000.csv\n",
      "\n",
      "      OK - CSV generado: 0046_DLC12a_170_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0047_DLC12a_170_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0047_DLC12a_170_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0047_DLC12a_170_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0047_DLC12a_170_000.csv\n",
      "\n",
      "      OK - CSV generado: 0047_DLC12a_170_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0048_DLC12a_170_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0048_DLC12a_170_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0048_DLC12a_170_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0048_DLC12a_170_000.csv\n",
      "\n",
      "      OK - CSV generado: 0048_DLC12a_170_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0049_DLC12a_190_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0049_DLC12a_190_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0049_DLC12a_190_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0049_DLC12a_190_000.csv\n",
      "\n",
      "      OK - CSV generado: 0049_DLC12a_190_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0050_DLC12a_190_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0050_DLC12a_190_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0050_DLC12a_190_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0050_DLC12a_190_000.csv\n",
      "\n",
      "      OK - CSV generado: 0050_DLC12a_190_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0051_DLC12a_190_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0051_DLC12a_190_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0051_DLC12a_190_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0051_DLC12a_190_000.csv\n",
      "\n",
      "      OK - CSV generado: 0051_DLC12a_190_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0052_DLC12a_190_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0052_DLC12a_190_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0052_DLC12a_190_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0052_DLC12a_190_000.csv\n",
      "\n",
      "      OK - CSV generado: 0052_DLC12a_190_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0053_DLC12a_190_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0053_DLC12a_190_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0053_DLC12a_190_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0053_DLC12a_190_000.csv\n",
      "\n",
      "      OK - CSV generado: 0053_DLC12a_190_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0054_DLC12a_190_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0054_DLC12a_190_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0054_DLC12a_190_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0054_DLC12a_190_000.csv\n",
      "\n",
      "      OK - CSV generado: 0054_DLC12a_190_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0055_DLC12a_210_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0055_DLC12a_210_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0055_DLC12a_210_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0055_DLC12a_210_000.csv\n",
      "\n",
      "      OK - CSV generado: 0055_DLC12a_210_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0056_DLC12a_210_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0056_DLC12a_210_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0056_DLC12a_210_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0056_DLC12a_210_000.csv\n",
      "\n",
      "      OK - CSV generado: 0056_DLC12a_210_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0057_DLC12a_210_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0057_DLC12a_210_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0057_DLC12a_210_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0057_DLC12a_210_000.csv\n",
      "\n",
      "      OK - CSV generado: 0057_DLC12a_210_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0058_DLC12a_210_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0058_DLC12a_210_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0058_DLC12a_210_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0058_DLC12a_210_000.csv\n",
      "\n",
      "      OK - CSV generado: 0058_DLC12a_210_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0059_DLC12a_210_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0059_DLC12a_210_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0059_DLC12a_210_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0059_DLC12a_210_000.csv\n",
      "\n",
      "      OK - CSV generado: 0059_DLC12a_210_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0060_DLC12a_210_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0060_DLC12a_210_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0060_DLC12a_210_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0060_DLC12a_210_000.csv\n",
      "\n",
      "      OK - CSV generado: 0060_DLC12a_210_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0061_DLC12a_230_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0061_DLC12a_230_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0061_DLC12a_230_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0061_DLC12a_230_000.csv\n",
      "\n",
      "      OK - CSV generado: 0061_DLC12a_230_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0062_DLC12a_230_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0062_DLC12a_230_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0062_DLC12a_230_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0062_DLC12a_230_000.csv\n",
      "\n",
      "      OK - CSV generado: 0062_DLC12a_230_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0063_DLC12a_230_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0063_DLC12a_230_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0063_DLC12a_230_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0063_DLC12a_230_000.csv\n",
      "\n",
      "      OK - CSV generado: 0063_DLC12a_230_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0064_DLC12a_230_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0064_DLC12a_230_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0064_DLC12a_230_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0064_DLC12a_230_000.csv\n",
      "\n",
      "      OK - CSV generado: 0064_DLC12a_230_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0065_DLC12a_230_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0065_DLC12a_230_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0065_DLC12a_230_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0065_DLC12a_230_000.csv\n",
      "\n",
      "      OK - CSV generado: 0065_DLC12a_230_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0066_DLC12a_230_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0066_DLC12a_230_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0066_DLC12a_230_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0066_DLC12a_230_000.csv\n",
      "\n",
      "      OK - CSV generado: 0066_DLC12a_230_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0067_DLC12a_250_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0067_DLC12a_250_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0067_DLC12a_250_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0067_DLC12a_250_000.csv\n",
      "\n",
      "      OK - CSV generado: 0067_DLC12a_250_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0068_DLC12a_250_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0068_DLC12a_250_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0068_DLC12a_250_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0068_DLC12a_250_000.csv\n",
      "\n",
      "      OK - CSV generado: 0068_DLC12a_250_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0069_DLC12a_250_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0069_DLC12a_250_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0069_DLC12a_250_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0069_DLC12a_250_000.csv\n",
      "\n",
      "      OK - CSV generado: 0069_DLC12a_250_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0070_DLC12a_250_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0070_DLC12a_250_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0070_DLC12a_250_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0070_DLC12a_250_000.csv\n",
      "\n",
      "      OK - CSV generado: 0070_DLC12a_250_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0071_DLC12a_250_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0071_DLC12a_250_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0071_DLC12a_250_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0071_DLC12a_250_000.csv\n",
      "\n",
      "      OK - CSV generado: 0071_DLC12a_250_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO: 0072_DLC12a_250_000\n",
      "======================================================================\n",
      "Archivos encontrados: 1\n",
      "  - 0072_DLC12a_250_000\n",
      "\n",
      "[1/3] Leyendo headers...\n",
      "      OK - Headers leidos\n",
      "\n",
      "[2/3] Leyendo datos binarios...\n",
      "      Leyendo 6 se√±ales NO Aero...\n",
      "Simulation number [1 out of 1]\n",
      "      OK - Datos binarios leidos\n",
      "\n",
      "[3/3] Generando CSV...\n",
      "Creando CSV para variables: ['Hub_fixed', 'Hub_rotating', 'Pitch_actuator', 'Drive_train', 'Summary', 'External_controller']\n",
      "Procesando archivo: 0072_DLC12a_250_000\n",
      "Procesando 30001 pasos de tiempo\n",
      "Archivo CSV creado: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\\0072_DLC12a_250_000.csv\n",
      "\n",
      "      OK - CSV generado: 0072_DLC12a_250_000.csv\n",
      "      Shape: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "RESUMEN FINAL DEL PROCESAMIENTO\n",
      "======================================================================\n",
      "Archivos procesados correctamente: 72\n",
      "Archivos con errores:              0\n",
      "Total intentados:                  72\n",
      "\n",
      "CSVs generados en: C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train\n",
      "\n",
      "DataFrames cargados en memoria:\n",
      "  - 0001_DLC12a_030_000: (30001, 58)\n",
      "  - 0002_DLC12a_030_000: (30001, 58)\n",
      "  - 0003_DLC12a_030_000: (30001, 58)\n",
      "  - 0004_DLC12a_030_000: (30001, 58)\n",
      "  - 0005_DLC12a_030_000: (30001, 58)\n",
      "  - 0006_DLC12a_030_000: (30001, 58)\n",
      "  - 0007_DLC12a_050_000: (30001, 58)\n",
      "  - 0008_DLC12a_050_000: (30001, 58)\n",
      "  - 0009_DLC12a_050_000: (30001, 58)\n",
      "  - 0010_DLC12a_050_000: (30001, 58)\n",
      "  - 0011_DLC12a_050_000: (30001, 58)\n",
      "  - 0012_DLC12a_050_000: (30001, 58)\n",
      "  - 0013_DLC12a_070_000: (30001, 58)\n",
      "  - 0014_DLC12a_070_000: (30001, 58)\n",
      "  - 0015_DLC12a_070_000: (30001, 58)\n",
      "  - 0016_DLC12a_070_000: (30001, 58)\n",
      "  - 0017_DLC12a_070_000: (30001, 58)\n",
      "  - 0018_DLC12a_070_000: (30001, 58)\n",
      "  - 0019_DLC12a_090_000: (30001, 58)\n",
      "  - 0020_DLC12a_090_000: (30001, 58)\n",
      "  - 0021_DLC12a_090_000: (30001, 58)\n",
      "  - 0022_DLC12a_090_000: (30001, 58)\n",
      "  - 0023_DLC12a_090_000: (30001, 58)\n",
      "  - 0024_DLC12a_090_000: (30001, 58)\n",
      "  - 0025_DLC12a_110_000: (30001, 58)\n",
      "  - 0026_DLC12a_110_000: (30001, 58)\n",
      "  - 0027_DLC12a_110_000: (30001, 58)\n",
      "  - 0028_DLC12a_110_000: (30001, 58)\n",
      "  - 0029_DLC12a_110_000: (30001, 58)\n",
      "  - 0030_DLC12a_110_000: (30001, 58)\n",
      "  - 0031_DLC12a_130_000: (30001, 58)\n",
      "  - 0032_DLC12a_130_000: (30001, 58)\n",
      "  - 0033_DLC12a_130_000: (30001, 58)\n",
      "  - 0034_DLC12a_130_000: (30001, 58)\n",
      "  - 0035_DLC12a_130_000: (30001, 58)\n",
      "  - 0036_DLC12a_130_000: (30001, 58)\n",
      "  - 0037_DLC12a_150_000: (30001, 58)\n",
      "  - 0038_DLC12a_150_000: (30001, 58)\n",
      "  - 0039_DLC12a_150_000: (30001, 58)\n",
      "  - 0040_DLC12a_150_000: (30001, 58)\n",
      "  - 0041_DLC12a_150_000: (30001, 58)\n",
      "  - 0042_DLC12a_150_000: (30001, 58)\n",
      "  - 0043_DLC12a_170_000: (30001, 58)\n",
      "  - 0044_DLC12a_170_000: (30001, 58)\n",
      "  - 0045_DLC12a_170_000: (30001, 58)\n",
      "  - 0046_DLC12a_170_000: (30001, 58)\n",
      "  - 0047_DLC12a_170_000: (30001, 58)\n",
      "  - 0048_DLC12a_170_000: (30001, 58)\n",
      "  - 0049_DLC12a_190_000: (30001, 58)\n",
      "  - 0050_DLC12a_190_000: (30001, 58)\n",
      "  - 0051_DLC12a_190_000: (30001, 58)\n",
      "  - 0052_DLC12a_190_000: (30001, 58)\n",
      "  - 0053_DLC12a_190_000: (30001, 58)\n",
      "  - 0054_DLC12a_190_000: (30001, 58)\n",
      "  - 0055_DLC12a_210_000: (30001, 58)\n",
      "  - 0056_DLC12a_210_000: (30001, 58)\n",
      "  - 0057_DLC12a_210_000: (30001, 58)\n",
      "  - 0058_DLC12a_210_000: (30001, 58)\n",
      "  - 0059_DLC12a_210_000: (30001, 58)\n",
      "  - 0060_DLC12a_210_000: (30001, 58)\n",
      "  - 0061_DLC12a_230_000: (30001, 58)\n",
      "  - 0062_DLC12a_230_000: (30001, 58)\n",
      "  - 0063_DLC12a_230_000: (30001, 58)\n",
      "  - 0064_DLC12a_230_000: (30001, 58)\n",
      "  - 0065_DLC12a_230_000: (30001, 58)\n",
      "  - 0066_DLC12a_230_000: (30001, 58)\n",
      "  - 0067_DLC12a_250_000: (30001, 58)\n",
      "  - 0068_DLC12a_250_000: (30001, 58)\n",
      "  - 0069_DLC12a_250_000: (30001, 58)\n",
      "  - 0070_DLC12a_250_000: (30001, 58)\n",
      "  - 0071_DLC12a_250_000: (30001, 58)\n",
      "  - 0072_DLC12a_250_000: (30001, 58)\n",
      "\n",
      "======================================================================\n",
      "STEP 2 COMPLETADO\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2.7: PROCESAR CADA FILE_NAME POR SEPARADO\n",
    "# ============================================================================\n",
    "# Iterar sobre cada file_name y procesarlo independientemente\n",
    "# Cada file_name generara su propio CSV\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROCESANDO ARCHIVOS POR SEPARADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Contadores globales\n",
    "total_processed = 0\n",
    "total_failed = 0\n",
    "all_dataframes = {}\n",
    "\n",
    "# ITERAR SOBRE CADA FILE_NAME\n",
    "for file_name_pattern in file_names:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROCESANDO: {file_name_pattern}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Obtener archivos para este file_name\n",
    "    current_files = all_matching_files.get(file_name_pattern, [])\n",
    "    \n",
    "    if not current_files:\n",
    "        print(f\"SKIP - No se encontraron archivos para '{file_name_pattern}'\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Archivos encontrados: {len(current_files)}\")\n",
    "    for f in current_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "    \n",
    "    try:\n",
    "        # PASO A: Leer headers para este grupo de archivos\n",
    "        print(f\"\\n[1/3] Leyendo headers...\")\n",
    "        header_series = pp.read_hdr_files(current_files, signal_list)\n",
    "        print(f\"      OK - Headers leidos\")\n",
    "        \n",
    "        # PASO B: Leer datos binarios\n",
    "        print(f\"\\n[2/3] Leyendo datos binarios...\")\n",
    "        \n",
    "        # Separar se√±ales Aero de no-Aero\n",
    "        non_aero_signals = [s for s in signal_list if \"Aero\" not in s]\n",
    "        aero_signals = [s for s in signal_list if \"Aero\" in s]\n",
    "        \n",
    "        # Estructura para datos binarios\n",
    "        bin_series = {f: {} for f in current_files}\n",
    "        \n",
    "        # Leer se√±ales NO Aero\n",
    "        if non_aero_signals:\n",
    "            print(f\"      Leyendo {len(non_aero_signals)} se√±ales NO Aero...\")\n",
    "            non_aero_bin = pp.read_bin_files(current_files, header_series, non_aero_signals)\n",
    "            for f in current_files:\n",
    "                if f in non_aero_bin:\n",
    "                    bin_series[f].update(non_aero_bin[f])\n",
    "        \n",
    "        # Leer se√±ales Aero con posiciones especificas\n",
    "        if aero_signals:\n",
    "            print(f\"      Leyendo {len(aero_signals)} se√±ales Aero...\")\n",
    "            for aero_sig in aero_signals:\n",
    "                # Detectar variable base del header\n",
    "                first_file = current_files[0]\n",
    "                hdr_vars = header_series[first_file][aero_sig].get('variab', [])\n",
    "                \n",
    "                base_var = None\n",
    "                for v in hdr_vars:\n",
    "                    if 'incident' in v.lower() and 'axial' in v.lower() and 'wind speed' in v.lower():\n",
    "                        base_var = v\n",
    "                        break\n",
    "                \n",
    "                if base_var is None and hdr_vars:\n",
    "                    base_var = hdr_vars[0]\n",
    "                \n",
    "                aero_bin = pp.read_bin_files(\n",
    "                    current_files,\n",
    "                    header_series,\n",
    "                    [aero_sig],\n",
    "                    var_list=[base_var],\n",
    "                    reduced_mbr_list=AERO_POSITIONS\n",
    "                )\n",
    "                \n",
    "                for f in current_files:\n",
    "                    if f in aero_bin:\n",
    "                        bin_series[f].update(aero_bin[f])\n",
    "        \n",
    "        print(f\"      OK - Datos binarios leidos\")\n",
    "        \n",
    "        # PASO C: Crear CSV usando la funcion create_timeseries_csv\n",
    "        print(f\"\\n[3/3] Generando CSV...\")\n",
    "        \n",
    "        output_filename = file_name_pattern\n",
    "        create_timeseries_csv(\n",
    "            bin_series, \n",
    "            header_series, \n",
    "            current_files, \n",
    "            combined_var_dict, \n",
    "            resultspath, \n",
    "            output_filename, \n",
    "            add_units\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n      OK - CSV generado: {output_filename}.csv\")\n",
    "        total_processed += 1\n",
    "        \n",
    "        # Cargar el CSV recien creado como DataFrame para verificacion\n",
    "        csv_path = os.path.join(resultspath, f\"{output_filename}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            df_check = pd.read_csv(csv_path)\n",
    "            all_dataframes[file_name_pattern] = df_check\n",
    "            print(f\"      Shape: {df_check.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR procesando '{file_name_pattern}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        total_failed += 1\n",
    "\n",
    "# RESUMEN FINAL\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RESUMEN FINAL DEL PROCESAMIENTO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Archivos procesados correctamente: {total_processed}\")\n",
    "print(f\"Archivos con errores:              {total_failed}\")\n",
    "print(f\"Total intentados:                  {total_processed + total_failed}\")\n",
    "\n",
    "if total_processed > 0:\n",
    "    print(f\"\\nCSVs generados en: {resultspath}\")\n",
    "    print(\"\\nDataFrames cargados en memoria:\")\n",
    "    for fname, df in all_dataframes.items():\n",
    "        print(f\"  - {fname}: {df.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2 COMPLETADO\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33327da1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß STEP 3: Feature Engineering para ML Tradicional\n",
    "\n",
    "En este paso vamos a enriquecer los datos CSV de `data_train_traditional_ML` con nuevas features para modelos de Machine Learning tradicionales.\n",
    "\n",
    "### üéØ Features a crear:\n",
    "\n",
    "1. **Lags de VLOS (velocidades de viento)**: \n",
    "   - Lags desde 5 hasta 25 segundos (cada 1 segundo)\n",
    "   - Para todas las variables de velocidad del viento (Blade 1/2 Incident axial wind speed)\n",
    "\n",
    "2. **Componentes trigonom√©tricas del azimuth**:\n",
    "   - `sin(Rotor azimuth angle)` y `cos(Rotor azimuth angle)`\n",
    "   - Evita discontinuidades en 0¬∞-360¬∞\n",
    "\n",
    "3. **Componentes de frecuencia 1P y 2P de los momentos flectores**:\n",
    "   - Usando `Rotor Speed` (rpm) como frecuencia 1P\n",
    "   - Variables: `Blade root 1 My 1P`, `Blade root 1 My 2P`, `Blade root 2 My 1P`, `Blade root 2 My 2P`\n",
    "\n",
    "### üì¶ Proceso:\n",
    "1. Cargar cada CSV de `data_train_traditional_ML`\n",
    "2. Crear features nuevas\n",
    "3. Guardar CSV actualizado con todas las features (originales + nuevas)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 3.1: Configurar carpeta de datos para ML tradicional\n",
    "# ============================================================================\n",
    "\n",
    "# Carpeta con los CSVs para ML tradicional\n",
    "data_folder_ml = root_dir / \"data_train_traditional_ML\"\n",
    "\n",
    "# Verificar que existe\n",
    "if not data_folder_ml.exists():\n",
    "    print(\"ERROR: La carpeta data_train_traditional_ML no existe\")\n",
    "    print(\"Por favor, ejecuta primero el comando para copiar data_train\")\n",
    "else:\n",
    "    # Listar archivos CSV\n",
    "    csv_files = list(data_folder_ml.glob(\"*.csv\"))\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"CARPETA DE DATOS PARA ML TRADICIONAL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Ruta: {data_folder_ml}\")\n",
    "    print(f\"\\nArchivos CSV encontrados: {len(csv_files)}\")\n",
    "    \n",
    "    if csv_files:\n",
    "        print(\"\\nPrimeros 10 archivos:\")\n",
    "        for i, csv_file in enumerate(csv_files[:10], 1):\n",
    "            print(f\"  {i}. {csv_file.name}\")\n",
    "        if len(csv_files) > 10:\n",
    "            print(f\"  ... y {len(csv_files) - 10} archivos m√°s\")\n",
    "    \n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 3.2: Definir funcion para crear features con lags de VLOS\n",
    "# ============================================================================\n",
    "\n",
    "def create_vlos_lags(df, lag_seconds_list=[2, 5, 8, 11, 14, 17, 20, 23, 26]):\n",
    "    \"\"\"\n",
    "    Crea features de lag para las variables de velocidad del viento (VLOS).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con los datos\n",
    "        lag_seconds_list: Lista de lags en segundos a crear\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con las nuevas columnas de lag a√±adidas\n",
    "    \"\"\"\n",
    "    # Identificar columnas de velocidad del viento (VLOS)\n",
    "    vlos_columns = [col for col in df.columns if 'LAC_VLOS' in col]\n",
    "    \n",
    "    print(f\"Variables VLOS encontradas: {len(vlos_columns)}\")\n",
    "    for col in vlos_columns:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Calcular tiempo de muestreo (dt) asumiendo columna Time\n",
    "    if 'Time' in df.columns:\n",
    "        dt = df['Time'].iloc[1] - df['Time'].iloc[0]  # Segundos entre muestras\n",
    "        print(f\"\\nTiempo de muestreo detectado: {dt:.4f} segundos\")\n",
    "    else:\n",
    "        dt = 0.02  # Default 50Hz\n",
    "        print(f\"\\nTiempo de muestreo por defecto: {dt} segundos\")\n",
    "    \n",
    "    # Crear lags para cada variable VLOS\n",
    "    print(f\"\\nCreando {len(lag_seconds_list)} lags para cada variable VLOS...\")\n",
    "    \n",
    "    total_created = 0\n",
    "    for vlos_col in vlos_columns:\n",
    "        for lag_sec in lag_seconds_list:\n",
    "            # Calcular numero de muestras para el lag\n",
    "            lag_samples = int(round(lag_sec / dt))\n",
    "            \n",
    "            # Crear nombre de la nueva columna\n",
    "            new_col_name = f\"{vlos_col}_lag{lag_sec}s\"\n",
    "            \n",
    "            # Crear la columna con shift\n",
    "            df[new_col_name] = df[vlos_col].shift(lag_samples)\n",
    "            \n",
    "            total_created += 1\n",
    "    \n",
    "    print(f\"Total de features de lag creadas: {total_created}\")\n",
    "    print(f\"Shape del DataFrame: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"OK - Funcion create_vlos_lags definida correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f07c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 3.3: Definir funcion para crear componentes seno/coseno del azimuth\n",
    "# ============================================================================\n",
    "\n",
    "def create_azimuth_components(df):\n",
    "    \"\"\"\n",
    "    Crea componentes seno y coseno del angulo de azimuth del rotor.\n",
    "    Esto evita discontinuidades en 0-360 grados.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con los datos\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con las nuevas columnas sin(azimuth) y cos(azimuth)\n",
    "    \"\"\"\n",
    "    azimuth_col = 'Rotor azimuth angle'\n",
    "    \n",
    "    if azimuth_col not in df.columns:\n",
    "        print(f\"ADVERTENCIA: Columna '{azimuth_col}' no encontrada\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Creando componentes trigonometricas de '{azimuth_col}'...\")\n",
    "    \n",
    "    # Crear componentes (asumiendo que el angulo esta en grados)\n",
    "    # Si esta en radianes, no hace falta convertir\n",
    "    # Verificar rango de valores para determinar unidades\n",
    "    max_val = df[azimuth_col].max()\n",
    "    \n",
    "    if max_val > 6.5:  # Si es > 2*pi, probablemente en grados\n",
    "        print(f\"   Rango detectado: 0-{max_val:.1f} (grados)\")\n",
    "        # Convertir de grados a radianes\n",
    "        azimuth_rad = np.deg2rad(df[azimuth_col])\n",
    "    else:\n",
    "        print(f\"   Rango detectado: 0-{max_val:.1f} (radianes)\")\n",
    "        azimuth_rad = df[azimuth_col]\n",
    "    \n",
    "    # Crear componentes\n",
    "    df['sin_rotor_azimuth'] = np.sin(azimuth_rad)\n",
    "    df['cos_rotor_azimuth'] = np.cos(azimuth_rad)\n",
    "    \n",
    "    print(f\"   OK - Creadas 2 nuevas columnas: sin_rotor_azimuth, cos_rotor_azimuth\")\n",
    "    print(f\"   Shape del DataFrame: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"OK - Funcion create_azimuth_components definida correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_filter(signal_data, cutoff, fs, order=2):\n",
    "    \"\"\"\n",
    "    Aplica un filtro pasa-bajo Butterworth a la se√±al.\n",
    "    \n",
    "    Args:\n",
    "        signal_data (np.array): Se√±al de entrada\n",
    "        cutoff (float): Frecuencia de corte en Hz\n",
    "        fs (float): Frecuencia de muestreo en Hz\n",
    "        order (int): Orden del filtro\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Se√±al filtrada\n",
    "    \"\"\"\n",
    "    from scipy import signal as sp_signal\n",
    "    \n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    \n",
    "    # Asegurar que el valor est√° en el rango v√°lido (0, 1)\n",
    "    normal_cutoff = max(0.001, min(normal_cutoff, 0.999))\n",
    "    \n",
    "    sos = sp_signal.butter(order, normal_cutoff, btype='low', output='sos')\n",
    "    filtered_signal = sp_signal.sosfilt(sos, signal_data)\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "def lowpass_filter_safe(signal_data, cutoff, fs, order=2):\n",
    "    \"\"\"\n",
    "    Versi√≥n segura del filtro pasa-bajo que maneja errores.\n",
    "    \n",
    "    Args:\n",
    "        signal_data (np.array): Se√±al de entrada\n",
    "        cutoff (float): Frecuencia de corte en Hz\n",
    "        fs (float): Frecuencia de muestreo en Hz\n",
    "        order (int): Orden del filtro\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Se√±al filtrada, o se√±al original si falla el filtrado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return lowpass_filter(signal_data, cutoff, fs, order)\n",
    "    except Exception as e:\n",
    "        print(f\"      ADVERTENCIA: Fallo en filtrado pasa-bajo ({e}). Usando se√±al sin filtrar.\")\n",
    "        return signal_data\n",
    "\n",
    "\n",
    "def create_frequency_components_1P_2P(df, apply_filtering=True):\n",
    "    \"\"\"\n",
    "    Crea componentes de frecuencia 0P, 1P y 2P de los momentos flectores.\n",
    "    \n",
    "    DESCRIPCI√ìN:\n",
    "    Esta funci√≥n crea los siguientes targets a partir de los momentos flectores M1(t) y M2(t):\n",
    "    \n",
    "    1. Se√±ales suma y diferencia:\n",
    "       - M_Œ£(t) = (M1(t) + M2(t)) / 2  ‚Üí contiene componentes pares (2P, 4P, ...)\n",
    "       - M_Œî(t) = (M1(t) - M2(t)) / 2  ‚Üí contiene componentes impares (1P, 3P, ...)\n",
    "    \n",
    "    2. Componente 0P (lento/promedio):\n",
    "       - M_0(t) = M_Œ£(t)  ‚Üí componente lento\n",
    "    \n",
    "    3. Componente 1P (proyectado en ejes fijos):\n",
    "       - M_1c(t) = M_Œî(t) * cos(œà(t))\n",
    "       - M_1s(t) = M_Œî(t) * sin(œà(t))\n",
    "       donde œà(t) es el √°ngulo de azimut de la pala 1\n",
    "    \n",
    "    4. Componente 2P (proyectado en ejes fijos):\n",
    "       - M_2c(t) = M_Œ£(t) * cos(2œà(t))\n",
    "       - M_2s(t) = M_Œ£(t) * sin(2œà(t))\n",
    "    \n",
    "    IMPORTANTE: Para que 1P y 2P sean \"limpios\", se recomienda filtrar:\n",
    "       - M_Œî alrededor de 1P antes de proyectar (band-pass)\n",
    "       - M_Œ£ alrededor de 2P antes de proyectar (band-pass)\n",
    "    \n",
    "    Targets de salida: [M_0, M_1c, M_1s, M_2c, M_2s]\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con los datos de la simulaci√≥n.\n",
    "                          Debe contener al menos:\n",
    "                          - 'Time': tiempo en segundos\n",
    "                          - 'Rotor speed': velocidad del rotor en rpm\n",
    "                          - 'Rotor azimuth angle': √°ngulo de azimut del rotor (pala 1)\n",
    "                          - 'Blade root 1 My': momento flector pala 1\n",
    "                          - 'Blade root 2 My': momento flector pala 2\n",
    "        apply_filtering (bool): Si True, aplica filtrado pasa-banda antes de proyectar.\n",
    "                               Default: True\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las nuevas columnas:\n",
    "                     - 'M_0' (0P): componente lento\n",
    "                     - 'M_1c' (1P coseno): componente 1P en fase\n",
    "                     - 'M_1s' (1P seno): componente 1P en cuadratura\n",
    "                     - 'M_2c' (2P coseno): componente 2P en fase\n",
    "                     - 'M_2s' (2P seno): componente 2P en cuadratura\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: Si faltan columnas requeridas en el DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validar columnas requeridas\n",
    "    required_cols = ['Time', 'Rotor speed', 'Rotor azimuth angle', \n",
    "                     'Blade root 1 My', 'Blade root 2 My']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Columnas faltantes en el DataFrame: {missing_cols}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Creando componentes de frecuencia 0P, 1P y 2P...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 1: Obtener par√°metros b√°sicos\n",
    "    # =========================================================================\n",
    "    M1 = df['Blade root 1 My'].values\n",
    "    M2 = df['Blade root 2 My'].values\n",
    "    time = df['Time'].values\n",
    "    azimuth = df['Rotor azimuth angle'].values\n",
    "    rotor_speed_rpm = df['Rotor speed'].values\n",
    "    \n",
    "    # Convertir azimut a radianes si est√° en grados\n",
    "    if azimuth.max() > 6.5:\n",
    "        azimuth_rad = np.deg2rad(azimuth)\n",
    "        print(\"   Azimut convertido de grados a radianes\")\n",
    "    else:\n",
    "        azimuth_rad = azimuth\n",
    "        print(\"   Azimut ya est√° en radianes\")\n",
    "    \n",
    "    # Calcular frecuencias\n",
    "    freq_1P_Hz = rotor_speed_rpm / 60.0  # Convertir rpm a Hz\n",
    "    freq_2P_Hz = 2 * freq_1P_Hz\n",
    "    freq_1P_mean = freq_1P_Hz.mean()\n",
    "    freq_2P_mean = freq_2P_Hz.mean()\n",
    "    \n",
    "    # Calcular frecuencia de muestreo\n",
    "    if len(df) > 1:\n",
    "        dt = time[1] - time[0]\n",
    "        fs = 1.0 / dt\n",
    "    else:\n",
    "        dt = 0.02\n",
    "        fs = 50.0\n",
    "    \n",
    "    print(f\"\\n   Par√°metros:\")\n",
    "    print(f\"   - Rotor Speed promedio: {rotor_speed_rpm.mean():.2f} rpm\")\n",
    "    print(f\"   - Frecuencia 1P promedio: {freq_1P_mean:.3f} Hz\")\n",
    "    print(f\"   - Frecuencia 2P promedio: {freq_2P_mean:.3f} Hz\")\n",
    "    print(f\"   - Frecuencia de muestreo: {fs:.1f} Hz\")\n",
    "    print(f\"   - N√∫mero de muestras: {len(df)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 2: Calcular se√±ales suma (Œ£) y diferencia (Œî)\n",
    "    # =========================================================================\n",
    "    print(f\"\\n   Calculando M_Œ£ y M_Œî...\")\n",
    "    \n",
    "    M_sum = (M1 + M2) / 2.0  # M_Œ£: contiene componentes pares (2P, 4P, ...)\n",
    "    M_diff = (M1 - M2) / 2.0  # M_Œî: contiene componentes impares (1P, 3P, ...)\n",
    "    \n",
    "    print(f\"   - M_Œ£ (suma) calculada: rango [{M_sum.min():.2f}, {M_sum.max():.2f}]\")\n",
    "    print(f\"   - M_Œî (diferencia) calculada: rango [{M_diff.min():.2f}, {M_diff.max():.2f}]\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 3: Aplicar filtrado pasa-banda (opcional pero recomendado)\n",
    "    # =========================================================================\n",
    "    if apply_filtering:\n",
    "        print(f\"\\n   Aplicando filtrado pasa-banda...\")\n",
    "        \n",
    "        # Filtrar M_Œî alrededor de 1P\n",
    "        bandwidth_1P = 0.3  # Ancho de banda en Hz alrededor de 1P\n",
    "        lowcut_1P = max(0.01, freq_1P_mean - bandwidth_1P)\n",
    "        highcut_1P = min(fs/2 - 0.1, freq_1P_mean + bandwidth_1P)\n",
    "        \n",
    "        print(f\"   - Filtrando M_Œî alrededor de 1P: [{lowcut_1P:.3f}, {highcut_1P:.3f}] Hz\")\n",
    "        M_diff_filtered = bandpass_filter_safe(M_diff, lowcut_1P, highcut_1P, fs, order=2)\n",
    "        \n",
    "        # Filtrar M_Œ£ alrededor de 2P\n",
    "        bandwidth_2P = 0.5  # Ancho de banda en Hz alrededor de 2P\n",
    "        lowcut_2P = max(0.01, freq_2P_mean - bandwidth_2P)\n",
    "        highcut_2P = min(fs/2 - 0.1, freq_2P_mean + bandwidth_2P)\n",
    "        \n",
    "        print(f\"   - Filtrando M_Œ£ alrededor de 2P: [{lowcut_2P:.3f}, {highcut_2P:.3f}] Hz\")\n",
    "        M_sum_filtered = bandpass_filter_safe(M_sum, lowcut_2P, highcut_2P, fs, order=2)\n",
    "    else:\n",
    "        print(f\"\\n   Sin filtrado (apply_filtering=False)\")\n",
    "        M_diff_filtered = M_diff\n",
    "        M_sum_filtered = M_sum\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 4: Crear componentes 0P, 1P y 2P\n",
    "    # =========================================================================\n",
    "    print(f\"\\n   Creando componentes de frecuencia...\")\n",
    "    \n",
    "    # 0P: Componente DC (eliminar frecuencias pares 2P, 4P, ...)\n",
    "    if apply_filtering:\n",
    "        # Filtro pasa-bajo para quedarse solo con componente DC\n",
    "        # Corte por debajo de 1P para eliminar 2P, 4P, etc.\n",
    "        cutoff_0P = freq_1P_mean * 8 # Cortar a la mitad de 1P\n",
    "        print(f\"   - Filtrando M_0 (pasa-bajo) con corte en {cutoff_0P:.3f} Hz\")\n",
    "        M_0 = lowpass_filter_safe(M_sum, cutoff_0P, fs, order=2)\n",
    "        print(f\"   - M_0 (0P): componente DC creado (sin 2P, 4P, ...)\")\n",
    "    else:\n",
    "        M_0 = M_sum  # Sin filtrar\n",
    "        print(f\"   - M_0 (0P): componente lento creado (sin filtrar)\")\n",
    "    \n",
    "    # 1P: Proyecci√≥n de M_Œî en ejes fijos usando azimut\n",
    "    M_1c = M_diff_filtered * np.cos(azimuth_rad)  # Componente 1P en fase (coseno)\n",
    "    M_1s = M_diff_filtered * np.sin(azimuth_rad)  # Componente 1P en cuadratura (seno)\n",
    "    print(f\"   - M_1c, M_1s (1P): componentes creadas con proyecci√≥n en ejes fijos\")\n",
    "    \n",
    "    # 2P: Proyecci√≥n de M_Œ£ en ejes fijos usando 2*azimut\n",
    "    M_2c = M_sum_filtered * np.cos(2 * azimuth_rad)  # Componente 2P en fase (coseno)\n",
    "    M_2s = M_sum_filtered * np.sin(2 * azimuth_rad)  # Componente 2P en cuadratura (seno)\n",
    "    print(f\"   - M_2c, M_2s (2P): componentes creadas con proyecci√≥n en ejes fijos\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 5: Agregar al DataFrame\n",
    "    # =========================================================================\n",
    "    print(f\"\\n   Agregando columnas al DataFrame...\")\n",
    "    \n",
    "    df['M_0'] = M_0      # 0P\n",
    "    df['M_1c'] = M_1c    # 1P coseno\n",
    "    df['M_1s'] = M_1s    # 1P seno\n",
    "    df['M_2c'] = M_2c    # 2P coseno\n",
    "    df['M_2s'] = M_2s    # 2P seno\n",
    "    \n",
    "    new_columns = ['M_0', 'M_1c', 'M_1s', 'M_2c', 'M_2s']\n",
    "    print(f\"   - Columnas creadas: {new_columns}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 6: Resumen final\n",
    "    # =========================================================================\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"RESUMEN:\")\n",
    "    print(f\"=\" * 70)\n",
    "    print(f\"   Vector de salida: y(t) = [M_0, M_1c, M_1s, M_2c, M_2s]\")\n",
    "    print(f\"   - M_0:  componente 0P (lento)\")\n",
    "    print(f\"   - M_1c: componente 1P en fase (coseno)\")\n",
    "    print(f\"   - M_1s: componente 1P en cuadratura (seno)\")\n",
    "    print(f\"   - M_2c: componente 2P en fase (coseno)\")\n",
    "    print(f\"   - M_2s: componente 2P en cuadratura (seno)\")\n",
    "    print(f\"\\n   Shape final del DataFrame: {df.shape}\")\n",
    "    print(f\"=\" * 70)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def bandpass_filter(signal_data, lowcut, highcut, fs, order=2):\n",
    "    \"\"\"\n",
    "    Aplica un filtro pasa-banda Butterworth a la se√±al.\n",
    "    \n",
    "    Args:\n",
    "        signal_data (np.array): Se√±al de entrada\n",
    "        lowcut (float): Frecuencia de corte inferior en Hz\n",
    "        highcut (float): Frecuencia de corte superior en Hz\n",
    "        fs (float): Frecuencia de muestreo en Hz\n",
    "        order (int): Orden del filtro\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Se√±al filtrada\n",
    "    \"\"\"\n",
    "    from scipy import signal as sp_signal\n",
    "    \n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    \n",
    "    # Asegurar que los valores est√°n en el rango v√°lido (0, 1)\n",
    "    low = max(0.001, min(low, 0.999))\n",
    "    high = max(low + 0.001, min(high, 0.999))\n",
    "    \n",
    "    sos = sp_signal.butter(order, [low, high], btype='band', output='sos')\n",
    "    filtered_signal = sp_signal.sosfilt(sos, signal_data)\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "def bandpass_filter_safe(signal_data, lowcut, highcut, fs, order=2):\n",
    "    \"\"\"\n",
    "    Versi√≥n segura del filtro pasa-banda que maneja errores.\n",
    "    \n",
    "    Args:\n",
    "        signal_data (np.array): Se√±al de entrada\n",
    "        lowcut (float): Frecuencia de corte inferior en Hz\n",
    "        highcut (float): Frecuencia de corte superior en Hz\n",
    "        fs (float): Frecuencia de muestreo en Hz\n",
    "        order (int): Orden del filtro\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Se√±al filtrada, o se√±al original si falla el filtrado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return bandpass_filter(signal_data, lowcut, highcut, fs, order)\n",
    "    except Exception as e:\n",
    "        print(f\"      ADVERTENCIA: Fallo en filtrado ({e}). Usando se√±al sin filtrar.\")\n",
    "        return signal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f178cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pitch_coleman_features(df):\n",
    "    \"\"\"\n",
    "    Crea features de pitch en marco coherente (Coleman transformation).\n",
    "    \n",
    "    DESCRIPCI√ìN:\n",
    "    Esta funci√≥n transforma los √°ngulos de pitch de las palas individuales a un marco\n",
    "    de referencia fijo (no rotatorio) usando la transformaci√≥n de Coleman:\n",
    "    \n",
    "    1. Componentes colectivo y diferencial:\n",
    "       - Œ∏_0(t) = (Œ∏_1(t) + Œ∏_2(t)) / 2  ‚Üí colectivo (promedio)\n",
    "       - Œ∏_Œî(t) = (Œ∏_1(t) - Œ∏_2(t)) / 2  ‚Üí diferencial\n",
    "    \n",
    "    2. Proyecci√≥n del diferencial a ejes fijos (1P):\n",
    "       - Œ∏_1c(t) = Œ∏_Œî(t) * cos(œà(t))\n",
    "       - Œ∏_1s(t) = Œ∏_Œî(t) * sin(œà(t))\n",
    "       donde œà(t) es el √°ngulo de azimut\n",
    "    \n",
    "    3. Rates (derivadas temporales):\n",
    "       - Œ∏Ãá_0(t) ‚âà (Œ∏_0(t) - Œ∏_0(t-Œît)) / Œît\n",
    "       - Œ∏Ãá_1c(t), Œ∏Ãá_1s(t) de forma similar\n",
    "    \n",
    "    4. Rotor speed rate:\n",
    "       - Œ©Ãá(t) ‚âà (Œ©(t) - Œ©(t-Œît)) / Œît\n",
    "    \n",
    "    IMPORTANTE: Estos features son coherentes con targets en Coleman (M_0, M_1c, M_1s).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con los datos. Debe contener:\n",
    "                          - 'Time': tiempo en segundos\n",
    "                          - 'Blade 1 pitch angle': pitch pala 1 (grados)\n",
    "                          - 'Blade 2 pitch angle': pitch pala 2 (grados)\n",
    "                          - 'Rotor azimuth angle': √°ngulo de azimut\n",
    "                          - 'Rotor speed': velocidad del rotor (rpm)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las nuevas columnas:\n",
    "                     - 'pitch_0': componente colectivo Œ∏_0\n",
    "                     - 'pitch_1c': componente 1P coseno Œ∏_1c\n",
    "                     - 'pitch_1s': componente 1P seno Œ∏_1s\n",
    "                     - 'pitch_0_rate': derivada temporal de Œ∏_0\n",
    "                     - 'pitch_1c_rate': derivada temporal de Œ∏_1c\n",
    "                     - 'pitch_1s_rate': derivada temporal de Œ∏_1s\n",
    "                     - 'rotor_speed_rate': derivada temporal de Œ©\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: Si faltan columnas requeridas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validar columnas requeridas\n",
    "    required_cols = ['Time', 'Blade 1 pitch angle', 'Blade 2 pitch angle', \n",
    "                     'Rotor azimuth angle', 'Rotor speed']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Columnas faltantes para pitch Coleman: {missing_cols}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Creando features de pitch en marco Coleman...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 1: Obtener datos b√°sicos\n",
    "    # =========================================================================\n",
    "    theta_1 = df['Blade 1 pitch angle'].values\n",
    "    theta_2 = df['Blade 2 pitch angle'].values\n",
    "    time = df['Time'].values\n",
    "    azimuth = df['Rotor azimuth angle'].values\n",
    "    rotor_speed = df['Rotor speed'].values\n",
    "    \n",
    "    # Convertir azimut a radianes si est√° en grados\n",
    "    if azimuth.max() > 6.5:\n",
    "        azimuth_rad = np.deg2rad(azimuth)\n",
    "        print(\"   Azimut convertido de grados a radianes\")\n",
    "    else:\n",
    "        azimuth_rad = azimuth\n",
    "        print(\"   Azimut ya est√° en radianes\")\n",
    "    \n",
    "    # Calcular dt (paso de tiempo)\n",
    "    if len(time) > 1:\n",
    "        dt = time[1] - time[0]\n",
    "    else:\n",
    "        dt = 0.1  # default\n",
    "    \n",
    "    print(f\"\\n   Par√°metros:\")\n",
    "    print(f\"   - N√∫mero de muestras: {len(df)}\")\n",
    "    print(f\"   - Œît (paso temporal): {dt:.4f} s\")\n",
    "    print(f\"   - Frecuencia de muestreo: {1.0/dt:.1f} Hz\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 2: Transformaci√≥n Coleman - Colectivo y Diferencial\n",
    "    # =========================================================================\n",
    "    print(f\"\\n   [1/3] Calculando componentes colectivo y diferencial...\")\n",
    "    \n",
    "    # Œ∏_0: componente colectivo (promedio)\n",
    "    theta_0 = (theta_1 + theta_2) / 2.0\n",
    "    \n",
    "    # Œ∏_Œî: componente diferencial\n",
    "    theta_delta = (theta_1 - theta_2) / 2.0\n",
    "    \n",
    "    print(f\"   - Œ∏_0 (colectivo): rango [{theta_0.min():.2f}, {theta_0.max():.2f}]¬∞\")\n",
    "    print(f\"   - Œ∏_Œî (diferencial): rango [{theta_delta.min():.2f}, {theta_delta.max():.2f}]¬∞\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 3: Proyecci√≥n del diferencial a ejes fijos (1P)\n",
    "    # =========================================================================\n",
    "    print(f\"\\n   [2/3] Proyectando Œ∏_Œî a ejes fijos (1P)...\")\n",
    "    \n",
    "    # Œ∏_1c: componente 1P en fase (coseno)\n",
    "    theta_1c = theta_delta * np.cos(azimuth_rad)\n",
    "    \n",
    "    # Œ∏_1s: componente 1P en cuadratura (seno)\n",
    "    theta_1s = theta_delta * np.sin(azimuth_rad)\n",
    "    \n",
    "    print(f\"   - Œ∏_1c (1P coseno): rango [{theta_1c.min():.2f}, {theta_1c.max():.2f}]¬∞\")\n",
    "    print(f\"   - Œ∏_1s (1P seno): rango [{theta_1s.min():.2f}, {theta_1s.max():.2f}]¬∞\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 4: Calcular rates (derivadas temporales)\n",
    "    # =========================================================================\n",
    "    print(f\"\\n   [3/3] Calculando rates (derivadas temporales)...\")\n",
    "    \n",
    "    # Derivadas usando diferencias finitas hacia atr√°s\n",
    "    # rate(t) ‚âà (value(t) - value(t-Œît)) / Œît\n",
    "    \n",
    "    # Œ∏Ãá_0: rate del colectivo\n",
    "    theta_0_rate = np.zeros_like(theta_0)\n",
    "    theta_0_rate[1:] = (theta_0[1:] - theta_0[:-1]) / dt\n",
    "    theta_0_rate[0] = theta_0_rate[1]  # primera muestra = segunda\n",
    "    \n",
    "    # Œ∏Ãá_1c: rate de 1P coseno\n",
    "    theta_1c_rate = np.zeros_like(theta_1c)\n",
    "    theta_1c_rate[1:] = (theta_1c[1:] - theta_1c[:-1]) / dt\n",
    "    theta_1c_rate[0] = theta_1c_rate[1]\n",
    "    \n",
    "    # Œ∏Ãá_1s: rate de 1P seno\n",
    "    theta_1s_rate = np.zeros_like(theta_1s)\n",
    "    theta_1s_rate[1:] = (theta_1s[1:] - theta_1s[:-1]) / dt\n",
    "    theta_1s_rate[0] = theta_1s_rate[1]\n",
    "    \n",
    "    # Œ©Ãá: rate de rotor speed\n",
    "    rotor_speed_rate = np.zeros_like(rotor_speed)\n",
    "    rotor_speed_rate[1:] = (rotor_speed[1:] - rotor_speed[:-1]) / dt\n",
    "    rotor_speed_rate[0] = rotor_speed_rate[1]\n",
    "    \n",
    "    print(f\"   - Œ∏Ãá_0 rate: rango [{theta_0_rate.min():.2f}, {theta_0_rate.max():.2f}] ¬∞/s\")\n",
    "    print(f\"   - Œ∏Ãá_1c rate: rango [{theta_1c_rate.min():.2f}, {theta_1c_rate.max():.2f}] ¬∞/s\")\n",
    "    print(f\"   - Œ∏Ãá_1s rate: rango [{theta_1s_rate.min():.2f}, {theta_1s_rate.max():.2f}] ¬∞/s\")\n",
    "    print(f\"   - Œ©Ãá rate: rango [{rotor_speed_rate.min():.2f}, {rotor_speed_rate.max():.2f}] rpm/s\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 5: Agregar al DataFrame\n",
    "    # =========================================================================\n",
    "    print(f\"\\n   Agregando columnas al DataFrame...\")\n",
    "    \n",
    "    df['pitch_0'] = theta_0\n",
    "    df['pitch_1c'] = theta_1c\n",
    "    df['pitch_1s'] = theta_1s\n",
    "    df['pitch_0_rate'] = theta_0_rate\n",
    "    df['pitch_1c_rate'] = theta_1c_rate\n",
    "    df['pitch_1s_rate'] = theta_1s_rate\n",
    "    df['rotor_speed_rate'] = rotor_speed_rate\n",
    "    \n",
    "    new_columns = ['pitch_0', 'pitch_1c', 'pitch_1s', \n",
    "                   'pitch_0_rate', 'pitch_1c_rate', 'pitch_1s_rate',\n",
    "                   'rotor_speed_rate']\n",
    "    \n",
    "    print(f\"   - Columnas creadas: {len(new_columns)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 6: Resumen final\n",
    "    # =========================================================================\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"RESUMEN:\")\n",
    "    print(f\"=\" * 70)\n",
    "    print(f\"   Features Coleman de pitch creados:\")\n",
    "    print(f\"   - pitch_0:  colectivo Œ∏_0 = (Œ∏_1 + Œ∏_2)/2\")\n",
    "    print(f\"   - pitch_1c: 1P coseno Œ∏_1c = Œ∏_Œî¬∑cos(œà)\")\n",
    "    print(f\"   - pitch_1s: 1P seno Œ∏_1s = Œ∏_Œî¬∑sin(œà)\")\n",
    "    print(f\"\\n   Rates (derivadas temporales):\")\n",
    "    print(f\"   - pitch_0_rate:  Œ∏Ãá_0\")\n",
    "    print(f\"   - pitch_1c_rate: Œ∏Ãá_1c\")\n",
    "    print(f\"   - pitch_1s_rate: Œ∏Ãá_1s\")\n",
    "    print(f\"   - rotor_speed_rate: Œ©Ãá\")\n",
    "    print(f\"\\n   üí° Estos features son coherentes con targets Coleman (M_0, M_1c, M_1s)\")\n",
    "    print(f\"   üí° Los rates capturan din√°mica ‚Üí mejoran predicci√≥n de componentes 1P\")\n",
    "    print(f\"\\n   Shape final del DataFrame: {df.shape}\")\n",
    "    print(f\"=\" * 70)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"OK - Funcion create_pitch_coleman_features definida correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "M√≥dulo para procesamiento de estad√≠sticas del campo de viento LIDAR con ROTACI√ìN configurable.\n",
    "\n",
    "Este m√≥dulo permite ROTAR la configuraci√≥n de beams para alinear correctamente\n",
    "el sistema de coordenadas del LIDAR con el sistema del rotor.\n",
    "\n",
    "C√ìMO USAR:\n",
    "- Cambia el par√°metro ROTATION_OFFSET para rotar todos los beams\n",
    "- ROTATION_OFFSET = 0  ‚Üí Sin rotaci√≥n (configuraci√≥n original)\n",
    "- ROTATION_OFFSET = 1  ‚Üí Rota 45¬∞ en sentido horario (beam 1 pasa a ser \"arriba\")\n",
    "- ROTATION_OFFSET = 2  ‚Üí Rota 90¬∞ en sentido horario (beam 2 pasa a ser \"arriba\")\n",
    "- ROTATION_OFFSET = -1 ‚Üí Rota 45¬∞ en sentido antihorario (beam 7 pasa a ser \"arriba\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PAR√ÅMETRO DE ROTACI√ìN - CAMBIA ESTE VALOR PARA TESTEAR\n",
    "# =============================================================================\n",
    "# ROTATION_OFFSET = 2  # üëà CAMBIA ESTE VALOR para rotar los beams\n",
    "                     # Cada unidad = 45¬∞ de rotaci√≥n\n",
    "\n",
    "\n",
    "def create_wind_field_statistics(df, rotation_offset=None):\n",
    "    \"\"\"\n",
    "    Crea estad√≠sticas del campo de viento LIDAR con configuraci√≥n ROTABLE de beams.\n",
    "    \n",
    "    PAR√ÅMETROS DE ROTACI√ìN:\n",
    "    - rotation_offset: N√∫mero de posiciones a rotar (None usa ROTATION_OFFSET global)\n",
    "                      +1 = rotar 45¬∞ CW, -1 = rotar 45¬∞ CCW\n",
    "    \n",
    "    DESCRIPCI√ìN:\n",
    "    Esta funci√≥n calcula caracter√≠sticas agregadas del campo de viento medido por el LIDAR\n",
    "    que capturan:\n",
    "    1. Intensidad del viento (media)\n",
    "    2. Turbulencia/heterogeneidad (desviaci√≥n est√°ndar)\n",
    "    3. Shear vertical (gradiente arriba-abajo)\n",
    "    4. Gradiente horizontal (diferencia izquierda-derecha, relacionado con yaw misalignment)\n",
    "    \n",
    "    F√ìRMULAS:\n",
    "    - U_mean = mean(VLOS de todos los BEAMs v√°lidos)  ‚Üí ayuda a predecir M_0\n",
    "    - U_std = std(VLOS de todos los BEAMs v√°lidos)    ‚Üí captura turbulencia/heterogeneidad\n",
    "    - U_shear_vert = mean(BEAMs arriba) - mean(BEAMs abajo)  ‚Üí shear vertical\n",
    "    - U_shear_horiz = mean(BEAMs izquierda) - mean(BEAMs derecha)  ‚Üí gradiente lateral\n",
    "    \n",
    "    CONFIGURACI√ìN BASE (antes de rotar):\n",
    "                    0¬∞ (‚Üë)\n",
    "                    BEAM 0\n",
    "                     |\n",
    "        315¬∞ BEAM 7  |  45¬∞ BEAM 1\n",
    "               ‚ï≤     |     ‚ï±\n",
    "                ‚ï≤    |    ‚ï±\n",
    "        270¬∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ+‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 90¬∞\n",
    "        BEAM 6       |       BEAM 2\n",
    "                ‚ï±    |    ‚ï≤\n",
    "               ‚ï±     |     ‚ï≤\n",
    "        225¬∞ BEAM 5  |  135¬∞ BEAM 3\n",
    "                     |\n",
    "                  BEAM 4\n",
    "                 180¬∞ (‚Üì)\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con columnas LAC_VLOS de diferentes BEAMs.\n",
    "        rotation_offset (int): Offset de rotaci√≥n (None usa valor global)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las nuevas columnas:\n",
    "                     - 'U_mean': velocidad media del campo de viento\n",
    "                     - 'U_std': desviaci√≥n est√°ndar (heterogeneidad)\n",
    "                     - 'U_shear_vert': shear vertical (arriba - abajo)\n",
    "                     - 'U_shear_horiz': gradiente horizontal (izquierda - derecha)\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: Si no se encuentran columnas VLOS en el DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usar valor global si no se especifica\n",
    "    if rotation_offset is None:\n",
    "        rotation_offset = ROTATION_OFFSET\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Creando estad√≠sticas del campo de viento LIDAR...\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üîÑ ROTACI√ìN APLICADA: {rotation_offset} posiciones ({rotation_offset * 45}¬∞)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 1: Identificar columnas VLOS (sin lag) y FILTRAR BEAMS VAC√çOS\n",
    "    # =========================================================================\n",
    "    print(\"\\n   [1/4] Identificando y filtrando columnas VLOS...\")\n",
    "    \n",
    "    # Buscar todas las columnas que contengan 'LAC_VLOS' pero NO 'lag'\n",
    "    vlos_cols = [col for col in df.columns if 'LAC_VLOS' in col and 'lag' not in col.lower()]\n",
    "    \n",
    "    if len(vlos_cols) == 0:\n",
    "        raise ValueError(\"No se encontraron columnas LAC_VLOS en el DataFrame\")\n",
    "    \n",
    "    print(f\"   - Columnas VLOS encontradas: {len(vlos_cols)}\")\n",
    "    \n",
    "    # Extraer n√∫meros de BEAM y FILTRAR por porcentaje de NaNs\n",
    "    beam_numbers = []\n",
    "    beam_to_col = {}\n",
    "    \n",
    "    for col in vlos_cols:\n",
    "        match = re.search(r'BEAM(\\d+)', col)\n",
    "        if match:\n",
    "            beam_num = int(match.group(1))\n",
    "            \n",
    "            # FILTRADO: Calcular porcentaje de NaNs\n",
    "            nan_percentage = df[col].isna().sum() / len(df) * 100\n",
    "            \n",
    "            if nan_percentage > 90:\n",
    "                # Beam vac√≠o ‚Üí ignorar\n",
    "                print(f\"   ‚ö†Ô∏è  BEAM {beam_num} ignorado ({nan_percentage:.1f}% NaNs)\")\n",
    "                continue\n",
    "            else:\n",
    "                # Beam v√°lido ‚Üí incluir\n",
    "                beam_numbers.append(beam_num)\n",
    "                beam_to_col[beam_num] = col\n",
    "                print(f\"   ‚úì  BEAM {beam_num} v√°lido ({nan_percentage:.1f}% NaNs)\")\n",
    "    \n",
    "    beam_numbers = sorted(set(beam_numbers))\n",
    "    print(f\"\\n   ‚Üí BEAMs v√°lidos detectados: {beam_numbers}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 2: Configuraci√≥n de posiciones de BEAMs CON ROTACI√ìN\n",
    "    # =========================================================================\n",
    "    print(\"\\n   [2/4] Configurando posiciones de BEAMs...\")\n",
    "    \n",
    "    num_beams = len(beam_numbers)\n",
    "    \n",
    "    # CONFIGURACI√ìN ESPEC√çFICA PARA 8 BEAMS (0-7) CON ROTACI√ìN\n",
    "    if num_beams == 8:\n",
    "        print(f\"   ‚Üí Configuraci√≥n de 8 BEAMs (distribuidos cada 45¬∞):\")\n",
    "        \n",
    "        # CONFIGURACI√ìN BASE (sin rotar)\n",
    "        # Arriba: 0¬∞, 45¬∞, 315¬∞ ‚Üí beams [0, 1, 7]\n",
    "        # Abajo: 135¬∞, 180¬∞, 225¬∞ ‚Üí beams [3, 4, 5]\n",
    "        # Izquierda: 225¬∞, 270¬∞, 315¬∞ ‚Üí beams [5, 6, 7]\n",
    "        # Derecha: 45¬∞, 90¬∞, 135¬∞ ‚Üí beams [1, 2, 3]\n",
    "        \n",
    "        # APLICAR ROTACI√ìN: cada beam se mueve rotation_offset posiciones\n",
    "        def rotate_beam(beam_num, offset):\n",
    "            \"\"\"Rota un beam aplicando m√≥dulo 8\"\"\"\n",
    "            return (beam_num + offset) % 8\n",
    "        \n",
    "        # Rotar cada grupo\n",
    "        beams_up_base = [0, 1, 7]\n",
    "        beams_down_base = [3, 4, 5]\n",
    "        beams_left_base = [5, 6, 7]\n",
    "        beams_right_base = [1, 2, 3]\n",
    "        \n",
    "        beams_up = [rotate_beam(b, rotation_offset) for b in beams_up_base]\n",
    "        beams_down = [rotate_beam(b, rotation_offset) for b in beams_down_base]\n",
    "        beams_left = [rotate_beam(b, rotation_offset) for b in beams_left_base]\n",
    "        beams_right = [rotate_beam(b, rotation_offset) for b in beams_right_base]\n",
    "        \n",
    "        # Calcular √°ngulos correspondientes\n",
    "        angles_up = [(b * 45) % 360 for b in beams_up]\n",
    "        angles_down = [(b * 45) % 360 for b in beams_down]\n",
    "        angles_left = [(b * 45) % 360 for b in beams_left]\n",
    "        angles_right = [(b * 45) % 360 for b in beams_right]\n",
    "        \n",
    "        print(f\"      Arriba (‚Üë):    {beams_up}    ‚Üí √°ngulos: {angles_up}\")\n",
    "        print(f\"      Abajo (‚Üì):     {beams_down}    ‚Üí √°ngulos: {angles_down}\")\n",
    "        print(f\"      Izquierda (‚Üê): {beams_left}    ‚Üí √°ngulos: {angles_left}\")\n",
    "        print(f\"      Derecha (‚Üí):   {beams_right}    ‚Üí √°ngulos: {angles_right}\")\n",
    "        \n",
    "    else:\n",
    "        # CONFIGURACI√ìN GEN√âRICA para otros n√∫meros de beams\n",
    "        print(f\"   ‚Üí Configuraci√≥n gen√©rica para {num_beams} BEAMs:\")\n",
    "        \n",
    "        if num_beams >= 4:\n",
    "            # Dividir en cuadrantes aproximadamente\n",
    "            quarter = num_beams // 4\n",
    "            \n",
    "            beams_up = beam_numbers[:quarter + 1]\n",
    "            beams_right = beam_numbers[quarter:2*quarter + 1]\n",
    "            beams_down = beam_numbers[2*quarter:3*quarter + 1]\n",
    "            beams_left = beam_numbers[3*quarter:] + beam_numbers[:1]\n",
    "            \n",
    "        else:\n",
    "            # Si hay muy pocos BEAMs, usar todos para cada c√°lculo\n",
    "            print(\"   ‚ö†Ô∏è  Pocos BEAMs detectados. Usando configuraci√≥n simplificada.\")\n",
    "            beams_up = beam_numbers[:len(beam_numbers)//2]\n",
    "            beams_down = beam_numbers[len(beam_numbers)//2:]\n",
    "            beams_left = beam_numbers[:len(beam_numbers)//2]\n",
    "            beams_right = beam_numbers[len(beam_numbers)//2:]\n",
    "        \n",
    "        print(f\"      Arriba (‚Üë):    {beams_up}\")\n",
    "        print(f\"      Derecha (‚Üí):   {beams_right}\")\n",
    "        print(f\"      Abajo (‚Üì):     {beams_down}\")\n",
    "        print(f\"      Izquierda (‚Üê): {beams_left}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 3: Calcular estad√≠sticas del campo de viento\n",
    "    # =========================================================================\n",
    "    print(\"\\n   [3/4] Calculando estad√≠sticas...\")\n",
    "    \n",
    "    # Crear DataFrame solo con columnas VLOS v√°lidas para c√°lculos eficientes\n",
    "    vlos_data = df[[beam_to_col[b] for b in beam_numbers]]\n",
    "    \n",
    "    # --- 3.1: U_mean (media del campo de viento) ---\n",
    "    U_mean = vlos_data.mean(axis=1)\n",
    "    print(f\"   - U_mean: rango [{U_mean.min():.2f}, {U_mean.max():.2f}] m/s\")\n",
    "    \n",
    "    # --- 3.2: U_std (heterogeneidad/turbulencia) ---\n",
    "    U_std = vlos_data.std(axis=1)\n",
    "    print(f\"   - U_std: rango [{U_std.min():.2f}, {U_std.max():.2f}] m/s\")\n",
    "    \n",
    "    # --- 3.3: U_shear_vert (shear vertical: arriba - abajo) ---\n",
    "    if len(beams_up) > 0 and len(beams_down) > 0:\n",
    "        cols_up = [beam_to_col[b] for b in beams_up if b in beam_to_col]\n",
    "        cols_down = [beam_to_col[b] for b in beams_down if b in beam_to_col]\n",
    "        \n",
    "        U_up = df[cols_up].mean(axis=1)\n",
    "        U_down = df[cols_down].mean(axis=1)\n",
    "        U_shear_vert = U_up - U_down\n",
    "        \n",
    "        print(f\"   - U_shear_vert: rango [{U_shear_vert.min():.2f}, {U_shear_vert.max():.2f}] m/s\")\n",
    "        print(f\"                   media: {U_shear_vert.mean():.3f} m/s\")\n",
    "    else:\n",
    "        U_shear_vert = pd.Series(0.0, index=df.index)\n",
    "        print(f\"   - U_shear_vert: no se pudo calcular (BEAMs insuficientes)\")\n",
    "    \n",
    "    # --- 3.4: U_shear_horiz (gradiente horizontal: izquierda - derecha) ---\n",
    "    if len(beams_left) > 0 and len(beams_right) > 0:\n",
    "        cols_left = [beam_to_col[b] for b in beams_left if b in beam_to_col]\n",
    "        cols_right = [beam_to_col[b] for b in beams_right if b in beam_to_col]\n",
    "        \n",
    "        U_left = df[cols_left].mean(axis=1)\n",
    "        U_right = df[cols_right].mean(axis=1)\n",
    "        U_shear_horiz = U_left - U_right\n",
    "        \n",
    "        print(f\"   - U_shear_horiz: rango [{U_shear_horiz.min():.2f}, {U_shear_horiz.max():.2f}] m/s\")\n",
    "        print(f\"                    media: {U_shear_horiz.mean():.3f} m/s\")\n",
    "    else:\n",
    "        U_shear_horiz = pd.Series(0.0, index=df.index)\n",
    "        print(f\"   - U_shear_horiz: no se pudo calcular (BEAMs insuficientes)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 4: Agregar al DataFrame\n",
    "    # =========================================================================\n",
    "    print(\"\\n   [4/4] Agregando columnas al DataFrame...\")\n",
    "    \n",
    "    df['U_mean'] = U_mean\n",
    "    df['U_std'] = U_std\n",
    "    df['U_shear_vert'] = U_shear_vert\n",
    "    df['U_shear_horiz'] = U_shear_horiz\n",
    "    \n",
    "    new_columns = ['U_mean', 'U_std', 'U_shear_vert', 'U_shear_horiz']\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PASO 5: Resumen final\n",
    "    # =========================================================================\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"RESUMEN:\")\n",
    "    print(f\"=\" * 70)\n",
    "    print(f\"   Estad√≠sticas del campo de viento creadas:\")\n",
    "    print(f\"   - U_mean:        velocidad media ‚Üí predice M_0\")\n",
    "    print(f\"   - U_std:         heterogeneidad/turbulencia\")\n",
    "    print(f\"   - U_shear_vert:  shear vertical (‚Üë - ‚Üì)\")\n",
    "    print(f\"   - U_shear_horiz: gradiente lateral (‚Üê - ‚Üí)\")\n",
    "    print(f\"\\n   üí° Estas variables capturan la estructura espacial del viento\")\n",
    "    print(f\"   üí° U_shear_vert y U_shear_horiz ayudan a predecir componentes 1P\")\n",
    "    print(f\"   üí° BEAMs vac√≠os (>90% NaN) fueron filtrados autom√°ticamente\")\n",
    "    print(f\"   üîÑ Rotaci√≥n aplicada: {rotation_offset} √ó 45¬∞ = {rotation_offset * 45}¬∞\")\n",
    "    print(f\"\\n   Shape final del DataFrame: {df.shape}\")\n",
    "    print(f\"=\" * 70)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_wind_statistics_lags(df, lag_times=[2, 5, 8, 11, 14, 17, 20, 23, 26]):\n",
    "    \"\"\"\n",
    "    Crea lags de las estad√≠sticas del campo de viento (U_mean, U_std, U_shear_vert, U_shear_horiz).\n",
    "    \n",
    "    DESCRIPCI√ìN:\n",
    "    Esta funci√≥n crea versiones desplazadas temporalmente de las estad√≠sticas del viento,\n",
    "    permitiendo al modelo capturar c√≥mo las condiciones de viento pasadas afectan las\n",
    "    cargas actuales en las palas.\n",
    "    \n",
    "    IMPORTANTE: Ejecutar despu√©s de create_wind_field_statistics().\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con las columnas U_mean, U_std, U_shear_vert, U_shear_horiz\n",
    "        lag_times (list): Lista de tiempos de lag en segundos\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con columnas adicionales:\n",
    "                     - 'U_mean_lag{X}s', 'U_std_lag{X}s', etc. para cada lag\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: Si faltan columnas de estad√≠sticas de viento.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validar que existen las columnas base\n",
    "    required_cols = ['U_mean', 'U_std', 'U_shear_vert', 'U_shear_horiz', 'Time']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Faltan columnas de estad√≠sticas de viento: {missing_cols}. \"\n",
    "                        f\"Ejecuta create_wind_field_statistics() primero.\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Creando lags de estad√≠sticas del campo de viento...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Calcular sampling rate\n",
    "    time_values = df['Time'].values\n",
    "    if len(time_values) > 1:\n",
    "        dt = time_values[1] - time_values[0]\n",
    "        fs = 1.0 / dt\n",
    "    else:\n",
    "        dt = 0.1\n",
    "        fs = 10.0\n",
    "    \n",
    "    print(f\"\\n   Par√°metros:\")\n",
    "    print(f\"   - Sampling rate: {fs:.1f} Hz (dt = {dt:.3f} s)\")\n",
    "    print(f\"   - Lags a crear: {lag_times[0]}s - {lag_times[-1]}s ({len(lag_times)} lags)\")\n",
    "    \n",
    "    # Variables base\n",
    "    base_vars = ['U_mean', 'U_std', 'U_shear_vert', 'U_shear_horiz']\n",
    "    \n",
    "    print(f\"   - Variables base: {len(base_vars)}\")\n",
    "    \n",
    "    # Contador de columnas creadas\n",
    "    created_cols = 0\n",
    "    \n",
    "    # Crear lags para cada variable\n",
    "    for var in base_vars:\n",
    "        for lag_sec in lag_times:\n",
    "            # Calcular lag en muestras\n",
    "            lag_samples = int(round(lag_sec * fs))\n",
    "            \n",
    "            # Nombre de la nueva columna\n",
    "            col_name = f\"{var}_lag{lag_sec}s\"\n",
    "            \n",
    "            # Crear lag usando shift\n",
    "            df[col_name] = df[var].shift(lag_samples)\n",
    "            \n",
    "            created_cols += 1\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ Columnas de lag creadas: {created_cols}\")\n",
    "    print(f\"      ({len(base_vars)} variables √ó {len(lag_times)} lags)\")\n",
    "    \n",
    "    # Resumen\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"RESUMEN:\")\n",
    "    print(f\"=\" * 70)\n",
    "    print(f\"   Lags de estad√≠sticas de viento creados:\")\n",
    "    print(f\"   - U_mean_lag{lag_times[0]}s ... U_mean_lag{lag_times[-1]}s\")\n",
    "    print(f\"   - U_std_lag{lag_times[0]}s ... U_std_lag{lag_times[-1]}s\")\n",
    "    print(f\"   - U_shear_vert_lag{lag_times[0]}s ... U_shear_vert_lag{lag_times[-1]}s\")\n",
    "    print(f\"   - U_shear_horiz_lag{lag_times[0]}s ... U_shear_horiz_lag{lag_times[-1]}s\")\n",
    "    print(f\"\\n   üí° Total de nuevas features: {created_cols}\")\n",
    "    print(f\"   üí° Estas capturas temporales del viento son cruciales para la predicci√≥n\")\n",
    "    print(f\"\\n   Shape final del DataFrame: {df.shape}\")\n",
    "    print(f\"=\" * 70)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 3.5: PROCESAR TODOS LOS ARCHIVOS CSV Y CREAR FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROCESANDO ARCHIVOS CSV - FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Contadores\n",
    "processed_count = 0\n",
    "failed_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "# Verificar que la carpeta existe\n",
    "if not data_folder_ml.exists():\n",
    "    print(\"ERROR: La carpeta data_train_traditional_ML no existe\")\n",
    "else:\n",
    "    csv_files = list(data_folder_ml.glob(\"*.csv\"))\n",
    "    \n",
    "    print(f\"\\nTotal de archivos CSV a procesar: {len(csv_files)}\\n\")\n",
    "    \n",
    "    # Procesar cada archivo CSV\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"Procesando: {csv_file.name}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # Cargar CSV\n",
    "            print(\"[1/5] Cargando CSV...\")\n",
    "            df = pd.read_csv(csv_file)\n",
    "            print(f\"      Shape original: {df.shape}\")\n",
    "            print(f\"      Columnas originales: {len(df.columns)}\")\n",
    "            \n",
    "            # Guardar numero de filas original\n",
    "            original_rows = len(df)\n",
    "            \n",
    "            # FEATURE 1: Lags de VLOS\n",
    "            print(\"\\n[2/5] Creando lags de VLOS (5-25 segundos)...\")\n",
    "            df = create_vlos_lags(df)\n",
    "            \n",
    "            # FEATURE 2: Componentes trigonometricas del azimuth\n",
    "            print(\"\\n[3/5] Creando componentes sin/cos del azimuth...\")\n",
    "            df = create_azimuth_components(df)\n",
    "            \n",
    "            # FEATURE 3: Componentes 1P y 2P de momentos flectores\n",
    "            print(\"\\n[4/5] Creando componentes 1P y 2P de momentos...\")\n",
    "            df = create_frequency_components_1P_2P(df)\n",
    "\n",
    "            # FEATURE 4: Componentes de pitch en marco Coleman\n",
    "            print(\"\\n[4.5/5] Creando componentes de pitch en marco Coleman...\")\n",
    "            df = create_pitch_coleman_features(df)\n",
    "\n",
    "            # FEATURE 5: Estad√≠sticas del campo de viento LIDAR\n",
    "            print(\"\\n[5/5] Creando estad√≠sticas del campo de viento LIDAR...\")\n",
    "            df = create_wind_field_statistics(df, rotation_offset=0)\n",
    "            df = create_wind_statistics_lags(df)\n",
    "\n",
    "            # Eliminar filas con NaN generadas por los lags\n",
    "            print(\"\\n[5/5] Limpiando datos...\")\n",
    "            df_cleaned = df.dropna()\n",
    "            rows_removed = original_rows - len(df_cleaned)\n",
    "            print(f\"      Filas con NaN eliminadas: {rows_removed}\")\n",
    "            print(f\"      Shape final: {df_cleaned.shape}\")\n",
    "            print(f\"      Columnas finales: {len(df_cleaned.columns)}\")\n",
    "            \n",
    "            # Guardar CSV actualizado\n",
    "            df_cleaned.to_csv(csv_file, index=False)\n",
    "            print(f\"\\n      OK - CSV actualizado guardado\")\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR procesando {csv_file.name}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            failed_count += 1\n",
    "        \n",
    "        print()  # Linea en blanco entre archivos\n",
    "    \n",
    "    # Resumen final\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"RESUMEN DEL PROCESAMIENTO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Archivos procesados exitosamente: {processed_count}\")\n",
    "    print(f\"Archivos con errores:              {failed_count}\")\n",
    "    print(f\"Total intentados:                  {len(csv_files)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if processed_count > 0:\n",
    "        print(f\"\\nCSVs actualizados en: {data_folder_ml}\")\n",
    "        print(\"\\nNuevas features a√±adidas a cada CSV:\")\n",
    "        print(\"  1. Lags de VLOS: 21 lags x N variables VLOS\")\n",
    "        print(\"  2. sin_rotor_azimuth y cos_rotor_azimuth\")\n",
    "        print(\"  3. Blade root 1 My 1P y Blade root 1 My 2P\")\n",
    "        print(\"  4. Blade root 2 My 1P y Blade root 2 My 2P\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 3 COMPLETADO - FEATURE ENGINEERING\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZACI√ìN: Componentes 0P, 1P, 2P y Reconstrucci√≥n de se√±ales\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Cargar el CSV\n",
    "csv_path = Path(r\"C:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\data_train_traditional_ML\\0040_DLC1.2_150_000_000_04.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"CSV cargado: {csv_path.name}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumnas disponibles que contienen 'M_':\")\n",
    "print([col for col in df.columns if 'M_' in col or 'Blade root' in col])\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE 1: Plot de las 5 componentes de frecuencia (M_0, M_1c, M_1s, M_2c, M_2s)\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "fig.suptitle('Componentes de Frecuencia 0P, 1P y 2P', fontsize=16, fontweight='bold')\n",
    "\n",
    "time = df['Time'].values\n",
    "\n",
    "# M_0 (0P - componente lento)\n",
    "axes[0].plot(time, df['M_0'].values, 'b-', linewidth=1)\n",
    "axes[0].set_ylabel('M_0 (0P)\\n[kNm]', fontsize=10, fontweight='bold')\n",
    "axes[0].set_title('Componente 0P (lento/promedio)', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# M_1c (1P coseno)\n",
    "axes[1].plot(time, df['M_1c'].values, 'r-', linewidth=1)\n",
    "axes[1].set_ylabel('M_1c (1P cos)\\n[kNm]', fontsize=10, fontweight='bold')\n",
    "axes[1].set_title('Componente 1P en fase (coseno)', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# M_1s (1P seno)\n",
    "axes[2].plot(time, df['M_1s'].values, 'g-', linewidth=1)\n",
    "axes[2].set_ylabel('M_1s (1P sin)\\n[kNm]', fontsize=10, fontweight='bold')\n",
    "axes[2].set_title('Componente 1P en cuadratura (seno)', fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# M_2c (2P coseno)\n",
    "axes[3].plot(time, df['M_2c'].values, 'm-', linewidth=1)\n",
    "axes[3].set_ylabel('M_2c (2P cos)\\n[kNm]', fontsize=10, fontweight='bold')\n",
    "axes[3].set_title('Componente 2P en fase (coseno)', fontsize=11)\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "# M_2s (2P seno)\n",
    "axes[4].plot(time, df['M_2s'].values, 'c-', linewidth=1)\n",
    "axes[4].set_ylabel('M_2s (2P sin)\\n[kNm]', fontsize=10, fontweight='bold')\n",
    "axes[4].set_title('Componente 2P en cuadratura (seno)', fontsize=11)\n",
    "axes[4].set_xlabel('Tiempo [s]', fontsize=11, fontweight='bold')\n",
    "axes[4].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE 2: Reconstrucci√≥n de Blade root 1 My y Blade root 2 My\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECONSTRUYENDO SE√ëALES ORIGINALES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Obtener datos necesarios\n",
    "M_0 = df['M_0'].values\n",
    "M_1c = df['M_1c'].values\n",
    "M_1s = df['M_1s'].values\n",
    "M_2c = df['M_2c'].values\n",
    "M_2s = df['M_2s'].values\n",
    "azimuth = df['Rotor azimuth angle'].values\n",
    "\n",
    "# Convertir azimut a radianes si est√° en grados\n",
    "if azimuth.max() > 6.5:\n",
    "    azimuth_rad = np.deg2rad(azimuth)\n",
    "    print(\"Azimut convertido de grados a radianes\")\n",
    "else:\n",
    "    azimuth_rad = azimuth\n",
    "    print(\"Azimut ya est√° en radianes\")\n",
    "\n",
    "# Reconstruir componentes diferenciales y de suma\n",
    "print(\"\\nReconstruyendo componentes intermedias...\")\n",
    "\n",
    "# Recuperar M_diff (1P) desde M_1c y M_1s\n",
    "M_diff_reconstructed = M_1c * np.cos(azimuth_rad) + M_1s * np.sin(azimuth_rad)\n",
    "print(\"   ‚úì M_Œî (1P) reconstruido\")\n",
    "\n",
    "# Recuperar M_sum (2P) desde M_2c y M_2s\n",
    "M_sum_reconstructed = M_2c * np.cos(2 * azimuth_rad) + M_2s * np.sin(2 * azimuth_rad)\n",
    "print(\"   ‚úì M_Œ£ (2P) reconstruido\")\n",
    "\n",
    "# Reconstruir las se√±ales originales\n",
    "# Opci√≥n 1: Solo con 1P (sin 2P)\n",
    "M1_reconstructed_1P = M_0 + M_diff_reconstructed \n",
    "M2_reconstructed_1P = M_0 - M_diff_reconstructed \n",
    "\n",
    "# Opci√≥n 2: Con 1P y 2P (m√°s completa)\n",
    "M1_reconstructed_full = M_0 + M_diff_reconstructed \n",
    "M2_reconstructed_full = M_0 - M_diff_reconstructed \n",
    "\n",
    "print(\"   ‚úì Blade root 1 My reconstruido\")\n",
    "print(\"   ‚úì Blade root 2 My reconstruido\")\n",
    "\n",
    "# Se√±ales originales\n",
    "M1_original = df['Blade root 1 My'].values\n",
    "M2_original = df['Blade root 2 My'].values\n",
    "\n",
    "# Calcular errores de reconstrucci√≥n\n",
    "error_M1_1P = np.sqrt(np.mean((M1_original - M1_reconstructed_1P)**2))\n",
    "error_M2_1P = np.sqrt(np.mean((M2_original - M2_reconstructed_1P)**2))\n",
    "error_M1_full = np.sqrt(np.mean((M1_original - M1_reconstructed_full)**2))\n",
    "error_M2_full = np.sqrt(np.mean((M2_original - M2_reconstructed_full)**2))\n",
    "\n",
    "print(f\"\\nErrores de reconstrucci√≥n (RMSE):\")\n",
    "print(f\"   Blade root 1 My (solo 1P):    {error_M1_1P:.2f} kNm\")\n",
    "print(f\"   Blade root 2 My (solo 1P):    {error_M2_1P:.2f} kNm\")\n",
    "print(f\"   Blade root 1 My (1P + 2P):    {error_M1_full:.2f} kNm\")\n",
    "print(f\"   Blade root 2 My (1P + 2P):    {error_M2_full:.2f} kNm\")\n",
    "\n",
    "# ============================================================================\n",
    "# PLOT: Comparaci√≥n Original vs Reconstrucci√≥n\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "fig.suptitle('Reconstrucci√≥n de Momentos Flectores desde Componentes 0P, 1P y 2P', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# SUBPLOT 1: Blade root 1 My\n",
    "axes[0].plot(time, M1_original, 'b-', linewidth=2, label='Original', alpha=0.7)\n",
    "axes[0].plot(time, M1_reconstructed_1P, 'r--', linewidth=1.5, \n",
    "             label=f'Reconstrucci√≥n (0P + 1P) - RMSE={error_M1_1P:.2f}', alpha=0.8)\n",
    "axes[0].plot(time, M1_reconstructed_full, 'g:', linewidth=1.5, \n",
    "             label=f'Reconstrucci√≥n (0P + 1P + 2P) - RMSE={error_M1_full:.2f}', alpha=0.8)\n",
    "axes[0].set_ylabel('Blade root 1 My [kNm]', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Blade root 1 My - Original vs Reconstrucci√≥n', fontsize=12)\n",
    "axes[0].legend(loc='upper right', fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# SUBPLOT 2: Blade root 2 My\n",
    "axes[1].plot(time, M2_original, 'b-', linewidth=2, label='Original', alpha=0.7)\n",
    "axes[1].plot(time, M2_reconstructed_1P, 'r--', linewidth=1.5, \n",
    "             label=f'Reconstrucci√≥n (0P + 1P) - RMSE={error_M2_1P:.2f}', alpha=0.8)\n",
    "axes[1].plot(time, M2_reconstructed_full, 'g:', linewidth=1.5, \n",
    "             label=f'Reconstrucci√≥n (0P + 1P + 2P) - RMSE={error_M2_full:.2f}', alpha=0.8)\n",
    "axes[1].set_ylabel('Blade root 2 My [kNm]', fontsize=11, fontweight='bold')\n",
    "axes[1].set_xlabel('Tiempo [s]', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Blade root 2 My - Original vs Reconstrucci√≥n', fontsize=12)\n",
    "axes[1].legend(loc='upper right', fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# PLOT ADICIONAL: Error de reconstrucci√≥n en el tiempo\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "fig.suptitle('Error de Reconstrucci√≥n en el Tiempo', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Error Blade root 1 My\n",
    "error_1P_M1 = M1_original - M1_reconstructed_1P\n",
    "error_full_M1 = M1_original - M1_reconstructed_full\n",
    "\n",
    "axes[0].plot(time, error_1P_M1, 'r-', linewidth=1, label='Error (0P + 1P)', alpha=0.7)\n",
    "axes[0].plot(time, error_full_M1, 'g-', linewidth=1, label='Error (0P + 1P + 2P)', alpha=0.7)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[0].set_ylabel('Error [kNm]', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Blade root 1 My - Error de Reconstrucci√≥n', fontsize=12)\n",
    "axes[0].legend(loc='upper right', fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error Blade root 2 My\n",
    "error_1P_M2 = M2_original - M2_reconstructed_1P\n",
    "error_full_M2 = M2_original - M2_reconstructed_full\n",
    "\n",
    "axes[1].plot(time, error_1P_M2, 'r-', linewidth=1, label='Error (0P + 1P)', alpha=0.7)\n",
    "axes[1].plot(time, error_full_M2, 'g-', linewidth=1, label='Error (0P + 1P + 2P)', alpha=0.7)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[1].set_ylabel('Error [kNm]', fontsize=11, fontweight='bold')\n",
    "axes[1].set_xlabel('Tiempo [s]', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Blade root 2 My - Error de Reconstrucci√≥n', fontsize=12)\n",
    "axes[1].legend(loc='upper right', fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZACI√ìN COMPLETADA\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZACI√ìN: SE√ëALES LAC_VLOS DE BEAMS INDIVIDUALES (0-7)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Archivo a analizar\n",
    "csv_path = Path(r\"C:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\data_train_traditional_ML\\0040_DLC1.2_150_000_000_04.csv\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VISUALIZACI√ìN: SE√ëALES LAC_VLOS POR BEAM\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Archivo: {csv_path.name}\\n\")\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"‚úÖ Datos cargados: {df.shape[0]:,} filas, {df.shape[1]} columnas\\n\")\n",
    "\n",
    "# Buscar columnas LAC_VLOS (sin lag, sin beam8/beam9)\n",
    "vlos_cols = []\n",
    "for col in df.columns:\n",
    "    if 'LAC_VLOS' in col and 'lag' not in col.lower():\n",
    "        # Extraer n√∫mero de beam\n",
    "        match = re.search(r'BEAM(\\d+)', col)\n",
    "        if match:\n",
    "            beam_num = int(match.group(1))\n",
    "            # Excluir beams 8 y 9\n",
    "            if beam_num not in [8, 9]:\n",
    "                vlos_cols.append((beam_num, col))\n",
    "\n",
    "# Ordenar por n√∫mero de beam\n",
    "vlos_cols.sort(key=lambda x: x[0])\n",
    "\n",
    "print(f\"Columnas LAC_VLOS encontradas (beams 0-7):\")\n",
    "for beam_num, col_name in vlos_cols:\n",
    "    nan_pct = df[col_name].isna().sum() / len(df) * 100\n",
    "    mean_val = df[col_name].mean()\n",
    "    print(f\"  BEAM {beam_num}: {col_name}\")\n",
    "    print(f\"    - Media: {mean_val:.3f} m/s | NaN: {nan_pct:.1f}%\")\n",
    "\n",
    "if len(vlos_cols) == 0:\n",
    "    print(\"\\n‚ùå No se encontraron columnas LAC_VLOS v√°lidas\")\n",
    "else:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"GR√ÅFICO: SERIES TEMPORALES DE LAC_VLOS (BEAMS 0-7)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Crear figura con subplots (uno por beam)\n",
    "    n_beams = len(vlos_cols)\n",
    "    fig, axes = plt.subplots(n_beams, 1, figsize=(16, 2.5 * n_beams))\n",
    "    fig.suptitle(f'LAC_VLOS - Series Temporales por BEAM\\n{csv_path.name}', \n",
    "                 fontsize=14, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # Asegurar que axes sea una lista\n",
    "    if n_beams == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Tiempo\n",
    "    time = df['Time'].values if 'Time' in df.columns else np.arange(len(df))\n",
    "    \n",
    "    # Colores por beam (usar colormap)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 8))\n",
    "    \n",
    "    # Plotear cada beam\n",
    "    for idx, (beam_num, col_name) in enumerate(vlos_cols):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Datos\n",
    "        vlos = df[col_name].values\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(time, vlos, linewidth=0.8, color=colors[beam_num], alpha=0.9)\n",
    "        ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5, alpha=0.3)\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        mean_val = np.nanmean(vlos)\n",
    "        std_val = np.nanstd(vlos)\n",
    "        min_val = np.nanmin(vlos)\n",
    "        max_val = np.nanmax(vlos)\n",
    "        \n",
    "        # Labels\n",
    "        ax.set_ylabel(f'BEAM {beam_num}\\n(m/s)', fontsize=10, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # T√≠tulo con √°ngulo\n",
    "        angle = beam_num * 45\n",
    "        ax.set_title(f'BEAM {beam_num} - √Ångulo: {angle}¬∞ | Media: {mean_val:.2f} m/s | Std: {std_val:.2f} m/s', \n",
    "                    fontsize=10, fontweight='bold', loc='left')\n",
    "        \n",
    "        # Estad√≠sticas en cuadro\n",
    "        stats_text = f'Rango: [{min_val:.2f}, {max_val:.2f}]'\n",
    "        ax.text(0.98, 0.95, stats_text, transform=ax.transAxes, \n",
    "               verticalalignment='top', horizontalalignment='right',\n",
    "               fontsize=8, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "    \n",
    "    # X label solo en el √∫ltimo\n",
    "    axes[-1].set_xlabel('Time (s)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°fico de series temporales generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # AN√ÅLISIS COMPARATIVO: MEDIAS POR BEAM\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"AN√ÅLISIS: COMPARACI√ìN DE MEDIAS POR BEAM\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Extraer medias\n",
    "    beam_stats = []\n",
    "    for beam_num, col_name in vlos_cols:\n",
    "        mean_val = df[col_name].mean()\n",
    "        std_val = df[col_name].std()\n",
    "        beam_stats.append({\n",
    "            'Beam': beam_num,\n",
    "            '√Ångulo': beam_num * 45,\n",
    "            'Media': mean_val,\n",
    "            'Std': std_val\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(beam_stats)\n",
    "    print(stats_df.to_string(index=False))\n",
    "    \n",
    "    # Gr√°fico de barras con medias\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('Comparaci√≥n de Estad√≠sticas por BEAM', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Subplot 1: Medias\n",
    "    ax1.bar(stats_df['Beam'], stats_df['Media'], color=colors[:len(stats_df)], \n",
    "           edgecolor='black', alpha=0.8)\n",
    "    ax1.set_xlabel('BEAM', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('Media VLOS (m/s)', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Media de Velocidad por BEAM', fontsize=11, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    ax1.set_xticks(stats_df['Beam'])\n",
    "    \n",
    "    # A√±adir √°ngulos como segundo eje X\n",
    "    ax1_top = ax1.twiny()\n",
    "    ax1_top.set_xlim(ax1.get_xlim())\n",
    "    ax1_top.set_xticks(stats_df['Beam'])\n",
    "    ax1_top.set_xticklabels([f\"{a}¬∞\" for a in stats_df['√Ångulo']])\n",
    "    ax1_top.set_xlabel('√Ångulo', fontsize=10)\n",
    "    \n",
    "    # Subplot 2: Desviaciones est√°ndar\n",
    "    ax2.bar(stats_df['Beam'], stats_df['Std'], color=colors[:len(stats_df)], \n",
    "           edgecolor='black', alpha=0.8)\n",
    "    ax2.set_xlabel('BEAM', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylabel('Std VLOS (m/s)', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('Desviaci√≥n Est√°ndar por BEAM', fontsize=11, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    ax2.set_xticks(stats_df['Beam'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°fico de comparaci√≥n generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DIAGN√ìSTICO: SHEAR CALCULADO A MANO\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"DIAGN√ìSTICO: C√ÅLCULO MANUAL DE SHEAR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Configuraci√≥n actual\n",
    "    beams_up = [0, 1, 7]\n",
    "    beams_down = [3, 4, 5]\n",
    "    beams_left = [5, 6, 7]\n",
    "    beams_right = [1, 2, 3]\n",
    "    \n",
    "    # Extraer medias por grupo\n",
    "    mean_up = np.mean([stats_df[stats_df['Beam'] == b]['Media'].values[0] \n",
    "                       for b in beams_up if b in stats_df['Beam'].values])\n",
    "    mean_down = np.mean([stats_df[stats_df['Beam'] == b]['Media'].values[0] \n",
    "                         for b in beams_down if b in stats_df['Beam'].values])\n",
    "    mean_left = np.mean([stats_df[stats_df['Beam'] == b]['Media'].values[0] \n",
    "                         for b in beams_left if b in stats_df['Beam'].values])\n",
    "    mean_right = np.mean([stats_df[stats_df['Beam'] == b]['Media'].values[0] \n",
    "                          for b in beams_right if b in stats_df['Beam'].values])\n",
    "    \n",
    "    shear_vert = mean_up - mean_down\n",
    "    shear_horiz = mean_left - mean_right\n",
    "    \n",
    "    print(f\"Configuraci√≥n actual:\")\n",
    "    print(f\"  Arriba (‚Üë):     beams {beams_up}     ‚Üí media: {mean_up:.3f} m/s\")\n",
    "    print(f\"  Abajo (‚Üì):      beams {beams_down}   ‚Üí media: {mean_down:.3f} m/s\")\n",
    "    print(f\"  Izquierda (‚Üê):  beams {beams_left}   ‚Üí media: {mean_left:.3f} m/s\")\n",
    "    print(f\"  Derecha (‚Üí):    beams {beams_right}  ‚Üí media: {mean_right:.3f} m/s\")\n",
    "    print(f\"\\nüîç SHEAR CALCULADO:\")\n",
    "    print(f\"  U_shear_vert  = {mean_up:.3f} - {mean_down:.3f} = {shear_vert:+.3f} m/s\")\n",
    "    print(f\"  U_shear_horiz = {mean_left:.3f} - {mean_right:.3f} = {shear_horiz:+.3f} m/s\")\n",
    "    \n",
    "    # Comparar con valores guardados en el CSV\n",
    "    if 'U_shear_vert' in df.columns:\n",
    "        csv_shear_vert = df['U_shear_vert'].mean()\n",
    "        csv_shear_horiz = df['U_shear_horiz'].mean()\n",
    "        print(f\"\\nüìä VALORES EN EL CSV:\")\n",
    "        print(f\"  U_shear_vert  (CSV): {csv_shear_vert:+.3f} m/s\")\n",
    "        print(f\"  U_shear_horiz (CSV): {csv_shear_horiz:+.3f} m/s\")\n",
    "        print(f\"\\n‚úì Diferencia vertical:   {abs(shear_vert - csv_shear_vert):.4f} m/s\")\n",
    "        print(f\"‚úì Diferencia horizontal: {abs(shear_horiz - csv_shear_horiz):.4f} m/s\")\n",
    "    \n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZACI√ìN DE NUEVAS FEATURES: PITCH COLEMAN Y ESTAD√çSTICAS DE VIENTO\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Archivo a analizar\n",
    "csv_path = Path(r\"C:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\data_train_traditional_ML\\0040_DLC1.2_150_000_000_04.csv\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VISUALIZACI√ìN DE NUEVAS FEATURES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Archivo: {csv_path.name}\\n\")\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"‚úÖ Datos cargados: {df.shape[0]:,} filas, {df.shape[1]} columnas\\n\")\n",
    "\n",
    "# Variables a plotear\n",
    "pitch_vars = ['pitch_0', 'pitch_1c', 'pitch_1s', \n",
    "              'pitch_0_rate', 'pitch_1c_rate', 'pitch_1s_rate', 'rotor_speed_rate']\n",
    "\n",
    "wind_vars = ['U_mean', 'U_std', 'U_shear_vert', 'U_shear_horiz']\n",
    "\n",
    "# Verificar qu√© variables existen\n",
    "pitch_available = [v for v in pitch_vars if v in df.columns]\n",
    "wind_available = [v for v in wind_vars if v in df.columns]\n",
    "\n",
    "print(f\"Variables de pitch Coleman encontradas: {len(pitch_available)}/{len(pitch_vars)}\")\n",
    "print(f\"Variables de estad√≠sticas de viento encontradas: {len(wind_available)}/{len(wind_vars)}\")\n",
    "\n",
    "if len(pitch_available) == 0 and len(wind_available) == 0:\n",
    "    print(\"\\n‚ùå ERROR: No se encontraron las variables. Ejecuta create_pitch_coleman_features() \"\n",
    "          \"y create_wind_field_statistics() primero.\")\n",
    "else:\n",
    "    # ========================================================================\n",
    "    # GR√ÅFICO 1: PITCH COLEMAN - TIME SERIES\n",
    "    # ========================================================================\n",
    "    \n",
    "    if len(pitch_available) >= 3:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"GR√ÅFICO 1: PITCH COLEMAN - COMPONENTES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "        fig.suptitle(f'Pitch Coleman Transformation - {csv_path.name}', \n",
    "                     fontsize=14, fontweight='bold', y=0.995)\n",
    "        \n",
    "        time = df['Time'].values if 'Time' in df.columns else np.arange(len(df))\n",
    "        \n",
    "        # Subplot 1: pitch_0 (colectivo)\n",
    "        if 'pitch_0' in df.columns:\n",
    "            ax = axes[0]\n",
    "            ax.plot(time, df['pitch_0'], linewidth=1, color='steelblue', alpha=0.8)\n",
    "            ax.set_ylabel('Œ∏‚ÇÄ (¬∞)', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Componente Colectivo', fontsize=11, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            # Estad√≠sticas\n",
    "            mean_val = df['pitch_0'].mean()\n",
    "            std_val = df['pitch_0'].std()\n",
    "            ax.text(0.02, 0.95, f'Media: {mean_val:.2f}¬∞\\nStd: {std_val:.2f}¬∞',\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                   fontsize=9)\n",
    "        \n",
    "        # Subplot 2: pitch_1c y pitch_1s (componentes 1P)\n",
    "        ax = axes[1]\n",
    "        if 'pitch_1c' in df.columns:\n",
    "            ax.plot(time, df['pitch_1c'], linewidth=1, color='red', \n",
    "                   alpha=0.7, label='Œ∏‚ÇÅc (coseno)')\n",
    "        if 'pitch_1s' in df.columns:\n",
    "            ax.plot(time, df['pitch_1s'], linewidth=1, color='blue', \n",
    "                   alpha=0.7, label='Œ∏‚ÇÅs (seno)')\n",
    "        ax.set_ylabel('Œ∏‚ÇÅc, Œ∏‚ÇÅs (¬∞)', fontsize=11, fontweight='bold')\n",
    "        ax.set_title('Componentes 1P (Ejes Fijos)', fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(loc='upper right', fontsize=10)\n",
    "        \n",
    "        # Subplot 3: Rates\n",
    "        ax = axes[2]\n",
    "        if 'pitch_0_rate' in df.columns:\n",
    "            ax.plot(time, df['pitch_0_rate'], linewidth=0.8, color='green', \n",
    "                   alpha=0.6, label='Œ∏Ãá‚ÇÄ')\n",
    "        if 'pitch_1c_rate' in df.columns:\n",
    "            ax.plot(time, df['pitch_1c_rate'], linewidth=0.8, color='orange', \n",
    "                   alpha=0.6, label='Œ∏Ãá‚ÇÅc')\n",
    "        if 'pitch_1s_rate' in df.columns:\n",
    "            ax.plot(time, df['pitch_1s_rate'], linewidth=0.8, color='purple', \n",
    "                   alpha=0.6, label='Œ∏Ãá‚ÇÅs')\n",
    "        ax.set_xlabel('Time (s)', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Rates (¬∞/s)', fontsize=11, fontweight='bold')\n",
    "        ax.set_title('Derivadas Temporales (Velocidades Angulares)', \n",
    "                    fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(loc='upper right', fontsize=10, ncol=3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Gr√°fico 1 generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # GR√ÅFICO 2: PITCH - SCATTER PLOTS (relaciones)\n",
    "    # ========================================================================\n",
    "    \n",
    "    if 'pitch_1c' in df.columns and 'pitch_1s' in df.columns:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"GR√ÅFICO 2: PITCH COLEMAN - SCATTER PLOTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        fig.suptitle(f'Pitch Coleman - Relaciones - {csv_path.name}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Subplot 1: Œ∏‚ÇÅc vs Œ∏‚ÇÅs (deber√≠a formar un c√≠rculo/elipse)\n",
    "        ax = axes[0]\n",
    "        scatter = ax.scatter(df['pitch_1c'], df['pitch_1s'], \n",
    "                           c=time, cmap='viridis', s=2, alpha=0.6)\n",
    "        ax.set_xlabel('Œ∏‚ÇÅc (¬∞)', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Œ∏‚ÇÅs (¬∞)', fontsize=11, fontweight='bold')\n",
    "        ax.set_title('Componentes 1P: Œ∏‚ÇÅc vs Œ∏‚ÇÅs', fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.axis('equal')\n",
    "        plt.colorbar(scatter, ax=ax, label='Time (s)')\n",
    "        \n",
    "        # Subplot 2: Œ∏‚ÇÄ vs amplitud 1P\n",
    "        if 'pitch_0' in df.columns:\n",
    "            ax = axes[1]\n",
    "            amplitude_1P = np.sqrt(df['pitch_1c']**2 + df['pitch_1s']**2)\n",
    "            ax.scatter(df['pitch_0'], amplitude_1P, s=2, alpha=0.5, color='steelblue')\n",
    "            ax.set_xlabel('Œ∏‚ÇÄ - Colectivo (¬∞)', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('Amplitud 1P (¬∞)', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Colectivo vs Amplitud 1P', fontsize=11, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            # Correlaci√≥n\n",
    "            corr = np.corrcoef(df['pitch_0'].dropna(), amplitude_1P.dropna())[0, 1]\n",
    "            ax.text(0.02, 0.95, f'Correlaci√≥n: {corr:.3f}',\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                   fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Gr√°fico 2 generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # GR√ÅFICO 3: ESTAD√çSTICAS DE VIENTO - TIME SERIES\n",
    "    # ========================================================================\n",
    "    \n",
    "    if len(wind_available) > 0:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"GR√ÅFICO 3: ESTAD√çSTICAS DEL CAMPO DE VIENTO\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "        fig.suptitle(f'Estad√≠sticas del Campo de Viento LIDAR - {csv_path.name}', \n",
    "                     fontsize=14, fontweight='bold', y=0.995)\n",
    "        \n",
    "        time = df['Time'].values if 'Time' in df.columns else np.arange(len(df))\n",
    "        \n",
    "        # Subplot 1: U_mean\n",
    "        if 'U_mean' in df.columns:\n",
    "            ax = axes[0]\n",
    "            ax.plot(time, df['U_mean'], linewidth=1, color='navy', alpha=0.8)\n",
    "            ax.set_ylabel('U_mean (m/s)', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Velocidad Media del Campo de Viento', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            mean_val = df['U_mean'].mean()\n",
    "            std_val = df['U_mean'].std()\n",
    "            ax.text(0.02, 0.95, f'Media: {mean_val:.2f} m/s\\nStd: {std_val:.2f} m/s',\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                   fontsize=9)\n",
    "        \n",
    "        # Subplot 2: U_std\n",
    "        if 'U_std' in df.columns:\n",
    "            ax = axes[1]\n",
    "            ax.plot(time, df['U_std'], linewidth=1, color='darkorange', alpha=0.8)\n",
    "            ax.set_ylabel('U_std (m/s)', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Heterogeneidad del Campo (Desviaci√≥n Est√°ndar)', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            mean_val = df['U_std'].mean()\n",
    "            ax.text(0.02, 0.95, f'Media: {mean_val:.2f} m/s',\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                   fontsize=9)\n",
    "        \n",
    "        # Subplot 3: U_shear_vert\n",
    "        if 'U_shear_vert' in df.columns:\n",
    "            ax = axes[2]\n",
    "            ax.plot(time, df['U_shear_vert'], linewidth=1, color='green', alpha=0.8)\n",
    "            ax.axhline(y=0, color='black', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "            ax.set_ylabel('U_shear_vert (m/s)', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Shear Vertical (Arriba - Abajo)', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            mean_val = df['U_shear_vert'].mean()\n",
    "            ax.text(0.02, 0.95, f'Media: {mean_val:.2f} m/s',\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "                   fontsize=9)\n",
    "        \n",
    "        # Subplot 4: U_shear_horiz\n",
    "        if 'U_shear_horiz' in df.columns:\n",
    "            ax = axes[3]\n",
    "            ax.plot(time, df['U_shear_horiz'], linewidth=1, color='purple', alpha=0.8)\n",
    "            ax.axhline(y=0, color='black', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "            ax.set_xlabel('Time (s)', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('U_shear_horiz (m/s)', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Gradiente Horizontal (Izquierda - Derecha)', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            mean_val = df['U_shear_horiz'].mean()\n",
    "            ax.text(0.02, 0.95, f'Media: {mean_val:.2f} m/s',\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='plum', alpha=0.8),\n",
    "                   fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Gr√°fico 3 generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # GR√ÅFICO 4: HISTOGRAMAS Y DISTRIBUCIONES\n",
    "    # ========================================================================\n",
    "    \n",
    "    all_vars = pitch_available + wind_available\n",
    "    \n",
    "    if len(all_vars) > 0:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"GR√ÅFICO 4: DISTRIBUCIONES (HISTOGRAMAS)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Calcular grid size\n",
    "        n_vars = len(all_vars)\n",
    "        n_cols = 4\n",
    "        n_rows = int(np.ceil(n_vars / n_cols))\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 3*n_rows))\n",
    "        fig.suptitle(f'Distribuciones de Features - {csv_path.name}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        \n",
    "        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "        \n",
    "        for idx, var in enumerate(all_vars):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            data = df[var].dropna()\n",
    "            \n",
    "            # Histograma\n",
    "            ax.hist(data, bins=50, color='steelblue', edgecolor='black', \n",
    "                   alpha=0.7, density=True)\n",
    "            \n",
    "            # Curva de densidad (KDE)\n",
    "            from scipy import stats\n",
    "            if len(data) > 10:\n",
    "                try:\n",
    "                    kde = stats.gaussian_kde(data)\n",
    "                    x_range = np.linspace(data.min(), data.max(), 200)\n",
    "                    ax.plot(x_range, kde(x_range), 'r-', linewidth=2, \n",
    "                           label='KDE', alpha=0.8)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Estad√≠sticas\n",
    "            mean_val = data.mean()\n",
    "            median_val = data.median()\n",
    "            std_val = data.std()\n",
    "            \n",
    "            ax.axvline(mean_val, color='red', linestyle='--', linewidth=1.5, \n",
    "                      alpha=0.7, label=f'Media: {mean_val:.2f}')\n",
    "            ax.axvline(median_val, color='green', linestyle='--', linewidth=1.5, \n",
    "                      alpha=0.7, label=f'Mediana: {median_val:.2f}')\n",
    "            \n",
    "            ax.set_title(var, fontsize=10, fontweight='bold')\n",
    "            ax.set_xlabel('Valor', fontsize=9)\n",
    "            ax.set_ylabel('Densidad', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.legend(fontsize=7, loc='upper right')\n",
    "            \n",
    "            # Texto con estad√≠sticas\n",
    "            stats_text = f'Œº={mean_val:.2f}\\nœÉ={std_val:.2f}'\n",
    "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "                   verticalalignment='top', fontsize=8,\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.6))\n",
    "        \n",
    "        # Ocultar axes sobrantes\n",
    "        for idx in range(len(all_vars), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Gr√°fico 4 generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # GR√ÅFICO 5: CORRELACIONES ENTRE VARIABLES (INCLUYENDO TARGETS)\n",
    "    # ========================================================================\n",
    "    \n",
    "    # A√±adir targets de momento para la correlaci√≥n\n",
    "    target_vars = ['M_0', 'M_1c', 'M_1s']\n",
    "    target_available = [v for v in target_vars if v in df.columns]\n",
    "    \n",
    "    # Variables para correlaci√≥n: features + targets\n",
    "    corr_vars = all_vars + target_available\n",
    "    \n",
    "    if len(corr_vars) >= 2:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"GR√ÅFICO 5: MATRIZ DE CORRELACI√ìN (FEATURES + TARGETS)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"Variables incluidas en correlaci√≥n:\")\n",
    "        print(f\"  - Features pitch: {len(pitch_available)}\")\n",
    "        print(f\"  - Features viento: {len(wind_available)}\")\n",
    "        print(f\"  - Targets: {len(target_available)} {target_available}\")\n",
    "        \n",
    "        # Calcular matriz de correlaci√≥n\n",
    "        corr_matrix = df[corr_vars].corr()\n",
    "        \n",
    "        # Tama√±o de figura adaptativo\n",
    "        fig_size = max(12, len(corr_vars) * 0.8)\n",
    "        fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
    "        \n",
    "        import seaborn as sns\n",
    "        \n",
    "        # Crear anotaciones personalizadas (m√°s grandes para targets)\n",
    "        annot_size = 8 if len(corr_vars) <= 12 else 6\n",
    "        \n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                   center=0, vmin=-1, vmax=1, square=True, ax=ax,\n",
    "                   cbar_kws={'label': 'Correlaci√≥n de Pearson'},\n",
    "                   linewidths=0.5, linecolor='gray',\n",
    "                   annot_kws={'size': annot_size})\n",
    "        \n",
    "        ax.set_title(f'Matriz de Correlaci√≥n - Features y Targets\\n{csv_path.name}', \n",
    "                    fontsize=14, fontweight='bold', pad=20)\n",
    "        \n",
    "        # Resaltar targets en los labels\n",
    "        ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "        xlabels = [label.get_text() for label in ax.get_xticklabels()]\n",
    "        \n",
    "        # Poner en negrita los targets\n",
    "        new_ylabels = []\n",
    "        for label in ylabels:\n",
    "            if label in target_available:\n",
    "                new_ylabels.append(f'**{label}**')\n",
    "            else:\n",
    "                new_ylabels.append(label)\n",
    "        \n",
    "        new_xlabels = []\n",
    "        for label in xlabels:\n",
    "            if label in target_available:\n",
    "                new_xlabels.append(f'**{label}**')\n",
    "            else:\n",
    "                new_xlabels.append(label)\n",
    "        \n",
    "        ax.set_yticklabels(new_ylabels, fontweight='bold' if len(target_available) > 0 else 'normal')\n",
    "        ax.set_xticklabels(new_xlabels, fontweight='bold' if len(target_available) > 0 else 'normal')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Gr√°fico 5 generado\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # An√°lisis de correlaciones m√°s relevantes con targets\n",
    "        # ====================================================================\n",
    "        \n",
    "        if len(target_available) > 0:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"AN√ÅLISIS DE CORRELACIONES CON TARGETS\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            for target in target_available:\n",
    "                print(f\"\\nüéØ Target: {target}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                # Obtener correlaciones con el target\n",
    "                target_corrs = corr_matrix[target].drop(target).abs().sort_values(ascending=False)\n",
    "                \n",
    "                # Top 5 correlaciones\n",
    "                top_n = min(5, len(target_corrs))\n",
    "                print(f\"Top {top_n} correlaciones (valor absoluto):\")\n",
    "                for i, (var, corr_val) in enumerate(target_corrs.head(top_n).items(), 1):\n",
    "                    # Obtener correlaci√≥n con signo\n",
    "                    corr_signed = corr_matrix.loc[var, target]\n",
    "                    print(f\"  {i}. {var:20s}: {corr_signed:+.3f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RESUMEN ESTAD√çSTICO\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESUMEN ESTAD√çSTICO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    summary = df[all_vars].describe()\n",
    "    print(summary)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"AN√ÅLISIS COMPLETADO\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a4c25",
   "metadata": {},
   "source": [
    "## PASO 3.6: Unir todos los CSVs en un dataset completo\n",
    "\n",
    "Una vez que todos los archivos CSV individuales han sido procesados y tienen sus nuevas features, los combinaremos en un √∫nico DataFrame gigante que contendr√° todos los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 3.6: UNIR TODOS LOS CSVs EN UN DATASET COMPLETO (OPTIMIZADO PARA MEMORIA)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREANDO DATASET COMPLETO - 0000_Complete_dataset.csv\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Funcion auxiliar para optimizar tipos de datos y reducir memoria\n",
    "def optimize_dataframe_memory(df):\n",
    "    \"\"\"\n",
    "    Optimiza el uso de memoria de un DataFrame convirtiendo tipos de datos.\n",
    "    - float64 -> float32 (reduce 50% memoria)\n",
    "    - int64 -> int16 si es posible\n",
    "    \"\"\"\n",
    "    memory_before = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Convertir float64 a float32\n",
    "        if col_type == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        \n",
    "        # Convertir int64 a int16 si los valores lo permiten\n",
    "        elif col_type == 'int64':\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if col_min >= -32768 and col_max <= 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            elif col_min >= -2147483648 and col_max <= 2147483647:\n",
    "                df[col] = df[col].astype('int32')\n",
    "    \n",
    "    memory_after = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    memory_saved = memory_before - memory_after\n",
    "    \n",
    "    return df, memory_before, memory_after, memory_saved\n",
    "\n",
    "# Variables de control\n",
    "loaded_files = 0\n",
    "failed_files = 0\n",
    "total_memory_before = 0\n",
    "total_memory_after = 0\n",
    "batch_size = 10  # Procesar en lotes de 10 archivos\n",
    "\n",
    "# Obtener lista de CSVs en data_train_traditional_ML\n",
    "csv_files = list(data_folder_ml.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"\\nTotal de archivos CSV encontrados: {len(csv_files)}\")\n",
    "print(f\"Procesamiento por lotes de {batch_size} archivos para optimizar memoria\\n\")\n",
    "\n",
    "# Lista para almacenar DataFrames concatenados por lote\n",
    "batched_dataframes = []\n",
    "\n",
    "# Procesar en lotes\n",
    "num_batches = (len(csv_files) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROCESANDO LOTE {batch_idx + 1}/{num_batches}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Obtener archivos del lote actual\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(csv_files))\n",
    "    batch_files = csv_files[start_idx:end_idx]\n",
    "    \n",
    "    # Lista temporal para este lote\n",
    "    batch_dfs = []\n",
    "    \n",
    "    # Cargar archivos del lote\n",
    "    for csv_file in batch_files:\n",
    "        try:\n",
    "            print(f\"  Cargando: {csv_file.name}\", end=\"\")\n",
    "            df_temp = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Optimizar tipos de datos inmediatamente\n",
    "            df_temp, mem_before, mem_after, mem_saved = optimize_dataframe_memory(df_temp)\n",
    "            \n",
    "            batch_dfs.append(df_temp)\n",
    "            loaded_files += 1\n",
    "            total_memory_before += mem_before\n",
    "            total_memory_after += mem_after\n",
    "            \n",
    "            print(f\" ... OK ({df_temp.shape[0]} filas, {df_temp.shape[1]} cols, {mem_saved:.2f} MB ahorrados)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" ... ERROR: {str(e)}\")\n",
    "            failed_files += 1\n",
    "    \n",
    "    # Concatenar lote actual si hay archivos cargados\n",
    "    if batch_dfs:\n",
    "        print(f\"\\n  Concatenando lote {batch_idx + 1}...\")\n",
    "        batch_concatenated = pd.concat(batch_dfs, ignore_index=True)\n",
    "        batched_dataframes.append(batch_concatenated)\n",
    "        print(f\"  OK - Lote {batch_idx + 1} concatenado: {batch_concatenated.shape}\")\n",
    "        \n",
    "        # Liberar memoria del lote\n",
    "        del batch_dfs\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Archivos cargados exitosamente: {loaded_files}\")\n",
    "print(f\"Archivos con errores:           {failed_files}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nOptimizaci√≥n de memoria:\")\n",
    "print(f\"  - Memoria ANTES:   {total_memory_before:.2f} MB\")\n",
    "print(f\"  - Memoria DESPU√âS: {total_memory_after:.2f} MB\")\n",
    "print(f\"  - Memoria AHORRADA: {total_memory_before - total_memory_after:.2f} MB ({(1 - total_memory_after/total_memory_before)*100:.1f}% reducci√≥n)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "if loaded_files > 0:\n",
    "    # Concatenar todos los lotes\n",
    "    print(\"Concatenando todos los lotes en DataFrame final...\")\n",
    "    df_complete = pd.concat(batched_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Liberar memoria de lotes\n",
    "    del batched_dataframes\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Dataset completo creado:\")\n",
    "    print(f\"  - Total de filas:    {df_complete.shape[0]:,}\")\n",
    "    print(f\"  - Total de columnas: {df_complete.shape[1]:,}\")\n",
    "    print(f\"  - Tama√±o en memoria: {df_complete.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Verificar que no hay NaN\n",
    "    nan_count = df_complete.isna().sum().sum()\n",
    "    print(f\"  - Total de valores NaN: {nan_count}\")\n",
    "    \n",
    "    if nan_count > 0:\n",
    "        print(f\"\\nADVERTENCIA: Se encontraron {nan_count} valores NaN en el dataset completo\")\n",
    "    \n",
    "    # Guardar el dataset completo\n",
    "    output_path = data_folder_ml / \"0000_Complete_dataset.csv\"\n",
    "    print(f\"\\nGuardando dataset completo en:\")\n",
    "    print(f\"  {output_path}\")\n",
    "    \n",
    "    df_complete.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATASET COMPLETO CREADO EXITOSAMENTE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nArchivo: 0000_Complete_dataset.csv\")\n",
    "    print(f\"Ubicaci√≥n: {data_folder_ml}\")\n",
    "    print(f\"\\nEste dataset contiene:\")\n",
    "    print(f\"  - Todas las simulaciones de viento combinadas\")\n",
    "    print(f\"  - Todas las features originales\")\n",
    "    print(f\"  - Lags de VLOS (5-25 segundos)\")\n",
    "    print(f\"  - Componentes sin/cos del azimuth\")\n",
    "    print(f\"  - Componentes 1P y 2P de momentos flectores\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Mostrar primeras columnas del dataset\n",
    "    print(\"\\nPrimeras 5 filas del dataset completo:\")\n",
    "    print(df_complete.head())\n",
    "    \n",
    "else:\n",
    "    print(\"\\nERROR: No se pudo cargar ning√∫n archivo CSV\")\n",
    "    print(\"Verifica que los archivos existan en data_train_traditional_ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29052eb3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä STEP 4: An√°lisis Exploratorio de Datos (EDA) - Modelos Tradicionales\n",
    "\n",
    "En este paso vamos a realizar un an√°lisis exploratorio exhaustivo del dataset completo para modelos de Machine Learning tradicionales.\n",
    "\n",
    "### üéØ Objetivos del EDA:\n",
    "\n",
    "1. **Distribuciones de variables**: Histogramas y estad√≠sticas descriptivas\n",
    "2. **Correlaciones**: Matrices de correlaci√≥n entre features y targets\n",
    "3. **Series temporales**: Visualizaci√≥n de se√±ales en el tiempo\n",
    "4. **Outliers**: Detecci√≥n de valores at√≠picos\n",
    "5. **Relaciones bivariadas**: Scatter plots entre features importantes y targets\n",
    "\n",
    "### üìÅ Organizaci√≥n:\n",
    "Todos los gr√°ficos se guardar√°n en: `notebook/00_EDA_traditional_ML/`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaea9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 4.1: Configurar carpeta para guardar gr√°ficos del EDA\n",
    "# ============================================================================\n",
    "\n",
    "# Crear carpeta para gr√°ficos del EDA dentro de notebook/\n",
    "eda_folder = Path.cwd() / \"00_EDA_traditional_ML\"\n",
    "eda_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACI√ìN EDA - AN√ÅLISIS EXPLORATORIO DE DATOS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta para gr√°ficos: {eda_folder}\")\n",
    "print(f\"Estado: {'‚úÖ Carpeta creada/verificada' if eda_folder.exists() else '‚ùå Error al crear carpeta'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configurar estilo de gr√°ficos para el EDA\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuraci√≥n de visualizaci√≥n aplicada\")\n",
    "print(\"üìä Listo para generar gr√°ficos del EDA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39798f7",
   "metadata": {},
   "source": [
    "### üìä PASO 4.2: Histogramas de Variables F√≠sicas\n",
    "\n",
    "Generaremos histogramas detallados de las variables f√≠sicas est√°ndar para analizar sus distribuciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 4.2: GENERAR HISTOGRAMAS DE VARIABLES F√çSICAS (LECTURA POR CHUNKS)\n",
    "# ============================================================================\n",
    "\n",
    "# Crear subcarpeta para histogramas\n",
    "histograms_folder = eda_folder / \"00_Histogramas\"\n",
    "histograms_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERANDO HISTOGRAMAS - VARIABLES F√çSICAS EST√ÅNDAR\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {histograms_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar dataset completo\n",
    "complete_dataset_path = data_folder_ml / \"0000_Complete_dataset.csv\"\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "    print(\"   Por favor, ejecuta primero el PASO 3.6 para crear el dataset completo\")\n",
    "else:\n",
    "    print(f\"\\nArchivo: {complete_dataset_path.name}\")\n",
    "    print(\"‚öôÔ∏è  M√©todo: Lectura por CHUNKS (optimizado para memoria)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 1: Leer primera l√≠nea para obtener nombres de columnas\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[1/3] Leyendo columnas del CSV...\")\n",
    "    df_sample = pd.read_csv(complete_dataset_path, nrows=0)  # Solo headers\n",
    "    all_columns = df_sample.columns.tolist()\n",
    "    print(f\"‚úÖ Total de columnas: {len(all_columns)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 2: Definir variables f√≠sicas est√°ndar a graficar\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[2/3] Seleccionando variables a graficar...\")\n",
    "    \n",
    "    variables_to_plot = []\n",
    "    \n",
    "    # 1. Rotor Speed\n",
    "    if 'Rotor speed' in all_columns:\n",
    "        variables_to_plot.append('Rotor speed')\n",
    "    \n",
    "    # 2. Pitch angles\n",
    "    pitch_vars = ['Blade 1 pitch angle', 'Blade 2 pitch angle']\n",
    "    for var in pitch_vars:\n",
    "        if var in all_columns:\n",
    "            variables_to_plot.append(var)\n",
    "    \n",
    "    # 3. Azimuth components\n",
    "    azimuth_vars = ['sin_rotor_azimuth', 'cos_rotor_azimuth']\n",
    "    for var in azimuth_vars:\n",
    "        if var in all_columns:\n",
    "            variables_to_plot.append(var)\n",
    "    \n",
    "    # 4. Blade moments (TARGETS)\n",
    "    moment_vars = ['Blade root 1 My', 'Blade root 2 My']\n",
    "    for var in moment_vars:\n",
    "        if var in all_columns:\n",
    "            variables_to_plot.append(var)\n",
    "\n",
    "    # 5. Componentes de frecuencia (0P, 1P, 2P)\n",
    "    frequency_components = ['M_0', 'M_1c', 'M_1s', 'M_2c', 'M_2s']\n",
    "    for var in frequency_components:\n",
    "        if var in all_columns:\n",
    "            variables_to_plot.append(var)\n",
    "    \n",
    "    # 6. VLOS variables (sin lags)\n",
    "    vlos_vars = [col for col in all_columns \n",
    "                 if 'LAC_VLOS' in col and 'lag' not in col]\n",
    "    variables_to_plot.extend(vlos_vars)\n",
    "    \n",
    "    print(f\"\\nüìä Variables a graficar: {len(variables_to_plot)}\")\n",
    "    print(\"\\nCategor√≠as:\")\n",
    "    print(f\"  - Rotor speed: 1\")\n",
    "    print(f\"  - Pitch angles: {len([v for v in variables_to_plot if 'pitch' in v.lower()])}\")\n",
    "    print(f\"  - Azimuth components: {len([v for v in variables_to_plot if 'azimuth' in v.lower()])}\")\n",
    "    print(f\"  - Blade moments: {len([v for v in variables_to_plot if 'My' in v])}\")\n",
    "    print(f\"  - VLOS (sin lags): {len(vlos_vars)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 3: Leer CSV por chunks y acumular datos para cada variable\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[3/3] Leyendo CSV por chunks...\")\n",
    "    \n",
    "    chunk_size = 100000  # 100k filas por chunk\n",
    "    data_accumulated = {var: [] for var in variables_to_plot}\n",
    "    \n",
    "    # Contador de chunks procesados\n",
    "    chunk_count = 0\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Leer CSV en chunks\n",
    "    for chunk in pd.read_csv(complete_dataset_path, chunksize=chunk_size, \n",
    "                             usecols=variables_to_plot):\n",
    "        chunk_count += 1\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        # Acumular datos de cada variable\n",
    "        for var in variables_to_plot:\n",
    "            if var in chunk.columns:\n",
    "                # Extraer valores no-NaN y convertir a lista\n",
    "                valid_data = chunk[var].dropna().values.tolist()\n",
    "                data_accumulated[var].extend(valid_data)\n",
    "        \n",
    "        print(f\"  Chunk {chunk_count} procesado ({len(chunk):,} filas) - Total acumulado: {total_rows:,}\")\n",
    "        \n",
    "        # Liberar memoria del chunk\n",
    "        del chunk\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Lectura completada: {total_rows:,} filas procesadas en {chunk_count} chunks\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 4: Generar histogramas con datos acumulados\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"GENERANDO HISTOGRAMAS...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    num_bins = 50  # N√∫mero de bins para detalle\n",
    "    generated_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for var in variables_to_plot:\n",
    "        try:\n",
    "            # Obtener datos acumulados\n",
    "            data = np.array(data_accumulated[var])\n",
    "            \n",
    "            if len(data) == 0:\n",
    "                print(f\"  ‚ö†Ô∏è  {var}: Sin datos v√°lidos\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Crear figura\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Crear histograma\n",
    "            n, bins, patches = ax.hist(data, bins=num_bins, \n",
    "                                       color='steelblue', \n",
    "                                       edgecolor='black', \n",
    "                                       alpha=0.7)\n",
    "            \n",
    "            # Calcular estad√≠sticas\n",
    "            mean_val = np.mean(data)\n",
    "            median_val = np.median(data)\n",
    "            std_val = np.std(data)\n",
    "            min_val = np.min(data)\n",
    "            max_val = np.max(data)\n",
    "            \n",
    "            # A√±adir l√≠neas de media y mediana\n",
    "            ax.axvline(mean_val, color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Media: {mean_val:.2f}')\n",
    "            ax.axvline(median_val, color='green', linestyle='--', \n",
    "                      linewidth=2, label=f'Mediana: {median_val:.2f}')\n",
    "            \n",
    "            # Configurar t√≠tulo y etiquetas\n",
    "            ax.set_title(f'Distribuci√≥n de {var}', \n",
    "                        fontsize=14, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel(var, fontsize=12)\n",
    "            ax.set_ylabel('Frecuencia', fontsize=12)\n",
    "            \n",
    "            # A√±adir cuadro de estad√≠sticas\n",
    "            stats_text = f'n = {len(data):,}\\nMedia = {mean_val:.2f}\\nMediana = {median_val:.2f}\\nStd = {std_val:.2f}\\nMin = {min_val:.2f}\\nMax = {max_val:.2f}'\n",
    "            ax.text(0.02, 0.98, stats_text,\n",
    "                   transform=ax.transAxes,\n",
    "                   verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                   fontsize=10)\n",
    "            \n",
    "            # Leyenda\n",
    "            ax.legend(loc='upper right')\n",
    "            \n",
    "            # Grid\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            # Ajustar layout\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Guardar figura\n",
    "            safe_filename = var.replace(' ', '_').replace('/', '_')\n",
    "            output_path = histograms_folder / f\"hist_{safe_filename}.png\"\n",
    "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            generated_count += 1\n",
    "            print(f\"  ‚úÖ {var}\")\n",
    "            \n",
    "            # Liberar memoria de datos\n",
    "            del data\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            print(f\"  ‚ùå {var}: {str(e)}\")\n",
    "    \n",
    "    # Liberar memoria de datos acumulados\n",
    "    del data_accumulated\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Resumen final\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESUMEN - GENERACI√ìN DE HISTOGRAMAS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Histogramas generados:  {generated_count}\")\n",
    "    print(f\"Errores:                {failed_count}\")\n",
    "    print(f\"Total intentados:       {len(variables_to_plot)}\")\n",
    "    print(f\"\\nüìÅ Gr√°ficos guardados en: {histograms_folder}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633b46b",
   "metadata": {},
   "source": [
    "### üåç PASO 4.3: An√°lisis del Efecto Gravitacional en las Palas\n",
    "\n",
    "Analizaremos la relaci√≥n entre el √°ngulo de azimuth (posici√≥n angular del rotor) y las cargas flectoras en las palas. Se espera observar una **onda senoidal clara** que confirma que el sensor mide correctamente el efecto del peso de la pala al girar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbcd30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 4.4: EFECTO GRAVITACIONAL - AZIMUTH VS CARGAS\n",
    "# ============================================================================\n",
    "\n",
    "# Crear subcarpeta para efecto gravitacional\n",
    "gravity_folder = eda_folder / \"01_EfectoGravedad\"\n",
    "gravity_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AN√ÅLISIS EFECTO GRAVITACIONAL - AZIMUTH VS CARGAS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {gravity_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar dataset completo\n",
    "complete_dataset_path = data_folder_ml / \"0000_Complete_dataset.csv\"\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    print(f\"\\nArchivo: {complete_dataset_path.name}\")\n",
    "    print(\"‚öôÔ∏è  M√©todo: Lectura por CHUNKS + Muestreo para gr√°ficos\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 1: Leer columnas necesarias por chunks con muestreo\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[1/3] Cargando datos (muestreo 1 de cada 10 filas)...\")\n",
    "    \n",
    "    # Columnas necesarias\n",
    "    columns_needed = ['Rotor azimuth angle', 'sin_rotor_azimuth', 'cos_rotor_azimuth',\n",
    "                  'Blade root 1 My', 'Blade root 2 My',\n",
    "                  'M_0', 'M_1c', 'M_1s', 'M_2c', 'M_2s']\n",
    "    \n",
    "    # Leer con muestreo (skiprows para reducir datos)\n",
    "    # Tomar 1 de cada 10 filas para reducir memoria\n",
    "    sample_rate = 10\n",
    "    df_sample = pd.read_csv(complete_dataset_path, \n",
    "                           usecols=columns_needed,\n",
    "                           skiprows=lambda i: i > 0 and i % sample_rate != 0)\n",
    "    \n",
    "    print(f\"‚úÖ Datos cargados: {df_sample.shape[0]:,} filas (muestreadas), {df_sample.shape[1]} columnas\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 2: Reconstruir azimuth a grados 0-360\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[2/3] Reconstruyendo azimuth a grados 0-360...\")\n",
    "    \n",
    "    # Verificar si ya est√° en grados o radianes\n",
    "    if df_sample['Rotor azimuth angle'].max() > 6.5:\n",
    "        print(\"   Azimuth ya est√° en grados\")\n",
    "        azimuth_degrees = df_sample['Rotor azimuth angle']\n",
    "    else:\n",
    "        print(\"   Convirtiendo azimuth de radianes a grados\")\n",
    "        azimuth_degrees = np.rad2deg(df_sample['Rotor azimuth angle'])\n",
    "    \n",
    "    # Normalizar a rango 0-360\n",
    "    azimuth_degrees = azimuth_degrees % 360\n",
    "    \n",
    "    print(f\"   Rango azimuth: {azimuth_degrees.min():.1f}¬∞ - {azimuth_degrees.max():.1f}¬∞\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 3: Generar gr√°ficos\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[3/3] Generando gr√°ficos...\")\n",
    "    \n",
    "    # Variables a graficar contra azimuth\n",
    "    y_variables = [\n",
    "        'Blade root 1 My',\n",
    "        'Blade root 2 My',\n",
    "        'Blade root 1 My 1P',\n",
    "        'Blade root 1 My 2P',\n",
    "        'Blade root 2 My 1P',\n",
    "        'Blade root 2 My 2P'\n",
    "    ]\n",
    "    \n",
    "    generated_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for y_var in y_variables:\n",
    "        if y_var not in df_sample.columns:\n",
    "            print(f\"  ‚ö†Ô∏è  {y_var}: No existe en el dataset\")\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Datos limpios (sin NaN)\n",
    "            mask = ~(df_sample[y_var].isna() | azimuth_degrees.isna())\n",
    "            x_data = azimuth_degrees[mask]\n",
    "            y_data = df_sample[y_var][mask]\n",
    "            \n",
    "            if len(x_data) == 0:\n",
    "                print(f\"  ‚ö†Ô∏è  {y_var}: Sin datos v√°lidos\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # -------------------------------------------------------------------\n",
    "            # GR√ÅFICO 1: SCATTER PLOT\n",
    "            # -------------------------------------------------------------------\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            # Scatter plot con transparencia\n",
    "            scatter = ax.scatter(x_data, y_data, \n",
    "                               alpha=0.3, s=1, c=y_data, \n",
    "                               cmap='viridis')\n",
    "            \n",
    "            # Colorbar\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label(f'{y_var} (kNm)', fontsize=10)\n",
    "            \n",
    "            # Etiquetas y t√≠tulo\n",
    "            ax.set_xlabel('√Ångulo de Azimuth (grados)', fontsize=12)\n",
    "            ax.set_ylabel(f'{y_var} (kNm)', fontsize=12)\n",
    "            ax.set_title(f'Efecto Gravitacional: Azimuth vs {y_var}', \n",
    "                        fontsize=14, fontweight='bold', pad=20)\n",
    "            \n",
    "            # Grid\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            # Ajustar l√≠mites del eje X\n",
    "            ax.set_xlim(0, 360)\n",
    "            \n",
    "            # Ticks cada 45 grados\n",
    "            ax.set_xticks(np.arange(0, 361, 45))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Guardar scatter\n",
    "            safe_filename = y_var.replace(' ', '_').replace('/', '_')\n",
    "            output_scatter = gravity_folder / f\"scatter_azimuth_vs_{safe_filename}.png\"\n",
    "            plt.savefig(output_scatter, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # -------------------------------------------------------------------\n",
    "            # GR√ÅFICO 2: HEXBIN PLOT (Mapa de densidad)\n",
    "            # -------------------------------------------------------------------\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            # Hexbin plot\n",
    "            hexbin = ax.hexbin(x_data, y_data, \n",
    "                              gridsize=50, cmap='YlOrRd', \n",
    "                              mincnt=1, bins='log')\n",
    "            \n",
    "            # Colorbar\n",
    "            cbar = plt.colorbar(hexbin, ax=ax)\n",
    "            cbar.set_label('Densidad (log scale)', fontsize=10)\n",
    "            \n",
    "            # Etiquetas y t√≠tulo\n",
    "            ax.set_xlabel('√Ångulo de Azimuth (grados)', fontsize=12)\n",
    "            ax.set_ylabel(f'{y_var} (kNm)', fontsize=12)\n",
    "            ax.set_title(f'Mapa de Densidad: Azimuth vs {y_var}', \n",
    "                        fontsize=14, fontweight='bold', pad=20)\n",
    "            \n",
    "            # Grid\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            # Ajustar l√≠mites del eje X\n",
    "            ax.set_xlim(0, 360)\n",
    "            \n",
    "            # Ticks cada 45 grados\n",
    "            ax.set_xticks(np.arange(0, 361, 45))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Guardar hexbin\n",
    "            output_hexbin = gravity_folder / f\"hexbin_azimuth_vs_{safe_filename}.png\"\n",
    "            plt.savefig(output_hexbin, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            generated_count += 1\n",
    "            print(f\"  ‚úÖ {y_var} (scatter + hexbin)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            print(f\"  ‚ùå {y_var}: {str(e)}\")\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del df_sample\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RESUMEN FINAL\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESUMEN - AN√ÅLISIS EFECTO GRAVITACIONAL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Variables procesadas:    {generated_count}\")\n",
    "    print(f\"Errores:                 {failed_count}\")\n",
    "    print(f\"Total gr√°ficos:          {generated_count * 2} (scatter + hexbin)\")\n",
    "    print(f\"\\nüìÅ Gr√°ficos guardados en: {gravity_folder}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚úÖ Se espera observar una ONDA SENOIDAL clara\")\n",
    "    print(\"   Esto confirma que el sensor mide correctamente el peso de la pala al girar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53592c7",
   "metadata": {},
   "source": [
    "### üîó PASO 4.4: Matriz de Correlaci√≥n - Variables F√≠sicas\n",
    "\n",
    "Calcularemos y visualizaremos la **matriz de correlaci√≥n de Pearson** entre las variables f√≠sicas est√°ndar (sin lags) para entender las relaciones lineales entre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e929e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 4.5: MATRIZ DE CORRELACI√ìN - VARIABLES F√çSICAS\n",
    "# ============================================================================\n",
    "\n",
    "# Crear subcarpeta para correlaciones\n",
    "correlation_folder = eda_folder / \"02_Correlations\"\n",
    "correlation_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MATRIZ DE CORRELACI√ìN - VARIABLES F√çSICAS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {correlation_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar dataset completo\n",
    "complete_dataset_path = data_folder_ml / \"0000_Complete_dataset.csv\"\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    print(f\"\\nArchivo: {complete_dataset_path.name}\")\n",
    "    print(\"‚öôÔ∏è  M√©todo: Lectura por CHUNKS + Correlaci√≥n incremental\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 1: Definir variables f√≠sicas (sin lags)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[1/4] Definiendo variables f√≠sicas a correlacionar...\")\n",
    "    \n",
    "    # Variables f√≠sicas est√°ndar (sin lags de VLOS)\n",
    "    physical_vars = [\n",
    "        'Rotor speed',\n",
    "        'Blade 1 pitch angle',\n",
    "        'Blade 2 pitch angle',\n",
    "        'Rotor azimuth angle',\n",
    "        'sin_rotor_azimuth',\n",
    "        'cos_rotor_azimuth',\n",
    "        'Blade root 1 My',\n",
    "        'Blade root 2 My',\n",
    "        'pitch_0',\n",
    "        'pitch_1c',\n",
    "        'pitch_1s',\n",
    "        'pitch_0_rate',\n",
    "        'pitch_1c_rate',\n",
    "        'pitch_1s_rate', \n",
    "        'U_mean',\n",
    "        'U_std',\n",
    "        'U_shear_vert',\n",
    "        'M_0', \n",
    "        'M_1c', \n",
    "        'M_1s', \n",
    "        'M_2c', \n",
    "        'M_2s',\n",
    "    ]\n",
    "    \n",
    "    # A√±adir VLOS sin lags\n",
    "    print(\"   Leyendo columnas disponibles...\")\n",
    "    df_sample = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_sample.columns.tolist()\n",
    "    \n",
    "    vlos_vars = [col for col in all_columns \n",
    "                 if 'LAC_VLOS' in col and 'lag' not in col]\n",
    "    \n",
    "    physical_vars.extend(vlos_vars)\n",
    "    \n",
    "    # Filtrar solo las que existen en el dataset\n",
    "    available_vars = [var for var in physical_vars if var in all_columns]\n",
    "    \n",
    "    print(f\"\\nüìä Variables a correlacionar: {len(available_vars)}\")\n",
    "    print(\"\\nCategor√≠as:\")\n",
    "    print(f\"  - Rotor speed: 1\")\n",
    "    print(f\"  - Pitch angles: {len([v for v in available_vars if 'pitch' in v.lower()])}\")\n",
    "    print(f\"  - Azimuth: {len([v for v in available_vars if 'azimuth' in v.lower()])}\")\n",
    "    print(f\"  - Blade moments: {len([v for v in available_vars if 'My' in v])}\")\n",
    "    print(f\"  - VLOS (sin lags): {len([v for v in available_vars if 'LAC_VLOS' in v])}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 2: Cargar datos con muestreo\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[2/4] Cargando datos (muestreo para eficiencia)...\")\n",
    "    \n",
    "    # Cargar con muestreo (1 de cada 5 filas)\n",
    "    sample_rate = 5\n",
    "    df_corr = pd.read_csv(complete_dataset_path,\n",
    "                          usecols=available_vars,\n",
    "                          skiprows=lambda i: i > 0 and i % sample_rate != 0)\n",
    "    \n",
    "    print(f\"‚úÖ Datos cargados: {df_corr.shape[0]:,} filas, {df_corr.shape[1]} columnas\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 3: Calcular matriz de correlaci√≥n\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[3/4] Calculando matriz de correlaci√≥n de Pearson...\")\n",
    "    \n",
    "    # Calcular correlaci√≥n\n",
    "    corr_matrix = df_corr.corr(method='pearson')\n",
    "    \n",
    "    print(f\"‚úÖ Matriz de correlaci√≥n: {corr_matrix.shape}\")\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del df_corr\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 4: Generar heatmap\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[4/4] Generando heatmap...\")\n",
    "    \n",
    "    # Crear figura grande para ver todos los detalles\n",
    "    fig_size = max(12, len(available_vars) * 0.5)\n",
    "    fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
    "    \n",
    "    # Crear heatmap con seaborn\n",
    "    sns.heatmap(corr_matrix,\n",
    "                annot=True,           # Mostrar valores num√©ricos\n",
    "                fmt='.2f',            # Formato con 2 decimales\n",
    "                cmap='coolwarm',      # Mapa de color divergente\n",
    "                center=0,             # Centrar en 0\n",
    "                vmin=-1, vmax=1,      # Rango de correlaci√≥n\n",
    "                square=True,          # Celdas cuadradas\n",
    "                linewidths=0.5,       # L√≠neas entre celdas\n",
    "                cbar_kws={'label': 'Correlaci√≥n de Pearson'},\n",
    "                ax=ax)\n",
    "    \n",
    "    # T√≠tulo\n",
    "    ax.set_title('Matriz de Correlaci√≥n - Variables F√≠sicas (sin lags)',\n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Rotar etiquetas para mejor legibilidad\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    \n",
    "    # Ajustar layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar figura\n",
    "    output_path = correlation_folder / \"correlation_matrix_physical_vars.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Heatmap generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 5: An√°lisis de correlaciones fuertes\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[5/4] Analizando correlaciones fuertes...\")\n",
    "    \n",
    "    # Encontrar correlaciones fuertes (|r| > 0.7, excluyendo diagonal)\n",
    "    strong_correlations = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_value = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.7:\n",
    "                var1 = corr_matrix.columns[i]\n",
    "                var2 = corr_matrix.columns[j]\n",
    "                strong_correlations.append((var1, var2, corr_value))\n",
    "    \n",
    "    # Ordenar por valor absoluto de correlaci√≥n\n",
    "    strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    print(f\"\\nüîç Correlaciones fuertes encontradas (|r| > 0.7): {len(strong_correlations)}\")\n",
    "    print(\"\\nTop 10 correlaciones m√°s fuertes:\")\n",
    "    for i, (var1, var2, corr_val) in enumerate(strong_correlations[:10], 1):\n",
    "        print(f\"  {i}. {var1} <-> {var2}: {corr_val:.3f}\")\n",
    "    \n",
    "    # Guardar correlaciones fuertes en CSV\n",
    "    if strong_correlations:\n",
    "        df_strong = pd.DataFrame(strong_correlations, \n",
    "                                columns=['Variable 1', 'Variable 2', 'Correlaci√≥n'])\n",
    "        csv_path = correlation_folder / \"strong_correlations.csv\"\n",
    "        df_strong.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nüíæ Correlaciones fuertes guardadas en: {csv_path.name}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RESUMEN FINAL\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESUMEN - MATRIZ DE CORRELACI√ìN\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Variables analizadas:      {len(available_vars)}\")\n",
    "    print(f\"Correlaciones calculadas:  {(len(available_vars) * (len(available_vars)-1)) // 2}\")\n",
    "    print(f\"Correlaciones fuertes:     {len(strong_correlations)} (|r| > 0.7)\")\n",
    "    print(f\"\\nüìÅ Archivos generados:\")\n",
    "    print(f\"  - Heatmap: correlation_matrix_physical_vars.png\")\n",
    "    print(f\"  - CSV: strong_correlations.csv\")\n",
    "    print(f\"\\nüìç Ubicaci√≥n: {correlation_folder}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚úÖ An√°lisis completado\")\n",
    "    print(\"   Usa el heatmap para identificar relaciones lineales entre variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911dcdc0",
   "metadata": {},
   "source": [
    "### üïí PASO 4.5: An√°lisis de Retardo (Lag) - Tiempo de Viaje del Viento\n",
    "\n",
    "Analizaremos las **correlaciones entre variables VLOS con lag** y las **cargas en las palas** para determinar el **tiempo de viaje del viento** desde el LIDAR hasta el rotor.\n",
    "\n",
    "**Metodolog√≠a:**\n",
    "- Calcular correlaci√≥n de Pearson entre cada variable `LAC_VLOS_BEAMX_RANGE5_lag_Xs` y los targets (`Blade root 1/2 My`)\n",
    "- Graficar: **Eje X** = Tiempo de lag (segundos), **Eje Y** = Correlaci√≥n\n",
    "- Identificar el **pico m√°ximo** de correlaci√≥n ‚Üí indica el tiempo √≥ptimo de viaje del viento\n",
    "\n",
    "**Objetivo:** El pico de la curva nos dice cu√°ntos segundos tarda, en promedio, el viento en viajar del LIDAR al rotor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 4.6: AN√ÅLISIS DE RETARDO (LAG) - TIEMPO DE VIAJE DEL VIENTO\n",
    "# ============================================================================\n",
    "\n",
    "# Crear subcarpeta para an√°lisis de lags\n",
    "lags_folder = eda_folder / \"03_Lags\"\n",
    "lags_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AN√ÅLISIS DE RETARDO - TIEMPO DE VIAJE DEL VIENTO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {lags_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar dataset completo\n",
    "complete_dataset_path = data_folder_ml / \"0000_Complete_dataset.csv\"\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    print(f\"\\nArchivo: {complete_dataset_path.name}\")\n",
    "    print(\"‚öôÔ∏è  M√©todo: C√°lculo de correlaciones lag por lag\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 1: Identificar columnas con lags y variables objetivo\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[1/4] Identificando variables con lag y targets...\")\n",
    "    \n",
    "    # Leer columnas del dataset\n",
    "    df_sample = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_sample.columns.tolist()\n",
    "    \n",
    "    # Identificar variables VLOS con lag\n",
    "    vlos_lag_columns = [col for col in all_columns \n",
    "                        if 'LAC_VLOS' in col and 'lag' in col]\n",
    "    \n",
    "    # Variables objetivo\n",
    "    target_vars = ['Blade root 1 My', 'Blade root 2 My']\n",
    "    \n",
    "    print(f\"\\nüìä Variables VLOS con lag encontradas: {len(vlos_lag_columns)}\")\n",
    "    print(f\"üìä Variables objetivo: {len(target_vars)}\")\n",
    "    \n",
    "    # Extraer informaci√≥n de lags\n",
    "    # Formato esperado: LAC_VLOS_BEAMX_RANGE5_lag6s, lag10s, etc\n",
    "    # Extraer BEAM y segundos de lag\n",
    "    \n",
    "    lag_info = {}\n",
    "    for col in vlos_lag_columns:\n",
    "        # Extraer BEAM number\n",
    "        if 'BEAM' in col:\n",
    "            beam_part = col.split('BEAM')[1].split('_')[0]\n",
    "            beam_num = int(beam_part)\n",
    "            \n",
    "            # Extraer segundos de lag (formato: lag6s, lag10s, etc)\n",
    "            if 'lag' in col:\n",
    "                # Buscar 'lag' seguido de d√≠gitos y 's'\n",
    "                import re\n",
    "                lag_match = re.search(r'lag(\\d+)s', col)\n",
    "                if lag_match:\n",
    "                    lag_seconds = int(lag_match.group(1))\n",
    "                    \n",
    "                    # Almacenar info\n",
    "                    if beam_num not in lag_info:\n",
    "                        lag_info[beam_num] = {}\n",
    "                    \n",
    "                    lag_info[beam_num][lag_seconds] = col\n",
    "    \n",
    "    print(f\"\\nüîç BEAMs identificados: {sorted(lag_info.keys())}\")\n",
    "    \n",
    "    # Obtener lista de tiempos de lag √∫nicos\n",
    "    all_lag_times = set()\n",
    "    for beam_data in lag_info.values():\n",
    "        all_lag_times.update(beam_data.keys())\n",
    "    lag_times_sorted = sorted(all_lag_times)\n",
    "    \n",
    "    print(f\"üîç Tiempos de lag: {lag_times_sorted[0]}s - {lag_times_sorted[-1]}s ({len(lag_times_sorted)} valores)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 2: Cargar datos necesarios\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[2/4] Cargando datos...\")\n",
    "    \n",
    "    # Columnas a cargar: targets + todas las VLOS con lag\n",
    "    columns_to_load = target_vars + vlos_lag_columns\n",
    "    \n",
    "    # Cargar con muestreo para eficiencia\n",
    "    sample_rate = 5\n",
    "    df_lags = pd.read_csv(complete_dataset_path,\n",
    "                          usecols=columns_to_load,\n",
    "                          skiprows=lambda i: i > 0 and i % sample_rate != 0)\n",
    "    \n",
    "    print(f\"‚úÖ Datos cargados: {df_lags.shape[0]:,} filas, {df_lags.shape[1]} columnas\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 3: Calcular correlaciones para cada BEAM\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[3/4] Calculando correlaciones lag por lag...\")\n",
    "    \n",
    "    # Diccionario para almacenar resultados\n",
    "    # Estructura: {beam_num: {target_var: {lag_time: correlation}}}\n",
    "    correlation_results = {}\n",
    "    \n",
    "    for beam_num in sorted(lag_info.keys()):\n",
    "        print(f\"\\n  üì° Procesando BEAM {beam_num}...\")\n",
    "        \n",
    "        correlation_results[beam_num] = {}\n",
    "        \n",
    "        for target_var in target_vars:\n",
    "            correlation_results[beam_num][target_var] = {}\n",
    "            \n",
    "            for lag_seconds, vlos_col in lag_info[beam_num].items():\n",
    "                # Calcular correlaci√≥n de Pearson\n",
    "                corr_value = df_lags[vlos_col].corr(df_lags[target_var])\n",
    "                correlation_results[beam_num][target_var][lag_seconds] = corr_value\n",
    "    \n",
    "    print(f\"\\n‚úÖ Correlaciones calculadas para {len(correlation_results)} BEAMs\")\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del df_lags\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 4: Generar gr√°ficos de correlaci√≥n vs tiempo de lag\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[4/4] Generando gr√°ficos...\")\n",
    "    \n",
    "    # Crear un gr√°fico por cada BEAM (con ambos targets)\n",
    "    for beam_num in sorted(correlation_results.keys()):\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        fig.suptitle(f'Correlaci√≥n Lag vs Tiempo - BEAM {beam_num}', \n",
    "                     fontsize=16, fontweight='bold', y=0.995)\n",
    "        \n",
    "        for idx, target_var in enumerate(target_vars):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Extraer datos para este target\n",
    "            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n",
    "            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n",
    "            \n",
    "            # Graficar curva de correlaci√≥n\n",
    "            ax.plot(lag_times, correlations, 'o-', linewidth=2, markersize=6,\n",
    "                   color='steelblue', label='Correlaci√≥n')\n",
    "            \n",
    "            # Encontrar el m√°ximo\n",
    "            max_corr = max(correlations)\n",
    "            max_lag = lag_times[correlations.index(max_corr)]\n",
    "            \n",
    "            # Marcar el m√°ximo con l√≠nea vertical\n",
    "            ax.axvline(x=max_lag, color='red', linestyle='--', linewidth=2,\n",
    "                      label=f'M√°ximo: {max_lag}s (r={max_corr:.3f})')\n",
    "            \n",
    "            # A√±adir anotaci√≥n en el pico\n",
    "            ax.annotate(f'{max_lag}s\\nr={max_corr:.3f}',\n",
    "                       xy=(max_lag, max_corr),\n",
    "                       xytext=(max_lag + 2, max_corr - 0.05),\n",
    "                       fontsize=11, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "            \n",
    "            # Configurar ejes y etiquetas\n",
    "            ax.set_xlabel('Tiempo de Lag (segundos)', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Correlaci√≥n de Pearson', fontsize=12, fontweight='bold')\n",
    "            ax.set_title(f'Target: {target_var}', fontsize=13, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.legend(loc='best', fontsize=10)\n",
    "            \n",
    "            # Ajustar l√≠mites del eje Y para mejor visualizaci√≥n\n",
    "            y_min = min(correlations) - 0.05\n",
    "            y_max = max(correlations) + 0.05\n",
    "            ax.set_ylim([y_min, y_max])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar figura\n",
    "        output_path = lags_folder / f\"lag_correlation_BEAM{beam_num}.png\"\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Imprimir m√°ximos para este BEAM\n",
    "        max_info = []\n",
    "        for target_var in target_vars:\n",
    "            lag_times_temp = sorted(correlation_results[beam_num][target_var].keys())\n",
    "            correlations_temp = [correlation_results[beam_num][target_var][t] for t in lag_times_temp]\n",
    "            max_corr = max(correlations_temp)\n",
    "            max_lag = lag_times_temp[correlations_temp.index(max_corr)]\n",
    "            max_info.append(f\"{target_var}={max_lag}s\")\n",
    "        \n",
    "        print(f\"  ‚úÖ BEAM {beam_num}: M√°ximos en \" + \", \".join(max_info))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 5: Crear gr√°fico resumen con todos los BEAMs\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[5/4] Generando gr√°fico resumen...\")\n",
    "    \n",
    "    # Crear figura con subplots para cada target\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    fig.suptitle('Resumen: Correlaci√≥n Lag vs Tiempo - Todos los BEAMs', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # Paleta de colores para distinguir BEAMs\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(correlation_results)))\n",
    "    \n",
    "    for idx, target_var in enumerate(target_vars):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        max_correlations_summary = []\n",
    "        \n",
    "        for beam_idx, beam_num in enumerate(sorted(correlation_results.keys())):\n",
    "            # Extraer datos\n",
    "            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n",
    "            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n",
    "            \n",
    "            # Graficar curva\n",
    "            ax.plot(lag_times, correlations, 'o-', linewidth=1.5, markersize=4,\n",
    "                   color=colors[beam_idx], label=f'BEAM {beam_num}', alpha=0.8)\n",
    "            \n",
    "            # Encontrar m√°ximo\n",
    "            max_corr = max(correlations)\n",
    "            max_lag = lag_times[correlations.index(max_corr)]\n",
    "            max_correlations_summary.append((beam_num, max_lag, max_corr))\n",
    "            \n",
    "            # Marcar m√°ximo con punto destacado\n",
    "            ax.plot(max_lag, max_corr, 'o', markersize=10, color=colors[beam_idx],\n",
    "                   markeredgecolor='black', markeredgewidth=1.5)\n",
    "        \n",
    "        # Configurar ejes y etiquetas\n",
    "        ax.set_xlabel('Tiempo de Lag (segundos)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Correlaci√≥n de Pearson', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'Target: {target_var}', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(loc='best', fontsize=9, ncol=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar figura resumen\n",
    "    output_path = lags_folder / \"lag_correlation_summary_all_beams.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Gr√°fico resumen generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 6: Guardar resultados en CSV\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[6/4] Guardando resultados en CSV...\")\n",
    "    \n",
    "    # Crear DataFrame con todos los resultados\n",
    "    results_list = []\n",
    "    \n",
    "    for beam_num in sorted(correlation_results.keys()):\n",
    "        for target_var in target_vars:\n",
    "            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n",
    "            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n",
    "            \n",
    "            # Encontrar m√°ximo\n",
    "            max_corr = max(correlations)\n",
    "            max_lag = lag_times[correlations.index(max_corr)]\n",
    "            \n",
    "            for lag_time, corr_value in zip(lag_times, correlations):\n",
    "                results_list.append({\n",
    "                    'BEAM': beam_num,\n",
    "                    'Target': target_var,\n",
    "                    'Lag_Seconds': lag_time,\n",
    "                    'Correlation': corr_value,\n",
    "                    'Is_Maximum': (lag_time == max_lag)\n",
    "                })\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    csv_path = lags_folder / \"lag_correlations_detailed.csv\"\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Resultados detallados guardados en: {csv_path.name}\")\n",
    "    \n",
    "    # Crear CSV resumen con m√°ximos\n",
    "    max_results = []\n",
    "    for beam_num in sorted(correlation_results.keys()):\n",
    "        for target_var in target_vars:\n",
    "            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n",
    "            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n",
    "            \n",
    "            max_corr = max(correlations)\n",
    "            max_lag = lag_times[correlations.index(max_corr)]\n",
    "            \n",
    "            max_results.append({\n",
    "                'BEAM': beam_num,\n",
    "                'Target': target_var,\n",
    "                'Optimal_Lag_Seconds': max_lag,\n",
    "                'Max_Correlation': max_corr\n",
    "            })\n",
    "    \n",
    "    df_max_results = pd.DataFrame(max_results)\n",
    "    csv_max_path = lags_folder / \"optimal_lag_summary.csv\"\n",
    "    df_max_results.to_csv(csv_max_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Resumen de m√°ximos guardado en: {csv_max_path.name}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RESUMEN FINAL\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESUMEN - AN√ÅLISIS DE RETARDO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"BEAMs analizados:          {len(correlation_results)}\")\n",
    "    print(f\"Targets analizados:        {len(target_vars)}\")\n",
    "    print(f\"Rango de lags:             {lag_times_sorted[0]}s - {lag_times_sorted[-1]}s\")\n",
    "    print(f\"\\nüéØ TIEMPOS √ìPTIMOS DE VIAJE DEL VIENTO:\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for target_var in target_vars:\n",
    "        print(f\"\\n  Target: {target_var}\")\n",
    "        for beam_num in sorted(correlation_results.keys()):\n",
    "            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n",
    "            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n",
    "            max_corr = max(correlations)\n",
    "            max_lag = lag_times[correlations.index(max_corr)]\n",
    "            print(f\"    BEAM {beam_num}: {max_lag}s (r={max_corr:.3f})\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Archivos generados:\")\n",
    "    print(f\"  - Gr√°ficos individuales: lag_correlation_BEAMX.png ({len(correlation_results)} archivos)\")\n",
    "    print(f\"  - Gr√°fico resumen: lag_correlation_summary_all_beams.png\")\n",
    "    print(f\"  - CSV detallado: lag_correlations_detailed.csv\")\n",
    "    print(f\"  - CSV resumen: optimal_lag_summary.csv\")\n",
    "    print(f\"\\nüìç Ubicaci√≥n: {lags_folder}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚úÖ An√°lisis completado\")\n",
    "    print(\"   El pico de correlaci√≥n indica el tiempo de viaje del viento del LIDAR al rotor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40bee1",
   "metadata": {},
   "source": [
    "### üå¨Ô∏è PASO 4.6: An√°lisis de Retardo por Categor√≠a de Viento\n",
    "\n",
    "Analizaremos c√≥mo **el tiempo de viaje del viento var√≠a seg√∫n la velocidad del viento**, demostrando la necesidad de modelos no lineales.\n",
    "\n",
    "**Metodolog√≠a:**\n",
    "1. Calcular **wind_mean**: Promedio de todas las variables `LAC_VLOS` sin lag (velocidad instant√°nea del viento)\n",
    "2. Categorizar en:\n",
    "   - **Viento Bajo**: 0-9 m/s\n",
    "   - **Viento Medio**: 9-18 m/s  \n",
    "   - **Viento Alto**: 18-30 m/s\n",
    "3. Repetir an√°lisis de correlaci√≥n lag vs tiempo para cada categor√≠a\n",
    "4. Superponer perfiles en misma figura con colores distintos\n",
    "\n",
    "**Hip√≥tesis a demostrar:**\n",
    "- üî¥ **Viento Alto** ‚Üí Pico de correlaci√≥n temprano (izquierda) ‚Üí Delay corto\n",
    "- üü° **Viento Medio** ‚Üí Pico intermedio\n",
    "- üîµ **Viento Bajo** ‚Üí Pico de correlaci√≥n tard√≠o (derecha) ‚Üí Delay largo\n",
    "\n",
    "**Justificaci√≥n:** Los picos se desplazan seg√∫n la velocidad del viento, confirmando que la relaci√≥n es **no lineal** y variable con condiciones operativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c155bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 4.7: AN√ÅLISIS DE RETARDO POR CATEGOR√çA DE VIENTO\n",
    "# ============================================================================\n",
    "\n",
    "# Crear subcarpeta para an√°lisis por velocidad de viento\n",
    "wind_lags_folder = eda_folder / \"04_Lag_per_Wind\"\n",
    "wind_lags_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AN√ÅLISIS DE RETARDO POR CATEGOR√çA DE VIENTO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {wind_lags_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar dataset completo\n",
    "complete_dataset_path = data_folder_ml / \"0000_Complete_dataset.csv\"\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    print(f\"\\nArchivo: {complete_dataset_path.name}\")\n",
    "    print(\"‚öôÔ∏è  M√©todo: Estratificaci√≥n por velocidad de viento\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 1: Identificar columnas VLOS y lag\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[1/6] Identificando variables VLOS...\")\n",
    "    \n",
    "    # Leer columnas del dataset\n",
    "    df_sample = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_sample.columns.tolist()\n",
    "    \n",
    "    # VLOS sin lag (para calcular wind_mean)\n",
    "    vlos_base_columns = [col for col in all_columns \n",
    "                         if 'LAC_VLOS' in col and 'lag' not in col]\n",
    "    \n",
    "    # VLOS con lag\n",
    "    vlos_lag_columns = [col for col in all_columns \n",
    "                        if 'LAC_VLOS' in col and 'lag' in col]\n",
    "    \n",
    "    # Variables objetivo\n",
    "    target_vars = ['Blade root 1 My', 'Blade root 2 My']\n",
    "    \n",
    "    print(f\"\\nüìä Variables VLOS base (sin lag): {len(vlos_base_columns)}\")\n",
    "    print(f\"üìä Variables VLOS con lag: {len(vlos_lag_columns)}\")\n",
    "    print(f\"üìä Variables objetivo: {len(target_vars)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 2: Extraer informaci√≥n de lags\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[2/6] Extrayendo informaci√≥n de lags...\")\n",
    "    \n",
    "    lag_info = {}\n",
    "    import re\n",
    "    \n",
    "    for col in vlos_lag_columns:\n",
    "        # Extraer BEAM number\n",
    "        if 'BEAM' in col:\n",
    "            beam_part = col.split('BEAM')[1].split('_')[0]\n",
    "            beam_num = int(beam_part)\n",
    "            \n",
    "            # Extraer segundos de lag (formato: lag6s, lag10s, etc)\n",
    "            lag_match = re.search(r'lag(\\d+)s', col)\n",
    "            if lag_match:\n",
    "                lag_seconds = int(lag_match.group(1))\n",
    "                \n",
    "                # Almacenar info\n",
    "                if beam_num not in lag_info:\n",
    "                    lag_info[beam_num] = {}\n",
    "                \n",
    "                lag_info[beam_num][lag_seconds] = col\n",
    "    \n",
    "    print(f\"üîç BEAMs identificados: {sorted(lag_info.keys())}\")\n",
    "    \n",
    "    # Obtener lista de tiempos de lag √∫nicos\n",
    "    all_lag_times = set()\n",
    "    for beam_data in lag_info.values():\n",
    "        all_lag_times.update(beam_data.keys())\n",
    "    lag_times_sorted = sorted(all_lag_times)\n",
    "    \n",
    "    print(f\"üîç Tiempos de lag: {lag_times_sorted[0]}s - {lag_times_sorted[-1]}s ({len(lag_times_sorted)} valores)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 3: Cargar datos y calcular wind_mean\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[3/6] Cargando datos y calculando velocidad media del viento...\")\n",
    "    \n",
    "    # Columnas necesarias: VLOS base + VLOS lag + targets\n",
    "    columns_to_load = vlos_base_columns + vlos_lag_columns + target_vars\n",
    "    \n",
    "    # Cargar con muestreo\n",
    "    sample_rate = 5\n",
    "    df_wind = pd.read_csv(complete_dataset_path,\n",
    "                          usecols=columns_to_load,\n",
    "                          skiprows=lambda i: i > 0 and i % sample_rate != 0)\n",
    "    \n",
    "    print(f\"‚úÖ Datos cargados: {df_wind.shape[0]:,} filas, {df_wind.shape[1]} columnas\")\n",
    "    \n",
    "    # Calcular wind_mean: promedio de variables VLOS sin lag\n",
    "    print(\"\\n‚öôÔ∏è  Calculando wind_mean (promedio de VLOS sin lag)...\")\n",
    "    df_wind['wind_mean'] = df_wind[vlos_base_columns].mean(axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ wind_mean calculado\")\n",
    "    print(f\"   Rango: {df_wind['wind_mean'].min():.2f} - {df_wind['wind_mean'].max():.2f} m/s\")\n",
    "    print(f\"   Media: {df_wind['wind_mean'].mean():.2f} m/s\")\n",
    "    print(f\"   Mediana: {df_wind['wind_mean'].median():.2f} m/s\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 4: Categorizar por velocidad de viento\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[4/6] Categorizando por velocidad de viento...\")\n",
    "    \n",
    "    # Definir categor√≠as\n",
    "    def categorize_wind(wind_speed):\n",
    "        if wind_speed < 9:\n",
    "            return 'Bajo (0-9 m/s)'\n",
    "        elif wind_speed < 18:\n",
    "            return 'Medio (9-18 m/s)'\n",
    "        else:\n",
    "            return 'Alto (18-30 m/s)'\n",
    "    \n",
    "    df_wind['wind_category'] = df_wind['wind_mean'].apply(categorize_wind)\n",
    "    \n",
    "    # Contar muestras por categor√≠a\n",
    "    category_counts = df_wind['wind_category'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nüìä Distribuci√≥n de categor√≠as:\")\n",
    "    for category, count in category_counts.items():\n",
    "        percentage = (count / len(df_wind)) * 100\n",
    "        print(f\"   {category}: {count:,} muestras ({percentage:.1f}%)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 5: Calcular correlaciones por categor√≠a\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[5/6] Calculando correlaciones por categor√≠a de viento...\")\n",
    "    \n",
    "    # Estructura: {category: {beam_num: {target_var: {lag_time: correlation}}}}\n",
    "    correlation_by_wind = {}\n",
    "    \n",
    "    for category in ['Bajo (0-9 m/s)', 'Medio (9-18 m/s)', 'Alto (18-30 m/s)']:\n",
    "        print(f\"\\n  üå¨Ô∏è  Procesando: {category}...\")\n",
    "        \n",
    "        # Filtrar datos por categor√≠a\n",
    "        df_category = df_wind[df_wind['wind_category'] == category]\n",
    "        \n",
    "        if len(df_category) == 0:\n",
    "            print(f\"     ‚ö†Ô∏è  No hay datos para esta categor√≠a\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"     Muestras: {len(df_category):,}\")\n",
    "        \n",
    "        correlation_by_wind[category] = {}\n",
    "        \n",
    "        for beam_num in sorted(lag_info.keys()):\n",
    "            correlation_by_wind[category][beam_num] = {}\n",
    "            \n",
    "            for target_var in target_vars:\n",
    "                correlation_by_wind[category][beam_num][target_var] = {}\n",
    "                \n",
    "                for lag_seconds, vlos_col in lag_info[beam_num].items():\n",
    "                    # Calcular correlaci√≥n\n",
    "                    corr_value = df_category[vlos_col].corr(df_category[target_var])\n",
    "                    correlation_by_wind[category][beam_num][target_var][lag_seconds] = corr_value\n",
    "    \n",
    "    print(f\"\\n‚úÖ Correlaciones calculadas para {len(correlation_by_wind)} categor√≠as\")\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del df_wind\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 6: Generar gr√°ficos comparativos por categor√≠a de viento\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[6/6] Generando gr√°ficos comparativos...\")\n",
    "    \n",
    "    # Colores para cada categor√≠a\n",
    "    category_colors = {\n",
    "        'Bajo (0-9 m/s)': '#3498db',      # Azul\n",
    "        'Medio (9-18 m/s)': '#f39c12',    # Naranja\n",
    "        'Alto (18-30 m/s)': '#e74c3c'     # Rojo\n",
    "    }\n",
    "    \n",
    "    category_labels = {\n",
    "        'Bajo (0-9 m/s)': 'Viento Bajo (0-9 m/s)',\n",
    "        'Medio (9-18 m/s)': 'Viento Medio (9-18 m/s)',\n",
    "        'Alto (18-30 m/s)': 'Viento Alto (18-30 m/s)'\n",
    "    }\n",
    "    \n",
    "    # Crear un gr√°fico por cada BEAM (con ambos targets)\n",
    "    for beam_num in sorted(lag_info.keys()):\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        fig.suptitle(f'Perfil de Correlaci√≥n Temporal por Velocidad de Viento - BEAM {beam_num}', \n",
    "                     fontsize=16, fontweight='bold', y=0.995)\n",
    "        \n",
    "        for idx, target_var in enumerate(target_vars):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Graficar cada categor√≠a de viento\n",
    "            for category in ['Bajo (0-9 m/s)', 'Medio (9-18 m/s)', 'Alto (18-30 m/s)']:\n",
    "                \n",
    "                if category not in correlation_by_wind:\n",
    "                    continue\n",
    "                \n",
    "                # Extraer datos para esta categor√≠a y target\n",
    "                lag_times = sorted(correlation_by_wind[category][beam_num][target_var].keys())\n",
    "                correlations = [correlation_by_wind[category][beam_num][target_var][t] \n",
    "                              for t in lag_times]\n",
    "                \n",
    "                # Graficar curva\n",
    "                ax.plot(lag_times, correlations, 'o-', \n",
    "                       linewidth=2.5, markersize=7,\n",
    "                       color=category_colors[category], \n",
    "                       label=category_labels[category],\n",
    "                       alpha=0.85)\n",
    "                \n",
    "                # Encontrar y marcar el m√°ximo\n",
    "                max_corr = max(correlations)\n",
    "                max_lag = lag_times[correlations.index(max_corr)]\n",
    "                \n",
    "                # L√≠nea vertical en el pico\n",
    "                ax.axvline(x=max_lag, color=category_colors[category], \n",
    "                          linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "                \n",
    "                # Punto destacado en el m√°ximo\n",
    "                ax.plot(max_lag, max_corr, 'o', markersize=12, \n",
    "                       color=category_colors[category],\n",
    "                       markeredgecolor='black', markeredgewidth=2)\n",
    "                \n",
    "                # Anotaci√≥n con valor del pico\n",
    "                y_offset = 0.02 if category == 'Bajo (0-9 m/s)' else (-0.02 if category == 'Alto (18-30 m/s)' else 0)\n",
    "                ax.annotate(f'{max_lag}s',\n",
    "                           xy=(max_lag, max_corr),\n",
    "                           xytext=(max_lag, max_corr + y_offset),\n",
    "                           fontsize=9, fontweight='bold',\n",
    "                           color=category_colors[category],\n",
    "                           ha='center',\n",
    "                           bbox=dict(boxstyle='round,pad=0.3', \n",
    "                                   facecolor='white', \n",
    "                                   edgecolor=category_colors[category],\n",
    "                                   alpha=0.8))\n",
    "            \n",
    "            # Configurar ejes y etiquetas\n",
    "            ax.set_xlabel('Tiempo de Lag (segundos)', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Correlaci√≥n de Pearson', fontsize=12, fontweight='bold')\n",
    "            ax.set_title(f'Target: {target_var}', fontsize=13, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.legend(loc='best', fontsize=11, framealpha=0.95)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar figura\n",
    "        output_path = wind_lags_folder / f\"lag_correlation_by_wind_BEAM{beam_num}.png\"\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"  ‚úÖ BEAM {beam_num}: Gr√°fico comparativo generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 7: Crear gr√°fico resumen multi-BEAM por categor√≠a\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[7/6] Generando gr√°fico resumen multi-BEAM...\")\n",
    "    \n",
    "    # Crear figura con 3 subplots (uno por categor√≠a)\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 16))\n",
    "    fig.suptitle('Resumen: Perfil de Correlaci√≥n por Velocidad de Viento - Todos los BEAMs', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # Usar solo el primer target para simplificar\n",
    "    target_var = target_vars[0]\n",
    "    \n",
    "    # Colores para BEAMs\n",
    "    beam_colors = plt.cm.tab10(np.linspace(0, 1, len(lag_info)))\n",
    "    \n",
    "    for idx, category in enumerate(['Bajo (0-9 m/s)', 'Medio (9-18 m/s)', 'Alto (18-30 m/s)']):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        if category not in correlation_by_wind:\n",
    "            continue\n",
    "        \n",
    "        for beam_idx, beam_num in enumerate(sorted(lag_info.keys())):\n",
    "            # Extraer datos\n",
    "            lag_times = sorted(correlation_by_wind[category][beam_num][target_var].keys())\n",
    "            correlations = [correlation_by_wind[category][beam_num][target_var][t] \n",
    "                          for t in lag_times]\n",
    "            \n",
    "            # Graficar curva\n",
    "            ax.plot(lag_times, correlations, 'o-', \n",
    "                   linewidth=1.5, markersize=4,\n",
    "                   color=beam_colors[beam_idx], \n",
    "                   label=f'BEAM {beam_num}',\n",
    "                   alpha=0.8)\n",
    "            \n",
    "            # Marcar m√°ximo\n",
    "            max_corr = max(correlations)\n",
    "            max_lag = lag_times[correlations.index(max_corr)]\n",
    "            ax.plot(max_lag, max_corr, 'o', markersize=10, \n",
    "                   color=beam_colors[beam_idx],\n",
    "                   markeredgecolor='black', markeredgewidth=1.5)\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo de Lag (segundos)', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Correlaci√≥n de Pearson', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{category_labels[category]} - Target: {target_var}', \n",
    "                    fontsize=12, fontweight='bold',\n",
    "                    color=category_colors[category])\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(loc='best', fontsize=9, ncol=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    output_path = wind_lags_folder / \"lag_correlation_by_wind_summary.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Gr√°fico resumen generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 8: Guardar resultados en CSV\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[8/6] Guardando resultados en CSV...\")\n",
    "    \n",
    "    # CSV con picos √≥ptimos por categor√≠a\n",
    "    optimal_results = []\n",
    "    \n",
    "    for category in correlation_by_wind.keys():\n",
    "        for beam_num in sorted(lag_info.keys()):\n",
    "            for target_var in target_vars:\n",
    "                lag_times = sorted(correlation_by_wind[category][beam_num][target_var].keys())\n",
    "                correlations = [correlation_by_wind[category][beam_num][target_var][t] \n",
    "                              for t in lag_times]\n",
    "                \n",
    "                max_corr = max(correlations)\n",
    "                max_lag = lag_times[correlations.index(max_corr)]\n",
    "                \n",
    "                optimal_results.append({\n",
    "                    'Wind_Category': category,\n",
    "                    'BEAM': beam_num,\n",
    "                    'Target': target_var,\n",
    "                    'Optimal_Lag_Seconds': max_lag,\n",
    "                    'Max_Correlation': max_corr\n",
    "                })\n",
    "    \n",
    "    df_optimal = pd.DataFrame(optimal_results)\n",
    "    csv_path = wind_lags_folder / \"optimal_lag_by_wind_category.csv\"\n",
    "    df_optimal.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ CSV guardado: {csv_path.name}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RESUMEN FINAL\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESUMEN - AN√ÅLISIS POR CATEGOR√çA DE VIENTO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Categor√≠as analizadas:     {len(correlation_by_wind)}\")\n",
    "    print(f\"BEAMs analizados:          {len(lag_info)}\")\n",
    "    print(f\"Targets analizados:        {len(target_vars)}\")\n",
    "    \n",
    "    print(f\"\\nüéØ DESPLAZAMIENTO DE PICOS (DEMOSTRACI√ìN DE NO LINEALIDAD):\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for target_var in target_vars:\n",
    "        print(f\"\\n  Target: {target_var}\")\n",
    "        \n",
    "        for beam_num in sorted(lag_info.keys()):\n",
    "            print(f\"    BEAM {beam_num}:\")\n",
    "            \n",
    "            for category in ['Bajo (0-9 m/s)', 'Medio (9-18 m/s)', 'Alto (18-30 m/s)']:\n",
    "                if category in correlation_by_wind:\n",
    "                    lag_times = sorted(correlation_by_wind[category][beam_num][target_var].keys())\n",
    "                    correlations = [correlation_by_wind[category][beam_num][target_var][t] \n",
    "                                  for t in lag_times]\n",
    "                    max_corr = max(correlations)\n",
    "                    max_lag = lag_times[correlations.index(max_corr)]\n",
    "                    \n",
    "                    icon = 'üîµ' if 'Bajo' in category else ('üü°' if 'Medio' in category else 'üî¥')\n",
    "                    print(f\"      {icon} {category}: {max_lag}s (r={max_corr:.3f})\")\n",
    "    \n",
    "    print(f\"\\nüí° CONCLUSI√ìN:\")\n",
    "    print(\"   El pico de correlaci√≥n se DESPLAZA seg√∫n la velocidad del viento:\")\n",
    "    print(\"   - Viento Alto ‚Üí Pico temprano (delay corto)\")\n",
    "    print(\"   - Viento Bajo ‚Üí Pico tard√≠o (delay largo)\")\n",
    "    print(\"   Esto confirma la necesidad de MODELOS NO LINEALES que capturen\")\n",
    "    print(\"   esta variabilidad en el tiempo de viaje del viento.\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Archivos generados:\")\n",
    "    print(f\"  - Gr√°ficos comparativos: lag_correlation_by_wind_BEAMX.png\")\n",
    "    print(f\"  - Gr√°fico resumen: lag_correlation_by_wind_summary.png\")\n",
    "    print(f\"  - CSV: optimal_lag_by_wind_category.csv\")\n",
    "    print(f\"\\nüìç Ubicaci√≥n: {wind_lags_folder}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚úÖ An√°lisis completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc79f4",
   "metadata": {},
   "source": [
    "### üå¨Ô∏è PASO 4.8: An√°lisis Granular de Retardo - Bins de 1 m/s\n",
    "\n",
    "An√°lisis de alta resoluci√≥n del **desplazamiento del pico de correlaci√≥n** con bins de velocidad de viento de **1 m/s**, demostrando la naturaleza continua y no lineal de la relaci√≥n.\n",
    "\n",
    "**Metodolog√≠a:**\n",
    "1. Crear bins de 1 m/s: [0-1), [1-2), [2-3), ... [29-30) m/s\n",
    "2. Calcular correlaciones lag vs tiempo para cada bin\n",
    "3. Identificar el pico √≥ptimo en cada bin\n",
    "4. Graficar **Lag √ìptimo vs Velocidad de Viento**\n",
    "\n",
    "**Objetivo:** Demostrar que el lag √≥ptimo disminuye continuamente con la velocidad del viento, confirmando la necesidad de modelos adaptativos no lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 4.8: AN√ÅLISIS GRANULAR DE RETARDO - BINS DE 1 M/S\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AN√ÅLISIS GRANULAR - BINS DE 1 M/S\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {wind_lags_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Reutilizar la misma carpeta 04_Lag_per_Wind\n",
    "# Las variables ya est√°n cargadas del paso anterior\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    print(f\"\\nArchivo: {complete_dataset_path.name}\")\n",
    "    print(\"‚öôÔ∏è  M√©todo: Bins de velocidad de 1 m/s\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 1: Cargar datos con wind_mean\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[1/5] Cargando datos...\")\n",
    "    \n",
    "    # Cargar con muestreo\n",
    "    columns_to_load = vlos_base_columns + vlos_lag_columns + target_vars\n",
    "    sample_rate = 5\n",
    "    \n",
    "    df_wind_bins = pd.read_csv(complete_dataset_path,\n",
    "                               usecols=columns_to_load,\n",
    "                               skiprows=lambda i: i > 0 and i % sample_rate != 0)\n",
    "    \n",
    "    print(f\"‚úÖ Datos cargados: {df_wind_bins.shape[0]:,} filas\")\n",
    "    \n",
    "    # Calcular wind_mean\n",
    "    print(\"‚öôÔ∏è  Calculando wind_mean...\")\n",
    "    df_wind_bins['wind_mean'] = df_wind_bins[vlos_base_columns].mean(axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ wind_mean: {df_wind_bins['wind_mean'].min():.2f} - {df_wind_bins['wind_mean'].max():.2f} m/s\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 2: Crear bins de 1 m/s\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n[2/5] Creando bins de 1 m/s...\")\n",
    "    \n",
    "    # Definir bins de 1 m/s desde 0 hasta 30\n",
    "    wind_bins = list(range(0, 31))  # [0, 1, 2, ..., 30]\n",
    "    wind_bin_labels = [f\"{i}-{i+1} m/s\" for i in range(30)]\n",
    "    \n",
    "    # Asignar bin a cada fila\n",
    "    df_wind_bins['wind_bin'] = pd.cut(df_wind_bins['wind_mean'], \n",
    "                                       bins=wind_bins, \n",
    "                                       labels=wind_bin_labels,\n",
    "                                       include_lowest=True)\n",
    "    \n",
    "    # Contar muestras por bin\n",
    "    bin_counts = df_wind_bins['wind_bin'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"‚úÖ Bins creados: {len(bin_counts)} bins con datos\")\n",
    "    print(f\"\\nBins con m√°s de 100 muestras:\")\n",
    "    valid_bins = [bin_name for bin_name, count in bin_counts.items() if count >= 100]\n",
    "    for bin_name in valid_bins[:10]:\n",
    "        print(f\"   {bin_name}: {bin_counts[bin_name]:,} muestras\")\n",
    "    if len(valid_bins) > 10:\n",
    "        print(f\"   ... y {len(valid_bins) - 10} bins m√°s\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 3: Calcular correlaciones por bin\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[3/5] Calculando correlaciones por bin (solo bins con ‚â•100 muestras)...\")\n",
    "    \n",
    "    # Estructura: {bin_name: {beam_num: {target_var: {lag_time: correlation}}}}\n",
    "    correlation_by_bin = {}\n",
    "    \n",
    "    bins_processed = 0\n",
    "    bins_skipped = 0\n",
    "    \n",
    "    for bin_name in wind_bin_labels:\n",
    "        # Filtrar datos por bin\n",
    "        df_bin = df_wind_bins[df_wind_bins['wind_bin'] == bin_name]\n",
    "        \n",
    "        # Solo procesar si hay suficientes muestras\n",
    "        if len(df_bin) < 100:\n",
    "            bins_skipped += 1\n",
    "            continue\n",
    "        \n",
    "        bins_processed += 1\n",
    "        correlation_by_bin[bin_name] = {}\n",
    "        \n",
    "        for beam_num in sorted(lag_info.keys()):\n",
    "            correlation_by_bin[bin_name][beam_num] = {}\n",
    "            \n",
    "            for target_var in target_vars:\n",
    "                correlation_by_bin[bin_name][beam_num][target_var] = {}\n",
    "                \n",
    "                for lag_seconds, vlos_col in lag_info[beam_num].items():\n",
    "                    # Calcular correlaci√≥n\n",
    "                    corr_value = df_bin[vlos_col].corr(df_bin[target_var])\n",
    "                    correlation_by_bin[bin_name][beam_num][target_var][lag_seconds] = corr_value\n",
    "        \n",
    "        if bins_processed % 5 == 0:\n",
    "            print(f\"   Procesados: {bins_processed} bins...\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Bins procesados: {bins_processed}\")\n",
    "    print(f\"   Bins omitidos (< 100 muestras): {bins_skipped}\")\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del df_wind_bins\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 4: Extraer lag √≥ptimo por bin\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[4/5] Extrayendo lag √≥ptimo por velocidad de viento...\")\n",
    "    \n",
    "    # Estructura para almacenar resultados\n",
    "    optimal_lag_per_wind = []\n",
    "    \n",
    "    for bin_name in sorted(correlation_by_bin.keys(), \n",
    "                          key=lambda x: int(x.split('-')[0])):\n",
    "        \n",
    "        # Extraer velocidad media del bin (punto medio)\n",
    "        wind_min = int(bin_name.split('-')[0])\n",
    "        wind_mid = wind_min + 0.5  # Punto medio del bin\n",
    "        \n",
    "        for beam_num in sorted(lag_info.keys()):\n",
    "            for target_var in target_vars:\n",
    "                # Obtener correlaciones\n",
    "                lag_times = sorted(correlation_by_bin[bin_name][beam_num][target_var].keys())\n",
    "                correlations = [correlation_by_bin[bin_name][beam_num][target_var][t] \n",
    "                              for t in lag_times]\n",
    "                \n",
    "                # Encontrar m√°ximo\n",
    "                max_corr = max(correlations)\n",
    "                max_lag = lag_times[correlations.index(max_corr)]\n",
    "                \n",
    "                optimal_lag_per_wind.append({\n",
    "                    'Wind_Bin': bin_name,\n",
    "                    'Wind_Speed_Mid': wind_mid,\n",
    "                    'BEAM': beam_num,\n",
    "                    'Target': target_var,\n",
    "                    'Optimal_Lag_Seconds': max_lag,\n",
    "                    'Max_Correlation': max_corr\n",
    "                })\n",
    "    \n",
    "    df_optimal_wind = pd.DataFrame(optimal_lag_per_wind)\n",
    "    \n",
    "    print(f\"‚úÖ Lag √≥ptimo extra√≠do para {len(df_optimal_wind)} combinaciones\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 5: Generar gr√°ficos de Lag √ìptimo vs Velocidad de Viento\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[5/5] Generando gr√°ficos...\")\n",
    "    \n",
    "    # Gr√°fico 1: Lag √ìptimo vs Velocidad - Un gr√°fico por BEAM\n",
    "    for beam_num in sorted(lag_info.keys()):\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        fig.suptitle(f'Lag √ìptimo vs Velocidad de Viento - BEAM {beam_num}', \n",
    "                     fontsize=16, fontweight='bold', y=0.995)\n",
    "        \n",
    "        for idx, target_var in enumerate(target_vars):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Filtrar datos para este BEAM y target\n",
    "            df_plot = df_optimal_wind[\n",
    "                (df_optimal_wind['BEAM'] == beam_num) & \n",
    "                (df_optimal_wind['Target'] == target_var)\n",
    "            ].sort_values('Wind_Speed_Mid')\n",
    "            \n",
    "            # Graficar con l√≠nea y puntos\n",
    "            ax.plot(df_plot['Wind_Speed_Mid'], \n",
    "                   df_plot['Optimal_Lag_Seconds'],\n",
    "                   'o-', linewidth=2.5, markersize=7,\n",
    "                   color='steelblue', label='Lag √ìptimo')\n",
    "            \n",
    "            # Agregar l√≠nea de tendencia (regresi√≥n polin√≥mica grado 2)\n",
    "            if len(df_plot) >= 3:\n",
    "                from numpy.polynomial import polynomial as P\n",
    "                x = df_plot['Wind_Speed_Mid'].values\n",
    "                y = df_plot['Optimal_Lag_Seconds'].values\n",
    "                \n",
    "                # Ajustar polinomio de grado 2\n",
    "                coefs = np.polyfit(x, y, 2)\n",
    "                poly = np.poly1d(coefs)\n",
    "                x_smooth = np.linspace(x.min(), x.max(), 100)\n",
    "                y_smooth = poly(x_smooth)\n",
    "                \n",
    "                ax.plot(x_smooth, y_smooth, '--', \n",
    "                       linewidth=2, color='red', alpha=0.7,\n",
    "                       label='Tendencia (polin√≥mica)')\n",
    "            \n",
    "            # Configurar ejes\n",
    "            ax.set_xlabel('Velocidad del Viento (m/s)', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Lag √ìptimo (segundos)', fontsize=12, fontweight='bold')\n",
    "            ax.set_title(f'Target: {target_var}', fontsize=13, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.legend(loc='best', fontsize=11)\n",
    "            \n",
    "            # Invertir eje Y para mostrar que lag disminuye con velocidad\n",
    "            ax.invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar\n",
    "        output_path = wind_lags_folder / f\"optimal_lag_vs_wind_speed_BEAM{beam_num}.png\"\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"  ‚úÖ BEAM {beam_num}: Gr√°fico generado\")\n",
    "    \n",
    "    # Gr√°fico 2: Resumen multi-BEAM\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    fig.suptitle('Resumen: Lag √ìptimo vs Velocidad de Viento - Todos los BEAMs', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # Colores para BEAMs\n",
    "    beam_colors_dict = {beam: plt.cm.tab10(i) \n",
    "                       for i, beam in enumerate(sorted(lag_info.keys()))}\n",
    "    \n",
    "    for idx, target_var in enumerate(target_vars):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for beam_num in sorted(lag_info.keys()):\n",
    "            # Filtrar datos\n",
    "            df_plot = df_optimal_wind[\n",
    "                (df_optimal_wind['BEAM'] == beam_num) & \n",
    "                (df_optimal_wind['Target'] == target_var)\n",
    "            ].sort_values('Wind_Speed_Mid')\n",
    "            \n",
    "            # Graficar\n",
    "            ax.plot(df_plot['Wind_Speed_Mid'], \n",
    "                   df_plot['Optimal_Lag_Seconds'],\n",
    "                   'o-', linewidth=2, markersize=5,\n",
    "                   color=beam_colors_dict[beam_num],\n",
    "                   label=f'BEAM {beam_num}',\n",
    "                   alpha=0.8)\n",
    "        \n",
    "        # Configurar\n",
    "        ax.set_xlabel('Velocidad del Viento (m/s)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Lag √ìptimo (segundos)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'Target: {target_var}', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(loc='best', fontsize=10, ncol=2)\n",
    "        ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    output_path = wind_lags_folder / \"optimal_lag_vs_wind_speed_summary.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  ‚úÖ Resumen multi-BEAM generado\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 6: Guardar CSV detallado\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[6/5] Guardando resultados en CSV...\")\n",
    "    \n",
    "    csv_path = wind_lags_folder / \"optimal_lag_per_wind_speed_1ms_bins.csv\"\n",
    "    df_optimal_wind.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ CSV guardado: {csv_path.name}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 7: An√°lisis de tendencia\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[7/5] An√°lisis de tendencia...\")\n",
    "    \n",
    "    # Calcular tendencia promedio (todos los BEAMs, primer target)\n",
    "    target_var = target_vars[0]\n",
    "    \n",
    "    # Agrupar por velocidad de viento (promedio de todos los BEAMs)\n",
    "    df_trend = df_optimal_wind[df_optimal_wind['Target'] == target_var].groupby('Wind_Speed_Mid').agg({\n",
    "        'Optimal_Lag_Seconds': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    if len(df_trend) >= 2:\n",
    "        # Calcular pendiente\n",
    "        wind_speeds = df_trend['Wind_Speed_Mid'].values\n",
    "        lag_values = df_trend['Optimal_Lag_Seconds'].values\n",
    "        \n",
    "        # Regresi√≥n lineal simple\n",
    "        slope, intercept = np.polyfit(wind_speeds, lag_values, 1)\n",
    "        \n",
    "        print(f\"\\nüìä TENDENCIA LINEAL (promedio de todos los BEAMs):\")\n",
    "        print(f\"   Target: {target_var}\")\n",
    "        print(f\"   Pendiente: {slope:.3f} segundos/(m/s)\")\n",
    "        print(f\"   Intercepto: {intercept:.2f} segundos\")\n",
    "        print(f\"\\n   Interpretaci√≥n:\")\n",
    "        if slope < 0:\n",
    "            print(f\"   Por cada 1 m/s de aumento en velocidad del viento,\")\n",
    "            print(f\"   el lag √≥ptimo DISMINUYE en {abs(slope):.3f} segundos\")\n",
    "        else:\n",
    "            print(f\"   Por cada 1 m/s de aumento en velocidad del viento,\")\n",
    "            print(f\"   el lag √≥ptimo AUMENTA en {slope:.3f} segundos\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RESUMEN FINAL\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESUMEN - AN√ÅLISIS GRANULAR (BINS 1 M/S)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Bins procesados:           {bins_processed}\")\n",
    "    print(f\"Bins omitidos:             {bins_skipped}\")\n",
    "    print(f\"BEAMs analizados:          {len(lag_info)}\")\n",
    "    print(f\"Targets analizados:        {len(target_vars)}\")\n",
    "    \n",
    "    print(f\"\\nüí° CONCLUSI√ìN:\")\n",
    "    print(\"   El lag √≥ptimo presenta una relaci√≥n CONTINUA y NO LINEAL\")\n",
    "    print(\"   con la velocidad del viento:\")\n",
    "    print(\"   - A mayor velocidad ‚Üí Lag √≥ptimo menor (viento llega antes)\")\n",
    "    print(\"   - A menor velocidad ‚Üí Lag √≥ptimo mayor (viento llega tarde)\")\n",
    "    print(\"\\n   Esta variabilidad continua confirma que:\")\n",
    "    print(\"   1. Los modelos deben ser ADAPTATIVOS a la velocidad del viento\")\n",
    "    print(\"   2. Modelos lineales con lag fijo son SUB√ìPTIMOS\")\n",
    "    print(\"   3. Se requieren modelos NO LINEALES (ej: Neural Networks, XGBoost)\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Archivos generados:\")\n",
    "    print(f\"  - Gr√°ficos por BEAM: optimal_lag_vs_wind_speed_BEAMX.png\")\n",
    "    print(f\"  - Resumen multi-BEAM: optimal_lag_vs_wind_speed_summary.png\")\n",
    "    print(f\"  - CSV detallado: optimal_lag_per_wind_speed_1ms_bins.csv\")\n",
    "    print(f\"\\nüìç Ubicaci√≥n: {wind_lags_folder}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚úÖ An√°lisis completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a57bcd",
   "metadata": {},
   "source": [
    "### üå¨Ô∏è PASO 4.9: Lag √ìptimo por Viento (M√©todo Alineado con el Lag)\n",
    "\n",
    "En el paso anterior, los bins de viento se constru√≠an con **`wind_mean` sin lag** (viento ‚Äúen t‚Äù), pero la correlaci√≥n se calculaba con **`VLOS_lagXs`** (viento ‚Äúen t‚àíXs‚Äù). Eso puede mezclar reg√≠menes de viento y mover artificialmente el pico.\n",
    "\n",
    "**M√©todo alineado (recomendado):**\n",
    "- Para cada lag $X$ (p.ej. 6 s), calculamos **`wind_mean_lagXs`** como la media por fila de todas las columnas `LAC_VLOS...lagXs`.\n",
    "- Clasificamos cada fila en bins de 1 m/s usando **ese** `wind_mean_lagXs`.\n",
    "- Dentro de cada bin, calculamos la correlaci√≥n entre `VLOS_lagXs` y el target.\n",
    "\n",
    "Esto alinea la condici√≥n ‚Äúviento = X m/s‚Äù con el instante temporal correcto para ese lag.\n",
    "\n",
    "**Outputs (misma carpeta `04_Lag_per_Wind`):**\n",
    "- Heatmap $\\mathrm{corr}(\\text{VLOS}_\\mathrm{lag}, My)$ vs (viento, lag)\n",
    "- Curva **lag √≥ptimo vs viento** (por target)\n",
    "- CSVs con la rejilla completa y el √≥ptimo por bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db61e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 4.9: LAG √ìPTIMO POR VIENTO (M√âTODO ALINEADO CON EL LAG)\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LAG √ìPTIMO POR VIENTO (M√âTODO ALINEADO)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {wind_lags_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Par√°metros (ajusta si quieres m√°s/menos estabilidad)\n",
    "sample_rate = 5                 # 1 de cada N filas\n",
    "min_samples_per_bin = 200       # m√≠nimo filas por bin para calcular correlaci√≥n\n",
    "use_abs_for_optimum = True      # True: argmax(|corr|) ; False: argmax(corr)\n",
    "\n",
    "# Archivo\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Identificar columnas lag y construir mapping lag -> columnas (por BEAM)\n",
    "    # ------------------------------------------------------------------------\n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "\n",
    "    target_vars = ['Blade root 1 My', 'Blade root 2 My']\n",
    "    for t in target_vars:\n",
    "        if t not in all_columns:\n",
    "            raise ValueError(f\"No encuentro target '{t}' en el dataset\")\n",
    "\n",
    "    vlos_lag_cols = [c for c in all_columns if ('LAC_VLOS' in c and 'lag' in c)]\n",
    "    if len(vlos_lag_cols) == 0:\n",
    "        raise ValueError(\"No se encontraron columnas VLOS con lag. Revisa nombres de columnas.\")\n",
    "\n",
    "    lag_to_cols = {}\n",
    "    beam_set = set()\n",
    "\n",
    "    for col in vlos_lag_cols:\n",
    "        m_lag = re.search(r'lag(\\d+)s', col)\n",
    "        m_beam = re.search(r'BEAM(\\d+)', col)\n",
    "        if not (m_lag and m_beam):\n",
    "            continue\n",
    "        lag_s = int(m_lag.group(1))\n",
    "        beam = int(m_beam.group(1))\n",
    "        beam_set.add(beam)\n",
    "        lag_to_cols.setdefault(lag_s, []).append(col)\n",
    "\n",
    "    lags = sorted(lag_to_cols.keys())\n",
    "    beams = sorted(beam_set)\n",
    "\n",
    "    print(f\"\\nüìå Lags detectados: {lags[0]}s - {lags[-1]}s ({len(lags)} lags)\")\n",
    "    print(f\"üìå BEAMs detectados en lags: {beams}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2) Cargar datos (targets + lags). No cargamos VLOS sin lag: aqu√≠ binning es por wind_mean_lagXs\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n[1/4] Cargando datos (targets + VLOS_lag*)...\")\n",
    "    usecols = target_vars + vlos_lag_cols\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        complete_dataset_path,\n",
    "        usecols=usecols,\n",
    "        skiprows=lambda i: i > 0 and i % sample_rate != 0\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Cargado: {df.shape[0]:,} filas, {df.shape[1]} columnas (sample_rate={sample_rate})\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 3) Construir rejilla correlaci√≥n(viento_bin, lag) para cada target\n",
    "    #    Bin se define con wind_mean_lagXs = mean( columnas lagXs a trav√©s de BEAMs )\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n[2/4] Calculando correlaci√≥n por bin(1 m/s) y lag (alineado)...\")\n",
    "\n",
    "    wind_edges = np.arange(0, 31, 1)  # 0..30\n",
    "    wind_mids = wind_edges[:-1] + 0.5\n",
    "    wind_bin_labels = [f\"{i}-{i+1}\" for i in range(30)]\n",
    "\n",
    "    results_grids = {}   # target -> dict(grid, counts)\n",
    "\n",
    "    for target in target_vars:\n",
    "        corr_grid = np.full((len(wind_mids), len(lags)), np.nan, dtype=float)\n",
    "        n_grid = np.zeros((len(wind_mids), len(lags)), dtype=int)\n",
    "\n",
    "        for j, lag_s in enumerate(lags):\n",
    "            cols_lag = lag_to_cols[lag_s]\n",
    "\n",
    "            # wind_mean_lagXs para cada fila\n",
    "            wind_mean_lag = df[cols_lag].mean(axis=1).to_numpy()\n",
    "\n",
    "            # asignaci√≥n r√°pida de bin\n",
    "            bin_idx = np.digitize(wind_mean_lag, wind_edges, right=False) - 1\n",
    "\n",
    "            # para cada bin con suficientes muestras\n",
    "            for i_bin in range(len(wind_mids)):\n",
    "                mask = (bin_idx == i_bin)\n",
    "                n = int(mask.sum())\n",
    "                if n < min_samples_per_bin:\n",
    "                    continue\n",
    "\n",
    "                # correlaci√≥n por BEAM en este lag; luego promedio\n",
    "                # (cada col ya es un BEAM concreto a ese lag)\n",
    "                corr_per_beam = df.loc[mask, cols_lag].corrwith(df.loc[mask, target])\n",
    "                mean_corr = float(corr_per_beam.mean(skipna=True))\n",
    "\n",
    "                corr_grid[i_bin, j] = mean_corr\n",
    "                n_grid[i_bin, j] = n\n",
    "\n",
    "        results_grids[target] = {\"corr\": corr_grid, \"n\": n_grid}\n",
    "        \n",
    "        # Quick diagnostic\n",
    "        valid_cells = np.isfinite(corr_grid).sum()\n",
    "        print(f\"  - Target '{target}': celdas v√°lidas en rejilla = {valid_cells:,}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4) Extraer lag √≥ptimo por bin y generar plots\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n[3/4] Extrayendo lag √≥ptimo por bin y generando plots...\")\n",
    "\n",
    "    optimal_rows = []\n",
    "\n",
    "    for target in target_vars:\n",
    "        corr_grid = results_grids[target][\"corr\"]\n",
    "        n_grid = results_grids[target][\"n\"]\n",
    "\n",
    "        # Lag √≥ptimo por bin\n",
    "        optimal_lag = np.full(len(wind_mids), np.nan, dtype=float)\n",
    "        optimal_corr = np.full(len(wind_mids), np.nan, dtype=float)\n",
    "        optimal_n = np.full(len(wind_mids), 0, dtype=int)\n",
    "\n",
    "        for i_bin in range(len(wind_mids)):\n",
    "            row = corr_grid[i_bin, :]\n",
    "            if not np.isfinite(row).any():\n",
    "                continue\n",
    "\n",
    "            if use_abs_for_optimum:\n",
    "                j_best = int(np.nanargmax(np.abs(row)))\n",
    "            else:\n",
    "                j_best = int(np.nanargmax(row))\n",
    "\n",
    "            optimal_lag[i_bin] = lags[j_best]\n",
    "            optimal_corr[i_bin] = row[j_best]\n",
    "            optimal_n[i_bin] = int(n_grid[i_bin, j_best])\n",
    "\n",
    "            optimal_rows.append({\n",
    "                \"Target\": target,\n",
    "                \"Wind_Bin\": wind_bin_labels[i_bin],\n",
    "                \"Wind_Speed_Mid\": float(wind_mids[i_bin]),\n",
    "                \"Optimal_Lag_Seconds\": float(optimal_lag[i_bin]),\n",
    "                \"Optimal_Correlation\": float(optimal_corr[i_bin]),\n",
    "                \"N_at_Optimal\": int(optimal_n[i_bin]),\n",
    "            })\n",
    "\n",
    "        # ---- Plot A: Heatmap viento x lag ----\n",
    "        # (y=wind, x=lag)\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        sns.heatmap(\n",
    "            corr_grid,\n",
    "            ax=ax,\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            vmin=-1, vmax=1,\n",
    "            cbar_kws={'label': 'Correlaci√≥n Pearson (promedio BEAMs)'},\n",
    "            xticklabels=lags,\n",
    "            yticklabels=[f\"{w:.1f}\" for w in wind_mids],\n",
    "        )\n",
    "        ax.set_title(f\"Heatmap Corr vs (Viento, Lag) - M√©todo Alineado\\nTarget: {target}\", fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel(\"Lag (s)\")\n",
    "        ax.set_ylabel(\"Velocidad viento (m/s) [bin mid]\")\n",
    "        plt.tight_layout()\n",
    "        heatmap_path = wind_lags_folder / f\"heatmap_corr_aligned_target_{target.replace(' ', '_')}.png\"\n",
    "        plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # ---- Plot B: Lag √≥ptimo vs viento ----\n",
    "        df_opt = pd.DataFrame([r for r in optimal_rows if r[\"Target\"] == target])\n",
    "        df_opt = df_opt.sort_values(\"Wind_Speed_Mid\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(df_opt[\"Wind_Speed_Mid\"], df_opt[\"Optimal_Lag_Seconds\"], 'o-', linewidth=2.5, markersize=6, color='steelblue')\n",
    "        ax.set_title(f\"Lag √ìptimo vs Viento (Alineado) - Target: {target}\", fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel(\"Velocidad viento (m/s) [bin mid]\")\n",
    "        ax.set_ylabel(\"Lag √≥ptimo (s)\")\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "        # l√≠nea de tendencia simple (opcional)\n",
    "        if len(df_opt) >= 5:\n",
    "            x = df_opt[\"Wind_Speed_Mid\"].values\n",
    "            y = df_opt[\"Optimal_Lag_Seconds\"].values\n",
    "            coefs = np.polyfit(x, y, 1)\n",
    "            ax.plot(x, np.polyval(coefs, x), '--', color='red', alpha=0.7, label=f\"Tendencia lineal: {coefs[0]:.2f} s/(m/s)\")\n",
    "            ax.legend(loc='best')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        line_path = wind_lags_folder / f\"optimal_lag_vs_wind_aligned_target_{target.replace(' ', '_')}.png\"\n",
    "        plt.savefig(line_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"  ‚úÖ Target '{target}': guardados {heatmap_path.name} y {line_path.name}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5) Guardar CSVs\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n[4/4] Guardando CSVs...\")\n",
    "\n",
    "    # CSV √≥ptimos\n",
    "    df_optimal_aligned = pd.DataFrame(optimal_rows)\n",
    "    opt_csv = wind_lags_folder / \"optimal_lag_per_wind_speed_1ms_bins_ALIGNED.csv\"\n",
    "    df_optimal_aligned.to_csv(opt_csv, index=False)\n",
    "\n",
    "    # CSV rejilla (formato largo: target, wind_mid, lag, corr, n)\n",
    "    long_rows = []\n",
    "    for target in target_vars:\n",
    "        corr_grid = results_grids[target][\"corr\"]\n",
    "        n_grid = results_grids[target][\"n\"]\n",
    "        for i_bin, wmid in enumerate(wind_mids):\n",
    "            for j, lag_s in enumerate(lags):\n",
    "                cval = corr_grid[i_bin, j]\n",
    "                nval = int(n_grid[i_bin, j])\n",
    "                if not np.isfinite(cval):\n",
    "                    continue\n",
    "                long_rows.append({\n",
    "                    \"Target\": target,\n",
    "                    \"Wind_Speed_Mid\": float(wmid),\n",
    "                    \"Wind_Bin\": wind_bin_labels[i_bin],\n",
    "                    \"Lag_Seconds\": int(lag_s),\n",
    "                    \"Correlation_MeanAcrossBeams\": float(cval),\n",
    "                    \"N\": nval,\n",
    "                })\n",
    "\n",
    "    df_grid_long = pd.DataFrame(long_rows)\n",
    "    grid_csv = wind_lags_folder / \"corr_grid_windbin_lag_ALIGNED_long.csv\"\n",
    "    df_grid_long.to_csv(grid_csv, index=False)\n",
    "\n",
    "    print(f\"‚úÖ CSV √≥ptimos: {opt_csv.name}\")\n",
    "    print(f\"‚úÖ CSV rejilla: {grid_csv.name}\")\n",
    "\n",
    "    print(\"\\nüí° Nota:\")\n",
    "    print(\"- Este m√©todo alinea el binning de viento con el mismo instante del lag.\")\n",
    "    print(\"- Si a√∫n ves no-monoton√≠a, suele indicar mezcla de reg√≠menes (DLCs/transitorios) o que el pico real cae fuera del rango de lags.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d42896",
   "metadata": {},
   "source": [
    "### üì° PASO 4.10: Desplazamiento √ìptimo de Se√±al VLOS (Cross-Correlation)\n",
    "\n",
    "En este an√°lisis **desplazamos directamente cada se√±al LAC_VLOS (sin lag)** hacia adelante o atr√°s en el tiempo y calculamos la correlaci√≥n de Pearson con cada target en cada posici√≥n.\n",
    "\n",
    "**M√©todo:**\n",
    "- Para cada BEAM, cargamos la se√±al **LAC_VLOS_BEAMX_RANGE5** (original, sin lag).\n",
    "- La desplazamos $N$ samples hacia adelante (positivo) o atr√°s (negativo): $N \\in [-200, +200]$ (con $\\Delta t \\approx 0.1$ s ‚Üí rango ~¬±20 s).\n",
    "- Calculamos $\\text{corr}( \\text{VLOS}_\\text{shifted}, \\text{target} )$ para cada $N$.\n",
    "- El shift √≥ptimo es $N^* = \\arg\\max_N |\\text{corr}(N)|$ (m√°ximo en valor absoluto).\n",
    "\n",
    "**Targets evaluados:**\n",
    "- `Blade root 1 My`, `Blade root 2 My` (carga total)\n",
    "- `Blade root 1 My 1P`, `Blade root 2 My 1P` (componente 1P/gravedad)\n",
    "\n",
    "**Conversi√≥n a tiempo:**\n",
    "Si $\\Delta t = 0.1$ s y el shift √≥ptimo es $N^* = 80$ samples ‚Üí delay = $80 \\times 0.1 = 8$ s.\n",
    "\n",
    "**Outputs (carpeta `05_Lag_VLOS_signal`):**\n",
    "- Plots de correlaci√≥n vs shift para cada BEAM y target (con marcador en el pico).\n",
    "- CSV con shifts √≥ptimos por BEAM/target y correlaci√≥n m√°xima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 4.10: DESPLAZAMIENTO √ìPTIMO DE SE√ëAL VLOS (CROSS-CORRELATION)\n",
    "# ESTRATIFICADO POR BINS DE VELOCIDAD DE VIENTO\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "\n",
    "# Crear carpeta para resultados\n",
    "signal_shift_folder = eda_folder / \"05_Lag_VLOS_signal\"\n",
    "signal_shift_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DESPLAZAMIENTO √ìPTIMO DE SE√ëAL VLOS - ESTRATIFICADO POR VIENTO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {signal_shift_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Par√°metros\n",
    "max_shift_samples = 250   # rango: [-250, +250] samples\n",
    "sample_rate_load = 10     # cargar 1 de cada N filas para acelerar\n",
    "use_abs_corr = True       # True: argmax(|corr|), False: argmax(corr)\n",
    "wind_bin_width = 2.0      # Ancho del bin de viento en m/s\n",
    "min_samples_per_bin = 500 # M√≠nimo de muestras por bin para calcular\n",
    "\n",
    "# Archivo\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Identificar columnas VLOS sin lag y targets\n",
    "    # ------------------------------------------------------------------------\n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "\n",
    "    # VLOS sin lag\n",
    "    vlos_base_cols = [c for c in all_columns if 'LAC_VLOS' in c and 'lag' not in c]\n",
    "    \n",
    "    # Targets\n",
    "    target_vars = [\n",
    "        'Blade root 1 My',\n",
    "        'Blade root 2 My',\n",
    "        'Blade root 1 My 1P',\n",
    "        'Blade root 2 My 1P'\n",
    "    ]\n",
    "    \n",
    "    # Verificar que existen\n",
    "    missing = [t for t in target_vars if t not in all_columns]\n",
    "    if missing:\n",
    "        print(f\"\\n‚ö†Ô∏è Targets no encontrados: {missing}\")\n",
    "        target_vars = [t for t in target_vars if t in all_columns]\n",
    "    \n",
    "    print(f\"\\nüìå Columnas VLOS sin lag detectadas: {len(vlos_base_cols)}\")\n",
    "    print(f\"üìå Targets a evaluar: {len(target_vars)}\")\n",
    "    \n",
    "    # Extraer BEAMs\n",
    "    beams = []\n",
    "    vlos_by_beam = {}\n",
    "    for col in vlos_base_cols:\n",
    "        m = re.search(r'BEAM(\\d+)', col)\n",
    "        if m:\n",
    "            beam = int(m.group(1))\n",
    "            beams.append(beam)\n",
    "            vlos_by_beam[beam] = col\n",
    "    beams = sorted(set(beams))\n",
    "    \n",
    "    print(f\"üìå BEAMs detectados: {beams}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2) Cargar datos (VLOS sin lag + targets + calcular wind_mean)\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[1/5] Cargando datos (sample_rate={sample_rate_load})...\")\n",
    "    \n",
    "    usecols = list(vlos_by_beam.values()) + target_vars\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        complete_dataset_path,\n",
    "        usecols=usecols,\n",
    "        skiprows=lambda i: i > 0 and i % sample_rate_load != 0\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Cargado: {df.shape[0]:,} filas, {df.shape[1]} columnas\")\n",
    "    \n",
    "    # Calcular wind_mean (promedio de VLOS sin lag)\n",
    "    print(\"‚öôÔ∏è  Calculando wind_mean...\")\n",
    "    df['wind_mean'] = df[list(vlos_by_beam.values())].mean(axis=1)\n",
    "    print(f\"‚úÖ wind_mean: {df['wind_mean'].min():.2f} - {df['wind_mean'].max():.2f} m/s\")\n",
    "    \n",
    "    # Detectar sampling time (aproximado)\n",
    "    # Asumimos que Time existe o usamos √≠ndice\n",
    "    if 'Time' in all_columns:\n",
    "        df_time_sample = pd.read_csv(complete_dataset_path, usecols=['Time'], nrows=1000, \n",
    "                                     skiprows=lambda i: i > 0 and i % sample_rate_load != 0)\n",
    "        if len(df_time_sample) >= 2:\n",
    "            dt = float(df_time_sample['Time'].diff().median())\n",
    "        else:\n",
    "            dt = 0.1  # default\n",
    "    else:\n",
    "        dt = 0.1  # default\n",
    "    \n",
    "    print(f\"üìå Sampling time estimado: {dt:.3f} s\")\n",
    "    print(f\"üìå Rango de shifts: ¬±{max_shift_samples} samples ‚Üí ¬±{max_shift_samples*dt:.1f} s\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 3) Crear bins de velocidad de viento\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[2/5] Creando bins de velocidad de viento ({wind_bin_width} m/s)...\")\n",
    "    \n",
    "    wind_edges = np.arange(0, 30 + wind_bin_width, wind_bin_width)\n",
    "    df['wind_bin'] = pd.cut(df['wind_mean'], bins=wind_edges, include_lowest=True)\n",
    "    \n",
    "    bin_counts = df['wind_bin'].value_counts().sort_index()\n",
    "    valid_bins = [b for b, count in bin_counts.items() if count >= min_samples_per_bin]\n",
    "    \n",
    "    print(f\"‚úÖ Bins creados: {len(bin_counts)}\")\n",
    "    print(f\"‚úÖ Bins v√°lidos (‚â•{min_samples_per_bin} muestras): {len(valid_bins)}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4) Calcular correlaci√≥n vs shift POR BIN de viento\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[3/5] Calculando correlaci√≥n vs shift por bin de viento...\")\n",
    "    \n",
    "    shift_range = np.arange(-max_shift_samples, max_shift_samples + 1)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for wind_bin in valid_bins:\n",
    "        df_bin = df[df['wind_bin'] == wind_bin]\n",
    "        wind_mid = (wind_bin.left + wind_bin.right) / 2\n",
    "        \n",
    "        print(f\"\\n  üå¨Ô∏è  Bin: {wind_bin} (mid={wind_mid:.1f} m/s, N={len(df_bin):,})...\")\n",
    "        \n",
    "        for beam in beams:\n",
    "            vlos_col = vlos_by_beam[beam]\n",
    "            vlos_signal = df_bin[vlos_col].to_numpy()\n",
    "            \n",
    "            for target in target_vars:\n",
    "                target_signal = df_bin[target].to_numpy()\n",
    "                \n",
    "                corr_vs_shift = np.full(len(shift_range), np.nan, dtype=float)\n",
    "                \n",
    "                for i, shift in enumerate(shift_range):\n",
    "                    if shift >= 0:\n",
    "                        if shift == 0:\n",
    "                            v = vlos_signal\n",
    "                            tg = target_signal\n",
    "                        else:\n",
    "                            v = vlos_signal[:-shift]\n",
    "                            tg = target_signal[shift:]\n",
    "                    else:\n",
    "                        v = vlos_signal[-shift:]\n",
    "                        tg = target_signal[:shift]\n",
    "                    \n",
    "                    if len(v) >= 100 and len(tg) >= 100 and len(v) == len(tg):\n",
    "                        valid_mask = np.isfinite(v) & np.isfinite(tg)\n",
    "                        if valid_mask.sum() >= 100:\n",
    "                            corr_vs_shift[i] = np.corrcoef(v[valid_mask], tg[valid_mask])[0, 1]\n",
    "                \n",
    "                # Encontrar shift √≥ptimo\n",
    "                if use_abs_corr:\n",
    "                    idx_opt = int(np.nanargmax(np.abs(corr_vs_shift)))\n",
    "                else:\n",
    "                    idx_opt = int(np.nanargmax(corr_vs_shift))\n",
    "                \n",
    "                shift_opt = int(shift_range[idx_opt])\n",
    "                corr_opt = float(corr_vs_shift[idx_opt])\n",
    "                delay_opt = shift_opt * dt\n",
    "                \n",
    "                results.append({\n",
    "                    'Wind_Bin': str(wind_bin),\n",
    "                    'Wind_Speed_Mid': wind_mid,\n",
    "                    'BEAM': beam,\n",
    "                    'Target': target,\n",
    "                    'Optimal_Shift_Samples': shift_opt,\n",
    "                    'Optimal_Delay_Seconds': delay_opt,\n",
    "                    'Max_Correlation': corr_opt,\n",
    "                    'N_Samples': len(df_bin),\n",
    "                })\n",
    "    \n",
    "    print(f\"\\n‚úÖ An√°lisis completado para {len(valid_bins)} bins, {len(beams)} BEAMs, {len(target_vars)} targets\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5) Guardar resultados en CSV\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[4/5] Guardando resultados...\")\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    csv_path = signal_shift_folder / \"optimal_shift_per_wind_bin_beam_target.csv\"\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ CSV guardado: {csv_path.name}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 6) Generar plots: Delay √ìptimo vs Velocidad de Viento\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[5/5] Generando plots de delay vs velocidad de viento...\")\n",
    "    \n",
    "    # Plot por cada BEAM y target\n",
    "    for beam in beams:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(f'Delay √ìptimo vs Velocidad de Viento - BEAM {beam}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for idx, target in enumerate(target_vars):\n",
    "            ax = axes.flat[idx]\n",
    "            \n",
    "            df_plot = df_results[\n",
    "                (df_results['BEAM'] == beam) & \n",
    "                (df_results['Target'] == target)\n",
    "            ].sort_values('Wind_Speed_Mid')\n",
    "            \n",
    "            if len(df_plot) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Graficar delay √≥ptimo vs viento\n",
    "            ax.plot(df_plot['Wind_Speed_Mid'], df_plot['Optimal_Delay_Seconds'], \n",
    "                   'o-', linewidth=2.5, markersize=7, color='steelblue', label='Delay √≥ptimo')\n",
    "            \n",
    "            # Superponer curva te√≥rica 100/U\n",
    "            wind_theory = np.linspace(df_plot['Wind_Speed_Mid'].min(), \n",
    "                                     df_plot['Wind_Speed_Mid'].max(), 100)\n",
    "            delay_theory = 100 / wind_theory  # Asumiendo distancia = 100m\n",
    "            ax.plot(wind_theory, delay_theory, '--', linewidth=2, color='red', \n",
    "                   alpha=0.7, label='Te√≥rico: 100m/U')\n",
    "            \n",
    "            ax.set_xlabel('Velocidad del Viento (m/s)', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('Delay √ìptimo (s)', fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'Target: {target}', fontsize=12, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.legend(loc='best', fontsize=10)\n",
    "            ax.invert_yaxis()  # Invertir para mostrar que delay disminuye con viento\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = signal_shift_folder / f\"delay_vs_wind_BEAM{beam}.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"  ‚úÖ BEAM {beam}: delay_vs_wind_BEAM{beam}.png\")\n",
    "    \n",
    "    # Plot resumen: todos los BEAMs superpuestos (solo primer target)\n",
    "    target_main = target_vars[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    beam_colors_map = {b: plt.cm.tab10(i) for i, b in enumerate(beams)}\n",
    "    \n",
    "    for beam in beams:\n",
    "        df_plot = df_results[\n",
    "            (df_results['BEAM'] == beam) & \n",
    "            (df_results['Target'] == target_main)\n",
    "        ].sort_values('Wind_Speed_Mid')\n",
    "        \n",
    "        if len(df_plot) == 0:\n",
    "            continue\n",
    "        \n",
    "        ax.plot(df_plot['Wind_Speed_Mid'], df_plot['Optimal_Delay_Seconds'], \n",
    "               'o-', linewidth=2, markersize=5, color=beam_colors_map[beam], \n",
    "               label=f'BEAM {beam}', alpha=0.8)\n",
    "    \n",
    "    # Curva te√≥rica\n",
    "    wind_theory = np.linspace(5, 25, 100)\n",
    "    delay_theory = 100 / wind_theory\n",
    "    ax.plot(wind_theory, delay_theory, '--', linewidth=2.5, color='black', \n",
    "           alpha=0.7, label='Te√≥rico: 100m/U')\n",
    "    \n",
    "    ax.set_xlabel('Velocidad del Viento (m/s)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Delay √ìptimo (s)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Delay √ìptimo vs Velocidad - Todos los BEAMs\\nTarget: {target_main}', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.legend(loc='best', fontsize=10, ncol=2)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    summary_path = signal_shift_folder / \"delay_vs_wind_all_beams_summary.png\"\n",
    "    plt.savefig(summary_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  ‚úÖ Resumen: delay_vs_wind_all_beams_summary.png\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # RESUMEN FINAL\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESUMEN - DESPLAZAMIENTO √ìPTIMO ESTRATIFICADO POR VIENTO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Bins de viento analizados: {len(valid_bins)}\")\n",
    "    print(f\"BEAMs analizados:          {len(beams)}\")\n",
    "    print(f\"Targets analizados:        {len(target_vars)}\")\n",
    "    print(f\"Rango de b√∫squeda:         ¬±{max_shift_samples} samples (¬±{max_shift_samples*dt:.1f} s)\")\n",
    "    \n",
    "    print(f\"\\nüéØ AN√ÅLISIS DE CONSISTENCIA F√çSICA:\")\n",
    "    print(\"   Se compara delay √≥ptimo vs curva te√≥rica 100m/U\")\n",
    "    print(\"   - Si los puntos siguen la curva ‚Üí advecci√≥n simple validada\")\n",
    "    print(\"   - Desviaciones indican efectos de din√°mica/filtrado/control\")\n",
    "    \n",
    "    # Calcular error medio respecto a teor√≠a para cada BEAM\n",
    "    print(f\"\\nüìä Error vs teor√≠a (100m/U) por BEAM:\")\n",
    "    for beam in beams:\n",
    "        df_beam = df_results[df_results['BEAM'] == beam]\n",
    "        if len(df_beam) == 0:\n",
    "            continue\n",
    "        \n",
    "        delay_theory_beam = 100 / df_beam['Wind_Speed_Mid']\n",
    "        error_mean = (df_beam['Optimal_Delay_Seconds'] - delay_theory_beam).mean()\n",
    "        error_std = (df_beam['Optimal_Delay_Seconds'] - delay_theory_beam).std()\n",
    "        \n",
    "        print(f\"  BEAM {beam}: error medio = {error_mean:+.2f} ¬± {error_std:.2f} s\")\n",
    "    \n",
    "    print(f\"\\nüí° INTERPRETACI√ìN:\")\n",
    "    print(f\"   - Delay √≥ptimo debe DECRECER con velocidad (curva ~100/U)\")\n",
    "    print(f\"   - Estratificaci√≥n elimina mezcla de reg√≠menes ‚Üí picos m√°s claros\")\n",
    "    print(f\"   - Variaci√≥n entre BEAMs: geometr√≠a del cono LIDAR\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Archivos generados:\")\n",
    "    print(f\"  - Plots por BEAM: delay_vs_wind_BEAMX.png ({len(beams)} archivos)\")\n",
    "    print(f\"  - Plot resumen: delay_vs_wind_all_beams_summary.png\")\n",
    "    print(f\"  - CSV detallado: optimal_shift_per_wind_bin_beam_target.csv\")\n",
    "    print(f\"\\nüìç Ubicaci√≥n: {signal_shift_folder}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚úÖ An√°lisis completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca848dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 4.10.b: DELAY RESIDUAL TRAS ADELANTAR VLOS A 100/U\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DELAY RESIDUAL TRAS SHIFT TE√ìRICO 100/U\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "signal_shift_folder = eda_folder / \"05_Lag_VLOS_signal\"\n",
    "signal_shift_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Par√°metros espec√≠ficos de este an√°lisis\n",
    "distance_upwind_m = 100.0           # distancia advecci√≥n (m)\n",
    "sample_rate_residual = 10            # 1 de cada N filas\n",
    "wind_bin_width = 2.0                 # bins iguales a los del paso anterior\n",
    "min_samples_per_bin = 500\n",
    "min_overlap_samples = 150\n",
    "residual_shift_span = 80             # rango adicional alrededor del delay te√≥rico (samples)\n",
    "target_vars = ['Blade root 1 My', 'Blade root 2 My']\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"‚ùå No se encuentra el archivo {complete_dataset_path}\")\n",
    "else:\n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "\n",
    "    vlos_base_cols = [c for c in all_columns if 'LAC_VLOS' in c and 'lag' not in c]\n",
    "    if not vlos_base_cols:\n",
    "        raise ValueError(\"No se detectaron columnas LAC_VLOS sin lag para este an√°lisis\")\n",
    "\n",
    "    missing_targets = [t for t in target_vars if t not in all_columns]\n",
    "    if missing_targets:\n",
    "        print(f\"‚ö†Ô∏è Targets ausentes: {missing_targets}\")\n",
    "        target_vars = [t for t in target_vars if t in all_columns]\n",
    "    if not target_vars:\n",
    "        raise ValueError(\"No quedan targets v√°lidos tras la verificaci√≥n de columnas\")\n",
    "\n",
    "    # Mapeo BEAM -> columna\n",
    "    vlos_by_beam = {}\n",
    "    for col in vlos_base_cols:\n",
    "        match = re.search(r'BEAM(\\d+)', col)\n",
    "        if match:\n",
    "            vlos_by_beam[int(match.group(1))] = col\n",
    "    beams = sorted(vlos_by_beam.keys())\n",
    "    if not beams:\n",
    "        raise ValueError(\"No se pudieron identificar BEAMs en las columnas LAC_VLOS\")\n",
    "\n",
    "    usecols = list(vlos_by_beam.values()) + target_vars\n",
    "    df = pd.read_csv(\n",
    "        complete_dataset_path,\n",
    "        usecols=usecols,\n",
    "        skiprows=lambda i: i > 0 and i % sample_rate_residual != 0\n",
    "    )\n",
    "\n",
    "    df['wind_mean'] = df[list(vlos_by_beam.values())].mean(axis=1)\n",
    "\n",
    "    # Estimar dt a partir de la columna Time si existe\n",
    "    if 'Time' in all_columns:\n",
    "        df_time = pd.read_csv(\n",
    "            complete_dataset_path,\n",
    "            usecols=['Time'],\n",
    "            nrows=2000,\n",
    "            skiprows=lambda i: i > 0 and i % sample_rate_residual != 0\n",
    "        )\n",
    "        time_diff = df_time['Time'].diff().dropna().abs()\n",
    "        dt = float(time_diff.median()) if not time_diff.empty else 0.1\n",
    "    else:\n",
    "        dt = 0.1\n",
    "\n",
    "    print(f\"üìå dt estimado: {dt:.3f} s | Rango residual ¬±{residual_shift_span} samples (~¬±{residual_shift_span*dt:.1f} s)\")\n",
    "\n",
    "    wind_edges = np.arange(0, 30 + wind_bin_width, wind_bin_width)\n",
    "    df['wind_bin'] = pd.cut(df['wind_mean'], bins=wind_edges, include_lowest=True)\n",
    "    bin_counts = df['wind_bin'].value_counts().sort_index()\n",
    "    valid_bins = [b for b, n in bin_counts.items() if n >= min_samples_per_bin]\n",
    "\n",
    "    print(f\"üìä Bins v√°lidos: {len(valid_bins)}/{len(bin_counts)}\")\n",
    "\n",
    "    shift_range = np.arange(-residual_shift_span, residual_shift_span + 1)\n",
    "    results = []\n",
    "\n",
    "    def _align_signals(sig_vlos: np.ndarray, sig_target: np.ndarray, shift_samples: int):\n",
    "        \"\"\"Devuelve las partes solapadas tras aplicar shift (None si no queda solape √∫til).\"\"\"\n",
    "        if shift_samples == 0:\n",
    "            v = sig_vlos\n",
    "            t = sig_target\n",
    "        elif shift_samples > 0:\n",
    "            if shift_samples >= len(sig_vlos) or shift_samples >= len(sig_target):\n",
    "                return None, None\n",
    "            v = sig_vlos[:-shift_samples]\n",
    "            t = sig_target[shift_samples:]\n",
    "        else:\n",
    "            shift_abs = abs(shift_samples)\n",
    "            if shift_abs >= len(sig_vlos) or shift_abs >= len(sig_target):\n",
    "                return None, None\n",
    "            v = sig_vlos[shift_abs:]\n",
    "            t = sig_target[:-shift_abs]\n",
    "        if len(v) < min_overlap_samples:\n",
    "            return None, None\n",
    "        valid = np.isfinite(v) & np.isfinite(t)\n",
    "        if valid.sum() < min_overlap_samples:\n",
    "            return None, None\n",
    "        return v[valid], t[valid]\n",
    "\n",
    "    for wind_bin in valid_bins:\n",
    "        df_bin = df[df['wind_bin'] == wind_bin]\n",
    "        if df_bin.empty:\n",
    "            continue\n",
    "        wind_mid = float((wind_bin.left + wind_bin.right) / 2)\n",
    "        if not np.isfinite(wind_mid) or wind_mid <= 0.1:\n",
    "            continue\n",
    "        theoretical_delay = distance_upwind_m / wind_mid\n",
    "        theoretical_shift_samples = int(round(theoretical_delay / dt))\n",
    "\n",
    "        for beam in beams:\n",
    "            vlos_signal = df_bin[vlos_by_beam[beam]].to_numpy(dtype=float)\n",
    "            for target in target_vars:\n",
    "                target_signal = df_bin[target].to_numpy(dtype=float)\n",
    "                corr_vs_residual = np.full(len(shift_range), np.nan, dtype=float)\n",
    "\n",
    "                for idx, residual_shift in enumerate(shift_range):\n",
    "                    total_shift = theoretical_shift_samples + residual_shift\n",
    "                    aligned_v, aligned_t = _align_signals(vlos_signal, target_signal, total_shift)\n",
    "                    if aligned_v is None:\n",
    "                        continue\n",
    "                    corr_vs_residual[idx] = np.corrcoef(aligned_v, aligned_t)[0, 1]\n",
    "\n",
    "                if not np.isfinite(corr_vs_residual).any():\n",
    "                    continue\n",
    "                idx_opt = int(np.nanargmax(np.abs(corr_vs_residual)))\n",
    "                residual_opt = int(shift_range[idx_opt])\n",
    "                total_shift_opt = theoretical_shift_samples + residual_opt\n",
    "\n",
    "                results.append({\n",
    "                    'Wind_Bin': str(wind_bin),\n",
    "                    'Wind_Speed_Mid': wind_mid,\n",
    "                    'BEAM': beam,\n",
    "                    'Target': target,\n",
    "                    'N_Samples_Bin': len(df_bin),\n",
    "                    'dt_seconds': dt,\n",
    "                    'Theoretical_Delay_s': theoretical_delay,\n",
    "                    'Theoretical_Shift_samples': theoretical_shift_samples,\n",
    "                    'Residual_Shift_samples': residual_opt,\n",
    "                    'Residual_Delay_s': residual_opt * dt,\n",
    "                    'Total_Shift_samples': total_shift_opt,\n",
    "                    'Total_Delay_s': total_shift_opt * dt,\n",
    "                    'Max_Correlation': float(corr_vs_residual[idx_opt])\n",
    "                })\n",
    "\n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è No se obtuvieron resultados v√°lidos (revisa bins o par√°metros)\")\n",
    "    else:\n",
    "        df_results = pd.DataFrame(results)\n",
    "        csv_path = signal_shift_folder / \"residual_delay_after_100m_over_U.csv\"\n",
    "        df_results.to_csv(csv_path, index=False)\n",
    "        print(f\"‚úÖ Resultados guardados en {csv_path.name} ({len(df_results)} filas)\")\n",
    "\n",
    "        beam_colors = {beam: plt.cm.tab10(i % 10) for i, beam in enumerate(beams)}\n",
    "\n",
    "        for target in target_vars:\n",
    "            df_target = df_results[df_results['Target'] == target]\n",
    "            if df_target.empty:\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharex=True)\n",
    "            fig.suptitle(f\"Delay tras adelantar 100/U - Target: {target}\", fontsize=15, fontweight='bold')\n",
    "\n",
    "            for beam in beams:\n",
    "                df_plot = df_target[df_target['BEAM'] == beam].sort_values('Wind_Speed_Mid')\n",
    "                if df_plot.empty:\n",
    "                    continue\n",
    "                axes[0].plot(\n",
    "                    df_plot['Wind_Speed_Mid'],\n",
    "                    df_plot['Residual_Delay_s'],\n",
    "                    'o-',\n",
    "                    color=beam_colors[beam],\n",
    "                    label=f\"BEAM {beam}\",\n",
    "                    linewidth=2,\n",
    "                    markersize=5\n",
    "                )\n",
    "                axes[1].plot(\n",
    "                    df_plot['Wind_Speed_Mid'],\n",
    "                    df_plot['Total_Delay_s'],\n",
    "                    'o-',\n",
    "                    color=beam_colors[beam],\n",
    "                    label=f\"BEAM {beam}\",\n",
    "                    linewidth=2,\n",
    "                    markersize=5\n",
    "                )\n",
    "\n",
    "            axes[0].axhline(0, color='black', linewidth=1, linestyle='--', alpha=0.7)\n",
    "            axes[0].set_ylabel('Delay residual (s)')\n",
    "            axes[0].set_xlabel('Velocidad viento (m/s)')\n",
    "            axes[0].grid(True, linestyle='--', alpha=0.3)\n",
    "            axes[0].set_title('Delay adicional necesario')\n",
    "\n",
    "            axes[1].set_ylabel('Delay total (s)')\n",
    "            axes[1].set_xlabel('Velocidad viento (m/s)')\n",
    "            axes[1].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "            wind_min = max(0.5, df_target['Wind_Speed_Mid'].min())\n",
    "            wind_max = df_target['Wind_Speed_Mid'].max()\n",
    "            if wind_max > wind_min:\n",
    "                theory_wind = np.linspace(wind_min, wind_max, 200)\n",
    "                theory_delay = distance_upwind_m / theory_wind\n",
    "                axes[1].plot(theory_wind, theory_delay, '--', color='black', linewidth=2, alpha=0.8, label='Te√≥rico 100/U')\n",
    "            axes[1].set_title('Delay total vs te√≥rico 100/U')\n",
    "\n",
    "            axes[0].legend(loc='best', fontsize=9)\n",
    "            axes[1].legend(loc='best', fontsize=9)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plot_path = signal_shift_folder / f\"residual_delay_after_100overU_target_{target.replace(' ', '_')}.png\"\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"  üìà Guardado: {plot_path.name}\")\n",
    "\n",
    "        print(\"\\n‚úÖ Gr√°ficos residuales generados en 05_Lag_VLOS_signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92249649",
   "metadata": {},
   "source": [
    "### üå™Ô∏è PASO 4.11: Evoluci√≥n Temporal de R√°fagas en los Lags\n",
    "\n",
    "En este paso visualizamos c√≥mo una r√°faga de viento \"entra\" en la memoria del modelo y se desplaza a trav√©s de los distintos lags a lo largo del tiempo.\n",
    "\n",
    "**M√©todo:**\n",
    "- Seleccionamos una muestra consecutiva de datos (ej. √∫ltimas 2000 filas).\n",
    "- Extraemos todas las columnas de LAC_VLOS con lag (de 5s a 25s).\n",
    "- Creamos un heatmap donde:\n",
    "  - **Eje X**: Tiempo (√≠ndice de la muestra)\n",
    "  - **Eje Y**: Variables de lag ordenadas (5s ‚Üí 25s)\n",
    "  - **Color**: Velocidad del viento\n",
    "\n",
    "**Patr√≥n esperado:**\n",
    "- Deber√≠an observarse **patrones diagonales de abajo-izquierda hacia arriba-derecha** ‚ÜóÔ∏è\n",
    "- Una r√°faga que aparece en lag=5s (abajo) \"envejece\" y sube hacia lag=25s (arriba) conforme pasa el tiempo\n",
    "- Interpretaci√≥n f√≠sica: lag5s=\"hace 5s\", lag10s=\"hace 10s\" ‚Üí una r√°faga se mueve de memoria reciente a memoria antigua\n",
    "- Esto confirma visualmente el mecanismo de advecci√≥n temporal y la coherencia de las se√±ales VLOS con lag.\n",
    "\n",
    "**Outputs (carpeta `06_Wind_Evolution`):**\n",
    "- Heatmap de evoluci√≥n temporal de r√°fagas por BEAM\n",
    "- CSV con los datos utilizados para la visualizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe405257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 4.11: EVOLUCI√ìN TEMPORAL DE R√ÅFAGAS EN LOS LAGS\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Crear carpeta para resultados\n",
    "wind_evolution_folder = eda_folder / \"06_Wind_Evolution\"\n",
    "wind_evolution_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö° EVOLUCI√ìN TEMPORAL DE R√ÅFAGAS EN LOS LAGS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {wind_evolution_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Par√°metros\n",
    "n_samples = 2000  # N√∫mero de filas consecutivas a visualizar\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Identificar columnas VLOS con lag\n",
    "    # ------------------------------------------------------------------------\n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "    \n",
    "    # Extraer columnas con lag\n",
    "    vlos_lag_cols = [c for c in all_columns if 'LAC_VLOS' in c and 'lag' in c]\n",
    "    \n",
    "    if not vlos_lag_cols:\n",
    "        print(\"\\n‚ùå ERROR: No se encontraron columnas VLOS con lag\")\n",
    "    else:\n",
    "        print(f\"\\nüìå Columnas VLOS con lag detectadas: {len(vlos_lag_cols)}\")\n",
    "        \n",
    "        # Extraer lag values y organizar\n",
    "        lag_info = []\n",
    "        for col in vlos_lag_cols:\n",
    "            m_lag = re.search(r'lag(\\d+)s', col)\n",
    "            m_beam = re.search(r'BEAM(\\d+)', col)\n",
    "            if m_lag and m_beam:\n",
    "                lag_s = int(m_lag.group(1))\n",
    "                beam = int(m_beam.group(1))\n",
    "                lag_info.append({\n",
    "                    'column': col,\n",
    "                    'lag_s': lag_s,\n",
    "                    'beam': beam\n",
    "                })\n",
    "        \n",
    "        df_lag_info = pd.DataFrame(lag_info)\n",
    "        lags_sorted = sorted(df_lag_info['lag_s'].unique())\n",
    "        beams = sorted(df_lag_info['beam'].unique())\n",
    "        \n",
    "        print(f\"üìå Lags detectados: {lags_sorted[0]}s - {lags_sorted[-1]}s ({len(lags_sorted)} lags)\")\n",
    "        print(f\"üìå BEAMs detectados: {beams}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 2) Cargar √∫ltimas n_samples filas\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[1/3] Cargando √∫ltimas {n_samples} filas...\")\n",
    "        \n",
    "        # Determinar n√∫mero total de filas\n",
    "        total_rows = sum(1 for _ in open(complete_dataset_path)) - 1  # -1 para header\n",
    "        start_row = max(1, total_rows - n_samples)\n",
    "        \n",
    "        df_sample = pd.read_csv(\n",
    "            complete_dataset_path,\n",
    "            usecols=vlos_lag_cols,\n",
    "            skiprows=range(1, start_row)\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Cargado: {df_sample.shape[0]:,} filas, {df_sample.shape[1]} columnas\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 3) Crear heatmaps por BEAM\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[2/3] Generando heatmaps de evoluci√≥n temporal...\")\n",
    "        \n",
    "        for beam in beams:\n",
    "            # Filtrar columnas de este BEAM\n",
    "            beam_cols = df_lag_info[df_lag_info['beam'] == beam].sort_values('lag_s')\n",
    "            \n",
    "            if beam_cols.empty:\n",
    "                continue\n",
    "            \n",
    "            # Crear matriz: filas = lags (ordenados), columnas = tiempo\n",
    "            data_matrix = []\n",
    "            lag_labels = []\n",
    "            \n",
    "            for _, row in beam_cols.iterrows():\n",
    "                col_name = row['column']\n",
    "                lag_s = row['lag_s']\n",
    "                \n",
    "                if col_name in df_sample.columns:\n",
    "                    data_matrix.append(df_sample[col_name].values)\n",
    "                    lag_labels.append(f\"{lag_s}s\")\n",
    "            \n",
    "            if not data_matrix:\n",
    "                continue\n",
    "            \n",
    "            data_matrix = np.array(data_matrix)  # shape: (n_lags, n_samples)\n",
    "            \n",
    "            # Crear heatmap\n",
    "            fig, ax = plt.subplots(figsize=(18, 10))\n",
    "            \n",
    "            im = ax.imshow(\n",
    "                data_matrix,\n",
    "                aspect='auto',\n",
    "                cmap='RdYlBu_r',\n",
    "                interpolation='nearest',\n",
    "                origin='lower'\n",
    "            )\n",
    "            \n",
    "            # Configurar ejes\n",
    "            ax.set_xlabel('Tiempo (muestras) ‚Üí', fontsize=13, fontweight='bold')\n",
    "            ax.set_ylabel('Lag (segundos) ‚Üë [memoria antigua ‚Üê reciente]', fontsize=13, fontweight='bold')\n",
    "            ax.set_title(f'Evoluci√≥n Temporal de R√°fagas - BEAM {beam}\\n(R√°fagas \"envejecen\" diagonal ‚ÜóÔ∏è: abajo-izq ‚Üí arriba-der)', \n",
    "                        fontsize=15, fontweight='bold')\n",
    "            \n",
    "            # Etiquetas eje Y (lags)\n",
    "            ax.set_yticks(range(len(lag_labels)))\n",
    "            ax.set_yticklabels(lag_labels)\n",
    "            \n",
    "            # A√±adir l√≠nea diagonal de referencia para visualizar mejor\n",
    "            n_points_diag = min(len(df_sample), len(lag_labels))\n",
    "            if n_points_diag > 10:\n",
    "                # Diagonal que sube de izquierda a derecha\n",
    "                x_diag = np.linspace(0, len(df_sample)-1, n_points_diag)\n",
    "                y_diag = np.linspace(0, len(lag_labels)-1, n_points_diag)\n",
    "                ax.plot(x_diag, y_diag, 'w--', linewidth=2, alpha=0.6, label='Trayectoria te√≥rica ‚ÜóÔ∏è')\n",
    "            \n",
    "            # Etiquetas eje X (tiempo) - mostrar cada N muestras\n",
    "            x_ticks_step = max(1, len(df_sample) // 10)\n",
    "            x_ticks = range(0, len(df_sample), x_ticks_step)\n",
    "            ax.set_xticks(x_ticks)\n",
    "            ax.set_xticklabels([str(i) for i in x_ticks])\n",
    "            \n",
    "            # Colorbar\n",
    "            cbar = plt.colorbar(im, ax=ax, label='Velocidad Viento (m/s)')\n",
    "            cbar.ax.tick_params(labelsize=11)\n",
    "            \n",
    "            # Grid sutil para visualizar mejor\n",
    "            ax.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)\n",
    "            ax.legend(loc='upper left', fontsize=10, framealpha=0.8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Guardar\n",
    "            plot_path = wind_evolution_folder / f\"wind_evolution_heatmap_BEAM{beam}.png\"\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  ‚úÖ BEAM {beam}: {plot_path.name}\")\n",
    "            \n",
    "            # Guardar CSV de datos utilizados\n",
    "            df_beam_data = df_sample[[col for col in beam_cols['column'] if col in df_sample.columns]]\n",
    "            csv_path = wind_evolution_folder / f\"wind_evolution_data_BEAM{beam}.csv\"\n",
    "            df_beam_data.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 4) Crear heatmap promedio (todos los BEAMs)\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[3/3] Generando heatmap promedio (todos los BEAMs)...\")\n",
    "        \n",
    "        # Para cada lag, promediar todos los BEAMs\n",
    "        avg_data_matrix = []\n",
    "        avg_lag_labels = []\n",
    "        \n",
    "        for lag_s in lags_sorted:\n",
    "            lag_cols = df_lag_info[df_lag_info['lag_s'] == lag_s]['column'].tolist()\n",
    "            valid_cols = [c for c in lag_cols if c in df_sample.columns]\n",
    "            \n",
    "            if valid_cols:\n",
    "                avg_signal = df_sample[valid_cols].mean(axis=1).values\n",
    "                avg_data_matrix.append(avg_signal)\n",
    "                avg_lag_labels.append(f\"{lag_s}s\")\n",
    "        \n",
    "        if avg_data_matrix:\n",
    "            avg_data_matrix = np.array(avg_data_matrix)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(18, 10))\n",
    "            \n",
    "            im = ax.imshow(\n",
    "                avg_data_matrix,\n",
    "                aspect='auto',\n",
    "                cmap='RdYlBu_r',\n",
    "                interpolation='nearest',\n",
    "                origin='lower'\n",
    "            )\n",
    "            \n",
    "            ax.set_xlabel('Tiempo (muestras) ‚Üí', fontsize=13, fontweight='bold')\n",
    "            ax.set_ylabel('Lag (segundos) ‚Üë [memoria antigua ‚Üê reciente]', fontsize=13, fontweight='bold')\n",
    "            ax.set_title('Evoluci√≥n Temporal de R√°fagas - Promedio Todos los BEAMs\\n(R√°fagas \"envejecen\" diagonal ‚ÜóÔ∏è: abajo-izq ‚Üí arriba-der)', \n",
    "                        fontsize=15, fontweight='bold')\n",
    "            \n",
    "            ax.set_yticks(range(len(avg_lag_labels)))\n",
    "            ax.set_yticklabels(avg_lag_labels)\n",
    "            \n",
    "            # A√±adir l√≠nea diagonal de referencia\n",
    "            n_points_diag = min(len(df_sample), len(avg_lag_labels))\n",
    "            if n_points_diag > 10:\n",
    "                x_diag = np.linspace(0, len(df_sample)-1, n_points_diag)\n",
    "                y_diag = np.linspace(0, len(avg_lag_labels)-1, n_points_diag)\n",
    "                ax.plot(x_diag, y_diag, 'w--', linewidth=2, alpha=0.6, label='Trayectoria te√≥rica ‚ÜóÔ∏è')\n",
    "            \n",
    "            x_ticks_step = max(1, len(df_sample) // 10)\n",
    "            x_ticks = range(0, len(df_sample), x_ticks_step)\n",
    "            ax.set_xticks(x_ticks)\n",
    "            ax.set_xticklabels([str(i) for i in x_ticks])\n",
    "            \n",
    "            cbar = plt.colorbar(im, ax=ax, label='Velocidad Viento (m/s)')\n",
    "            cbar.ax.tick_params(labelsize=11)\n",
    "            \n",
    "            ax.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)\n",
    "            ax.legend(loc='upper left', fontsize=10, framealpha=0.8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path = wind_evolution_folder / \"wind_evolution_heatmap_ALL_BEAMS_avg.png\"\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  ‚úÖ Promedio: {plot_path.name}\")\n",
    "            \n",
    "            # CSV promedio\n",
    "            df_avg = pd.DataFrame(\n",
    "                avg_data_matrix.T,\n",
    "                columns=avg_lag_labels\n",
    "            )\n",
    "            csv_path = wind_evolution_folder / \"wind_evolution_data_ALL_BEAMS_avg.csv\"\n",
    "            df_avg.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # RESUMEN\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"RESUMEN - EVOLUCI√ìN TEMPORAL DE R√ÅFAGAS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Muestras analizadas:  {len(df_sample):,}\")\n",
    "        print(f\"Lags visualizados:    {len(lags_sorted)} ({lags_sorted[0]}s - {lags_sorted[-1]}s)\")\n",
    "        print(f\"BEAMs analizados:     {len(beams)}\")\n",
    "        \n",
    "        print(f\"\\nüéØ INTERPRETACI√ìN:\")\n",
    "        print(\"   - Patrones DIAGONALES ‚ÜóÔ∏è (abajo-izq ‚Üí arriba-der) indican advecci√≥n temporal\")\n",
    "        print(\"   - Una r√°faga aparece en lag=5s (abajo) y 'envejece' subiendo a lag=25s (arriba)\")\n",
    "        print(\"   - F√≠sica: lag5s='hace 5s', lag10s='hace 10s' ‚Üí memoria reciente a antigua\")\n",
    "        print(\"   - Diagonales claras = coherencia temporal del LIDAR\")\n",
    "        print(\"   - Patrones verticales = transitorios/cambios abruptos\")\n",
    "        \n",
    "        print(f\"\\nüìä VISUALIZACI√ìN:\")\n",
    "        print(\"   - Rojo = viento alto (r√°faga)\")\n",
    "        print(\"   - Azul = viento bajo\")\n",
    "        print(\"   - Inclinaci√≥n diagonal ~ dt/dlag\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Archivos generados:\")\n",
    "        print(f\"  - Heatmaps por BEAM: wind_evolution_heatmap_BEAMX.png ({len(beams)} archivos)\")\n",
    "        print(f\"  - Heatmap promedio: wind_evolution_heatmap_ALL_BEAMS_avg.png\")\n",
    "        print(f\"  - CSVs con datos: wind_evolution_data_*.csv\")\n",
    "        print(f\"\\nüìç Ubicaci√≥n: {wind_evolution_folder}\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n‚úÖ An√°lisis completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e95075",
   "metadata": {},
   "source": [
    "### üå¨Ô∏è PASO 4.11.b: Evoluci√≥n Espacial de R√°fagas (Distancia al Rotor)\n",
    "\n",
    "En este an√°lisis visualizamos c√≥mo una r√°faga de viento se propaga desde la ubicaci√≥n del LIDAR (100m upwind) hasta el rotor.\n",
    "\n",
    "**M√©todo:**\n",
    "- Seleccionamos las √∫ltimas 2000 muestras del dataset completo\n",
    "- En el **Eje Y**: Distancia al rotor (m), donde:\n",
    "  - 100m = se√±al VLOS sin lag (medida en ubicaci√≥n del LIDAR)\n",
    "  - ~0m = llegada al rotor (calculada con advecci√≥n seg√∫n velocidad del viento)\n",
    "- En el **Eje X**: Tiempo (√≠ndice de muestra)\n",
    "- **Color**: Velocidad del viento\n",
    "\n",
    "**Interpretaci√≥n f√≠sica:**\n",
    "- lag = 0s ‚Üí d = 100m (LIDAR mide a 100m upwind)\n",
    "- lag = 100/U ‚Üí d ‚âà 0m (viento llega al rotor)\n",
    "- Para cada lag intermedio: distancia = 100 - U √ó lag\n",
    "\n",
    "**Patr√≥n esperado:**\n",
    "- Diagonales descendentes ‚ÜòÔ∏è (arriba-izq ‚Üí abajo-der)\n",
    "- Una r√°faga medida a 100m \"viaja\" hacia el rotor (d‚Üí0) conforme pasa el tiempo\n",
    "- Esto muestra la propagaci√≥n f√≠sica del viento desde el punto de medici√≥n hasta la turbina\n",
    "\n",
    "**Outputs (carpeta `06_Wind_Evolution`):**\n",
    "- Heatmaps espaciales por BEAM y promedio\n",
    "- CSV con datos de evoluci√≥n espacial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 4.11.b: EVOLUCI√ìN ESPACIAL DE R√ÅFAGAS (DISTANCIA AL ROTOR)\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üå¨Ô∏è EVOLUCI√ìN ESPACIAL DE R√ÅFAGAS (100m ‚Üí ROTOR)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {wind_evolution_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Par√°metros\n",
    "n_samples_spatial = 2000  # √öltimas N filas\n",
    "distance_lidar_m = 100.0  # Distancia del LIDAR al rotor (m)\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Cargar columnas VLOS con y sin lag + calcular velocidad promedio\n",
    "    # ------------------------------------------------------------------------\n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "    \n",
    "    # Columnas VLOS con lag\n",
    "    vlos_lag_cols = [c for c in all_columns if 'LAC_VLOS' in c and 'lag' in c]\n",
    "    # Columnas VLOS sin lag (medici√≥n en 100m)\n",
    "    vlos_base_cols = [c for c in all_columns if 'LAC_VLOS' in c and 'lag' not in c]\n",
    "    \n",
    "    if not vlos_lag_cols or not vlos_base_cols:\n",
    "        print(\"\\n‚ùå ERROR: No se encontraron columnas VLOS necesarias\")\n",
    "    else:\n",
    "        print(f\"\\nüìå Columnas VLOS con lag: {len(vlos_lag_cols)}\")\n",
    "        print(f\"üìå Columnas VLOS sin lag: {len(vlos_base_cols)}\")\n",
    "        \n",
    "        # Extraer informaci√≥n de lags\n",
    "        lag_info = []\n",
    "        for col in vlos_lag_cols:\n",
    "            m_lag = re.search(r'lag(\\d+)s', col)\n",
    "            m_beam = re.search(r'BEAM(\\d+)', col)\n",
    "            if m_lag and m_beam:\n",
    "                lag_s = int(m_lag.group(1))\n",
    "                beam = int(m_beam.group(1))\n",
    "                lag_info.append({\n",
    "                    'column': col,\n",
    "                    'lag_s': lag_s,\n",
    "                    'beam': beam\n",
    "                })\n",
    "        \n",
    "        # Informaci√≥n de VLOS sin lag\n",
    "        base_info = []\n",
    "        for col in vlos_base_cols:\n",
    "            m_beam = re.search(r'BEAM(\\d+)', col)\n",
    "            if m_beam:\n",
    "                beam = int(m_beam.group(1))\n",
    "                base_info.append({\n",
    "                    'column': col,\n",
    "                    'lag_s': 0,  # lag = 0 para sin lag\n",
    "                    'beam': beam\n",
    "                })\n",
    "        \n",
    "        # Combinar\n",
    "        all_info = base_info + lag_info\n",
    "        df_all_info = pd.DataFrame(all_info)\n",
    "        \n",
    "        lags_sorted = sorted(df_all_info['lag_s'].unique())\n",
    "        beams = sorted(df_all_info['beam'].unique())\n",
    "        \n",
    "        print(f\"üìå Lags (con 0s=sin lag): {lags_sorted[0]}s - {lags_sorted[-1]}s ({len(lags_sorted)} lags)\")\n",
    "        print(f\"üìå BEAMs detectados: {beams}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 2) Cargar √∫ltimas n_samples_spatial filas\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[1/3] Cargando √∫ltimas {n_samples_spatial} filas...\")\n",
    "        \n",
    "        all_vlos_cols = vlos_base_cols + vlos_lag_cols\n",
    "        \n",
    "        # Determinar n√∫mero total de filas\n",
    "        total_rows = sum(1 for _ in open(complete_dataset_path)) - 1\n",
    "        start_row = max(1, total_rows - n_samples_spatial)\n",
    "        \n",
    "        df_sample = pd.read_csv(\n",
    "            complete_dataset_path,\n",
    "            usecols=all_vlos_cols,\n",
    "            skiprows=range(1, start_row)\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Cargado: {df_sample.shape[0]:,} filas, {df_sample.shape[1]} columnas\")\n",
    "        \n",
    "        # Calcular velocidad promedio de viento (sin lag) por fila\n",
    "        wind_mean = df_sample[vlos_base_cols].mean(axis=1).values\n",
    "        print(f\"‚úÖ Velocidad promedio: {wind_mean.min():.2f} - {wind_mean.max():.2f} m/s\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 3) Crear heatmaps espaciales por BEAM\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[2/3] Generando heatmaps espaciales por BEAM...\")\n",
    "        \n",
    "        for beam in beams:\n",
    "            # Filtrar columnas de este BEAM, ordenar por lag\n",
    "            beam_data = df_all_info[df_all_info['beam'] == beam].sort_values('lag_s')\n",
    "            \n",
    "            if beam_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Crear matriz de datos y calcular distancias\n",
    "            data_matrix = []\n",
    "            distance_labels = []\n",
    "            \n",
    "            for _, row in beam_data.iterrows():\n",
    "                col_name = row['column']\n",
    "                lag_s = row['lag_s']\n",
    "                \n",
    "                if col_name in df_sample.columns:\n",
    "                    data_matrix.append(df_sample[col_name].values)\n",
    "                    \n",
    "                    # Calcular distancia promedio (d = 100 - U*lag)\n",
    "                    # Usar velocidad promedio del periodo\n",
    "                    avg_wind = wind_mean.mean()\n",
    "                    distance_m = distance_lidar_m - avg_wind * lag_s\n",
    "                    distance_m = max(0, distance_m)  # No negativo\n",
    "                    distance_labels.append(distance_m)\n",
    "            \n",
    "            if not data_matrix:\n",
    "                continue\n",
    "            \n",
    "            data_matrix = np.array(data_matrix)  # shape: (n_distances, n_samples)\n",
    "            distance_labels = np.array(distance_labels)\n",
    "            \n",
    "            # Invertir para que 100m est√© arriba y 0m abajo\n",
    "            data_matrix = data_matrix[::-1]\n",
    "            distance_labels = distance_labels[::-1]\n",
    "            \n",
    "            # Crear heatmap\n",
    "            fig, ax = plt.subplots(figsize=(18, 10))\n",
    "            \n",
    "            im = ax.imshow(\n",
    "                data_matrix,\n",
    "                aspect='auto',\n",
    "                cmap='RdYlBu_r',\n",
    "                interpolation='nearest',\n",
    "                origin='lower',\n",
    "                extent=[0, len(df_sample), distance_labels.min(), distance_labels.max()]\n",
    "            )\n",
    "            \n",
    "            # Configurar ejes\n",
    "            ax.set_xlabel('Tiempo (muestras) ‚Üí', fontsize=13, fontweight='bold')\n",
    "            ax.set_ylabel('Distancia al rotor (m) ‚Üì [100m LIDAR ‚Üí 0m rotor]', fontsize=13, fontweight='bold')\n",
    "            ax.set_title(f'Propagaci√≥n Espacial de R√°fagas - BEAM {beam}\\n(R√°fagas viajan hacia el rotor ‚ÜòÔ∏è: arriba-izq ‚Üí abajo-der)', \n",
    "                        fontsize=15, fontweight='bold')\n",
    "            \n",
    "            # Etiquetas eje Y (distancias)\n",
    "            n_yticks = min(10, len(distance_labels))\n",
    "            y_indices = np.linspace(0, len(distance_labels)-1, n_yticks, dtype=int)\n",
    "            ax.set_yticks(distance_labels[y_indices])\n",
    "            ax.set_yticklabels([f\"{d:.0f}m\" for d in distance_labels[y_indices]])\n",
    "            \n",
    "            # A√±adir l√≠nea diagonal de referencia (descendente)\n",
    "            n_points_diag = min(len(df_sample), len(distance_labels))\n",
    "            if n_points_diag > 10:\n",
    "                x_diag = np.linspace(0, len(df_sample)-1, n_points_diag)\n",
    "                y_diag = np.linspace(distance_labels.max(), distance_labels.min(), n_points_diag)\n",
    "                ax.plot(x_diag, y_diag, 'w--', linewidth=2, alpha=0.6, label='Trayectoria te√≥rica ‚ÜòÔ∏è')\n",
    "            \n",
    "            # Etiquetas eje X\n",
    "            x_ticks_step = max(1, len(df_sample) // 10)\n",
    "            x_ticks = range(0, len(df_sample), x_ticks_step)\n",
    "            ax.set_xticks(x_ticks)\n",
    "            ax.set_xticklabels([str(i) for i in x_ticks])\n",
    "            \n",
    "            # Colorbar\n",
    "            cbar = plt.colorbar(im, ax=ax, label='Velocidad Viento (m/s)')\n",
    "            cbar.ax.tick_params(labelsize=11)\n",
    "            \n",
    "            # Grid y leyenda\n",
    "            ax.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)\n",
    "            ax.legend(loc='upper right', fontsize=10, framealpha=0.8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Guardar\n",
    "            plot_path = wind_evolution_folder / f\"wind_spatial_evolution_BEAM{beam}.png\"\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  ‚úÖ BEAM {beam}: {plot_path.name}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 4) Crear heatmap espacial promedio (todos los BEAMs)\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[3/3] Generando heatmap espacial promedio...\")\n",
    "        \n",
    "        # Promediar por lag a trav√©s de todos los BEAMs\n",
    "        avg_data_matrix = []\n",
    "        avg_distance_labels = []\n",
    "        \n",
    "        for lag_s in lags_sorted:\n",
    "            lag_cols = df_all_info[df_all_info['lag_s'] == lag_s]['column'].tolist()\n",
    "            valid_cols = [c for c in lag_cols if c in df_sample.columns]\n",
    "            \n",
    "            if valid_cols:\n",
    "                avg_signal = df_sample[valid_cols].mean(axis=1).values\n",
    "                avg_data_matrix.append(avg_signal)\n",
    "                \n",
    "                # Distancia promedio para este lag\n",
    "                avg_wind = wind_mean.mean()\n",
    "                distance_m = distance_lidar_m - avg_wind * lag_s\n",
    "                distance_m = max(0, distance_m)\n",
    "                avg_distance_labels.append(distance_m)\n",
    "        \n",
    "        if avg_data_matrix:\n",
    "            avg_data_matrix = np.array(avg_data_matrix)\n",
    "            avg_distance_labels = np.array(avg_distance_labels)\n",
    "            \n",
    "            # Invertir para que 100m est√© arriba\n",
    "            avg_data_matrix = avg_data_matrix[::-1]\n",
    "            avg_distance_labels = avg_distance_labels[::-1]\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(18, 10))\n",
    "            \n",
    "            im = ax.imshow(\n",
    "                avg_data_matrix,\n",
    "                aspect='auto',\n",
    "                cmap='RdYlBu_r',\n",
    "                interpolation='nearest',\n",
    "                origin='lower',\n",
    "                extent=[0, len(df_sample), avg_distance_labels.min(), avg_distance_labels.max()]\n",
    "            )\n",
    "            \n",
    "            ax.set_xlabel('Tiempo (muestras) ‚Üí', fontsize=13, fontweight='bold')\n",
    "            ax.set_ylabel('Distancia al rotor (m) ‚Üì [100m LIDAR ‚Üí 0m rotor]', fontsize=13, fontweight='bold')\n",
    "            ax.set_title('Propagaci√≥n Espacial de R√°fagas - Promedio Todos los BEAMs\\n(R√°fagas viajan hacia el rotor ‚ÜòÔ∏è: arriba-izq ‚Üí abajo-der)', \n",
    "                        fontsize=15, fontweight='bold')\n",
    "            \n",
    "            # Etiquetas eje Y\n",
    "            n_yticks = min(10, len(avg_distance_labels))\n",
    "            y_indices = np.linspace(0, len(avg_distance_labels)-1, n_yticks, dtype=int)\n",
    "            ax.set_yticks(avg_distance_labels[y_indices])\n",
    "            ax.set_yticklabels([f\"{d:.0f}m\" for d in avg_distance_labels[y_indices]])\n",
    "            \n",
    "            # Diagonal descendente\n",
    "            n_points_diag = min(len(df_sample), len(avg_distance_labels))\n",
    "            if n_points_diag > 10:\n",
    "                x_diag = np.linspace(0, len(df_sample)-1, n_points_diag)\n",
    "                y_diag = np.linspace(avg_distance_labels.max(), avg_distance_labels.min(), n_points_diag)\n",
    "                ax.plot(x_diag, y_diag, 'w--', linewidth=2, alpha=0.6, label='Trayectoria te√≥rica ‚ÜòÔ∏è')\n",
    "            \n",
    "            # Etiquetas eje X\n",
    "            x_ticks_step = max(1, len(df_sample) // 10)\n",
    "            x_ticks = range(0, len(df_sample), x_ticks_step)\n",
    "            ax.set_xticks(x_ticks)\n",
    "            ax.set_xticklabels([str(i) for i in x_ticks])\n",
    "            \n",
    "            cbar = plt.colorbar(im, ax=ax, label='Velocidad Viento (m/s)')\n",
    "            cbar.ax.tick_params(labelsize=11)\n",
    "            \n",
    "            ax.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)\n",
    "            ax.legend(loc='upper right', fontsize=10, framealpha=0.8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path = wind_evolution_folder / \"wind_spatial_evolution_ALL_BEAMS_avg.png\"\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  ‚úÖ Promedio: {plot_path.name}\")\n",
    "            \n",
    "            # CSV con datos espaciales\n",
    "            df_spatial = pd.DataFrame(\n",
    "                avg_data_matrix.T,\n",
    "                columns=[f\"{d:.1f}m\" for d in avg_distance_labels]\n",
    "            )\n",
    "            csv_path = wind_evolution_folder / \"wind_spatial_evolution_data.csv\"\n",
    "            df_spatial.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # RESUMEN\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"RESUMEN - EVOLUCI√ìN ESPACIAL DE R√ÅFAGAS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Muestras analizadas:  {len(df_sample):,}\")\n",
    "        print(f\"Distancia LIDAR:      {distance_lidar_m:.0f} m\")\n",
    "        print(f\"Velocidad promedio:   {wind_mean.mean():.2f} m/s\")\n",
    "        print(f\"BEAMs analizados:     {len(beams)}\")\n",
    "        \n",
    "        print(f\"\\nüéØ INTERPRETACI√ìN:\")\n",
    "        print(\"   - Patrones DIAGONALES ‚ÜòÔ∏è (arriba-izq ‚Üí abajo-der) muestran propagaci√≥n espacial\")\n",
    "        print(\"   - Una r√°faga medida a 100m (LIDAR) viaja hacia el rotor (0m)\")\n",
    "        print(\"   - F√≠sica: distancia = 100 - U*lag, donde U = velocidad del viento\")\n",
    "        print(\"   - Diagonales claras = advecci√≥n consistente desde LIDAR a turbina\")\n",
    "        print(\"   - Velocidad de ca√≠da diagonal ~ velocidad del viento\")\n",
    "        \n",
    "        print(f\"\\nüìä VISUALIZACI√ìN:\")\n",
    "        print(\"   - Rojo = viento alto (r√°faga)\")\n",
    "        print(\"   - Azul = viento bajo\")\n",
    "        print(\"   - Diagonal descendente muestra propagaci√≥n f√≠sica del viento\")\n",
    "        print(\"   - Inclinaci√≥n diagonal ~ 1/velocidad del viento\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Archivos generados:\")\n",
    "        print(f\"  - Heatmaps espaciales por BEAM: wind_spatial_evolution_BEAMX.png\")\n",
    "        print(f\"  - Heatmap espacial promedio: wind_spatial_evolution_ALL_BEAMS_avg.png\")\n",
    "        print(f\"  - CSV: wind_spatial_evolution_data.csv\")\n",
    "        print(f\"\\nüìç Ubicaci√≥n: {wind_evolution_folder}\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n‚úÖ An√°lisis espacial completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9fafaf",
   "metadata": {},
   "source": [
    "### üåÄ PASO 4.12: Intensidad de Turbulencia vs Variabilidad de Cargas\n",
    "\n",
    "Hasta ahora hemos analizado **correlaciones promedio** entre viento y cargas. Sin embargo, en fatiga de palas, lo cr√≠tico no es la media sino la **desviaci√≥n est√°ndar** (variabilidad/picos).\n",
    "\n",
    "**Hip√≥tesis f√≠sica:**\n",
    "- A la misma velocidad de viento media (ej. 12 m/s), una turbulencia alta genera **picos de carga mucho mayores** que una baja\n",
    "- Mayor turbulencia ‚Üí mayor \"ruido\" en las cargas ‚Üí mayor da√±o por fatiga\n",
    "\n",
    "**M√©todo:**\n",
    "1. Dividir la serie temporal en **ventanas de 10 minutos** (tiempo=25s a 600s, luego 600s a 1175s, etc.)\n",
    "2. Para cada ventana, calcular:\n",
    "   - **TI (Turbulence Intensity)**: $\\text{TI} = \\frac{\\sigma_{\\text{viento}}}{\\mu_{\\text{viento}}}$\n",
    "   - **std(Blade root My)**: Desviaci√≥n est√°ndar de la carga en esa ventana\n",
    "   - **mean(viento)**: Velocidad promedio del viento (para colorear)\n",
    "3. Crear gr√°fico de dispersi√≥n: TI (X) vs std(carga) (Y), coloreado por velocidad media\n",
    "\n",
    "**Valor esperado:**\n",
    "- **Correlaci√≥n positiva fuerte**: A mayor TI, mayor std(carga)\n",
    "- Esto valida que el modelo debe capturar **varianza**, no solo media\n",
    "- Punto a favor para redes neuronales probabil√≠sticas o Loss Functions que penalicen outliers\n",
    "\n",
    "**Outputs (carpeta `07_Turbulence`):**\n",
    "- Scatter plots TI vs std(carga) para ambas palas\n",
    "- CSV con TI, std(carga) y velocidad media por ventana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 4.12: INTENSIDAD DE TURBULENCIA VS VARIABILIDAD DE CARGAS\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Crear carpeta para resultados\n",
    "turbulence_folder = eda_folder / \"07_Turbulence\"\n",
    "turbulence_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üåÄ INTENSIDAD DE TURBULENCIA VS VARIABILIDAD DE CARGAS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta destino: {turbulence_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Par√°metros\n",
    "window_start = 25      # Inicio de la primera ventana (s)\n",
    "window_duration = 575  # Duraci√≥n de cada ventana: 600 - 25 = 575s (~10 min)\n",
    "chunk_size = 600       # Tama√±o total del chunk (s)\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Verificar columnas necesarias\n",
    "    # ------------------------------------------------------------------------\n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "    \n",
    "    # Verificar columnas necesarias\n",
    "    required_cols = ['Time', 'Blade root 1 My', 'Blade root 2 My']\n",
    "    missing_cols = [c for c in required_cols if c not in all_columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\n‚ùå ERROR: Faltan columnas necesarias: {missing_cols}\")\n",
    "    else:\n",
    "        # Columnas VLOS sin lag (para calcular TI del viento)\n",
    "        vlos_base_cols = [c for c in all_columns if 'LAC_VLOS' in c and 'lag' not in c]\n",
    "        \n",
    "        if not vlos_base_cols:\n",
    "            print(\"\\n‚ùå ERROR: No se encontraron columnas VLOS sin lag para calcular TI\")\n",
    "        else:\n",
    "            print(f\"\\nüìå Columnas VLOS sin lag detectadas: {len(vlos_base_cols)}\")\n",
    "            print(f\"üìå Ventana de an√°lisis: {window_duration}s (~{window_duration/60:.1f} min)\")\n",
    "            print(f\"üìå Periodo total por chunk: {chunk_size}s\")\n",
    "            \n",
    "            # ------------------------------------------------------------------------\n",
    "            # 2) Leer datos y detectar reinicios de series temporales\n",
    "            # ------------------------------------------------------------------------\n",
    "            print(f\"\\n[1/3] Leyendo datos y detectando series temporales concatenadas...\")\n",
    "            \n",
    "            usecols = ['Time'] + vlos_base_cols + ['Blade root 1 My', 'Blade root 2 My']\n",
    "            \n",
    "            # Leer todo el dataset\n",
    "            df = pd.read_csv(complete_dataset_path, usecols=usecols)\n",
    "            \n",
    "            print(f\"‚úÖ Cargado: {len(df):,} filas\")\n",
    "            \n",
    "            # Detectar reinicios de tiempo (cuando Time disminuye)\n",
    "            time_diff = df['Time'].diff()\n",
    "            restarts = np.where(time_diff < 0)[0]  # √çndices donde el tiempo reinicia\n",
    "            \n",
    "            # Crear segmentos (cada segmento es una serie temporal continua)\n",
    "            segment_starts = [0] + restarts.tolist()\n",
    "            segment_ends = restarts.tolist() + [len(df)]\n",
    "            \n",
    "            print(f\"üìå Series temporales detectadas: {len(segment_starts)}\")\n",
    "            print(f\"   Cada serie va de Time={window_start}s a ~{chunk_size}s\")\n",
    "            \n",
    "            # ------------------------------------------------------------------------\n",
    "            # 3) Procesar cada segmento independientemente\n",
    "            # ------------------------------------------------------------------------\n",
    "            print(f\"\\n[2/3] Procesando cada serie temporal (ventanas {window_start}s-{window_start+window_duration}s)...\")\n",
    "            \n",
    "            results = []\n",
    "            \n",
    "            for seg_idx, (start_idx, end_idx) in enumerate(zip(segment_starts, segment_ends)):\n",
    "                df_segment = df.iloc[start_idx:end_idx].copy()\n",
    "                \n",
    "                # Verificar que el segmento tiene datos en el rango de inter√©s\n",
    "                if df_segment['Time'].min() > window_start + window_duration:\n",
    "                    continue  # Este segmento no tiene datos √∫tiles\n",
    "                if df_segment['Time'].max() < window_start:\n",
    "                    continue\n",
    "                \n",
    "                # Filtrar ventana de inter√©s: del segundo 25 al 600\n",
    "                mask = (df_segment['Time'] >= window_start) & (df_segment['Time'] <= window_start + window_duration)\n",
    "                df_window = df_segment[mask]\n",
    "                \n",
    "                if len(df_window) < 50:  # M√≠nimo de muestras para c√°lculo confiable\n",
    "                    continue\n",
    "                \n",
    "                # Calcular viento promedio (media de todos los BEAMs)\n",
    "                wind_values = df_window[vlos_base_cols].values\n",
    "                wind_mean_per_sample = np.nanmean(wind_values, axis=1)\n",
    "                \n",
    "                # TI: std(viento) / mean(viento)\n",
    "                wind_mean = np.nanmean(wind_mean_per_sample)\n",
    "                wind_std = np.nanstd(wind_mean_per_sample)\n",
    "                \n",
    "                if wind_mean > 0.1:  # Evitar divisi√≥n por cero\n",
    "                    TI = wind_std / wind_mean\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Desviaci√≥n est√°ndar de las cargas\n",
    "                load1_std = df_window['Blade root 1 My'].std()\n",
    "                load2_std = df_window['Blade root 2 My'].std()\n",
    "                \n",
    "                # Desviaci√≥n est√°ndar de las se√±ales VLOS (variabilidad espacial del viento)\n",
    "                # Para cada muestra temporal, calcular std entre los BEAMs\n",
    "                vlos_std_per_sample = np.nanstd(wind_values, axis=1)\n",
    "                vlos_std_mean = np.nanmean(vlos_std_per_sample)\n",
    "                \n",
    "                # Guardar resultados\n",
    "                results.append({\n",
    "                    'Segment_ID': seg_idx,\n",
    "                    'Time_Start_s': df_window['Time'].min(),\n",
    "                    'Time_End_s': df_window['Time'].max(),\n",
    "                    'Wind_Mean_ms': wind_mean,\n",
    "                    'Wind_Std_ms': wind_std,\n",
    "                    'VLOS_Std_Spatial_ms': vlos_std_mean,\n",
    "                    'Turbulence_Intensity_TI': TI,\n",
    "                    'Blade1_My_Std': load1_std,\n",
    "                    'Blade2_My_Std': load2_std,\n",
    "                    'N_Samples': len(df_window)\n",
    "                })\n",
    "                \n",
    "                if (seg_idx + 1) % 100 == 0:\n",
    "                    print(f\"  ... procesados {seg_idx + 1} segmentos, {len(results)} ventanas v√°lidas\")\n",
    "            \n",
    "            if not results:\n",
    "                print(\"\\n‚ö†Ô∏è No se obtuvieron ventanas v√°lidas\")\n",
    "            else:\n",
    "                df_results = pd.DataFrame(results)\n",
    "                print(f\"\\n‚úÖ Total series temporales analizadas: {len(df_results):,}\")\n",
    "                print(f\"   TI rango: {df_results['Turbulence_Intensity_TI'].min():.3f} - {df_results['Turbulence_Intensity_TI'].max():.3f}\")\n",
    "                print(f\"   Viento medio rango: {df_results['Wind_Mean_ms'].min():.2f} - {df_results['Wind_Mean_ms'].max():.2f} m/s\")\n",
    "                \n",
    "                # ------------------------------------------------------------------------\n",
    "                # 4) Guardar CSV\n",
    "                # ------------------------------------------------------------------------\n",
    "                csv_path = turbulence_folder / \"turbulence_intensity_vs_load_variability.csv\"\n",
    "                df_results.to_csv(csv_path, index=False)\n",
    "                print(f\"\\n[3/3] CSV guardado: {csv_path.name}\")\n",
    "                \n",
    "                # ------------------------------------------------------------------------\n",
    "                # 5) Crear gr√°ficos de dispersi√≥n\n",
    "                # ------------------------------------------------------------------------\n",
    "                print(f\"\\nGenerando gr√°ficos de dispersi√≥n...\")\n",
    "                \n",
    "                for blade_num, blade_col in [(1, 'Blade1_My_Std'), (2, 'Blade2_My_Std')]:\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    \n",
    "                    # Scatter plot con color por velocidad de viento\n",
    "                    scatter = ax.scatter(\n",
    "                        df_results['Turbulence_Intensity_TI'],\n",
    "                        df_results[blade_col],\n",
    "                        c=df_results['Wind_Mean_ms'],\n",
    "                        cmap='viridis',\n",
    "                        s=30,\n",
    "                        alpha=0.6,\n",
    "                        edgecolors='k',\n",
    "                        linewidth=0.3\n",
    "                    )\n",
    "                    \n",
    "                    # Colorbar\n",
    "                    cbar = plt.colorbar(scatter, ax=ax, label='Velocidad Viento Media (m/s)')\n",
    "                    cbar.ax.tick_params(labelsize=10)\n",
    "                    \n",
    "                    # Ajustar l√≠nea de tendencia\n",
    "                    if len(df_results) >= 10:\n",
    "                        # Regresi√≥n lineal\n",
    "                        from scipy.stats import linregress\n",
    "                        valid_mask = np.isfinite(df_results['Turbulence_Intensity_TI']) & np.isfinite(df_results[blade_col])\n",
    "                        x_fit = df_results.loc[valid_mask, 'Turbulence_Intensity_TI'].values\n",
    "                        y_fit = df_results.loc[valid_mask, blade_col].values\n",
    "                        \n",
    "                        if len(x_fit) >= 10:\n",
    "                            slope, intercept, r_value, p_value, std_err = linregress(x_fit, y_fit)\n",
    "                            x_line = np.linspace(x_fit.min(), x_fit.max(), 100)\n",
    "                            y_line = slope * x_line + intercept\n",
    "                            \n",
    "                            ax.plot(x_line, y_line, 'r--', linewidth=2, alpha=0.8,\n",
    "                                   label=f'Tendencia: R¬≤={r_value**2:.3f}, p={p_value:.2e}')\n",
    "                            ax.legend(loc='upper left', fontsize=10, framealpha=0.9)\n",
    "                    \n",
    "                    # Etiquetas y t√≠tulo\n",
    "                    ax.set_xlabel('Intensidad de Turbulencia (TI = œÉ/Œº)', fontsize=12, fontweight='bold')\n",
    "                    ax.set_ylabel(f'Desviaci√≥n Est√°ndar Blade {blade_num} My (kNm)', fontsize=12, fontweight='bold')\n",
    "                    ax.set_title(f'TI vs Variabilidad de Carga - Blade {blade_num}\\n(Mayor turbulencia ‚Üí Mayor variabilidad de carga)', \n",
    "                                fontsize=14, fontweight='bold')\n",
    "                    \n",
    "                    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    # Guardar\n",
    "                    plot_path = turbulence_folder / f\"TI_vs_Load_Std_Blade{blade_num}.png\"\n",
    "                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    \n",
    "                    print(f\"  ‚úÖ Blade {blade_num}: {plot_path.name}\")\n",
    "                \n",
    "                # ------------------------------------------------------------------------\n",
    "                # Gr√°fico combinado (ambas palas)\n",
    "                # ------------------------------------------------------------------------\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "                fig.suptitle('TI vs Variabilidad de Carga - Comparaci√≥n Ambas Palas', \n",
    "                           fontsize=16, fontweight='bold')\n",
    "                \n",
    "                for idx, (blade_num, blade_col) in enumerate([(1, 'Blade1_My_Std'), (2, 'Blade2_My_Std')]):\n",
    "                    ax = axes[idx]\n",
    "                    \n",
    "                    scatter = ax.scatter(\n",
    "                        df_results['Turbulence_Intensity_TI'],\n",
    "                        df_results[blade_col],\n",
    "                        c=df_results['Wind_Mean_ms'],\n",
    "                        cmap='viridis',\n",
    "                        s=30,\n",
    "                        alpha=0.6,\n",
    "                        edgecolors='k',\n",
    "                        linewidth=0.3\n",
    "                    )\n",
    "                    \n",
    "                    cbar = plt.colorbar(scatter, ax=ax, label='Viento (m/s)')\n",
    "                    cbar.ax.tick_params(labelsize=9)\n",
    "                    \n",
    "                    # L√≠nea de tendencia\n",
    "                    if len(df_results) >= 10:\n",
    "                        valid_mask = np.isfinite(df_results['Turbulence_Intensity_TI']) & np.isfinite(df_results[blade_col])\n",
    "                        x_fit = df_results.loc[valid_mask, 'Turbulence_Intensity_TI'].values\n",
    "                        y_fit = df_results.loc[valid_mask, blade_col].values\n",
    "                        \n",
    "                        if len(x_fit) >= 10:\n",
    "                            slope, intercept, r_value, _, _ = linregress(x_fit, y_fit)\n",
    "                            x_line = np.linspace(x_fit.min(), x_fit.max(), 100)\n",
    "                            y_line = slope * x_line + intercept\n",
    "                            ax.plot(x_line, y_line, 'r--', linewidth=2, alpha=0.8, label=f'R¬≤={r_value**2:.3f}')\n",
    "                            ax.legend(loc='upper left', fontsize=9)\n",
    "                    \n",
    "                    ax.set_xlabel('TI (œÉ/Œº)', fontsize=11, fontweight='bold')\n",
    "                    ax.set_ylabel(f'std(Blade {blade_num} My) [kNm]', fontsize=11, fontweight='bold')\n",
    "                    ax.set_title(f'Blade {blade_num}', fontsize=12, fontweight='bold')\n",
    "                    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                plot_path = turbulence_folder / \"TI_vs_Load_Std_Combined.png\"\n",
    "                plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"  ‚úÖ Combinado: {plot_path.name}\")\n",
    "                \n",
    "                # ------------------------------------------------------------------------\n",
    "                # RESUMEN\n",
    "                # ------------------------------------------------------------------------\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(\"RESUMEN - INTENSIDAD DE TURBULENCIA VS VARIABILIDAD\")\n",
    "                print(\"=\"*70)\n",
    "                print(f\"Series temporales analizadas: {len(df_results):,}\")\n",
    "                print(f\"Duraci√≥n ventana por serie:   {window_duration}s (~{window_duration/60:.1f} min)\")\n",
    "                print(f\"Rango temporal por serie:     {window_start}s - {window_start+window_duration}s\")\n",
    "                print(f\"TI m√≠nimo:                    {df_results['Turbulence_Intensity_TI'].min():.4f}\")\n",
    "                print(f\"TI m√°ximo:                    {df_results['Turbulence_Intensity_TI'].max():.4f}\")\n",
    "                print(f\"TI medio:                     {df_results['Turbulence_Intensity_TI'].mean():.4f}\")\n",
    "                \n",
    "                # Correlaciones\n",
    "                from scipy.stats import pearsonr\n",
    "                corr1, p1 = pearsonr(df_results['Turbulence_Intensity_TI'], df_results['Blade1_My_Std'])\n",
    "                corr2, p2 = pearsonr(df_results['Turbulence_Intensity_TI'], df_results['Blade2_My_Std'])\n",
    "                \n",
    "                print(f\"\\nüéØ CORRELACIONES (Pearson):\")\n",
    "                print(f\"  Blade 1: r = {corr1:.3f}, p-value = {p1:.2e}\")\n",
    "                print(f\"  Blade 2: r = {corr2:.3f}, p-value = {p2:.2e}\")\n",
    "                \n",
    "                if corr1 > 0.5 or corr2 > 0.5:\n",
    "                    print(\"\\n‚úÖ VALIDADO: Correlaci√≥n positiva fuerte detectada!\")\n",
    "                    print(\"   ‚Üí Mayor turbulencia ‚Üí Mayor variabilidad de carga\")\n",
    "                    print(\"   ‚Üí El modelo DEBE capturar varianza, no solo media\")\n",
    "                elif corr1 > 0.3 or corr2 > 0.3:\n",
    "                    print(\"\\n‚ö†Ô∏è Correlaci√≥n moderada detectada\")\n",
    "                    print(\"   ‚Üí Se observa tendencia pero con dispersi√≥n\")\n",
    "                else:\n",
    "                    print(\"\\nüîµ Correlaci√≥n d√©bil\")\n",
    "                    print(\"   ‚Üí Revisar c√°lculo de TI o ventanas temporales\")\n",
    "                \n",
    "                print(f\"\\nüí° IMPLICACIONES PARA EL MODELO:\")\n",
    "                print(\"   1. Loss Functions deben penalizar outliers/picos (no solo MSE)\")\n",
    "                print(\"   2. Considerar redes neuronales probabil√≠sticas (Bayesianas, Dropout)\")\n",
    "                print(\"   3. Evaluar modelos con m√©tricas de varianza (std, quantiles)\")\n",
    "                print(\"   4. En fatiga, predecir distribuciones > predecir medias\")\n",
    "                \n",
    "                print(f\"\\nüìÅ Archivos generados:\")\n",
    "                print(f\"  - TI_vs_Load_Std_Blade1.png\")\n",
    "                print(f\"  - TI_vs_Load_Std_Blade2.png\")\n",
    "                print(f\"  - TI_vs_Load_Std_Combined.png\")\n",
    "                print(f\"  - turbulence_intensity_vs_load_variability.csv\")\n",
    "                print(f\"\\nüìç Ubicaci√≥n: {turbulence_folder}\")\n",
    "                print(\"=\"*70)\n",
    "                print(\"\\n‚úÖ An√°lisis de turbulencia completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64b200",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä PASO 5: FEATURE ENGINEERING - Desviaci√≥n Est√°ndar M√≥vil VLOS\n",
    "\n",
    "Ahora que hemos completado el EDA, procedemos a enriquecer el dataset con nuevas features que capturen la **variabilidad temporal del viento**.\n",
    "\n",
    "### üéØ Objetivo\n",
    "\n",
    "A√±adir al dataset completo (`0000_Complete_dataset.csv`) una nueva columna: **`VLOS_Rolling_Std`**\n",
    "\n",
    "**Definici√≥n:**\n",
    "- Calcula la desviaci√≥n est√°ndar m√≥vil (rolling window) sobre las se√±ales VLOS **sin lag** (raw)\n",
    "- Ventana m√≥vil: **30 segundos** (~300 muestras si dt ‚âà 0.1s)\n",
    "- Se calcula para cada BEAM y luego se promedian\n",
    "\n",
    "**Utilidad:**\n",
    "- Captura turbulencia de alta frecuencia no visible en la media\n",
    "- Feature importante para predecir variabilidad de cargas\n",
    "- Complementa TI (Turbulence Intensity) con informaci√≥n local/temporal\n",
    "\n",
    "**M√©todo:**\n",
    "1. Cargar dataset completo\n",
    "2. Para cada columna `LAC_VLOS_BEAMX_RANGE5` (sin lag):\n",
    "   - Calcular `rolling(window=300).std()`\n",
    "3. Promediar las std rolling de todos los BEAMs ‚Üí `VLOS_Rolling_Std`\n",
    "4. Guardar dataset actualizado\n",
    "\n",
    "**Nota:** Operaci√≥n puede tardar debido al tama√±o del dataset (~2M filas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddcbd11",
   "metadata": {},
   "source": [
    "### üìä PASO 5.2: Validaci√≥n Visual de VLOS_Rolling_Std\n",
    "\n",
    "Validaremos la nueva feature `VLOS_Rolling_Std` mediante visualizaciones para confirmar que:\n",
    "1. Captura correctamente la variabilidad temporal del viento\n",
    "2. Tiene correlaci√≥n con la variabilidad de cargas\n",
    "3. No contiene artefactos num√©ricos\n",
    "\n",
    "**Visualizaciones:**\n",
    "- Serie temporal: VLOS_Rolling_Std vs tiempo (muestra representativa)\n",
    "- Histograma de distribuci√≥n\n",
    "- Scatter: VLOS_Rolling_Std vs std(Blade My)\n",
    "- Comparaci√≥n: VLOS_Rolling_Std vs velocidad media del viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03485b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 5.2: VALIDACI√ìN VISUAL DE VLOS_Rolling_Std\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä VALIDACI√ìN VISUAL: VLOS_Rolling_Std\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear carpeta para resultados de validaci√≥n\n",
    "validation_folder = eda_folder / \"08_VLOS_Rolling_Std_Validation\"\n",
    "validation_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Carpeta destino: {validation_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Verificar que existe la columna VLOS_Rolling_Std\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[1/5] Verificando columna VLOS_Rolling_Std...\")\n",
    "    \n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "    \n",
    "    if 'VLOS_Rolling_Std' not in all_columns:\n",
    "        print(\"\\n‚ùå ERROR: La columna VLOS_Rolling_Std no existe en el dataset\")\n",
    "        print(\"   Por favor, ejecuta primero el PASO 5.1 para crear esta feature\")\n",
    "    else:\n",
    "        print(\"‚úÖ Columna VLOS_Rolling_Std encontrada\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 2) Cargar muestra representativa de datos\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[2/5] Cargando muestra representativa de datos...\")\n",
    "        \n",
    "        # Cargar √∫ltimas 5000 filas para an√°lisis visual\n",
    "        n_samples = 2000\n",
    "        \n",
    "        # Columnas necesarias\n",
    "        cols_needed = ['Time', 'VLOS_Rolling_Std', 'Blade root 1 My', 'Blade root 2 My']\n",
    "        \n",
    "        # A√±adir columnas VLOS sin lag para comparaci√≥n\n",
    "        vlos_base_cols = [c for c in all_columns if 'LAC_VLOS' in c and 'lag' not in c]\n",
    "        cols_needed.extend(vlos_base_cols)\n",
    "        \n",
    "        # Determinar total de filas\n",
    "        total_rows = sum(1 for _ in open(complete_dataset_path)) - 1\n",
    "        start_row = max(1, total_rows - n_samples)\n",
    "        \n",
    "        # Cargar muestra\n",
    "        df_sample = pd.read_csv(\n",
    "            complete_dataset_path,\n",
    "            usecols=cols_needed,\n",
    "            skiprows=range(1, start_row)\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Cargado: {len(df_sample):,} filas\")\n",
    "        print(f\"   Tiempo inicial: {df_sample['Time'].min():.1f}s\")\n",
    "        print(f\"   Tiempo final: {df_sample['Time'].max():.1f}s\")\n",
    "        \n",
    "        # Calcular velocidad media del viento\n",
    "        df_sample['Wind_Mean'] = df_sample[vlos_base_cols].mean(axis=1)\n",
    "        \n",
    "        # Estad√≠sticas b√°sicas\n",
    "        print(f\"\\nüìä Estad√≠sticas VLOS_Rolling_Std:\")\n",
    "        print(f\"   M√≠n:    {df_sample['VLOS_Rolling_Std'].min():.4f} m/s\")\n",
    "        print(f\"   M√°x:    {df_sample['VLOS_Rolling_Std'].max():.4f} m/s\")\n",
    "        print(f\"   Media:  {df_sample['VLOS_Rolling_Std'].mean():.4f} m/s\")\n",
    "        print(f\"   Mediana:{df_sample['VLOS_Rolling_Std'].median():.4f} m/s\")\n",
    "        print(f\"   NaNs:   {df_sample['VLOS_Rolling_Std'].isna().sum()}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 3) PLOT 1: Serie Temporal\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[3/5] Generando serie temporal...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)\n",
    "        fig.suptitle('Validaci√≥n VLOS_Rolling_Std - Serie Temporal', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Subplot 1: Velocidad media del viento\n",
    "        axes[0].plot(df_sample['Time'], df_sample['Wind_Mean'], \n",
    "                    linewidth=1, color='steelblue', label='Viento medio')\n",
    "        axes[0].set_ylabel('Velocidad Viento (m/s)', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title('Velocidad Media del Viento', fontsize=12, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "        axes[0].legend(loc='upper right')\n",
    "        \n",
    "        # Subplot 2: VLOS_Rolling_Std\n",
    "        axes[1].plot(df_sample['Time'], df_sample['VLOS_Rolling_Std'], \n",
    "                    linewidth=1, color='orange', label='VLOS Rolling Std')\n",
    "        axes[1].set_ylabel('Desv. Est√°ndar M√≥vil (m/s)', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title('VLOS_Rolling_Std - Captura Turbulencia Local', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3, linestyle='--')\n",
    "        axes[1].legend(loc='upper right')\n",
    "        \n",
    "        # Subplot 3: Cargas en las palas\n",
    "        axes[2].plot(df_sample['Time'], df_sample['Blade root 1 My'], \n",
    "                    linewidth=0.8, alpha=0.7, label='Blade 1 My')\n",
    "        axes[2].plot(df_sample['Time'], df_sample['Blade root 2 My'], \n",
    "                    linewidth=0.8, alpha=0.7, label='Blade 2 My')\n",
    "        axes[2].set_ylabel('Momento Flector (kNm)', fontsize=11, fontweight='bold')\n",
    "        axes[2].set_xlabel('Tiempo (s)', fontsize=11, fontweight='bold')\n",
    "        axes[2].set_title('Cargas en las Palas', fontsize=12, fontweight='bold')\n",
    "        axes[2].grid(True, alpha=0.3, linestyle='--')\n",
    "        axes[2].legend(loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = validation_folder / \"time_series_validation.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Serie temporal: {plot_path.name}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 4) PLOT 2: Histograma de distribuci√≥n\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[4/5] Generando histograma de distribuci√≥n...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Subplot 1: Histograma b√°sico\n",
    "        axes[0].hist(df_sample['VLOS_Rolling_Std'].dropna(), bins=50, \n",
    "                    color='teal', edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        mean_val = df_sample['VLOS_Rolling_Std'].mean()\n",
    "        median_val = df_sample['VLOS_Rolling_Std'].median()\n",
    "        \n",
    "        axes[0].axvline(mean_val, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Media: {mean_val:.3f}')\n",
    "        axes[0].axvline(median_val, color='green', linestyle='--', linewidth=2, \n",
    "                       label=f'Mediana: {median_val:.3f}')\n",
    "        \n",
    "        axes[0].set_xlabel('VLOS_Rolling_Std (m/s)', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_ylabel('Frecuencia', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title('Distribuci√≥n de VLOS_Rolling_Std', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].legend(loc='upper right')\n",
    "        axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Subplot 2: Box plot\n",
    "        bp = axes[1].boxplot(df_sample['VLOS_Rolling_Std'].dropna(), \n",
    "                            vert=True, patch_artist=True,\n",
    "                            boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                            medianprops=dict(color='red', linewidth=2),\n",
    "                            whiskerprops=dict(linewidth=1.5),\n",
    "                            capprops=dict(linewidth=1.5))\n",
    "        \n",
    "        axes[1].set_ylabel('VLOS_Rolling_Std (m/s)', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title('Box Plot - Detecci√≥n de Outliers', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = validation_folder / \"distribution_validation.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Distribuci√≥n: {plot_path.name}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 5) PLOT 3: Correlaciones con cargas\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[5/5] Generando correlaciones con cargas...\")\n",
    "        \n",
    "        # Calcular rolling std de las cargas (misma ventana)\n",
    "        rolling_window_samples = 300  # ~30s con dt=0.1s\n",
    "        df_sample['Blade1_My_Rolling_Std'] = df_sample['Blade root 1 My'].rolling(\n",
    "            window=rolling_window_samples, min_periods=50, center=True\n",
    "        ).std()\n",
    "        df_sample['Blade2_My_Rolling_Std'] = df_sample['Blade root 2 My'].rolling(\n",
    "            window=rolling_window_samples, min_periods=50, center=True\n",
    "        ).std()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        for idx, (blade_num, blade_col) in enumerate([\n",
    "            (1, 'Blade1_My_Rolling_Std'), \n",
    "            (2, 'Blade2_My_Rolling_Std')\n",
    "        ]):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Scatter plot\n",
    "            valid_mask = (\n",
    "                df_sample['VLOS_Rolling_Std'].notna() & \n",
    "                df_sample[blade_col].notna()\n",
    "            )\n",
    "            \n",
    "            scatter = ax.scatter(\n",
    "                df_sample.loc[valid_mask, 'VLOS_Rolling_Std'],\n",
    "                df_sample.loc[valid_mask, blade_col],\n",
    "                c=df_sample.loc[valid_mask, 'Wind_Mean'],\n",
    "                cmap='viridis',\n",
    "                s=20,\n",
    "                alpha=0.5,\n",
    "                edgecolors='none'\n",
    "            )\n",
    "            \n",
    "            # Colorbar\n",
    "            cbar = plt.colorbar(scatter, ax=ax, label='Viento medio (m/s)')\n",
    "            cbar.ax.tick_params(labelsize=9)\n",
    "            \n",
    "            # Calcular correlaci√≥n\n",
    "            from scipy.stats import pearsonr\n",
    "            x_corr = df_sample.loc[valid_mask, 'VLOS_Rolling_Std'].values\n",
    "            y_corr = df_sample.loc[valid_mask, blade_col].values\n",
    "            \n",
    "            if len(x_corr) >= 10:\n",
    "                corr, p_val = pearsonr(x_corr, y_corr)\n",
    "                \n",
    "                # L√≠nea de tendencia\n",
    "                from scipy.stats import linregress\n",
    "                slope, intercept, _, _, _ = linregress(x_corr, y_corr)\n",
    "                x_line = np.linspace(x_corr.min(), x_corr.max(), 100)\n",
    "                y_line = slope * x_line + intercept\n",
    "                ax.plot(x_line, y_line, 'r--', linewidth=2, alpha=0.8,\n",
    "                       label=f'R={corr:.3f}, p={p_val:.2e}')\n",
    "                ax.legend(loc='upper left', fontsize=10)\n",
    "            \n",
    "            ax.set_xlabel('VLOS_Rolling_Std (m/s)', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel(f'std(Blade {blade_num} My) [kNm]', fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'Correlaci√≥n: Turbulencia ‚Üî Variabilidad Carga (Blade {blade_num})', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = validation_folder / \"correlation_with_loads.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Correlaciones: {plot_path.name}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 6) PLOT 4: VLOS_Rolling_Std vs Velocidad Media\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[Bonus] Generando relaci√≥n con velocidad media...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        scatter = ax.scatter(\n",
    "            df_sample['Wind_Mean'],\n",
    "            df_sample['VLOS_Rolling_Std'],\n",
    "            c=df_sample['Time'],\n",
    "            cmap='plasma',\n",
    "            s=15,\n",
    "            alpha=0.4,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "        \n",
    "        cbar = plt.colorbar(scatter, ax=ax, label='Tiempo (s)')\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        \n",
    "        ax.set_xlabel('Velocidad Media del Viento (m/s)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('VLOS_Rolling_Std (m/s)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('VLOS_Rolling_Std vs Velocidad Media\\n(Turbulencia vs Intensidad del Viento)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = validation_folder / \"rolling_std_vs_wind_mean.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Relaci√≥n con viento: {plot_path.name}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # RESUMEN Y DIAGN√ìSTICO\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"RESUMEN - VALIDACI√ìN VLOS_Rolling_Std\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nüìä ESTAD√çSTICAS:\")\n",
    "        print(f\"   M√≠nimo:       {df_sample['VLOS_Rolling_Std'].min():.4f} m/s\")\n",
    "        print(f\"   M√°ximo:       {df_sample['VLOS_Rolling_Std'].max():.4f} m/s\")\n",
    "        print(f\"   Media:        {df_sample['VLOS_Rolling_Std'].mean():.4f} m/s\")\n",
    "        print(f\"   Mediana:      {df_sample['VLOS_Rolling_Std'].median():.4f} m/s\")\n",
    "        print(f\"   Desv. Std:    {df_sample['VLOS_Rolling_Std'].std():.4f} m/s\")\n",
    "        print(f\"   NaNs:         {df_sample['VLOS_Rolling_Std'].isna().sum()} ({df_sample['VLOS_Rolling_Std'].isna().sum()/len(df_sample)*100:.2f}%)\")\n",
    "        \n",
    "        print(f\"\\nüîç VALIDACI√ìN VISUAL:\")\n",
    "        print(\"   ‚úÖ Serie temporal: ¬øCaptura picos de turbulencia?\")\n",
    "        print(\"   ‚úÖ Distribuci√≥n: ¬øForma coherente sin outliers an√≥malos?\")\n",
    "        print(\"   ‚úÖ Correlaci√≥n: ¬øSe correlaciona con variabilidad de cargas?\")\n",
    "        print(\"   ‚úÖ Relaci√≥n viento: ¬øAumenta con velocidad o es independiente?\")\n",
    "        \n",
    "        # Calcular correlaciones con cargas\n",
    "        valid_mask = (\n",
    "            df_sample['VLOS_Rolling_Std'].notna() & \n",
    "            df_sample['Blade1_My_Rolling_Std'].notna() &\n",
    "            df_sample['Blade2_My_Rolling_Std'].notna()\n",
    "        )\n",
    "        \n",
    "        if valid_mask.sum() >= 10:\n",
    "            corr1, p1 = pearsonr(\n",
    "                df_sample.loc[valid_mask, 'VLOS_Rolling_Std'],\n",
    "                df_sample.loc[valid_mask, 'Blade1_My_Rolling_Std']\n",
    "            )\n",
    "            corr2, p2 = pearsonr(\n",
    "                df_sample.loc[valid_mask, 'VLOS_Rolling_Std'],\n",
    "                df_sample.loc[valid_mask, 'Blade2_My_Rolling_Std']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nüìà CORRELACIONES CON CARGAS:\")\n",
    "            print(f\"   Blade 1: r = {corr1:.3f}, p = {p1:.2e}\")\n",
    "            print(f\"   Blade 2: r = {corr2:.3f}, p = {p2:.2e}\")\n",
    "            \n",
    "            if corr1 > 0.3 or corr2 > 0.3:\n",
    "                print(\"\\n   ‚úÖ VALIDADO: Correlaci√≥n positiva detectada\")\n",
    "                print(\"      ‚Üí Mayor turbulencia ‚Üí Mayor variabilidad de carga\")\n",
    "                print(\"      ‚Üí La feature captura informaci√≥n relevante\")\n",
    "            else:\n",
    "                print(\"\\n   ‚ö†Ô∏è Correlaci√≥n d√©bil\")\n",
    "                print(\"      ‚Üí Puede indicar que la ventana m√≥vil es incorrecta\")\n",
    "                print(\"      ‚Üí O que la turbulencia no es el factor dominante en esta muestra\")\n",
    "        \n",
    "        print(f\"\\nüí° USO EN MODELO ML:\")\n",
    "        print(\"   - Feature para predecir picos/variabilidad de cargas\")\n",
    "        print(\"   - Complementa velocidad media con informaci√≥n de turbulencia local\")\n",
    "        print(\"   - √ötil en modelos que predicen distribuciones (no solo media)\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Archivos generados:\")\n",
    "        print(f\"   - time_series_validation.png\")\n",
    "        print(f\"   - distribution_validation.png\")\n",
    "        print(f\"   - correlation_with_loads.png\")\n",
    "        print(f\"   - rolling_std_vs_wind_mean.png\")\n",
    "        \n",
    "        print(f\"\\nüìç Ubicaci√≥n: {validation_folder}\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n‚úÖ Validaci√≥n visual completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab636ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 5.2: VALIDACI√ìN VISUAL DE VLOS_Rolling_Std\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä VALIDACI√ìN VISUAL: VLOS_Rolling_Std\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear carpeta para resultados de validaci√≥n\n",
    "validation_folder = eda_folder / \"08_VLOS_Rolling_Std_Validation\"\n",
    "validation_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Carpeta destino: {validation_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Verificar que existe la columna VLOS_Rolling_Std\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[1/5] Verificando columna VLOS_Rolling_Std...\")\n",
    "    \n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "    \n",
    "    if 'VLOS_Rolling_Std' not in all_columns:\n",
    "        print(\"\\n‚ùå ERROR: La columna VLOS_Rolling_Std no existe en el dataset\")\n",
    "        print(\"   Por favor, ejecuta primero el PASO 5.1 para crear esta feature\")\n",
    "    else:\n",
    "        print(\"‚úÖ Columna VLOS_Rolling_Std encontrada\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 2) Cargar muestra representativa de datos\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[2/5] Cargando muestra representativa de datos...\")\n",
    "        \n",
    "        # Cargar √∫ltimas 5000 filas para an√°lisis visual\n",
    "        n_samples = 5000\n",
    "        \n",
    "        # Columnas necesarias\n",
    "        cols_needed = ['Time', 'VLOS_Rolling_Std', 'Blade root 1 My', 'Blade root 2 My']\n",
    "        \n",
    "        # A√±adir columnas VLOS sin lag para comparaci√≥n\n",
    "        vlos_base_cols = [c for c in all_columns if 'LAC_VLOS' in c and 'lag' not in c]\n",
    "        cols_needed.extend(vlos_base_cols)\n",
    "        \n",
    "        # Determinar total de filas\n",
    "        total_rows = sum(1 for _ in open(complete_dataset_path)) - 1\n",
    "        start_row = max(1, total_rows - n_samples)\n",
    "        \n",
    "        # Cargar muestra\n",
    "        df_sample = pd.read_csv(\n",
    "            complete_dataset_path,\n",
    "            usecols=cols_needed,\n",
    "            skiprows=range(1, start_row)\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Cargado: {len(df_sample):,} filas\")\n",
    "        print(f\"   Tiempo inicial: {df_sample['Time'].min():.1f}s\")\n",
    "        print(f\"   Tiempo final: {df_sample['Time'].max():.1f}s\")\n",
    "        \n",
    "        # Calcular velocidad media del viento\n",
    "        df_sample['Wind_Mean'] = df_sample[vlos_base_cols].mean(axis=1)\n",
    "        \n",
    "        # Estad√≠sticas b√°sicas\n",
    "        print(f\"\\nüìä Estad√≠sticas VLOS_Rolling_Std:\")\n",
    "        print(f\"   M√≠n:    {df_sample['VLOS_Rolling_Std'].min():.4f} m/s\")\n",
    "        print(f\"   M√°x:    {df_sample['VLOS_Rolling_Std'].max():.4f} m/s\")\n",
    "        print(f\"   Media:  {df_sample['VLOS_Rolling_Std'].mean():.4f} m/s\")\n",
    "        print(f\"   Mediana:{df_sample['VLOS_Rolling_Std'].median():.4f} m/s\")\n",
    "        print(f\"   NaNs:   {df_sample['VLOS_Rolling_Std'].isna().sum()}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 3) PLOT 1: Serie Temporal\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[3/5] Generando serie temporal...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)\n",
    "        fig.suptitle('Validaci√≥n VLOS_Rolling_Std - Serie Temporal', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Subplot 1: Velocidad media del viento\n",
    "        axes[0].plot(df_sample['Time'], df_sample['Wind_Mean'], \n",
    "                    linewidth=1, color='steelblue', label='Viento medio')\n",
    "        axes[0].set_ylabel('Velocidad Viento (m/s)', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title('Velocidad Media del Viento', fontsize=12, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "        axes[0].legend(loc='upper right')\n",
    "        \n",
    "        # Subplot 2: VLOS_Rolling_Std\n",
    "        axes[1].plot(df_sample['Time'], df_sample['VLOS_Rolling_Std'], \n",
    "                    linewidth=1, color='orange', label='VLOS Rolling Std')\n",
    "        axes[1].set_ylabel('Desv. Est√°ndar M√≥vil (m/s)', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title('VLOS_Rolling_Std - Captura Turbulencia Local', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3, linestyle='--')\n",
    "        axes[1].legend(loc='upper right')\n",
    "        \n",
    "        # Subplot 3: Cargas en las palas\n",
    "        axes[2].plot(df_sample['Time'], df_sample['Blade root 1 My'], \n",
    "                    linewidth=0.8, alpha=0.7, label='Blade 1 My')\n",
    "        axes[2].plot(df_sample['Time'], df_sample['Blade root 2 My'], \n",
    "                    linewidth=0.8, alpha=0.7, label='Blade 2 My')\n",
    "        axes[2].set_ylabel('Momento Flector (kNm)', fontsize=11, fontweight='bold')\n",
    "        axes[2].set_xlabel('Tiempo (s)', fontsize=11, fontweight='bold')\n",
    "        axes[2].set_title('Cargas en las Palas', fontsize=12, fontweight='bold')\n",
    "        axes[2].grid(True, alpha=0.3, linestyle='--')\n",
    "        axes[2].legend(loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = validation_folder / \"time_series_validation.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Serie temporal: {plot_path.name}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 4) PLOT 2: Histograma de distribuci√≥n\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[4/5] Generando histograma de distribuci√≥n...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Subplot 1: Histograma b√°sico\n",
    "        axes[0].hist(df_sample['VLOS_Rolling_Std'].dropna(), bins=50, \n",
    "                    color='teal', edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        mean_val = df_sample['VLOS_Rolling_Std'].mean()\n",
    "        median_val = df_sample['VLOS_Rolling_Std'].median()\n",
    "        \n",
    "        axes[0].axvline(mean_val, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Media: {mean_val:.3f}')\n",
    "        axes[0].axvline(median_val, color='green', linestyle='--', linewidth=2, \n",
    "                       label=f'Mediana: {median_val:.3f}')\n",
    "        \n",
    "        axes[0].set_xlabel('VLOS_Rolling_Std (m/s)', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_ylabel('Frecuencia', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title('Distribuci√≥n de VLOS_Rolling_Std', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].legend(loc='upper right')\n",
    "        axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Subplot 2: Box plot\n",
    "        bp = axes[1].boxplot(df_sample['VLOS_Rolling_Std'].dropna(), \n",
    "                            vert=True, patch_artist=True,\n",
    "                            boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                            medianprops=dict(color='red', linewidth=2),\n",
    "                            whiskerprops=dict(linewidth=1.5),\n",
    "                            capprops=dict(linewidth=1.5))\n",
    "        \n",
    "        axes[1].set_ylabel('VLOS_Rolling_Std (m/s)', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title('Box Plot - Detecci√≥n de Outliers', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = validation_folder / \"distribution_validation.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Distribuci√≥n: {plot_path.name}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 5) PLOT 3: Correlaciones con cargas\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[5/5] Generando correlaciones con cargas...\")\n",
    "        \n",
    "        # Calcular rolling std de las cargas (misma ventana)\n",
    "        rolling_window_samples = 300  # ~30s con dt=0.1s\n",
    "        df_sample['Blade1_My_Rolling_Std'] = df_sample['Blade root 1 My'].rolling(\n",
    "            window=rolling_window_samples, min_periods=50, center=True\n",
    "        ).std()\n",
    "        df_sample['Blade2_My_Rolling_Std'] = df_sample['Blade root 2 My'].rolling(\n",
    "            window=rolling_window_samples, min_periods=50, center=True\n",
    "        ).std()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        for idx, (blade_num, blade_col) in enumerate([\n",
    "            (1, 'Blade1_My_Rolling_Std'), \n",
    "            (2, 'Blade2_My_Rolling_Std')\n",
    "        ]):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Scatter plot\n",
    "            valid_mask = (\n",
    "                df_sample['VLOS_Rolling_Std'].notna() & \n",
    "                df_sample[blade_col].notna()\n",
    "            )\n",
    "            \n",
    "            scatter = ax.scatter(\n",
    "                df_sample.loc[valid_mask, 'VLOS_Rolling_Std'],\n",
    "                df_sample.loc[valid_mask, blade_col],\n",
    "                c=df_sample.loc[valid_mask, 'Wind_Mean'],\n",
    "                cmap='viridis',\n",
    "                s=20,\n",
    "                alpha=0.5,\n",
    "                edgecolors='none'\n",
    "            )\n",
    "            \n",
    "            # Colorbar\n",
    "            cbar = plt.colorbar(scatter, ax=ax, label='Viento medio (m/s)')\n",
    "            cbar.ax.tick_params(labelsize=9)\n",
    "            \n",
    "            # Calcular correlaci√≥n\n",
    "            from scipy.stats import pearsonr\n",
    "            x_corr = df_sample.loc[valid_mask, 'VLOS_Rolling_Std'].values\n",
    "            y_corr = df_sample.loc[valid_mask, blade_col].values\n",
    "            \n",
    "            if len(x_corr) >= 10:\n",
    "                corr, p_val = pearsonr(x_corr, y_corr)\n",
    "                \n",
    "                # L√≠nea de tendencia\n",
    "                from scipy.stats import linregress\n",
    "                slope, intercept, _, _, _ = linregress(x_corr, y_corr)\n",
    "                x_line = np.linspace(x_corr.min(), x_corr.max(), 100)\n",
    "                y_line = slope * x_line + intercept\n",
    "                ax.plot(x_line, y_line, 'r--', linewidth=2, alpha=0.8,\n",
    "                       label=f'R={corr:.3f}, p={p_val:.2e}')\n",
    "                ax.legend(loc='upper left', fontsize=10)\n",
    "            \n",
    "            ax.set_xlabel('VLOS_Rolling_Std (m/s)', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel(f'std(Blade {blade_num} My) [kNm]', fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'Correlaci√≥n: Turbulencia ‚Üî Variabilidad Carga (Blade {blade_num})', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = validation_folder / \"correlation_with_loads.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Correlaciones: {plot_path.name}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 6) PLOT 4: VLOS_Rolling_Std vs Velocidad Media\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[Bonus] Generando relaci√≥n con velocidad media...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        scatter = ax.scatter(\n",
    "            df_sample['Wind_Mean'],\n",
    "            df_sample['VLOS_Rolling_Std'],\n",
    "            c=df_sample['Time'],\n",
    "            cmap='plasma',\n",
    "            s=15,\n",
    "            alpha=0.4,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "        \n",
    "        cbar = plt.colorbar(scatter, ax=ax, label='Tiempo (s)')\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        \n",
    "        ax.set_xlabel('Velocidad Media del Viento (m/s)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('VLOS_Rolling_Std (m/s)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('VLOS_Rolling_Std vs Velocidad Media\\n(Turbulencia vs Intensidad del Viento)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = validation_folder / \"rolling_std_vs_wind_mean.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Relaci√≥n con viento: {plot_path.name}\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # RESUMEN Y DIAGN√ìSTICO\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"RESUMEN - VALIDACI√ìN VLOS_Rolling_Std\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nüìä ESTAD√çSTICAS:\")\n",
    "        print(f\"   M√≠nimo:       {df_sample['VLOS_Rolling_Std'].min():.4f} m/s\")\n",
    "        print(f\"   M√°ximo:       {df_sample['VLOS_Rolling_Std'].max():.4f} m/s\")\n",
    "        print(f\"   Media:        {df_sample['VLOS_Rolling_Std'].mean():.4f} m/s\")\n",
    "        print(f\"   Mediana:      {df_sample['VLOS_Rolling_Std'].median():.4f} m/s\")\n",
    "        print(f\"   Desv. Std:    {df_sample['VLOS_Rolling_Std'].std():.4f} m/s\")\n",
    "        print(f\"   NaNs:         {df_sample['VLOS_Rolling_Std'].isna().sum()} ({df_sample['VLOS_Rolling_Std'].isna().sum()/len(df_sample)*100:.2f}%)\")\n",
    "        \n",
    "        print(f\"\\nüîç VALIDACI√ìN VISUAL:\")\n",
    "        print(\"   ‚úÖ Serie temporal: ¬øCaptura picos de turbulencia?\")\n",
    "        print(\"   ‚úÖ Distribuci√≥n: ¬øForma coherente sin outliers an√≥malos?\")\n",
    "        print(\"   ‚úÖ Correlaci√≥n: ¬øSe correlaciona con variabilidad de cargas?\")\n",
    "        print(\"   ‚úÖ Relaci√≥n viento: ¬øAumenta con velocidad o es independiente?\")\n",
    "        \n",
    "        # Calcular correlaciones con cargas\n",
    "        valid_mask = (\n",
    "            df_sample['VLOS_Rolling_Std'].notna() & \n",
    "            df_sample['Blade1_My_Rolling_Std'].notna() &\n",
    "            df_sample['Blade2_My_Rolling_Std'].notna()\n",
    "        )\n",
    "        \n",
    "        if valid_mask.sum() >= 10:\n",
    "            corr1, p1 = pearsonr(\n",
    "                df_sample.loc[valid_mask, 'VLOS_Rolling_Std'],\n",
    "                df_sample.loc[valid_mask, 'Blade1_My_Rolling_Std']\n",
    "            )\n",
    "            corr2, p2 = pearsonr(\n",
    "                df_sample.loc[valid_mask, 'VLOS_Rolling_Std'],\n",
    "                df_sample.loc[valid_mask, 'Blade2_My_Rolling_Std']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nüìà CORRELACIONES CON CARGAS:\")\n",
    "            print(f\"   Blade 1: r = {corr1:.3f}, p = {p1:.2e}\")\n",
    "            print(f\"   Blade 2: r = {corr2:.3f}, p = {p2:.2e}\")\n",
    "            \n",
    "            if corr1 > 0.3 or corr2 > 0.3:\n",
    "                print(\"\\n   ‚úÖ VALIDADO: Correlaci√≥n positiva detectada\")\n",
    "                print(\"      ‚Üí Mayor turbulencia ‚Üí Mayor variabilidad de carga\")\n",
    "                print(\"      ‚Üí La feature captura informaci√≥n relevante\")\n",
    "            else:\n",
    "                print(\"\\n   ‚ö†Ô∏è Correlaci√≥n d√©bil\")\n",
    "                print(\"      ‚Üí Puede indicar que la ventana m√≥vil es incorrecta\")\n",
    "                print(\"      ‚Üí O que la turbulencia no es el factor dominante en esta muestra\")\n",
    "        \n",
    "        print(f\"\\nüí° USO EN MODELO ML:\")\n",
    "        print(\"   - Feature para predecir picos/variabilidad de cargas\")\n",
    "        print(\"   - Complementa velocidad media con informaci√≥n de turbulencia local\")\n",
    "        print(\"   - √ötil en modelos que predicen distribuciones (no solo media)\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Archivos generados:\")\n",
    "        print(f\"   - time_series_validation.png\")\n",
    "        print(f\"   - distribution_validation.png\")\n",
    "        print(f\"   - correlation_with_loads.png\")\n",
    "        print(f\"   - rolling_std_vs_wind_mean.png\")\n",
    "        \n",
    "        print(f\"\\nüìç Ubicaci√≥n: {validation_folder}\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n‚úÖ Validaci√≥n visual completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c087987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 5.1: A√ëADIR DESVIACI√ìN EST√ÅNDAR M√ìVIL VLOS AL DATASET COMPLETO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FEATURE ENGINEERING: VLOS_Rolling_Std\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Par√°metros\n",
    "rolling_window_seconds = 30  # Ventana m√≥vil en segundos\n",
    "dt_estimated = 0.02          # Tiempo de muestreo estimado (s)\n",
    "rolling_window_samples = int(rolling_window_seconds / dt_estimated)\n",
    "\n",
    "print(f\"Ventana m√≥vil: {rolling_window_seconds}s (~{rolling_window_samples} muestras)\")\n",
    "print(f\"Dataset: {complete_dataset_path.name}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "else:\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Verificar columnas VLOS sin lag\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[1/5] Verificando columnas VLOS sin lag...\")\n",
    "    \n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "    \n",
    "    # Columnas VLOS sin lag\n",
    "    vlos_base_cols = [c for c in all_columns if 'LAC_VLOS' in c and 'lag' not in c]\n",
    "    \n",
    "    if not vlos_base_cols:\n",
    "        print(\"\\n‚ùå ERROR: No se encontraron columnas VLOS sin lag\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Columnas VLOS sin lag detectadas: {len(vlos_base_cols)}\")\n",
    "        for col in vlos_base_cols:\n",
    "            print(f\"   - {col}\")\n",
    "        \n",
    "        # Verificar si ya existe VLOS_Rolling_Std\n",
    "        if 'VLOS_Rolling_Std' in all_columns:\n",
    "            print(\"\\n‚ö†Ô∏è  La columna VLOS_Rolling_Std ya existe en el dataset\")\n",
    "            overwrite = input(\"   ¬øDeseas recalcularla? (s/n): \")\n",
    "            if overwrite.lower() != 's':\n",
    "                print(\"   Operaci√≥n cancelada\")\n",
    "                raise SystemExit\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 2) Cargar datos y detectar segmentos (series temporales concatenadas)\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[2/6] Cargando dataset y detectando series temporales concatenadas...\")\n",
    "        \n",
    "        # Cargar solo las columnas necesarias\n",
    "        usecols = vlos_base_cols + ['Time']\n",
    "        df = pd.read_csv(complete_dataset_path, usecols=usecols)\n",
    "        \n",
    "        print(f\"‚úÖ Cargado: {len(df):,} filas, {len(usecols)} columnas\")\n",
    "        \n",
    "        # Detectar reinicios de tiempo (series concatenadas)\n",
    "        time_diff = df['Time'].diff()\n",
    "        restarts = np.where(time_diff < 0)[0]  # √çndices donde Time reinicia\n",
    "        \n",
    "        # Crear segmentos\n",
    "        segment_starts = [0] + restarts.tolist()\n",
    "        segment_ends = restarts.tolist() + [len(df)]\n",
    "        \n",
    "        print(f\"üìå Series temporales detectadas: {len(segment_starts)}\")\n",
    "        \n",
    "        # Verificar dt real en el primer segmento\n",
    "        first_segment = df.iloc[segment_starts[0]:segment_ends[0]]\n",
    "        dt_real = first_segment['Time'].diff().abs().median()\n",
    "        rolling_window_samples = int(rolling_window_seconds / dt_real)\n",
    "        \n",
    "        print(f\"üìå dt real: {dt_real:.3f}s\")\n",
    "        print(f\"üìå Ventana ajustada: {rolling_window_samples} muestras\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 3) Calcular rolling std POR SEGMENTO para cada BEAM\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[3/6] Calculando rolling std por segmento para cada BEAM...\")\n",
    "        \n",
    "        # Inicializar array para almacenar resultados\n",
    "        vlos_rolling_std_full = np.full(len(df), np.nan)\n",
    "        \n",
    "        for col in vlos_base_cols:\n",
    "            print(f\"   Procesando {col}...\")\n",
    "            col_rolling_std = np.full(len(df), np.nan)\n",
    "            \n",
    "            # Procesar cada segmento independientemente\n",
    "            for seg_idx, (start_idx, end_idx) in enumerate(zip(segment_starts, segment_ends)):\n",
    "                segment_data = df[col].iloc[start_idx:end_idx]\n",
    "                \n",
    "                # Calcular rolling std solo dentro de este segmento\n",
    "                segment_rolling = segment_data.rolling(\n",
    "                    window=rolling_window_samples,\n",
    "                    min_periods=max(1, rolling_window_samples // 2),\n",
    "                    center=True\n",
    "                ).std()\n",
    "                \n",
    "                # Asignar resultados al array completo\n",
    "                col_rolling_std[start_idx:end_idx] = segment_rolling.values\n",
    "            \n",
    "            # Acumular para promediar despu√©s\n",
    "            rolling_stds.append(col_rolling_std)\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 4) Promediar rolling stds de todos los BEAMs\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[4/6] Promediando rolling stds de todos los BEAMs...\")\n",
    "        \n",
    "        rolling_stds_array = np.array(rolling_stds)\n",
    "        vlos_rolling_std = np.nanmean(rolling_stds_array, axis=0)\n",
    "        \n",
    "        print(f\"‚úÖ VLOS_Rolling_Std calculado\")\n",
    "        print(f\"   M√≠nimo: {np.nanmin(vlos_rolling_std):.3f} m/s\")\n",
    "        print(f\"   M√°ximo: {np.nanmax(vlos_rolling_std):.3f} m/s\")\n",
    "        print(f\"   Media: {np.nanmean(vlos_rolling_std):.3f} m/s\")\n",
    "        print(f\"   NaNs: {np.isnan(vlos_rolling_std).sum()} ({np.isnan(vlos_rolling_std).sum()/len(vlos_rolling_std)*100:.2f}%)\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 5) A√±adir columna al dataset completo con procesamiento por chunks\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n[5/6] A√±adiendo columna VLOS_Rolling_Std al dataset (procesamiento por chunks)...\")\n",
    "        \n",
    "        # Crear backup si no existe\n",
    "        backup_path = complete_dataset_path.parent / f\"{complete_dataset_path.stem}_backup{complete_dataset_path.suffix}\"\n",
    "        \n",
    "        if not backup_path.exists():\n",
    "            print(f\"   Creando backup: {backup_path.name}\")\n",
    "            import shutil\n",
    "            shutil.copy2(complete_dataset_path, backup_path)\n",
    "        \n",
    "        # Crear archivo temporal con la nueva columna\n",
    "        temp_output_path = complete_dataset_path.parent / \"temp_with_rolling_std.csv\"\n",
    "        \n",
    "        print(f\"   Procesando dataset por chunks...\")\n",
    "        chunk_size = 50000  # Chunks de 50k filas\n",
    "        first_chunk = True\n",
    "        row_idx = 0\n",
    "        \n",
    "        for chunk in pd.read_csv(complete_dataset_path, chunksize=chunk_size):\n",
    "            # A√±adir la columna calculada a este chunk\n",
    "            chunk_end = min(row_idx + len(chunk), len(vlos_rolling_std))\n",
    "            chunk['VLOS_Rolling_Std'] = vlos_rolling_std[row_idx:chunk_end]\n",
    "            \n",
    "            # Escribir chunk (con header solo en el primero)\n",
    "            mode = 'w' if first_chunk else 'a'\n",
    "            header = first_chunk\n",
    "            chunk.to_csv(temp_output_path, index=False, mode=mode, header=header)\n",
    "            \n",
    "            row_idx += len(chunk)\n",
    "            first_chunk = False\n",
    "            \n",
    "            if row_idx % 200000 == 0:\n",
    "                print(f\"   Procesadas {row_idx:,} filas...\")\n",
    "        \n",
    "        print(f\"   Total procesadas: {row_idx:,} filas\")\n",
    "        \n",
    "        # Reemplazar archivo original con el temporal\n",
    "        print(f\"   Reemplazando dataset original...\")\n",
    "        import os\n",
    "        os.replace(temp_output_path, complete_dataset_path)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dataset actualizado exitosamente\")\n",
    "        print(f\"   Archivo: {complete_dataset_path.name}\")\n",
    "        print(f\"   Total filas: {row_idx:,}\")\n",
    "        print(f\"   Total columnas: {len(df_complete.columns)}\")\n",
    "        print(f\"   Nueva feature: VLOS_Rolling_Std\")\n",
    "        \n",
    "        # ------------------------------------------------------------------------\n",
    "        # 6) RESUMEN Y VALIDACI√ìN\n",
    "        # ------------------------------------------------------------------------\n",
    "        # ------------------------------------------------------------------------\n",
    "        # RESUMEN\n",
    "        # ------------------------------------------------------------------------\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"RESUMEN - VLOS_Rolling_Std\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Ventana m√≥vil:          {rolling_window_seconds}s ({rolling_window_samples} muestras)\")\n",
    "        print(f\"BEAMs promediados:       {len(vlos_base_cols)}\")\n",
    "        print(f\"Filas procesadas:        {len(df_complete):,}\")\n",
    "        print(f\"\")\n",
    "        print(f\"üìä Estad√≠sticas VLOS_Rolling_Std:\")\n",
    "        print(f\"   M√≠nimo:  {vlos_rolling_std.min():.4f} m/s\")\n",
    "        print(f\"   M√°ximo:  {vlos_rolling_std.max():.4f} m/s\")\n",
    "        print(f\"   Media:   {vlos_rolling_std.mean():.4f} m/s\")\n",
    "        print(f\"   Mediana: {vlos_rolling_std.median():.4f} m/s\")\n",
    "        print(f\"   Std:     {vlos_rolling_std.std():.4f} m/s\")\n",
    "        print(f\"\")\n",
    "        print(f\"üí° INTERPRETACI√ìN:\")\n",
    "        print(f\"   - Valores altos ‚Üí Turbulencia local / R√°fagas\")\n",
    "        print(f\"   - Valores bajos ‚Üí Flujo estable\")\n",
    "        print(f\"   - Captura variabilidad temporal no visible en la media\")\n",
    "        print(f\"\")\n",
    "        print(f\"üéØ USO EN MODELO ML:\")\n",
    "        print(f\"   - Feature adicional para predecir cargas\")\n",
    "        print(f\"   - Especialmente √∫til para predecir variabilidad/picos\")\n",
    "        print(f\"   - Complementa TI con informaci√≥n temporal local\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n‚úÖ Feature Engineering completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc2d2e",
   "metadata": {},
   "source": [
    "## ANALISIS DE LAS FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43976cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INSPECCI√ìN DE COLUMNAS DEL DATASET COMPLETO\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INSPECCI√ìN DE COLUMNAS DEL DATASET COMPLETO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ruta del dataset COMPLETO\n",
    "complete_data_path = Path(\"C:\\\\Users\\\\aitorredondoruiz\\\\Desktop\\\\2B_energy\\\\__Git\\\\Lidar_My_validation_VLOS\\\\data_train_traditional_ML\\\\0000_Complete_dataset.csv\")\n",
    "\n",
    "if not complete_data_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_data_path}\")\n",
    "else:\n",
    "    print(f\"\\nArchivo: {complete_data_path}\")\n",
    "    \n",
    "    # Leer solo el header (primera fila)\n",
    "    df_header = pd.read_csv(complete_data_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "    \n",
    "    print(f\"\\nüìä Total de columnas: {len(all_columns)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CATEGORIZAR COLUMNAS POR TIPO\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Targets (momentos Coleman)\n",
    "    target_cols = [col for col in all_columns if col in ['M_0', 'M_1c', 'M_1s', 'M_2c', 'M_2s']]\n",
    "    \n",
    "    # Targets originales (por si a√∫n existen)\n",
    "    target_original = [col for col in all_columns if 'Blade root' in col and 'My' in col]\n",
    "    \n",
    "    # Time\n",
    "    time_cols = [col for col in all_columns if col == 'Time']\n",
    "    \n",
    "    # VLOS (sin lag)\n",
    "    vlos_base = [col for col in all_columns if 'LAC_VLOS' in col and 'lag' not in col.lower()]\n",
    "    \n",
    "    # VLOS lags\n",
    "    vlos_lags = [col for col in all_columns if 'LAC_VLOS' in col and 'lag' in col.lower()]\n",
    "    \n",
    "    # Estad√≠sticas de viento (sin lag)\n",
    "    wind_stats_base = [col for col in all_columns if col in ['U_mean', 'U_std', 'U_shear_vert', 'U_shear_horiz']]\n",
    "    \n",
    "    # Estad√≠sticas de viento (con lag)\n",
    "    wind_stats_lags = [col for col in all_columns if any(col.startswith(f'{base}_lag') for base in ['U_mean', 'U_std', 'U_shear_vert', 'U_shear_horiz'])]\n",
    "    \n",
    "    # Pitch Coleman (sin rate)\n",
    "    pitch_coleman = [col for col in all_columns if col in ['pitch_0', 'pitch_1c', 'pitch_1s']]\n",
    "    \n",
    "    # Pitch Coleman rates\n",
    "    pitch_rates = [col for col in all_columns if col in ['pitch_0_rate', 'pitch_1c_rate', 'pitch_1s_rate']]\n",
    "    \n",
    "    # Rotor speed y derivatives\n",
    "    rotor_cols = [col for col in all_columns if 'Rotor speed' in col or 'rotor_speed_rate' in col]\n",
    "    \n",
    "    # Azimuth (sin/cos)\n",
    "    azimuth_cols = [col for col in all_columns if 'azimuth' in col.lower()]\n",
    "    \n",
    "    # Pitch original (blades)\n",
    "    pitch_original = [col for col in all_columns if 'Blade' in col and 'pitch angle' in col]\n",
    "    \n",
    "    # Otras columnas\n",
    "    other_cols = [col for col in all_columns if col not in \n",
    "                  target_cols + target_original + time_cols + vlos_base + vlos_lags + \n",
    "                  wind_stats_base + wind_stats_lags + pitch_coleman + pitch_rates + \n",
    "                  rotor_cols + azimuth_cols + pitch_original]\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MOSTRAR RESUMEN POR CATEGOR√çAS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESUMEN POR CATEGOR√çAS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    categories = [\n",
    "        (\"üéØ TARGETS (Coleman)\", target_cols),\n",
    "        (\"üéØ TARGETS (Originales)\", target_original),\n",
    "        (\"‚è∞ TIME\", time_cols),\n",
    "        (\"üå¨Ô∏è  VLOS Base (sin lag)\", vlos_base),\n",
    "        (\"üå¨Ô∏è  VLOS Lags\", vlos_lags),\n",
    "        (\"üìä Estad√≠sticas Viento Base\", wind_stats_base),\n",
    "        (\"üìä Estad√≠sticas Viento Lags\", wind_stats_lags),\n",
    "        (\"üîß Pitch Coleman\", pitch_coleman),\n",
    "        (\"üîß Pitch Rates\", pitch_rates),\n",
    "        (\"‚öôÔ∏è  Rotor Speed\", rotor_cols),\n",
    "        (\"üîÑ Azimuth (sin/cos)\", azimuth_cols),\n",
    "        (\"üîß Pitch Original (Blades)\", pitch_original),\n",
    "        (\"‚ùì Otras\", other_cols),\n",
    "    ]\n",
    "    \n",
    "    for category_name, cols in categories:\n",
    "        if len(cols) > 0:\n",
    "            print(f\"\\n{category_name}: {len(cols)} columnas\")\n",
    "            if len(cols) <= 10:\n",
    "                for col in cols:\n",
    "                    print(f\"  - {col}\")\n",
    "            else:\n",
    "                print(f\"  Primeras 5:\")\n",
    "                for col in cols[:5]:\n",
    "                    print(f\"  - {col}\")\n",
    "                print(f\"  ...\")\n",
    "                print(f\"  √öltimas 3:\")\n",
    "                for col in cols[-3:]:\n",
    "                    print(f\"  - {col}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CONTEO DE LAGS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"AN√ÅLISIS DE LAGS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Detectar lags √∫nicos\n",
    "    import re\n",
    "    lag_times = set()\n",
    "    \n",
    "    for col in vlos_lags + wind_stats_lags:\n",
    "        match = re.search(r'lag(\\d+)s', col)\n",
    "        if match:\n",
    "            lag_times.add(int(match.group(1)))\n",
    "    \n",
    "    lag_times = sorted(lag_times)\n",
    "    \n",
    "    if len(lag_times) > 0:\n",
    "        print(f\"\\nLags detectados: {lag_times[0]}s - {lag_times[-1]}s ({len(lag_times)} lags)\")\n",
    "        print(f\"Lags: {lag_times}\")\n",
    "    \n",
    "    # Contar VLOS lags por BEAM\n",
    "    vlos_beams = set()\n",
    "    for col in vlos_lags:\n",
    "        match = re.search(r'BEAM(\\d+)', col)\n",
    "        if match:\n",
    "            vlos_beams.add(int(match.group(1)))\n",
    "    \n",
    "    vlos_beams = sorted(vlos_beams)\n",
    "    print(f\"\\nVLOS - BEAMs detectados: {vlos_beams}\")\n",
    "    print(f\"VLOS - Lags por BEAM: {len(vlos_lags) // len(vlos_beams) if len(vlos_beams) > 0 else 0}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FEATURES vs TARGETS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FEATURES vs TARGETS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    feature_cols = [col for col in all_columns if col not in target_cols + target_original + time_cols]\n",
    "    \n",
    "    print(f\"\\n‚úÖ FEATURES (X): {len(feature_cols)} columnas\")\n",
    "    print(f\"‚úÖ TARGETS (y): {len(target_cols)} columnas: {target_cols}\")\n",
    "    print(f\"‚è∞ TIME: {len(time_cols)} columna: {time_cols}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VERIFICAR EXISTENCIA DE NUEVAS FEATURES\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"VERIFICACI√ìN DE NUEVAS FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    checks = [\n",
    "        (\"Pitch Coleman (pitch_0, pitch_1c, pitch_1s)\", \n",
    "         all([col in all_columns for col in ['pitch_0', 'pitch_1c', 'pitch_1s']])),\n",
    "        (\"Pitch Rates (pitch_0_rate, pitch_1c_rate, pitch_1s_rate)\",\n",
    "         all([col in all_columns for col in ['pitch_0_rate', 'pitch_1c_rate', 'pitch_1s_rate']])),\n",
    "        (\"Rotor Speed Rate (rotor_speed_rate)\",\n",
    "         'rotor_speed_rate' in all_columns),\n",
    "        (\"Estad√≠sticas Viento Base (U_mean, U_std, U_shear_vert, U_shear_horiz)\",\n",
    "         all([col in all_columns for col in ['U_mean', 'U_std', 'U_shear_vert', 'U_shear_horiz']])),\n",
    "        (\"Estad√≠sticas Viento Lags\",\n",
    "         len(wind_stats_lags) > 0),\n",
    "        (\"Targets Coleman (M_0, M_1c, M_1s)\",\n",
    "         all([col in all_columns for col in ['M_0', 'M_1c', 'M_1s']])),\n",
    "        (\"VLOS Lags\",\n",
    "         len(vlos_lags) > 0),\n",
    "        (\"Azimuth sin/cos\",\n",
    "         all([col in all_columns for col in ['sin_rotor_azimuth', 'cos_rotor_azimuth']])),\n",
    "    ]\n",
    "    \n",
    "    for feature_name, exists in checks:\n",
    "        status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "        print(f\"{status} {feature_name}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # GUARDAR LISTA DE COLUMNAS EN TXT\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"GUARDANDO LISTA DE COLUMNAS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    output_path = Path(\"C:\\\\Users\\\\aitorredondoruiz\\\\Desktop\\\\2B_energy\\\\__Git\\\\Lidar_My_validation_VLOS\\\\data_train_traditional_ML\\\\complete_dataset_columns_summary.txt\")\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        f.write(f\"COLUMNAS DEL DATASET: {complete_data_path.name}\\n\")\n",
    "        f.write(f\"Total: {len(all_columns)} columnas\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        \n",
    "        for category_name, cols in categories:\n",
    "            if len(cols) > 0:\n",
    "                f.write(f\"\\n{category_name}: {len(cols)} columnas\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                for col in cols:\n",
    "                    f.write(f\"  - {col}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        f.write(f\"FEATURES (X): {len(feature_cols)}\\n\")\n",
    "        f.write(f\"TARGETS (y): {len(target_cols)}\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        f.write(\"LISTA COMPLETA DE COLUMNAS (en orden)\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        for i, col in enumerate(all_columns, 1):\n",
    "            f.write(f\"{i:3d}. {col}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Lista guardada en: {output_path}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MOSTRAR TODAS LAS COLUMNAS (OPCIONAL)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"LISTA COMPLETA DE COLUMNAS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"(Mostrando primeras 50 y √∫ltimas 20)\\n\")\n",
    "    \n",
    "    print(\"Primeras 50:\")\n",
    "    for i, col in enumerate(all_columns[:50], 1):\n",
    "        print(f'{i:3d}. {col}')\n",
    "    \n",
    "    if len(all_columns) > 70:\n",
    "        print(\"\\n... [columnas intermedias omitidas] ...\\n\")\n",
    "        \n",
    "        print(f\"√öltimas 20:\")\n",
    "        start_idx = len(all_columns) - 20\n",
    "        for i, col in enumerate(all_columns[-20:], start_idx + 1):\n",
    "            print(f'{i:3d}. {col}')\n",
    "    \n",
    "    print(f\"\\nüí° Para ver TODAS las columnas, consulta el archivo:\")\n",
    "    print(f\"   {output_path}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"INSPECCI√ìN COMPLETADA\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06815f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü§ñ PARTE 3: MACHINE LEARNING\n",
    "\n",
    "---\n",
    "\n",
    "Una vez completado el an√°lisis exploratorio (EDA) y feature engineering, procedemos con el entrenamiento de modelos de Machine Learning para predecir las cargas en las palas del aerogenerador.\n",
    "\n",
    "## üìã Objetivos\n",
    "\n",
    "1. **Cargar dataset completo** con todas las features engineered\n",
    "2. **Definir variables de entrada (X) y salida (y)**\n",
    "3. **Entrenar modelos de regresi√≥n** (Linear, Random Forest, Gradient Boosting, etc.)\n",
    "4. **Evaluar rendimiento** con m√©tricas de error (MAE, RMSE, R¬≤)\n",
    "5. **Analizar feature importance** para entender qu√© variables son m√°s relevantes\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ PASO 6: Preparaci√≥n de Datos para ML\n",
    "\n",
    "En este paso:\n",
    "- Cargaremos el dataset completo `0000_Complete_dataset.csv`\n",
    "- Definiremos **targets (y)**: `Blade root 1 My`, `Blade root 2 My`\n",
    "- Definiremos **features (X)**: \n",
    "  - Variables operacionales: `Rotor speed`, `Blade pitch angles`, `sin/cos rotor azimuth`\n",
    "  - Variables LIDAR: Todas las columnas con `LAC_VLOS` y `lag`\n",
    "  - Features engineered: `VLOS_Rolling_Std`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 6.1: CARGA DEL DATASET Y DEFINICI√ìN DE X e Y\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ü§ñ MACHINE LEARNING: Preparaci√≥n de Datos\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar que existe el dataset completo\n",
    "if not complete_dataset_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}\")\n",
    "    print(\"   Aseg√∫rate de haber ejecutado los pasos anteriores\")\n",
    "else:\n",
    "    print(f\"Dataset: {complete_dataset_path.name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Leer cabecera para identificar columnas disponibles\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[1/4] Leyendo cabecera del dataset...\")\n",
    "    \n",
    "    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n",
    "    all_columns = df_header.columns.tolist()\n",
    "    \n",
    "    print(f\"‚úÖ Total columnas disponibles: {len(all_columns)}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2) Definir TARGETS (y)\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[2/4] Definiendo targets (y)...\")\n",
    "    \n",
    "    target_cols = ['M_0', 'M_1c', 'M_1s',]\n",
    "    \n",
    "    # Verificar que existen\n",
    "    missing_targets = [col for col in target_cols if col not in all_columns]\n",
    "    if missing_targets:\n",
    "        print(f\"\\n‚ùå ERROR: Targets faltantes: {missing_targets}\")\n",
    "        raise ValueError(\"No se encontraron todas las columnas target\")\n",
    "    \n",
    "    print(f\"‚úÖ Targets definidos:\")\n",
    "    for i, target in enumerate(target_cols, 1):\n",
    "        print(f\"   {i}. {target}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 3) Definir FEATURES (X)\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[3/4] Definiendo features (X)...\")\n",
    "    \n",
    "    # Features base (operacionales)\n",
    "    base_features = [\n",
    "        'Rotor speed',\n",
    "        'Blade 1 pitch angle',\n",
    "        'Blade 2 pitch angle',\n",
    "        'sin_rotor_azimuth',\n",
    "        'cos_rotor_azimuth',\n",
    "        'U_mean',\n",
    "        'U_std',\n",
    "        'U_shear_vert',\n",
    "        'U_shear_horiz',\n",
    "        'U_mean_lag2s',\n",
    "        'U_mean_lag5s',\n",
    "        'U_mean_lag8s',\n",
    "        'U_mean_lag11s',\n",
    "        'U_mean_lag14s',\n",
    "        'U_mean_lag17s',\n",
    "        'U_mean_lag20s',\n",
    "        'U_mean_lag23s',\n",
    "        'U_mean_lag26s',\n",
    "        'U_std_lag2s',\n",
    "        'U_std_lag5s',\n",
    "        'U_std_lag8s',\n",
    "        'U_std_lag11s',\n",
    "        'U_std_lag14s',\n",
    "        'U_std_lag17s',\n",
    "        'U_std_lag20s',\n",
    "        'U_std_lag23s',\n",
    "        'U_std_lag26s',\n",
    "        'U_shear_vert_lag2s',\n",
    "        'U_shear_vert_lag5s',\n",
    "        'U_shear_vert_lag8s',\n",
    "        'U_shear_vert_lag11s',\n",
    "        'U_shear_vert_lag14s',\n",
    "        'U_shear_vert_lag17s',\n",
    "        'U_shear_vert_lag20s',\n",
    "        'U_shear_vert_lag23s',\n",
    "        'U_shear_vert_lag26s',\n",
    "        'U_shear_horiz_lag2s',\n",
    "        'U_shear_horiz_lag5s',\n",
    "        'U_shear_horiz_lag8s',\n",
    "        'U_shear_horiz_lag11s',\n",
    "        'U_shear_horiz_lag14s',\n",
    "        'U_shear_horiz_lag17s',\n",
    "        'U_shear_horiz_lag20s',\n",
    "        'U_shear_horiz_lag23s',\n",
    "        'U_shear_horiz_lag26s',      \n",
    "        'pitch_0',\n",
    "        'pitch_1c',\n",
    "        'pitch_1s',\n",
    "        'pitch_0_rate',\n",
    "        'pitch_1c_rate',\n",
    "        'pitch_1s_rate'\n",
    "    ]\n",
    "    \n",
    "    # Verificar que existen\n",
    "    missing_base = [col for col in base_features if col not in all_columns]\n",
    "    if missing_base:\n",
    "        print(f\"\\n‚ö†Ô∏è  Features base faltantes: {missing_base}\")\n",
    "        base_features = [col for col in base_features if col in all_columns]\n",
    "    \n",
    "    print(f\"\\nüìã Features base (operacionales):\")\n",
    "    for feat in base_features:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    # Features LIDAR (autom√°ticas: todas con 'LAC_VLOS' o 'lag')\n",
    "    lidar_features = [\n",
    "       col for col in all_columns \n",
    "       if 'LAC_VLOS' in col  # Incluye LAC_VLOS_BEAM0_RANGE5 y LAC_VLOS_BEAM0_RANGE5_lag5s\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüåÄ Features LIDAR (LAC_VLOS + lag):\")\n",
    "    print(f\"   Total detectadas: {len(lidar_features)}\")\n",
    "    \n",
    "    # Mostrar algunos ejemplos\n",
    "    if len(lidar_features) > 0:\n",
    "        print(f\"   Ejemplos:\")\n",
    "        for feat in lidar_features[:5]:\n",
    "            print(f\"   - {feat}\")\n",
    "        if len(lidar_features) > 5:\n",
    "            print(f\"   - ... y {len(lidar_features) - 5} m√°s\")\n",
    "    \n",
    "    # Features engineered (si existen)\n",
    "    engineered_features = []\n",
    "    if 'No_inlcuir_esto' in all_columns:\n",
    "        engineered_features.append('VLOS_Rolling_Std')\n",
    "    \n",
    "    if engineered_features:\n",
    "        print(f\"\\nüîß Features engineered:\")\n",
    "        for feat in engineered_features:\n",
    "            print(f\"   - {feat}\")\n",
    "    \n",
    "    # Combinar todas las features\n",
    "    feature_cols = base_features + lidar_features \n",
    "    # feature_cols = base_features + lidar_features + engineered_features\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä RESUMEN DE FEATURES:\")\n",
    "    print(f\"   - Features base:       {len(base_features)}\")\n",
    "    print(f\"   - Features LIDAR:      {len(lidar_features)}\")\n",
    "    # print(f\"   - Features engineered: {len(engineered_features)}\")\n",
    "    print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"   - TOTAL FEATURES (X):  {len(feature_cols)}\")\n",
    "    print(f\"   - TOTAL TARGETS (y):   {len(target_cols)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4) Cargar dataset completo por chunks (OPTIMIZADO)\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n[4/4] Cargando dataset completo (por chunks optimizados)...\")\n",
    "    \n",
    "    # Columnas a cargar: features + targets + Time (para identificar series)\n",
    "    usecols = feature_cols + target_cols\n",
    "    \n",
    "    # Agregar Time si existe (necesario para identificar series temporales)\n",
    "    if 'Time' in all_columns:\n",
    "        usecols.append('Time')\n",
    "        print(\"\\n   ‚úÖ Columna 'Time' incluida para identificaci√≥n de series temporales\")\n",
    "    else:\n",
    "        print(\"\\n   ‚ö†Ô∏è  Columna 'Time' no encontrada - No se podr√° hacer split por series\")\n",
    "    \n",
    "    # Verificar que todas existen\n",
    "    missing_cols = [col for col in usecols if col not in all_columns]\n",
    "    if missing_cols:\n",
    "        print(f\"\\n‚ö†Ô∏è  Columnas faltantes (se omitir√°n): {missing_cols}\")\n",
    "        usecols = [col for col in usecols if col in all_columns]\n",
    "    \n",
    "    print(f\"   Total columnas a cargar: {len(usecols)}\")\n",
    "    \n",
    "    # ‚ö° OPTIMIZACI√ìN: Chunks m√°s peque√±os + dtypes optimizados\n",
    "    chunk_size = 5000  # Reducido de 100,000 a 5,000 para evitar MemoryError\n",
    "    \n",
    "    # Definir dtypes optimizados (float32 en vez de float64 ahorra 50% de memoria)\n",
    "    dtype_dict = {col: 'float32' for col in usecols}\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    print(f\"\\n   üì¶ Configuraci√≥n de carga:\")\n",
    "    print(f\"      - Chunk size: {chunk_size:,} filas\")\n",
    "    print(f\"      - Dtype: float32 (ahorro 50% memoria vs float64)\")\n",
    "    print(f\"      - Memoria estimada: ~{len(usecols) * 4 / 1024:.1f} MB por chunk\")\n",
    "    print(f\"\\n   ‚è≥ Leyendo chunks...\")\n",
    "    \n",
    "    chunk_count = 0\n",
    "    try:\n",
    "        for chunk in pd.read_csv(\n",
    "            complete_dataset_path, \n",
    "            usecols=usecols, \n",
    "            chunksize=chunk_size,\n",
    "            dtype=dtype_dict,\n",
    "            low_memory=True\n",
    "        ):\n",
    "            chunks.append(chunk)\n",
    "            chunk_count += 1\n",
    "            \n",
    "            # Mostrar progreso cada 50 chunks (250k filas)\n",
    "            if chunk_count % 50 == 0:\n",
    "                total_rows_loaded = chunk_count * chunk_size\n",
    "                print(f\"      ‚úì Cargadas ~{total_rows_loaded:,} filas ({chunk_count} chunks)...\")\n",
    "        \n",
    "        print(f\"\\n   ‚úÖ Lectura completada: {chunk_count} chunks le√≠dos\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n   ‚ùå ERROR al leer chunks: {str(e)}\")\n",
    "        if chunks:\n",
    "            print(f\"      Se cargaron {len(chunks)} chunks antes del error\")\n",
    "            print(f\"      Continuando con datos parciales...\")\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Concatenar todos los chunks\n",
    "    print(f\"\\n   üîó Concatenando {len(chunks)} chunks...\")\n",
    "    df_ml = pd.concat(chunks, ignore_index=True)\n",
    "    del chunks  # Liberar memoria\n",
    "    # 5) Separar X e y (manteniendo Time como metadata)\n",
    "    print(f\"\\n‚úÖ Dataset cargado exitosamente\")\n",
    "    print(f\"   Total filas: {len(df_ml):,}\")\n",
    "    print(f\"   Total columnas: {len(df_ml.columns)}\")\n",
    "    print(f\"   Memoria utilizada: {df_ml.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5) Separar X e y\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üì¶ CREACI√ìN DE MATRICES X e Y\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Verificar que los targets est√°n en el dataframe\n",
    "    available_targets = [col for col in target_cols if col in df_ml.columns]\n",
    "    available_features = [col for col in feature_cols if col in df_ml.columns]\n",
    "    \n",
    "    if len(available_targets) == 0:\n",
    "    # Crear matrices (sin Time, que se mantendr√° en df_ml para el split)\n",
    "        raise ValueError(\"Targets no disponibles\")\n",
    "    \n",
    "    if len(available_features) == 0:\n",
    "        print(\"\\n‚ùå ERROR: No se encontr√≥ ninguna feature\")\n",
    "        raise ValueError(\"Features no disponibles\")\n",
    "    \n",
    "    # Crear matrices\n",
    "    X = df_ml[available_features].copy()\n",
    "    y = df_ml[available_targets].copy()\n",
    "    # Verificar si Time est√° disponible\n",
    "    if 'Time' in df_ml.columns:\n",
    "        print(f\"   ‚ÑπÔ∏è  'Time' disponible en df_ml para identificar series temporales\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  'Time' no disponible - Split ser√° secuencial simple\")\n",
    "    \n",
    "    # Verificar NaNs\n",
    "    nan_features = X.isna().sum().sum()\n",
    "    nan_targets = y.isna().sum().sum()\n",
    "    \n",
    "    print(f\"\\nüîç Verificaci√≥n de NaNs:\")\n",
    "    print(f\"   NaNs en X: {nan_features:,} ({nan_features / X.size * 100:.2f}%)\")\n",
    "    print(f\"   NaNs en y: {nan_targets:,} ({nan_targets / y.size * 100:.2f}%)\")\n",
    "    print(f\"   NaNs en y: {nan_targets:,} ({nan_targets / y.size * 100:.2f}%)\")\n",
    "    if nan_features > 0 or nan_targets > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Se detectaron NaNs. Opciones:\")\n",
    "        print(f\"      1. Eliminar filas con NaNs: df_ml.dropna()\")\n",
    "        print(f\"      2. Imputar valores: SimpleImputer\")\n",
    "        print(f\"      3. Dejar como est√° si el modelo lo soporta (XGBoost, LightGBM)\")\n",
    "    \n",
    "    # Estad√≠sticas b√°sicas de X\n",
    "    print(f\"\\nüìä ESTAD√çSTICAS DE FEATURES (X):\")\n",
    "    print(f\"\\n   Top 5 features con mayor varianza:\")\n",
    "    feature_variances = X.var().sort_values(ascending=False).head(5)\n",
    "    for feat, var in feature_variances.items():\n",
    "        print(f\"      - {feat}: {var:.2e}\")\n",
    "    \n",
    "    # Estad√≠sticas de targets\n",
    "    print(f\"\\nüìä ESTAD√çSTICAS DE TARGETS (y):\")\n",
    "    for target in available_targets:\n",
    "        print(f\"\\n   {target}:\")\n",
    "        print(f\"      Min:    {y[target].min():.2f}\")\n",
    "        print(f\"      Max:    {y[target].max():.2f}\")\n",
    "        print(f\"      Mean:   {y[target].mean():.2f}\")\n",
    "        print(f\"      Median: {y[target].median():.2f}\")\n",
    "        print(f\"      Std:    {y[target].std():.2f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"‚úÖ PREPARACI√ìN DE DATOS COMPLETADA\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìå PR√ìXIMOS PASOS:\")\n",
    "\n",
    "    print(f\"   1. Tratamiento de NaNs (si es necesario)\")\n",
    "    print(f\"\\nüìå PR√ìXIMOS PASOS:\")    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"   2. Split train/test\")\n",
    "    print(f\"   1. Tratamiento de NaNs (si es necesario)\")    \n",
    "    print(f\"   5. Evaluaci√≥n y m√©tricas\")\n",
    "\n",
    "    print(f\"   3. Normalizaci√≥n/Escalado (opcional)\")\n",
    "    print(f\"   2. Split train/test\")    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"   4. Entrenamiento de modelos\")\n",
    "    print(f\"   3. Normalizaci√≥n/Escalado (opcional)\")    \n",
    "    print(f\"   4. Entrenamiento de modelos\")\n",
    "    print(f\"   5. Evaluaci√≥n y m√©tricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43f1f6",
   "metadata": {},
   "source": [
    "## üîÑ PASO 6.2: Divisi√≥n Train/Test (Por Bloques de Series Temporales)\n",
    "\n",
    "**Estrategia inteligente**: \n",
    "- Identificamos cada serie temporal completa usando la columna **Time** (25s ‚Üí 600s)\n",
    "- Hacemos split a nivel de **series completas**, no de filas individuales\n",
    "- Distribuimos series de **vientos bajos, medios y altos** en train y test\n",
    "- Evitamos **data leakage** manteniendo cada serie √≠ntegra en un solo conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50443685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 6.2: DIVISI√ìN TRAIN/TEST (POR BLOQUES DE SERIES TEMPORALES)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ DIVISI√ìN TRAIN/TEST (Por Bloques de Series Temporales)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar que tenemos la columna Time\n",
    "if 'Time' not in df_ml.columns:\n",
    "    print(\"\\n‚ùå ERROR: Se requiere la columna 'Time' para identificar series temporales\")\n",
    "    raise ValueError(\"Columna 'Time' no encontrada\")\n",
    "\n",
    "print(\"‚úÖ Columna 'Time' detectada\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Identificar cada serie temporal completa\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n[1/5] Identificando series temporales (bloques completos)...\")\n",
    "\n",
    "time_col = df_ml['Time'].copy()\n",
    "\n",
    "# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\n",
    "series_id = np.zeros(len(time_col), dtype=int)\n",
    "current_series = 0\n",
    "\n",
    "for i in range(1, len(time_col)):\n",
    "    if time_col.iloc[i] < time_col.iloc[i-1]:\n",
    "        current_series += 1\n",
    "    series_id[i] = current_series\n",
    "\n",
    "# Agregar series_id al dataframe\n",
    "df_ml['series_id'] = series_id\n",
    "\n",
    "n_series = series_id.max() + 1\n",
    "\n",
    "print(f\"\\n   ‚úÖ Series temporales identificadas: {n_series}\")\n",
    "\n",
    "# Analizar cada serie\n",
    "print(f\"\\n   üìä Primeras 10 series:\")\n",
    "for sid in range(min(10, n_series)):\n",
    "    mask = df_ml['series_id'] == sid\n",
    "    n_rows = mask.sum()\n",
    "    time_min = df_ml.loc[mask, 'Time'].min()\n",
    "    time_max = df_ml.loc[mask, 'Time'].max()\n",
    "    print(f\"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s\")\n",
    "\n",
    "if n_series > 10:\n",
    "    print(f\"      ... y {n_series - 10} series m√°s\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) Dividir series en train (80%) y test (20%)\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n[2/5] Dividiendo series en Train/Test (80/20)...\")\n",
    "\n",
    "train_ratio = 0.8\n",
    "n_train_series = int(n_series * train_ratio)\n",
    "n_test_series = n_series - n_train_series\n",
    "\n",
    "print(f\"\\n   Total series:    {n_series}\")\n",
    "print(f\"   Ratio objetivo:  {train_ratio*100:.0f}% train / {(1-train_ratio)*100:.0f}% test\")\n",
    "print(f\"   Series train:    {n_train_series} ({n_train_series/n_series*100:.1f}%)\")\n",
    "print(f\"   Series test:     {n_test_series} ({n_test_series/n_series*100:.1f}%)\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3) Estrategia de distribuci√≥n: Intercalar para mezclar vientos\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n[3/5] Distribuyendo series (intercaladas para mezclar velocidades)...\")\n",
    "\n",
    "# Crear lista de series alternando para train y test\n",
    "# Esto asegura que train y test tengan vientos bajos, medios y altos\n",
    "series_indices = np.arange(n_series)\n",
    "\n",
    "# Estrategia: tomar cada 5ta serie para test (aproximadamente 20%)\n",
    "# Ejemplo: train=[0,1,2,3, 5,6,7,8, 10,11,12,13, ...], test=[4, 9, 14, ...]\n",
    "test_series = series_indices[4::5]  # Cada 5ta serie\n",
    "train_series = np.array([s for s in series_indices if s not in test_series])\n",
    "\n",
    "print(f\"\\n   üìã Estrategia: Cada 5ta serie va a TEST\")\n",
    "print(f\"   Series reales en TRAIN: {len(train_series)}\")\n",
    "print(f\"   Series reales en TEST:  {len(test_series)}\")\n",
    "\n",
    "# Ajustar si es necesario para acercarse m√°s a 80/20\n",
    "if len(test_series) < n_test_series:\n",
    "    # Agregar m√°s series a test desde el final\n",
    "    additional_needed = n_test_series - len(test_series)\n",
    "    remaining_train = [s for s in train_series if s not in test_series]\n",
    "    additional_test = remaining_train[-additional_needed:]\n",
    "    test_series = np.concatenate([test_series, additional_test])\n",
    "    train_series = np.array([s for s in series_indices if s not in test_series])\n",
    "\n",
    "print(f\"\\n   ‚úÖ Series ajustadas:\")\n",
    "print(f\"      TRAIN: {len(train_series)} series ({len(train_series)/n_series*100:.1f}%)\")\n",
    "print(f\"      TEST:  {len(test_series)} series ({len(test_series)/n_series*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   üî¢ Series en TRAIN: {train_series[:10].tolist()}{'...' if len(train_series) > 10 else ''}\")\n",
    "print(f\"   üî¢ Series en TEST:  {test_series[:10].tolist()}{'...' if len(test_series) > 10 else ''}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4) Crear m√°scaras y dividir datos\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n[4/5] Creando conjuntos train y test...\")\n",
    "\n",
    "# Crear m√°scaras booleanas\n",
    "train_mask = df_ml['series_id'].isin(train_series)\n",
    "test_mask = df_ml['series_id'].isin(test_series)\n",
    "\n",
    "# Dividir X e y\n",
    "X_train = X[train_mask].copy()\n",
    "X_test = X[test_mask].copy()\n",
    "\n",
    "y_train = y[train_mask].copy()\n",
    "y_test = y[test_mask].copy()\n",
    "\n",
    "# Guardar metadata temporal (Time y series_id) para an√°lisis posterior\n",
    "Time_train = df_ml.loc[train_mask, 'Time'].copy()\n",
    "Time_test = df_ml.loc[test_mask, 'Time'].copy()\n",
    "\n",
    "series_id_train = df_ml.loc[train_mask, 'series_id'].copy()\n",
    "series_id_test = df_ml.loc[test_mask, 'series_id'].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Divisi√≥n completada:\")\n",
    "print(f\"   X_train: {X_train.shape} ({len(X_train):,} filas)\")\n",
    "print(f\"   X_test:  {X_test.shape} ({len(X_test):,} filas)\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   y_test:  {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nüìã Metadata temporal guardada:\")\n",
    "print(f\"   Time_train: {len(Time_train):,} valores\")\n",
    "print(f\"   Time_test:  {len(Time_test):,} valores\")\n",
    "print(f\"   series_id_train: {len(series_id_train):,} valores (series √∫nicas: {series_id_train.nunique()})\")\n",
    "print(f\"   series_id_test:  {len(series_id_test):,} valores (series √∫nicas: {series_id_test.nunique()})\")\n",
    "\n",
    "print(f\"\\nüí° Nota: Time y series_id NO est√°n en X (solo son metadata)\")\n",
    "print(f\"   X contiene √∫nicamente features predictivas para el modelo\")\n",
    "\n",
    "# Porcentajes reales de filas\n",
    "actual_train_pct = len(X_train) / len(X) * 100\n",
    "actual_test_pct = len(X_test) / len(X) * 100\n",
    "\n",
    "print(f\"\\nüìä Porcentajes reales (filas):\")\n",
    "print(f\"   Train: {actual_train_pct:.2f}%\")\n",
    "print(f\"   Test:  {actual_test_pct:.2f}%\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5) An√°lisis de distribuci√≥n de velocidades de viento (proxy: targets)\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n[5/5] Verificando distribuci√≥n de cargas (proxy para velocidad)...\")\n",
    "\n",
    "for target in y.columns:\n",
    "    print(f\"\\n   üìä {target}:\")\n",
    "    print(f\"      {'':12} {'TRAIN':>12} {'TEST':>12} {'DIFERENCIA':>12}\")\n",
    "    print(f\"      {'‚îÄ'*12} {'‚îÄ'*12} {'‚îÄ'*12} {'‚îÄ'*12}\")\n",
    "    \n",
    "    train_min = y_train[target].min()\n",
    "    test_min = y_test[target].min()\n",
    "    train_max = y_train[target].max()\n",
    "    test_max = y_test[target].max()\n",
    "    train_mean = y_train[target].mean()\n",
    "    test_mean = y_test[target].mean()\n",
    "    train_std = y_train[target].std()\n",
    "    test_std = y_test[target].std()\n",
    "    \n",
    "    print(f\"      {'Min:':12} {train_min:>12.2f} {test_min:>12.2f} {abs(train_min-test_min):>12.2f}\")\n",
    "    print(f\"      {'Max:':12} {train_max:>12.2f} {test_max:>12.2f} {abs(train_max-test_max):>12.2f}\")\n",
    "    print(f\"      {'Mean:':12} {train_mean:>12.2f} {test_mean:>12.2f} {abs(train_mean-test_mean):>12.2f}\")\n",
    "    print(f\"      {'Std:':12} {train_std:>12.2f} {test_std:>12.2f} {abs(train_std-test_std):>12.2f}\")\n",
    "    \n",
    "    # Evaluar si las distribuciones son similares\n",
    "    mean_diff_pct = abs(train_mean - test_mean) / train_mean * 100\n",
    "    if mean_diff_pct < 10:\n",
    "        print(f\"      ‚úÖ Distribuciones similares (diff media: {mean_diff_pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"      ‚ö†Ô∏è  Distribuciones diferentes (diff media: {mean_diff_pct:.1f}%)\")\n",
    "\n",
    "# Verificar NaNs\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üîç VERIFICACI√ìN DE CALIDAD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "nan_X_train = X_train.isna().sum().sum()\n",
    "nan_X_test = X_test.isna().sum().sum()\n",
    "nan_y_train = y_train.isna().sum().sum()\n",
    "nan_y_test = y_test.isna().sum().sum()\n",
    "\n",
    "print(f\"\\nüìä NaNs por conjunto:\")\n",
    "print(f\"   X_train: {nan_X_train:,} ({nan_X_train / X_train.size * 100:.2f}%)\")\n",
    "print(f\"   X_test:  {nan_X_test:,} ({nan_X_test / X_test.size * 100:.2f}%)\")\n",
    "print(f\"   y_train: {nan_y_train:,} ({nan_y_train / y_train.size * 100:.2f}%)\")\n",
    "print(f\"   y_test:  {nan_y_test:,} ({nan_y_test / y_test.size * 100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ DIVISI√ìN TRAIN/TEST COMPLETADA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìå VENTAJAS DE ESTE ENFOQUE:\")\n",
    "print(f\"   ‚úì Cada serie temporal completa en train O test (no mezcladas)\")\n",
    "print(f\"   ‚úì Train y test tienen series de diferentes velocidades de viento\")\n",
    "print(f\"   ‚úì Sin data leakage temporal\")\n",
    "print(f\"   ‚úì Distribuciones de cargas similares entre train y test\")\n",
    "\n",
    "print(f\"\\nüìå DATOS LISTOS PARA MODELADO:\")\n",
    "print(f\"   ‚úì X_train: {X_train.shape}\")\n",
    "print(f\"   ‚úì X_test:  {X_test.shape}\")\n",
    "print(f\"   ‚úì y_train: {y_train.shape}\")\n",
    "print(f\"   ‚úì y_test:  {y_test.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMEN DE SERIES TEMPORALES IDENTIFICADAS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üìã RESUMEN FINAL: SERIES TEMPORALES IDENTIFICADAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüî¢ TOTAL DE SERIES TEMPORALES DETECTADAS: {n_series}\")\n",
    "print(f\"\\n   Distribuci√≥n:\")\n",
    "print(f\"   ‚Ä¢ {len(train_series)} series en TRAIN ({len(train_series)/n_series*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ {len(test_series)} series en TEST  ({len(test_series)/n_series*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Rango de IDs:\")\n",
    "print(f\"   ‚Ä¢ Serie m√≠nima: 0\")\n",
    "print(f\"   ‚Ä¢ Serie m√°xima: {n_series - 1}\")\n",
    "\n",
    "print(f\"\\n   Filas totales por conjunto:\")\n",
    "print(f\"   ‚Ä¢ TRAIN: {len(X_train):,} filas\")\n",
    "print(f\"   ‚Ä¢ TEST:  {len(X_test):,} filas\")\n",
    "print(f\"   ‚Ä¢ TOTAL: {len(X):,} filas\")\n",
    "\n",
    "print(f\"\\nüí° VERIFICACI√ìN: Si tu dataset tiene una cantidad conocida de simulaciones,\")\n",
    "print(f\"   comprueba que el n√∫mero {n_series} coincida con lo esperado.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82724352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PASO 6.3: NORMALIZACI√ìN - Part 2/4: FIT SCALERS (INDEPENDIENTES)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PASO 6.3 - Part 2/4: AJUSTE DE ESCALADORES (INDEPENDIENTES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =========================================================================\n",
    "# VERIFICACI√ìN Y LIMPIEZA DE COLUMNAS DUPLICADAS\n",
    "# =========================================================================\n",
    "\n",
    "print(f\"\\nüîç Verificaci√≥n de datos:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "\n",
    "# Detectar columnas duplicadas\n",
    "n_cols = len(X_train.columns)\n",
    "n_unique = len(X_train.columns.unique())\n",
    "n_duplicated = n_cols - n_unique\n",
    "\n",
    "print(f\"   Total columnas: {n_cols}\")\n",
    "print(f\"   Columnas √∫nicas: {n_unique}\")\n",
    "\n",
    "if n_duplicated > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: {n_duplicated} nombres de columnas duplicados\")\n",
    "    print(f\"   Renombrando columnas duplicadas...\")\n",
    "    \n",
    "    # Renombrar columnas duplicadas agregando sufijo\n",
    "    new_columns = []\n",
    "    seen = {}\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            new_col = f\"{col}_dup{seen[col]}\"\n",
    "            new_columns.append(new_col)\n",
    "            print(f\"      '{col}' ‚Üí '{new_col}'\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            new_columns.append(col)\n",
    "    \n",
    "    # Aplicar nuevos nombres\n",
    "    X_train.columns = new_columns\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ Columnas renombradas. Ahora todas son √∫nicas.\")\n",
    "    print(f\"   Total columnas despu√©s: {len(X_train.columns)}\")\n",
    "\n",
    "# =========================================================================\n",
    "# CREAR CARPETA PARA ESCALADORES\n",
    "# =========================================================================\n",
    "\n",
    "models_folder = Path(\"01_Models_scaler\")\n",
    "models_folder.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\n‚úÖ Carpeta para escaladores: {models_folder}\")\n",
    "\n",
    "# =========================================================================\n",
    "# CREAR ESCALADORES INDEPENDIENTES PARA CADA FEATURE (X)\n",
    "# =========================================================================\n",
    "\n",
    "print(f\"\\n[1/2] Creando escaladores independientes para FEATURES (X)...\")\n",
    "print(f\"      Total features: {len(X_train.columns)}\")\n",
    "\n",
    "# Diccionario para almacenar un escalador por cada feature\n",
    "scalers_X = {}\n",
    "\n",
    "# Ajustar un escalador por cada columna de X_train\n",
    "for i, col in enumerate(X_train.columns, 1):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[[col]])\n",
    "    scalers_X[col] = scaler\n",
    "    \n",
    "    # Mostrar progreso cada 100 features\n",
    "    if i % 100 == 0 or i == len(X_train.columns):\n",
    "        print(f\"      ‚úÖ {i}/{len(X_train.columns)} escaladores ajustados...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total escaladores X creados: {len(scalers_X)}\")\n",
    "\n",
    "# Verificaci√≥n\n",
    "if len(scalers_X) != len(X_train.columns):\n",
    "    print(f\"‚ùå ERROR: N√∫mero de escaladores ({len(scalers_X)}) != n√∫mero de columnas ({len(X_train.columns)})\")\n",
    "    print(f\"   Esto NO deber√≠a ocurrir despu√©s de renombrar duplicados.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Verificaci√≥n: Todos los features tienen escalador\")\n",
    "\n",
    "# =========================================================================\n",
    "# CREAR ESCALADORES INDEPENDIENTES PARA CADA TARGET (Y)\n",
    "# =========================================================================\n",
    "\n",
    "print(f\"\\n[2/2] Creando escaladores independientes para TARGETS (Y)...\")\n",
    "\n",
    "# Diccionario para almacenar un escalador por cada target\n",
    "scalers_y = {}\n",
    "\n",
    "# Ajustar un escalador por cada columna de y_train\n",
    "for col in y_train.columns:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(y_train[[col]])\n",
    "    scalers_y[col] = scaler\n",
    "    print(f\"      ‚úÖ Escalador creado para: {col}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total escaladores Y creados: {len(scalers_y)}\")\n",
    "\n",
    "# =========================================================================\n",
    "# GUARDAR ESCALADORES\n",
    "# =========================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GUARDANDO ESCALADORES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Guardar escaladores de features\n",
    "scalers_X_path = models_folder / 'scalers_X.pkl'\n",
    "joblib.dump(scalers_X, scalers_X_path)\n",
    "print(f\"‚úÖ Escaladores X guardados: {scalers_X_path}\")\n",
    "print(f\"   Tipo: Diccionario con {len(scalers_X)} escaladores\")\n",
    "print(f\"   Tama√±o: {scalers_X_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# Guardar escaladores de targets\n",
    "scalers_y_path = models_folder / 'scalers_y.pkl'\n",
    "joblib.dump(scalers_y, scalers_y_path)\n",
    "print(f\"‚úÖ Escaladores Y guardados: {scalers_y_path}\")\n",
    "print(f\"   Tipo: Diccionario con {len(scalers_y)} escaladores\")\n",
    "\n",
    "# =========================================================================\n",
    "# VERIFICACI√ìN FINAL\n",
    "# =========================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"VERIFICACI√ìN FINAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar que todas las columnas tienen escalador\n",
    "missing_X = set(X_train.columns) - set(scalers_X.keys())\n",
    "missing_y = set(y_train.columns) - set(scalers_y.keys())\n",
    "\n",
    "if missing_X:\n",
    "    print(f\"‚ùå Faltan escaladores para features: {missing_X}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Todos los features ({len(X_train.columns)}) tienen escalador\")\n",
    "\n",
    "if missing_y:\n",
    "    print(f\"‚ùå Faltan escaladores para targets: {missing_y}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Todos los targets ({len(y_train.columns)}) tienen escalador\")\n",
    "\n",
    "# =========================================================================\n",
    "# RESUMEN\n",
    "# =========================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RESUMEN - PASO 2/4 COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Escaladores X (features): {len(scalers_X)}\")\n",
    "print(f\"üìä Escaladores Y (targets):  {len(scalers_y)}\")\n",
    "print(f\"\\nüí° M√©todo: StandardScaler independiente por variable\")\n",
    "print(f\"üí° Ventaja: Cada variable se escala seg√∫n su propia distribuci√≥n\")\n",
    "print(f\"üí° Archivos guardados en: {models_folder}\")\n",
    "print(f\"   - scalers_X.pkl ({len(scalers_X)} escaladores)\")\n",
    "print(f\"   - scalers_y.pkl ({len(scalers_y)} escaladores)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e572293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_norm.py - Normalizaci√≥n de datos con escaladores independientes\n",
    "\n",
    "Este script normaliza los datos de entrenamiento usando escaladores independientes\n",
    "por columna (un StandardScaler por cada feature y target).\n",
    "\n",
    "Prerequisitos:\n",
    "    - Los escaladores deben estar creados y guardados en:\n",
    "      * notebook/01_Models_scaler/scalers_X.pkl\n",
    "      * notebook/01_Models_scaler/scalers_y.pkl\n",
    "    - Los datos train/test deben estar disponibles en memoria (X_train, y_train)\n",
    "\n",
    "Outputs:\n",
    "    - notebook/02_Data_ML_traditional/X_train_norm.csv\n",
    "    - notebook/02_Data_ML_traditional/y_train_norm.csv\n",
    "    - notebook/02_Data_ML_traditional/normalization_metadata.json\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "class DataNormalizer:\n",
    "    \"\"\"\n",
    "    Clase para normalizar datos usando escaladores independientes por columna.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scalers_folder: str = \"notebook/01_Models_scaler\"):\n",
    "        \"\"\"\n",
    "        Inicializa el normalizador cargando los escaladores.\n",
    "        \n",
    "        Args:\n",
    "            scalers_folder: Carpeta donde est√°n guardados los escaladores\n",
    "        \"\"\"\n",
    "        self.scalers_folder = Path(scalers_folder)\n",
    "        self.scalers_X = None\n",
    "        self.scalers_y = None\n",
    "        \n",
    "    def load_scalers(self) -> None:\n",
    "        \"\"\"\n",
    "        Carga los escaladores desde archivos pickle.\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"CARGANDO ESCALADORES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Cargar escaladores de features\n",
    "        scalers_X_path = self.scalers_folder / 'scalers_X.pkl'\n",
    "        if not scalers_X_path.exists():\n",
    "            raise FileNotFoundError(f\"No se encuentra {scalers_X_path}\")\n",
    "        \n",
    "        self.scalers_X = joblib.load(scalers_X_path)\n",
    "        print(f\"‚úÖ Escaladores X cargados: {len(self.scalers_X)} escaladores\")\n",
    "        \n",
    "        # Cargar escaladores de targets\n",
    "        scalers_y_path = self.scalers_folder / 'scalers_y.pkl'\n",
    "        if not scalers_y_path.exists():\n",
    "            raise FileNotFoundError(f\"No se encuentra {scalers_y_path}\")\n",
    "        \n",
    "        self.scalers_y = joblib.load(scalers_y_path)\n",
    "        print(f\"‚úÖ Escaladores Y cargados: {len(self.scalers_y)} escaladores\")\n",
    "        print()\n",
    "        \n",
    "    def normalize_X_chunked(self, X: pd.DataFrame, chunk_size: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normaliza features X por chunks para eficiencia de memoria.\n",
    "        \n",
    "        Args:\n",
    "            X: DataFrame con features\n",
    "            chunk_size: N√∫mero de columnas a procesar por chunk\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame normalizado\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"NORMALIZANDO FEATURES (X)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nüîç Verificaci√≥n inicial:\")\n",
    "        print(f\"   X shape: {X.shape}\")\n",
    "        print(f\"   Escaladores disponibles: {len(self.scalers_X)}\")\n",
    "        \n",
    "        total_cols = len(X.columns)\n",
    "        n_chunks = int(np.ceil(total_cols / chunk_size))\n",
    "        \n",
    "        print(f\"\\nüìä Procesamiento por chunks:\")\n",
    "        print(f\"   Total columnas: {total_cols}\")\n",
    "        print(f\"   Chunk size: {chunk_size}\")\n",
    "        print(f\"   Total chunks: {n_chunks}\")\n",
    "        \n",
    "        chunks_list = []\n",
    "        \n",
    "        for chunk_idx in range(n_chunks):\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = min((chunk_idx + 1) * chunk_size, total_cols)\n",
    "            cols_chunk = X.columns[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"\\n[Chunk {chunk_idx+1}/{n_chunks}] Columnas {start_idx}-{end_idx-1} ({len(cols_chunk)} cols)\")\n",
    "            \n",
    "            # Crear lista de arrays normalizados\n",
    "            normalized_arrays = []\n",
    "            normalized_cols = []\n",
    "            \n",
    "            # Normalizar cada columna con su escalador\n",
    "            for i, col in enumerate(cols_chunk, 1):\n",
    "                if col not in self.scalers_X:\n",
    "                    raise KeyError(f\"No se encuentra escalador para columna: {col}\")\n",
    "                \n",
    "                # Transformar y agregar a la lista\n",
    "                arr_norm = self.scalers_X[col].transform(X[[col]]).ravel()\n",
    "                normalized_arrays.append(arr_norm)\n",
    "                normalized_cols.append(col)\n",
    "                \n",
    "                if i % 20 == 0 or i == len(cols_chunk):\n",
    "                    print(f\"      ‚úÖ {i}/{len(cols_chunk)} columnas normalizadas...\")\n",
    "            \n",
    "            # Crear DataFrame del chunk con np.column_stack\n",
    "            chunk_df = pd.DataFrame(\n",
    "                np.column_stack(normalized_arrays),\n",
    "                index=X.index,\n",
    "                columns=normalized_cols\n",
    "            )\n",
    "            chunks_list.append(chunk_df)\n",
    "            \n",
    "            print(f\"      ‚úÖ Chunk {chunk_idx+1} completado: {chunk_df.shape}\")\n",
    "        \n",
    "        # Concatenar todos los chunks\n",
    "        print(f\"\\nConcatenando {len(chunks_list)} chunks...\")\n",
    "        X_norm = pd.concat(chunks_list, axis=1)\n",
    "        \n",
    "        # Liberar memoria\n",
    "        del chunks_list\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"‚úÖ Normalizaci√≥n completada: {X_norm.shape}\")\n",
    "        \n",
    "        # Verificaci√≥n estad√≠stica\n",
    "        print(f\"\\nüìä Verificaci√≥n (primeras 5 columnas):\")\n",
    "        stats = X_norm.iloc[:, :5].describe().loc[['mean', 'std']]\n",
    "        print(stats)\n",
    "        print()\n",
    "        \n",
    "        return X_norm\n",
    "    \n",
    "    def normalize_y(self, y: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normaliza targets Y columna por columna.\n",
    "        \n",
    "        Args:\n",
    "            y: DataFrame con targets\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame normalizado\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"NORMALIZANDO TARGETS (Y)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nüîç Verificaci√≥n inicial:\")\n",
    "        print(f\"   y shape: {y.shape}\")\n",
    "        print(f\"   Escaladores disponibles: {len(self.scalers_y)}\")\n",
    "        \n",
    "        # Crear listas para datos normalizados\n",
    "        normalized_arrays = []\n",
    "        normalized_cols = []\n",
    "        \n",
    "        print(f\"\\nNormalizando {len(y.columns)} targets:\")\n",
    "        \n",
    "        # Normalizar cada target con su escalador\n",
    "        for col in y.columns:\n",
    "            if col not in self.scalers_y:\n",
    "                raise KeyError(f\"No se encuentra escalador para target: {col}\")\n",
    "            \n",
    "            arr_norm = self.scalers_y[col].transform(y[[col]]).ravel()\n",
    "            normalized_arrays.append(arr_norm)\n",
    "            normalized_cols.append(col)\n",
    "            print(f\"   ‚úÖ Target normalizado: {col}\")\n",
    "        \n",
    "        # Crear DataFrame con np.column_stack\n",
    "        y_norm = pd.DataFrame(\n",
    "            np.column_stack(normalized_arrays),\n",
    "            index=y.index,\n",
    "            columns=normalized_cols\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Normalizaci√≥n completada: {y_norm.shape}\")\n",
    "        \n",
    "        # Verificaci√≥n estad√≠stica\n",
    "        print(f\"\\nüìä Verificaci√≥n estad√≠stica:\")\n",
    "        stats = y_norm.describe().loc[['mean', 'std']]\n",
    "        print(stats)\n",
    "        print()\n",
    "        \n",
    "        return y_norm\n",
    "    \n",
    "    def save_normalized_data(self, \n",
    "                            X_norm: pd.DataFrame, \n",
    "                            y_norm: pd.DataFrame,\n",
    "                            output_folder: str = \"02_Data_ML_traditional\") -> None:\n",
    "        \"\"\"\n",
    "        Guarda los datos normalizados y metadata.\n",
    "        \n",
    "        Args:\n",
    "            X_norm: Features normalizados\n",
    "            y_norm: Targets normalizados\n",
    "            output_folder: Carpeta de destino\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"GUARDANDO DATOS NORMALIZADOS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Crear carpeta\n",
    "        output_path = Path(output_folder)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüìÅ Carpeta destino: {output_path}\")\n",
    "        \n",
    "        # Guardar X_train_norm\n",
    "        print(f\"\\n[1/3] Guardando X_train_norm...\")\n",
    "        X_norm_path = output_path / 'X_train_norm.csv'\n",
    "        X_norm.to_csv(X_norm_path, index=False)\n",
    "        \n",
    "        file_size_mb = X_norm_path.stat().st_size / (1024**2)\n",
    "        print(f\"   ‚úÖ Guardado: {X_norm_path.name}\")\n",
    "        print(f\"   Shape: {X_norm.shape}\")\n",
    "        print(f\"   Tama√±o: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Guardar y_train_norm\n",
    "        print(f\"\\n[2/3] Guardando y_train_norm...\")\n",
    "        y_norm_path = output_path / 'y_train_norm.csv'\n",
    "        y_norm.to_csv(y_norm_path, index=False)\n",
    "        \n",
    "        file_size_mb = y_norm_path.stat().st_size / (1024**2)\n",
    "        print(f\"   ‚úÖ Guardado: {y_norm_path.name}\")\n",
    "        print(f\"   Shape: {y_norm.shape}\")\n",
    "        print(f\"   Tama√±o: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Guardar metadata\n",
    "        print(f\"\\n[3/3] Guardando metadata...\")\n",
    "        \n",
    "        metadata = {\n",
    "            \"normalization_method\": \"StandardScaler independiente por columna\",\n",
    "            \"n_features\": len(X_norm.columns),\n",
    "            \"n_targets\": len(y_norm.columns),\n",
    "            \"n_samples_train\": len(X_norm),\n",
    "            \"feature_columns\": X_norm.columns.tolist(),\n",
    "            \"target_columns\": y_norm.columns.tolist(),\n",
    "            \"scalers_X_path\": str(self.scalers_folder / \"scalers_X.pkl\"),\n",
    "            \"scalers_y_path\": str(self.scalers_folder / \"scalers_y.pkl\"),\n",
    "            \"scalers_X_count\": len(self.scalers_X),\n",
    "            \"scalers_y_count\": len(self.scalers_y)\n",
    "        }\n",
    "        \n",
    "        metadata_path = output_path / 'normalization_metadata.json'\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"   ‚úÖ Guardado: {metadata_path.name}\")\n",
    "        \n",
    "        # Resumen final\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"RESUMEN\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"‚úÖ X_train_norm: {X_norm.shape}\")\n",
    "        print(f\"‚úÖ y_train_norm: {y_norm.shape}\")\n",
    "        print(f\"\\nüìÅ Archivos guardados:\")\n",
    "        print(f\"   - {X_norm_path}\")\n",
    "        print(f\"   - {y_norm_path}\")\n",
    "        print(f\"   - {metadata_path}\")\n",
    "        print(f\"\\nüìÅ Escaladores:\")\n",
    "        print(f\"   - {self.scalers_folder / 'scalers_X.pkl'} ({len(self.scalers_X)} escaladores)\")\n",
    "        print(f\"   - {self.scalers_folder / 'scalers_y.pkl'} ({len(self.scalers_y)} escaladores)\")\n",
    "        print(f\"\\nüí° M√©todo: Escalador independiente por cada variable\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "def normalize_train_data(X_train: pd.DataFrame, \n",
    "                         y_train: pd.DataFrame,\n",
    "                         scalers_folder: str = \"notebook/01_Models_scaler\",\n",
    "                         output_folder: str = \"notebook/02_Data_ML_traditional\",\n",
    "                         chunk_size: int = 100) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para normalizar datos de entrenamiento.\n",
    "    \n",
    "    Args:\n",
    "        X_train: DataFrame con features de entrenamiento\n",
    "        y_train: DataFrame con targets de entrenamiento\n",
    "        scalers_folder: Carpeta donde est√°n los escaladores\n",
    "        output_folder: Carpeta donde guardar los datos normalizados\n",
    "        chunk_size: Tama√±o de chunk para procesamiento de X\n",
    "        \n",
    "    Returns:\n",
    "        Tuple con (X_train_norm, y_train_norm)\n",
    "        \n",
    "    Example:\n",
    "        >>> X_train_norm, y_train_norm = normalize_train_data(X_train, y_train)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NORMALIZACI√ìN DE DATOS DE ENTRENAMIENTO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Entrada:\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # Crear normalizador\n",
    "    normalizer = DataNormalizer(scalers_folder=scalers_folder)\n",
    "    \n",
    "    # Cargar escaladores\n",
    "    normalizer.load_scalers()\n",
    "    \n",
    "    # Normalizar X_train\n",
    "    X_train_norm = normalizer.normalize_X_chunked(X_train, chunk_size=chunk_size)\n",
    "    \n",
    "    # Normalizar y_train\n",
    "    y_train_norm = normalizer.normalize_y(y_train)\n",
    "    \n",
    "    # Guardar datos\n",
    "    normalizer.save_normalized_data(X_train_norm, y_train_norm, output_folder=output_folder)\n",
    "    \n",
    "    print(\"\\n‚úÖ PROCESO COMPLETADO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return X_train_norm, y_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98373414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalizar datos (asumiendo que X_train y y_train ya existen)\n",
    "X_train_norm, y_train_norm = normalize_train_data(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    scalers_folder=\"01_Models_scaler\",\n",
    "    output_folder=\"02_Data_ML_traditional\",\n",
    "    chunk_size=100  # Ajusta seg√∫n tu memoria disponible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a02bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar escaladores\n",
    "import joblib\n",
    "\n",
    "scalers_X = joblib.load(\"01_Models_scaler/scalers_X.pkl\")\n",
    "scalers_y = joblib.load(\"01_Models_scaler/scalers_y.pkl\")\n",
    "\n",
    "print(f\"Escaladores X: {len(scalers_X)}\")\n",
    "print(f\"Escaladores Y: {len(scalers_y)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Verificar que los nombres coinciden\n",
    "print(f\"\\nColumnas en X_train: {len(X_train.columns)}\")\n",
    "print(f\"Columnas en escaladores: {len(scalers_X)}\")\n",
    "print(f\"¬øCoinciden? {set(X_train.columns) == set(scalers_X.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb49069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_data.py - Guardado de datasets de Machine Learning\n",
    "\n",
    "Este script guarda todos los datasets de machine learning en formato pickle:\n",
    "- Datos originales (X_train, y_train, X_test, y_test)\n",
    "- Datos normalizados (X_train_norm, y_train_norm)\n",
    "- Informaci√≥n temporal (Time_train, Time_test)\n",
    "- Identificadores de series (series_id_train, series_id_test)\n",
    "- Archivo de metadatos CSV\n",
    "\n",
    "Prerequisitos:\n",
    "    - Los datos deben estar disponibles en memoria\n",
    "    - Carpeta de destino: notebook/02_Data_ML_traditional/\n",
    "\n",
    "Outputs:\n",
    "    - 10 archivos .pkl con los datasets\n",
    "    - 1 archivo datasets_metadata.csv con informaci√≥n\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "\n",
    "class DataSaver:\n",
    "    \"\"\"\n",
    "    Clase para guardar datasets de Machine Learning de forma organizada.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_folder: str = \"notebook/02_Data_ML_traditional\"):\n",
    "        \"\"\"\n",
    "        Inicializa el guardador de datos.\n",
    "        \n",
    "        Args:\n",
    "            output_folder: Carpeta donde guardar los datasets\n",
    "        \"\"\"\n",
    "        self.output_folder = Path(output_folder)\n",
    "        self.file_paths = {}\n",
    "        \n",
    "    def save_all_datasets(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.DataFrame,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_test: pd.DataFrame,\n",
    "        X_train_norm: pd.DataFrame,\n",
    "        y_train_norm: pd.DataFrame,\n",
    "        Time_train: pd.Series,\n",
    "        Time_test: pd.Series,\n",
    "        series_id_train: pd.Series,\n",
    "        series_id_test: pd.Series\n",
    "    ) -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Guarda todos los datasets de ML.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Features de entrenamiento (original)\n",
    "            y_train: Targets de entrenamiento (original)\n",
    "            X_test: Features de test (original)\n",
    "            y_test: Targets de test (original)\n",
    "            X_train_norm: Features de entrenamiento (normalizado)\n",
    "            y_train_norm: Targets de entrenamiento (normalizado)\n",
    "            Time_train: Timestamps de entrenamiento\n",
    "            Time_test: Timestamps de test\n",
    "            series_id_train: IDs de series de entrenamiento\n",
    "            series_id_test: IDs de series de test\n",
    "            \n",
    "        Returns:\n",
    "            Diccionario con rutas de archivos guardados\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üíæ GUARDANDO DATASETS DE MACHINE LEARNING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Crear carpeta de destino\n",
    "        self._create_output_folder()\n",
    "        \n",
    "        # Guardar datos originales\n",
    "        self._save_original_data(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Guardar datos normalizados\n",
    "        self._save_normalized_data(X_train_norm, y_train_norm)\n",
    "        \n",
    "        # Guardar informaci√≥n temporal y series\n",
    "        self._save_temporal_info(Time_train, Time_test, series_id_train, series_id_test)\n",
    "        \n",
    "        # Crear metadatos\n",
    "        self._create_metadata(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            X_train_norm, y_train_norm,\n",
    "            Time_train, Time_test,\n",
    "            series_id_train, series_id_test\n",
    "        )\n",
    "        \n",
    "        # Resumen final\n",
    "        self._print_summary(X_train, X_test, y_train)\n",
    "        \n",
    "        return self.file_paths\n",
    "    \n",
    "    def _create_output_folder(self) -> None:\n",
    "        \"\"\"Crea la carpeta de salida si no existe.\"\"\"\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "        print(f\"\\nüìÅ Carpeta de destino: {self.output_folder}\")\n",
    "    \n",
    "    def _save_original_data(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.DataFrame,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_test: pd.DataFrame\n",
    "    ) -> None:\n",
    "        \"\"\"Guarda datos originales (sin normalizar).\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PASO 1: GUARDAR DATOS ORIGINALES (SIN NORMALIZAR)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # X_train\n",
    "        print(f\"\\n[1/4] Guardando X_train...\")\n",
    "        X_train_path = self.output_folder / 'X_train.pkl'\n",
    "        joblib.dump(X_train, X_train_path)\n",
    "        self.file_paths['X_train'] = X_train_path\n",
    "        print(f\"   ‚úÖ X_train guardado: {X_train.shape}\")\n",
    "        print(f\"      Tama√±o: {X_train_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "        \n",
    "        # y_train\n",
    "        print(f\"\\n[2/4] Guardando y_train...\")\n",
    "        y_train_path = self.output_folder / 'y_train.pkl'\n",
    "        joblib.dump(y_train, y_train_path)\n",
    "        self.file_paths['y_train'] = y_train_path\n",
    "        print(f\"   ‚úÖ y_train guardado: {y_train.shape}\")\n",
    "        print(f\"      Tama√±o: {y_train_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "        \n",
    "        # X_test\n",
    "        print(f\"\\n[3/4] Guardando X_test...\")\n",
    "        X_test_path = self.output_folder / 'X_test.pkl'\n",
    "        joblib.dump(X_test, X_test_path)\n",
    "        self.file_paths['X_test'] = X_test_path\n",
    "        print(f\"   ‚úÖ X_test guardado: {X_test.shape}\")\n",
    "        print(f\"      Tama√±o: {X_test_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "        \n",
    "        # y_test\n",
    "        print(f\"\\n[4/4] Guardando y_test...\")\n",
    "        y_test_path = self.output_folder / 'y_test.pkl'\n",
    "        joblib.dump(y_test, y_test_path)\n",
    "        self.file_paths['y_test'] = y_test_path\n",
    "        print(f\"   ‚úÖ y_test guardado: {y_test.shape}\")\n",
    "        print(f\"      Tama√±o: {y_test_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    def _save_normalized_data(\n",
    "        self,\n",
    "        X_train_norm: pd.DataFrame,\n",
    "        y_train_norm: pd.DataFrame\n",
    "    ) -> None:\n",
    "        \"\"\"Guarda datos normalizados.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PASO 2: GUARDAR DATOS NORMALIZADOS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # X_train_norm\n",
    "        print(f\"\\n[1/2] Guardando X_train_norm...\")\n",
    "        X_train_norm_path = self.output_folder / 'X_train_norm.pkl'\n",
    "        joblib.dump(X_train_norm, X_train_norm_path)\n",
    "        self.file_paths['X_train_norm'] = X_train_norm_path\n",
    "        print(f\"   ‚úÖ X_train_norm guardado: {X_train_norm.shape}\")\n",
    "        print(f\"      Tama√±o: {X_train_norm_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "        \n",
    "        # y_train_norm\n",
    "        print(f\"\\n[2/2] Guardando y_train_norm...\")\n",
    "        y_train_norm_path = self.output_folder / 'y_train_norm.pkl'\n",
    "        joblib.dump(y_train_norm, y_train_norm_path)\n",
    "        self.file_paths['y_train_norm'] = y_train_norm_path\n",
    "        print(f\"   ‚úÖ y_train_norm guardado: {y_train_norm.shape}\")\n",
    "        print(f\"      Tama√±o: {y_train_norm_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "        print(f\"      üí° Normalizado con scalers independientes por target\")\n",
    "    \n",
    "    def _save_temporal_info(\n",
    "        self,\n",
    "        Time_train: pd.Series,\n",
    "        Time_test: pd.Series,\n",
    "        series_id_train: pd.Series,\n",
    "        series_id_test: pd.Series\n",
    "    ) -> None:\n",
    "        \"\"\"Guarda informaci√≥n temporal y de series.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PASO 3: GUARDAR INFORMACI√ìN TEMPORAL Y SERIES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Time_train\n",
    "        print(f\"\\n[1/4] Guardando Time_train...\")\n",
    "        Time_train_path = self.output_folder / 'Time_train.pkl'\n",
    "        joblib.dump(Time_train, Time_train_path)\n",
    "        self.file_paths['Time_train'] = Time_train_path\n",
    "        print(f\"   ‚úÖ Time_train guardado: {Time_train.shape}\")\n",
    "        print(f\"      Tama√±o: {Time_train_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "        \n",
    "        # Time_test\n",
    "        print(f\"\\n[2/4] Guardando Time_test...\")\n",
    "        Time_test_path = self.output_folder / 'Time_test.pkl'\n",
    "        joblib.dump(Time_test, Time_test_path)\n",
    "        self.file_paths['Time_test'] = Time_test_path\n",
    "        print(f\"   ‚úÖ Time_test guardado: {Time_test.shape}\")\n",
    "        print(f\"      Tama√±o: {Time_test_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "        \n",
    "        # series_id_train\n",
    "        print(f\"\\n[3/4] Guardando series_id_train...\")\n",
    "        series_id_train_path = self.output_folder / 'series_id_train.pkl'\n",
    "        joblib.dump(series_id_train, series_id_train_path)\n",
    "        self.file_paths['series_id_train'] = series_id_train_path\n",
    "        print(f\"   ‚úÖ series_id_train guardado: {series_id_train.shape}\")\n",
    "        print(f\"      Tama√±o: {series_id_train_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "        \n",
    "        # series_id_test\n",
    "        print(f\"\\n[4/4] Guardando series_id_test...\")\n",
    "        series_id_test_path = self.output_folder / 'series_id_test.pkl'\n",
    "        joblib.dump(series_id_test, series_id_test_path)\n",
    "        self.file_paths['series_id_test'] = series_id_test_path\n",
    "        print(f\"   ‚úÖ series_id_test guardado: {series_id_test.shape}\")\n",
    "        print(f\"      Tama√±o: {series_id_test_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    def _create_metadata(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.DataFrame,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_test: pd.DataFrame,\n",
    "        X_train_norm: pd.DataFrame,\n",
    "        y_train_norm: pd.DataFrame,\n",
    "        Time_train: pd.Series,\n",
    "        Time_test: pd.Series,\n",
    "        series_id_train: pd.Series,\n",
    "        series_id_test: pd.Series\n",
    "    ) -> None:\n",
    "        \"\"\"Crea archivo de metadatos.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PASO 4: CREAR ARCHIVO DE METADATOS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Crear DataFrame con metadatos\n",
    "        metadata = {\n",
    "            'Dataset': [\n",
    "                'X_train', 'y_train', 'X_test', 'y_test',\n",
    "                'X_train_norm', 'y_train_norm', 'Time_train', 'Time_test',\n",
    "                'series_id_train', 'series_id_test'\n",
    "            ],\n",
    "            'Shape': [\n",
    "                str(X_train.shape), str(y_train.shape), str(X_test.shape), str(y_test.shape),\n",
    "                str(X_train_norm.shape), str(y_train_norm.shape), str(Time_train.shape), str(Time_test.shape),\n",
    "                str(series_id_train.shape), str(series_id_test.shape)\n",
    "            ],\n",
    "            'Tipo': [\n",
    "                'Features (original)', 'Targets (original)', 'Features (original)', 'Targets (original)',\n",
    "                'Features (normalizado)', 'Targets (norm. independiente)', 'Time info', 'Time info',\n",
    "                'Series ID', 'Series ID'\n",
    "            ],\n",
    "            'Archivo': [\n",
    "                'X_train.pkl', 'y_train.pkl', 'X_test.pkl', 'y_test.pkl',\n",
    "                'X_train_norm.pkl', 'y_train_norm.pkl', 'Time_train.pkl', 'Time_test.pkl',\n",
    "                'series_id_train.pkl', 'series_id_test.pkl'\n",
    "            ],\n",
    "            'Tama√±o_MB': [\n",
    "                f\"{self.file_paths['X_train'].stat().st_size / (1024**2):.2f}\",\n",
    "                f\"{self.file_paths['y_train'].stat().st_size / (1024**2):.2f}\",\n",
    "                f\"{self.file_paths['X_test'].stat().st_size / (1024**2):.2f}\",\n",
    "                f\"{self.file_paths['y_test'].stat().st_size / (1024**2):.2f}\",\n",
    "                f\"{self.file_paths['X_train_norm'].stat().st_size / (1024**2):.2f}\",\n",
    "                f\"{self.file_paths['y_train_norm'].stat().st_size / (1024**2):.2f}\",\n",
    "                f\"{self.file_paths['Time_train'].stat().st_size / (1024**2):.2f}\",\n",
    "                f\"{self.file_paths['Time_test'].stat().st_size / (1024**2):.2f}\",\n",
    "                f\"{self.file_paths['series_id_train'].stat().st_size / (1024**2):.2f}\",\n",
    "                f\"{self.file_paths['series_id_test'].stat().st_size / (1024**2):.2f}\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        metadata_df = pd.DataFrame(metadata)\n",
    "        metadata_path = self.output_folder / 'datasets_metadata.csv'\n",
    "        metadata_df.to_csv(metadata_path, index=False)\n",
    "        self.file_paths['metadata'] = metadata_path\n",
    "        \n",
    "        print(f\"\\n   ‚úÖ Metadatos guardados: {metadata_path.name}\")\n",
    "        print(f\"\\n   üìä RESUMEN:\")\n",
    "        print(metadata_df.to_string(index=False))\n",
    "    \n",
    "    def _print_summary(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_train: pd.DataFrame\n",
    "    ) -> None:\n",
    "        \"\"\"Imprime resumen final.\"\"\"\n",
    "        # Calcular tama√±o total\n",
    "        total_size = sum([\n",
    "            path.stat().st_size for key, path in self.file_paths.items() \n",
    "            if key != 'metadata'\n",
    "        ]) / (1024**2)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ DATASETS GUARDADOS CORRECTAMENTE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nüìä ESTAD√çSTICAS:\")\n",
    "        print(f\"   ‚Ä¢ Total de archivos: 10 + 1 metadata\")\n",
    "        print(f\"   ‚Ä¢ Tama√±o total: {total_size:.2f} MB\")\n",
    "        print(f\"   ‚Ä¢ Muestras train: {len(X_train):,}\")\n",
    "        print(f\"   ‚Ä¢ Muestras test: {len(X_test):,}\")\n",
    "        print(f\"   ‚Ä¢ Features: {X_train.shape[1]}\")\n",
    "        print(f\"   ‚Ä¢ Targets: {y_train.shape[1]} (M_0, M_1c, M_1s)\")\n",
    "        \n",
    "        print(f\"\\nüìÅ UBICACI√ìN:\")\n",
    "        print(f\"   {self.output_folder.absolute()}\")\n",
    "        \n",
    "        print(f\"\\nüí° C√ìMO CARGAR LOS DATOS:\")\n",
    "        print(f\"   ```python\")\n",
    "        print(f\"   import joblib\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Cargar datos originales\")\n",
    "        print(f\"   X_train = joblib.load('{self.output_folder / 'X_train.pkl'}')\")\n",
    "        print(f\"   y_train = joblib.load('{self.output_folder / 'y_train.pkl'}')\")\n",
    "        print(f\"   X_test = joblib.load('{self.output_folder / 'X_test.pkl'}')\")\n",
    "        print(f\"   y_test = joblib.load('{self.output_folder / 'y_test.pkl'}')\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Cargar datos normalizados\")\n",
    "        print(f\"   X_train_norm = joblib.load('{self.output_folder / 'X_train_norm.pkl'}')\")\n",
    "        print(f\"   y_train_norm = joblib.load('{self.output_folder / 'y_train_norm.pkl'}')\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # IMPORTANTE: y_train_norm usa scalers independientes por target\")\n",
    "        print(f\"   # Para normalizar/desnormalizar datos nuevos:\")\n",
    "        print(f\"   scalers_y = joblib.load('notebook/01_Models_scaler/scalers_y.pkl')\")\n",
    "        print(f\"   # Normalizar: y_norm[col] = scalers_y[col].transform(y[[col]])\")\n",
    "        print(f\"   # Desnormalizar: y[col] = scalers_y[col].inverse_transform(y_norm[[col]])\")\n",
    "        print(f\"   ```\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "def save_ml_datasets(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "    X_train_norm: pd.DataFrame,\n",
    "    y_train_norm: pd.DataFrame,\n",
    "    Time_train: pd.Series,\n",
    "    Time_test: pd.Series,\n",
    "    series_id_train: pd.Series,\n",
    "    series_id_test: pd.Series,\n",
    "    output_folder: str = \"02_Data_ML_traditional\"\n",
    ") -> Dict[str, Path]:\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para guardar todos los datasets de ML.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Features de entrenamiento (original)\n",
    "        y_train: Targets de entrenamiento (original)\n",
    "        X_test: Features de test (original)\n",
    "        y_test: Targets de test (original)\n",
    "        X_train_norm: Features de entrenamiento (normalizado)\n",
    "        y_train_norm: Targets de entrenamiento (normalizado)\n",
    "        Time_train: Timestamps de entrenamiento\n",
    "        Time_test: Timestamps de test\n",
    "        series_id_train: IDs de series de entrenamiento\n",
    "        series_id_test: IDs de series de test\n",
    "        output_folder: Carpeta de salida (default: notebook/02_Data_ML_traditional)\n",
    "        \n",
    "    Returns:\n",
    "        Diccionario con rutas de todos los archivos guardados\n",
    "        \n",
    "    Example:\n",
    "        >>> file_paths = save_ml_datasets(\n",
    "        ...     X_train, y_train, X_test, y_test,\n",
    "        ...     X_train_norm, y_train_norm,\n",
    "        ...     Time_train, Time_test,\n",
    "        ...     series_id_train, series_id_test\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    saver = DataSaver(output_folder)\n",
    "    return saver.save_all_datasets(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        X_train_norm, y_train_norm,\n",
    "        Time_train, Time_test,\n",
    "        series_id_train, series_id_test\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Guardar todos los datasets\n",
    "file_paths = save_ml_datasets(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    X_train_norm, y_train_norm,\n",
    "    Time_train, Time_test,\n",
    "    series_id_train, series_id_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7eb0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##   DE MOMENTO NO USAR ESTO ##\n",
    "\n",
    "# =============================================================================\n",
    "# GUARDAR DATASETS DE MACHINE LEARNING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ GUARDANDO DATASETS DE MACHINE LEARNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Crear carpeta para guardar los datasets\n",
    "data_ml_folder = root_dir / 'notebook' / '02_Data_ML_traditional'\n",
    "os.makedirs(data_ml_folder, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Carpeta de destino: {data_ml_folder}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Guardar datos originales (sin normalizar)\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: GUARDAR DATOS ORIGINALES (SIN NORMALIZAR)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/4] Guardando X_train...\")\n",
    "X_train_path = data_ml_folder / 'X_train.pkl'\n",
    "joblib.dump(X_train, X_train_path)\n",
    "print(f\"   ‚úÖ X_train guardado: {X_train.shape}\")\n",
    "print(f\"      Tama√±o: {X_train_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n[2/4] Guardando y_train...\")\n",
    "y_train_path = data_ml_folder / 'y_train.pkl'\n",
    "joblib.dump(y_train, y_train_path)\n",
    "print(f\"   ‚úÖ y_train guardado: {y_train.shape}\")\n",
    "print(f\"      Tama√±o: {y_train_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n[3/4] Guardando X_test...\")\n",
    "X_test_path = data_ml_folder / 'X_test.pkl'\n",
    "joblib.dump(X_test, X_test_path)\n",
    "print(f\"   ‚úÖ X_test guardado: {X_test.shape}\")\n",
    "print(f\"      Tama√±o: {X_test_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n[4/4] Guardando y_test...\")\n",
    "y_test_path = data_ml_folder / 'y_test.pkl'\n",
    "joblib.dump(y_test, y_test_path)\n",
    "print(f\"   ‚úÖ y_test guardado: {y_test.shape}\")\n",
    "print(f\"      Tama√±o: {y_test_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) Guardar datos normalizados\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: GUARDAR DATOS NORMALIZADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Guardando X_train_norm...\")\n",
    "X_train_norm_path = data_ml_folder / 'X_train_norm.pkl'\n",
    "joblib.dump(X_train_norm, X_train_norm_path)\n",
    "print(f\"   ‚úÖ X_train_norm guardado: {X_train_norm.shape}\")\n",
    "print(f\"      Tama√±o: {X_train_norm_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n[2/2] Guardando y_train_norm...\")\n",
    "y_train_norm_path = data_ml_folder / 'y_train_norm.pkl'\n",
    "joblib.dump(y_train_norm, y_train_norm_path)\n",
    "print(f\"   ‚úÖ y_train_norm guardado: {y_train_norm.shape}\")\n",
    "print(f\"      Tama√±o: {y_train_norm_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "print(f\"      üí° Normalizado con scalers independientes por target\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3) Guardar informaci√≥n adicional (Time_train, Time_test, series_id)\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 3: GUARDAR INFORMACI√ìN TEMPORAL Y SERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/4] Guardando Time_train...\")\n",
    "Time_train_path = data_ml_folder / 'Time_train.pkl'\n",
    "joblib.dump(Time_train, Time_train_path)\n",
    "print(f\"   ‚úÖ Time_train guardado: {Time_train.shape}\")\n",
    "print(f\"      Tama√±o: {Time_train_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n[2/4] Guardando Time_test...\")\n",
    "Time_test_path = data_ml_folder / 'Time_test.pkl'\n",
    "joblib.dump(Time_test, Time_test_path)\n",
    "print(f\"   ‚úÖ Time_test guardado: {Time_test.shape}\")\n",
    "print(f\"      Tama√±o: {Time_test_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n[3/4] Guardando series_id_train...\")\n",
    "series_id_train_path = data_ml_folder / 'series_id_train.pkl'\n",
    "joblib.dump(series_id_train, series_id_train_path)\n",
    "print(f\"   ‚úÖ series_id_train guardado: {series_id_train.shape}\")\n",
    "print(f\"      Tama√±o: {series_id_train_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n[4/4] Guardando series_id_test...\")\n",
    "series_id_test_path = data_ml_folder / 'series_id_test.pkl'\n",
    "joblib.dump(series_id_test, series_id_test_path)\n",
    "print(f\"   ‚úÖ series_id_test guardado: {series_id_test.shape}\")\n",
    "print(f\"      Tama√±o: {series_id_test_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4) Crear archivo de metadatos\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 4: CREAR ARCHIVO DE METADATOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calcular tama√±o total\n",
    "total_size = sum([\n",
    "    X_train_path.stat().st_size,\n",
    "    y_train_path.stat().st_size,\n",
    "    X_test_path.stat().st_size,\n",
    "    y_test_path.stat().st_size,\n",
    "    X_train_norm_path.stat().st_size,\n",
    "    y_train_norm_path.stat().st_size,\n",
    "    Time_train_path.stat().st_size,\n",
    "    Time_test_path.stat().st_size,\n",
    "    series_id_train_path.stat().st_size,\n",
    "    series_id_test_path.stat().st_size\n",
    "]) / (1024**2)\n",
    "\n",
    "# Crear DataFrame con metadatos\n",
    "metadata = {\n",
    "    'Dataset': [\n",
    "        'X_train', 'y_train', 'X_test', 'y_test',\n",
    "        'X_train_norm', 'y_train_norm', 'Time_train', 'Time_test',\n",
    "        'series_id_train', 'series_id_test'\n",
    "    ],\n",
    "    'Shape': [\n",
    "        str(X_train.shape), str(y_train.shape), str(X_test.shape), str(y_test.shape),\n",
    "        str(X_train_norm.shape), str(y_train_norm.shape), str(Time_train.shape), str(Time_test.shape),\n",
    "        str(series_id_train.shape), str(series_id_test.shape)\n",
    "    ],\n",
    "    'Tipo': [\n",
    "        'Features (original)', 'Targets (original)', 'Features (original)', 'Targets (original)',\n",
    "        'Features (normalizado)', 'Targets (norm. independiente)', 'Time info', 'Time info',\n",
    "        'Series ID', 'Series ID'\n",
    "    ],\n",
    "    'Archivo': [\n",
    "        'X_train.pkl', 'y_train.pkl', 'X_test.pkl', 'y_test.pkl',\n",
    "        'X_train_norm.pkl', 'y_train_norm.pkl', 'Time_train.pkl', 'Time_test.pkl',\n",
    "        'series_id_train.pkl', 'series_id_test.pkl'\n",
    "    ],\n",
    "    'Tama√±o_MB': [\n",
    "        f\"{X_train_path.stat().st_size / (1024**2):.2f}\",\n",
    "        f\"{y_train_path.stat().st_size / (1024**2):.2f}\",\n",
    "        f\"{X_test_path.stat().st_size / (1024**2):.2f}\",\n",
    "        f\"{y_test_path.stat().st_size / (1024**2):.2f}\",\n",
    "        f\"{X_train_norm_path.stat().st_size / (1024**2):.2f}\",\n",
    "        f\"{y_train_norm_path.stat().st_size / (1024**2):.2f}\",\n",
    "        f\"{Time_train_path.stat().st_size / (1024**2):.2f}\",\n",
    "        f\"{Time_test_path.stat().st_size / (1024**2):.2f}\",\n",
    "        f\"{series_id_train_path.stat().st_size / (1024**2):.2f}\",\n",
    "        f\"{series_id_test_path.stat().st_size / (1024**2):.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_path = data_ml_folder / 'datasets_metadata.csv'\n",
    "metadata_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "print(f\"\\n   ‚úÖ Metadatos guardados: {metadata_path.name}\")\n",
    "print(f\"\\n   üìä RESUMEN:\")\n",
    "print(metadata_df.to_string(index=False))\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# RESUMEN FINAL\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ DATASETS GUARDADOS CORRECTAMENTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä ESTAD√çSTICAS:\")\n",
    "print(f\"   ‚Ä¢ Total de archivos: 10 + 1 metadata\")\n",
    "print(f\"   ‚Ä¢ Tama√±o total: {total_size:.2f} MB\")\n",
    "print(f\"   ‚Ä¢ Muestras train: {len(X_train):,}\")\n",
    "print(f\"   ‚Ä¢ Muestras test: {len(X_test):,}\")\n",
    "print(f\"   ‚Ä¢ Features: {X_train.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Targets: {y_train.shape[1]} (M_0, M_1c, M_1s)\")\n",
    "\n",
    "print(f\"\\nüìÅ UBICACI√ìN:\")\n",
    "print(f\"   {data_ml_folder}\")\n",
    "\n",
    "print(f\"\\nüí° C√ìMO CARGAR LOS DATOS:\")\n",
    "print(f\"   ```python\")\n",
    "print(f\"   import joblib\")\n",
    "print(f\"   \")\n",
    "print(f\"   # Cargar datos originales\")\n",
    "print(f\"   X_train = joblib.load('{data_ml_folder / 'X_train.pkl'}')\")\n",
    "print(f\"   y_train = joblib.load('{data_ml_folder / 'y_train.pkl'}')\")\n",
    "print(f\"   X_test = joblib.load('{data_ml_folder / 'X_test.pkl'}')\")\n",
    "print(f\"   y_test = joblib.load('{data_ml_folder / 'y_test.pkl'}')\")\n",
    "print(f\"   \")\n",
    "print(f\"   # Cargar datos normalizados\")\n",
    "print(f\"   X_train_norm = joblib.load('{data_ml_folder / 'X_train_norm.pkl'}')\")\n",
    "print(f\"   y_train_norm = joblib.load('{data_ml_folder / 'y_train_norm.pkl'}')\")\n",
    "print(f\"   \")\n",
    "print(f\"   # IMPORTANTE: y_train_norm usa scalers independientes por target\")\n",
    "print(f\"   # Para normalizar/desnormalizar datos nuevos:\")\n",
    "print(f\"   scalers_y = joblib.load('01_Models_scaler/scalers_y.pkl')\")\n",
    "print(f\"   # Normalizar: y_norm[col] = scalers_y[col].transform(y[[col]])\")\n",
    "print(f\"   # Desnormalizar: y[col] = scalers_y[col].inverse_transform(y_norm[[col]])\")\n",
    "print(f\"   ```\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061809e3",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ PASO 7: MODELADO - BASELINE RIDGE REGRESSION\n",
    "---\n",
    "\n",
    "**Objetivo:** Crear un modelo baseline con Ridge Regression (regresi√≥n lineal con regularizaci√≥n L2)\n",
    "\n",
    "**¬øPor qu√© Ridge?**\n",
    "- Es un modelo lineal simple y r√°pido\n",
    "- La regularizaci√≥n L2 evita overfitting\n",
    "- Sirve como punto de referencia para comparar modelos m√°s complejos\n",
    "\n",
    "**Estrategia:**\n",
    "1. Entrenar con datos normalizados (X_train_norm, y_train_norm)\n",
    "2. Normalizar datos de test usando los scalers guardados\n",
    "3. Predecir sobre test normalizado\n",
    "4. Desnormalizar las predicciones para calcular m√©tricas en escala original\n",
    "5. Calcular RMSE y R¬≤ para cada pala por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c68840",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('03_ML_traditional_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 7.1: BASELINE - RIDGE REGRESSION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ MODELO BASELINE: RIDGE REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "\n",
    "# Crear carpeta para guardar resultados\n",
    "training_folder = Path('03_ML_traditional_models') / 'Linear_Ridge'\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Carpeta de entrenamiento: {training_folder}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Normalizar datos de TEST usando los scalers guardados\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: NORMALIZAR DATOS DE TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Normalizando X_test usando scaler guardado...\")\n",
    "\n",
    "# Cargar el scaler de X desde la carpeta 01_Models_scaler\n",
    "# ‚úÖ (scalers independientes)\n",
    "scalers_X_path = root_dir / 'notebook' / '01_Models_scaler' / 'scalers_X.pkl'\n",
    "scalers_X = joblib.load(scalers_X_path)\n",
    "\n",
    "# Transformar X_test (cada columna con su scaler)\n",
    "X_test_norm = pd.DataFrame(index=X_test.index)\n",
    "for col in X_test.columns:\n",
    "    if col not in scalers_X:\n",
    "        raise KeyError(f\"No se encuentra scaler para columna: {col}\")\n",
    "    X_test_norm[col] = scalers_X[col].transform(X_test[[col]]).ravel()\n",
    "\n",
    "print(f\"   ‚úÖ X_test normalizado: {X_test_norm.shape}\")\n",
    "print(f\"   üìÇ Scalers cargados de: {scalers_X_path}\")\n",
    "print(f\"   üí° {len(scalers_X)} scalers independientes usados\")\n",
    "\n",
    "print(f\"   ‚úÖ X_test normalizado: {X_test_norm.shape}\")\n",
    "print(f\"   üìÇ Scaler cargado de: {scaler_X_path}\")\n",
    "\n",
    "print(f\"\\n[2/2] Normalizando y_test usando scalers independientes...\")\n",
    "\n",
    "# Cargar los scalers de y desde la carpeta 01_Models_scaler\n",
    "scalers_y_path = root_dir / 'notebook' / '01_Models_scaler' / 'scalers_y.pkl'\n",
    "scalers_y = joblib.load(scalers_y_path)\n",
    "\n",
    "# Transformar y_test (cada columna con su scaler)\n",
    "y_test_norm = pd.DataFrame(index=y_test.index)\n",
    "for col in y_test.columns:\n",
    "    y_test_norm[col] = scalers_y[col].transform(y_test[[col]]).ravel()\n",
    "\n",
    "print(f\"   ‚úÖ y_test normalizado: {y_test_norm.shape}\")\n",
    "print(f\"   üìÇ Scalers cargados de: {scalers_y_path}\")\n",
    "print(f\"   üí° {len(scalers_y)} scalers independientes: {list(scalers_y.keys())}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) Verificar datos de entrenamiento normalizados\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"VERIFICACI√ìN: DATOS DE ENTRENAMIENTO NORMALIZADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   ‚úÖ X_train_norm disponible: {X_train_norm.shape}\")\n",
    "print(f\"   ‚úÖ y_train_norm disponible: {y_train_norm.shape}\")\n",
    "print(f\"\\n   üí° Usando datos ya normalizados en pasos anteriores\")\n",
    "print(f\"   üí° y_train_norm usa {len(scalers_y)} scalers independientes por target\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3) Configurar y entrenar modelo Ridge\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: ENTRENAMIENTO DEL MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/3] Configurando Ridge Regression...\")\n",
    "\n",
    "# Configurar Ridge con MultiOutputRegressor para predecir los 3 targets\n",
    "ridge_model = MultiOutputRegressor(\n",
    "    Ridge(\n",
    "        alpha=1.0,          # Par√°metro de regularizaci√≥n\n",
    "        fit_intercept=True,\n",
    "        solver='auto', #saga\n",
    "        max_iter=1000,      # Iteraciones m√°ximas\n",
    "        tol=1e-3,           # Tolerancia de convergencia\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"   ‚úÖ Modelo configurado:\")\n",
    "print(f\"      ‚Ä¢ Tipo: Ridge Regression\")\n",
    "print(f\"      ‚Ä¢ Alpha (regularizaci√≥n): 1.0\")\n",
    "print(f\"      ‚Ä¢ Outputs: {len(y_train_norm.columns)} targets (M_0, M_1c, M_1s)\")\n",
    "\n",
    "print(f\"\\n[2/3] Entrenando modelo con datos normalizados...\")\n",
    "print(f\"   ‚Ä¢ Datos de entrenamiento: {X_train_norm.shape}\")\n",
    "print(f\"   ‚Ä¢ Targets de entrenamiento: {y_train_norm.shape}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Entrenar el modelo con datos normalizados\n",
    "ridge_model.fit(X_train_norm, y_train_norm)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ‚úÖ Modelo entrenado en {training_time:.2f} segundos\")\n",
    "\n",
    "print(f\"\\n[3/3] Guardando modelo...\")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model_path = training_folder / 'ridge_model.pkl'\n",
    "joblib.dump(ridge_model, model_path)\n",
    "\n",
    "print(f\"   ‚úÖ Modelo guardado: {model_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4) Realizar predicciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 3: PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Prediciendo sobre conjunto de entrenamiento...\")\n",
    "\n",
    "y_train_pred_norm = ridge_model.predict(X_train_norm)\n",
    "y_train_pred_norm = pd.DataFrame(\n",
    "    y_train_pred_norm,\n",
    "    index=y_train_norm.index,\n",
    "    columns=y_train_norm.columns\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones train: {y_train_pred_norm.shape}\")\n",
    "\n",
    "print(f\"\\n[2/2] Prediciendo sobre conjunto de test...\")\n",
    "\n",
    "y_test_pred_norm = ridge_model.predict(X_test_norm)\n",
    "y_test_pred_norm = pd.DataFrame(\n",
    "    y_test_pred_norm,\n",
    "    index=y_test_norm.index,\n",
    "    columns=y_test_norm.columns\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones test: {y_test_pred_norm.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5) Desnormalizar predicciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 4: DESNORMALIZACI√ìN DE PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Desnormalizando predicciones de train con scalers independientes...\")\n",
    "\n",
    "# Usar los scalers independientes para desnormalizar\n",
    "y_train_pred = pd.DataFrame(index=y_train_pred_norm.index)\n",
    "for col in y_train_pred_norm.columns:\n",
    "    y_train_pred[col] = scalers_y[col].inverse_transform(y_train_pred_norm[[col]]).ravel()\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones train desnormalizadas\")\n",
    "\n",
    "print(f\"\\n[2/2] Desnormalizando predicciones de test con scalers independientes...\")\n",
    "\n",
    "# Usar los scalers independientes para desnormalizar\n",
    "y_test_pred = pd.DataFrame(index=y_test_pred_norm.index)\n",
    "for col in y_test_pred_norm.columns:\n",
    "    y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm[[col]]).ravel()\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones test desnormalizadas\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6) Calcular m√©tricas\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 5: C√ÅLCULO DE M√âTRICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Diccionarios para almacenar m√©tricas\n",
    "metrics_train = {}\n",
    "metrics_test = {}\n",
    "\n",
    "print(f\"\\n{'M√âTRICA':20} {'CONJUNTO':10} {'M_0':>15} {'M_1c':>15} {'M_1s':>15}\")\n",
    "print(f\"{'‚îÄ'*20} {'‚îÄ'*10} {'‚îÄ'*15} {'‚îÄ'*15} {'‚îÄ'*15}\")\n",
    "\n",
    "# Calcular m√©tricas para cada target\n",
    "for i, col in enumerate(y_train.columns):\n",
    "    # TRAIN\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train[col], y_train_pred[col]))\n",
    "    r2_train = r2_score(y_train[col], y_train_pred[col])\n",
    "    \n",
    "    # TEST\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test[col], y_test_pred[col]))\n",
    "    r2_test = r2_score(y_test[col], y_test_pred[col])\n",
    "    \n",
    "    # Guardar m√©tricas\n",
    "    metrics_train[col] = {'RMSE': rmse_train, 'R2': r2_train}\n",
    "    metrics_test[col] = {'RMSE': rmse_test, 'R2': r2_test}\n",
    "    \n",
    "    # Mostrar\n",
    "    if i == 0:\n",
    "        print(f\"{'RMSE':20} {'TRAIN':10} {rmse_train:>15.2f} {'-':>15} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {rmse_test:>15.2f} {'-':>15} {'-':>15}\")\n",
    "        print(f\"{'R¬≤':20} {'TRAIN':10} {r2_train:>15.4f} {'-':>15} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {r2_test:>15.4f} {'-':>15} {'-':>15}\")\n",
    "    elif i == 1:\n",
    "        print(f\"{'RMSE':20} {'TRAIN':10} {'-':>15} {rmse_train:>15.2f} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {rmse_test:>15.2f} {'-':>15}\")\n",
    "        print(f\"{'R¬≤':20} {'TRAIN':10} {'-':>15} {r2_train:>15.4f} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {r2_test:>15.4f} {'-':>15}\")\n",
    "    else:\n",
    "        print(f\"{'RMSE':20} {'TRAIN':10} {'-':>15} {'-':>15} {rmse_train:>15.2f}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {'-':>15} {rmse_test:>15.2f}\")\n",
    "        print(f\"{'R¬≤':20} {'TRAIN':10} {'-':>15} {'-':>15} {r2_train:>15.4f}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {'-':>15} {r2_test:>15.4f}\")\n",
    "\n",
    "# Calcular promedios\n",
    "avg_rmse_train = np.mean([m['RMSE'] for m in metrics_train.values()])\n",
    "avg_rmse_test = np.mean([m['RMSE'] for m in metrics_test.values()])\n",
    "avg_r2_train = np.mean([m['R2'] for m in metrics_train.values()])\n",
    "avg_r2_test = np.mean([m['R2'] for m in metrics_test.values()])\n",
    "\n",
    "print(f\"{'‚îÄ'*20} {'‚îÄ'*10} {'‚îÄ'*15} {'‚îÄ'*15} {'‚îÄ'*15}\")\n",
    "print(f\"{'PROMEDIO RMSE':20} {'TRAIN':10} {avg_rmse_train:>15.2f}\")\n",
    "print(f\"{'':20} {'TEST':10} {avg_rmse_test:>15.2f}\")\n",
    "print(f\"{'PROMEDIO R¬≤':20} {'TRAIN':10} {avg_r2_train:>15.4f}\")\n",
    "print(f\"{'':20} {'TEST':10} {avg_r2_test:>15.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 7) Visualizaciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 6: GENERACI√ìN DE GR√ÅFICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 7.1) Gr√°fica de m√©tricas por target\n",
    "print(f\"\\n[1/5] Creando gr√°fica de m√©tricas...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "targets = list(metrics_train.keys())\n",
    "rmse_train_vals = [metrics_train[t]['RMSE'] for t in targets]\n",
    "rmse_test_vals = [metrics_test[t]['RMSE'] for t in targets]\n",
    "\n",
    "x = np.arange(len(targets))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, rmse_train_vals, width, label='Train', alpha=0.8)\n",
    "axes[0].bar(x + width/2, rmse_test_vals, width, label='Test', alpha=0.8)\n",
    "axes[0].set_xlabel('Target')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE por Target - Ridge Regression')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(targets, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤\n",
    "r2_train_vals = [metrics_train[t]['R2'] for t in targets]\n",
    "r2_test_vals = [metrics_test[t]['R2'] for t in targets]\n",
    "\n",
    "axes[1].bar(x - width/2, r2_train_vals, width, label='Train', alpha=0.8)\n",
    "axes[1].bar(x + width/2, r2_test_vals, width, label='Test', alpha=0.8)\n",
    "axes[1].set_xlabel('Target')\n",
    "axes[1].set_ylabel('R¬≤')\n",
    "axes[1].set_title('R¬≤ por Target - Ridge Regression')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(targets, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "metrics_plot_path = training_folder / 'metrics_comparison.png'\n",
    "plt.savefig(metrics_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {metrics_plot_path.name}\")\n",
    "\n",
    "\n",
    "# 7.2) Gr√°ficas de predicciones vs reales\n",
    "print(f\"\\n[2/3] Creando gr√°ficas de predicciones vs reales...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 16))\n",
    "\n",
    "for idx, col in enumerate(y_train.columns):\n",
    "    row = idx\n",
    "    \n",
    "    # TRAIN\n",
    "    axes[row, 0].scatter(y_train[col], y_train_pred[col], alpha=0.3, s=1)\n",
    "    axes[row, 0].plot([y_train[col].min(), y_train[col].max()], \n",
    "                       [y_train[col].min(), y_train[col].max()], \n",
    "                       'r--', lw=2, label='Perfect prediction')\n",
    "    axes[row, 0].set_xlabel('Real')\n",
    "    axes[row, 0].set_ylabel('Predicho')\n",
    "    axes[row, 0].set_title(f'{col} - TRAIN (R¬≤={metrics_train[col][\"R2\"]:.4f})')\n",
    "    axes[row, 0].legend()\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # TEST\n",
    "    axes[row, 1].scatter(y_test[col], y_test_pred[col], alpha=0.3, s=1, color='orange')\n",
    "    axes[row, 1].plot([y_test[col].min(), y_test[col].max()], \n",
    "                       [y_test[col].min(), y_test[col].max()], \n",
    "                       'r--', lw=2, label='Perfect prediction')\n",
    "    axes[row, 1].set_xlabel('Real')\n",
    "    axes[row, 1].set_ylabel('Predicho')\n",
    "    axes[row, 1].set_title(f'{col} - TEST (R¬≤={metrics_test[col][\"R2\"]:.4f})')\n",
    "    axes[row, 1].legend()\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "predictions_plot_path = training_folder / 'predictions_vs_real.png'\n",
    "plt.savefig(predictions_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {predictions_plot_path.name}\")\n",
    "\n",
    "# 7.3) Gr√°fica de residuos\n",
    "print(f\"\\n[3/3] Creando gr√°fica de residuos...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 16))\n",
    "\n",
    "for idx, col in enumerate(y_train.columns):\n",
    "    row = idx\n",
    "    \n",
    "    # Calcular residuos\n",
    "    residuals_train = y_train[col] - y_train_pred[col]\n",
    "    residuals_test = y_test[col] - y_test_pred[col]\n",
    "    \n",
    "    # TRAIN\n",
    "    axes[row, 0].scatter(y_train_pred[col], residuals_train, alpha=0.3, s=1)\n",
    "    axes[row, 0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[row, 0].set_xlabel('Predicho')\n",
    "    axes[row, 0].set_ylabel('Residuo (Real - Predicho)')\n",
    "    axes[row, 0].set_title(f'{col} - Residuos TRAIN')\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # TEST\n",
    "    axes[row, 1].scatter(y_test_pred[col], residuals_test, alpha=0.3, s=1, color='orange')\n",
    "    axes[row, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[row, 1].set_xlabel('Predicho')\n",
    "    axes[row, 1].set_ylabel('Residuo (Real - Predicho)')\n",
    "    axes[row, 1].set_title(f'{col} - Residuos TEST')\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "residuals_plot_path = training_folder / 'residuals_analysis.png'\n",
    "plt.savefig(residuals_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {residuals_plot_path.name}\")\n",
    "\n",
    "# 7.4) Gr√°fica de time series: Real vs Predicho (3 series aleatorias de test)\n",
    "print(f\"\\n[4/5] Creando gr√°ficas de time series (Real vs Predicho)...\")\n",
    "\n",
    "# Obtener series √∫nicas del conjunto de test\n",
    "unique_test_series = series_id_test.unique()\n",
    "\n",
    "# Seleccionar 3 series aleatorias\n",
    "np.random.seed(42)\n",
    "selected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n",
    "\n",
    "print(f\"   ‚Ä¢ Series seleccionadas: {selected_series}\")\n",
    "\n",
    "# Crear figura con 3 filas x 3 columnas (3 series, 3 targets)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 14))\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Para cada target, graficar en subplot separado\n",
    "    for target_idx, col in enumerate(y_test.columns):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales y predichos\n",
    "        y_real = y_test.loc[series_indices, col]\n",
    "        y_pred = y_test_pred.loc[series_indices, col]\n",
    "        \n",
    "        # Calcular m√©tricas para esta serie y target\n",
    "        r2_series = r2_score(y_real, y_pred)\n",
    "        rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n",
    "        \n",
    "        # Graficar\n",
    "        ax.plot(time_series, y_real, label='Real', \n",
    "                linewidth=2, alpha=0.8, color='blue')\n",
    "        ax.plot(time_series, y_pred, label='Predicho', \n",
    "                linestyle='--', linewidth=2, alpha=0.8, color='red')\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Valor', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}', \n",
    "                     fontsize=10, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "timeseries_plot_path = training_folder / 'timeseries_comparison.png'\n",
    "plt.savefig(timeseries_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}\")\n",
    "\n",
    "# 7.5) Gr√°fica de time series con ZOOM (50 segundos)\n",
    "print(f\"\\n[5/5] Creando gr√°ficas de time series con zoom (50s)...\")\n",
    "\n",
    "# Crear figura con 3 filas x 3 columnas (3 series, 3 targets)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 14))\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Definir ventana de 50 segundos desde el inicio\n",
    "    time_min = time_series.min()\n",
    "    time_max_zoom = time_min + 50\n",
    "    \n",
    "    # Filtrar por ventana de tiempo\n",
    "    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n",
    "    zoom_indices = time_series[zoom_mask].index\n",
    "    time_zoom = time_series[zoom_mask]\n",
    "    \n",
    "    # Para cada target, graficar en subplot separado\n",
    "    for target_idx, col in enumerate(y_test.columns):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales y predichos (con zoom)\n",
    "        y_real_zoom = y_test.loc[zoom_indices, col]\n",
    "        y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n",
    "        \n",
    "        # Calcular m√©tricas para esta ventana\n",
    "        r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n",
    "        rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n",
    "        \n",
    "        # Graficar\n",
    "        ax.plot(time_zoom, y_real_zoom, label='Real', \n",
    "                linewidth=2.5, alpha=0.8, color='blue', marker='o', markersize=4)\n",
    "        ax.plot(time_zoom, y_pred_zoom, label='Predicho', \n",
    "                linestyle='--', linewidth=2.5, alpha=0.8, color='red', marker='x', markersize=5)\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Valor', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}', \n",
    "                     fontsize=10, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # A√±adir texto con informaci√≥n de puntos\n",
    "        n_points = len(zoom_indices)\n",
    "        ax.text(0.02, 0.02, f'Puntos: {n_points}', transform=ax.transAxes, fontsize=8,\n",
    "                verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "timeseries_zoom_plot_path = training_folder / 'timeseries_comparison_zoom50s.png'\n",
    "plt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 8) Guardar m√©tricas en archivo\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 7: GUARDAR RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Crear DataFrame con m√©tricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Blade': list(metrics_train.keys()) + ['PROMEDIO'],\n",
    "    'RMSE_Train': list([m['RMSE'] for m in metrics_train.values()]) + [avg_rmse_train],\n",
    "    'RMSE_Test': list([m['RMSE'] for m in metrics_test.values()]) + [avg_rmse_test],\n",
    "    'R2_Train': list([m['R2'] for m in metrics_train.values()]) + [avg_r2_train],\n",
    "    'R2_Test': list([m['R2'] for m in metrics_test.values()]) + [avg_r2_test]\n",
    "})\n",
    "\n",
    "metrics_csv_path = training_folder / 'metrics_results.csv'\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "print(f\"\\n   ‚úÖ M√©tricas guardadas: {metrics_csv_path.name}\")\n",
    "\n",
    "# Crear resumen de entrenamiento\n",
    "summary = {\n",
    "    'Model': 'Ridge Regression',\n",
    "    'Training_Time_seconds': training_time,\n",
    "    'Train_Samples': len(X_train),\n",
    "    'Test_Samples': len(X_test),\n",
    "    'Features': X_train.shape[1],\n",
    "    'Targets': y_train.shape[1],\n",
    "    'Avg_RMSE_Train': avg_rmse_train,\n",
    "    'Avg_RMSE_Test': avg_rmse_test,\n",
    "    'Avg_R2_Train': avg_r2_train,\n",
    "    'Avg_R2_Test': avg_r2_test,\n",
    "    'Alpha': 1.0\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_path = training_folder / 'training_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(f\"   ‚úÖ Resumen guardado: {summary_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# RESUMEN FINAL\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ ENTRENAMIENTO COMPLETADO - RIDGE REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESULTADOS FINALES:\")\n",
    "print(f\"   ‚Ä¢ Tiempo de entrenamiento: {training_time:.2f} segundos\")\n",
    "print(f\"   ‚Ä¢ RMSE promedio (Train): {avg_rmse_train:.2f}\")\n",
    "print(f\"   ‚Ä¢ RMSE promedio (Test):  {avg_rmse_test:.2f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ promedio (Train):   {avg_r2_train:.4f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ promedio (Test):    {avg_r2_test:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "print(f\"   ‚Ä¢ {model_path.name}\")\n",
    "print(f\"   ‚Ä¢ {metrics_csv_path.name}\")\n",
    "print(f\"   ‚Ä¢ {summary_path.name}\")\n",
    "print(f\"   ‚Ä¢ {metrics_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {predictions_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {residuals_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {timeseries_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {timeseries_zoom_plot_path.name}\")\n",
    "\n",
    "print(f\"\\nüí° El modelo Ridge sirve como baseline para comparar con modelos m√°s complejos.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e14908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A√±ade esto despu√©s de cargar los datos normalizados\n",
    "print(\"\\nüîç AN√ÅLISIS DE CORRELACIONES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Correlaci√≥n entre features y targets\n",
    "correlations = pd.concat([X_train, y_train], axis=1).corr()\n",
    "\n",
    "print(\"\\nTop 10 features correlacionadas con cada target:\")\n",
    "for target in y_train.columns:\n",
    "    print(f\"\\n{target}:\")\n",
    "    top_corr = correlations[target].drop(y_train.columns).abs().sort_values(ascending=False).head(10)\n",
    "    print(top_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f650d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Despu√©s de la normalizaci√≥n\n",
    "print(\"\\nüìä VARIABILIDAD DE LOS TARGETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for col in y_train.columns:\n",
    "    std_original = y_train[col].std()\n",
    "    mean_original = y_train[col].mean()\n",
    "    cv = std_original / abs(mean_original) if mean_original != 0 else 0\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"   Mean: {mean_original:.2e}\")\n",
    "    print(f\"   Std:  {std_original:.2e}\")\n",
    "    print(f\"   CV:   {cv:.4f} (coef. variaci√≥n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f1b01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.2 Random Forest Regressor\n",
    "\n",
    "**Random Forest** es un algoritmo de ensamble basado en √°rboles de decisi√≥n que:\n",
    "- Construye m√∫ltiples √°rboles de decisi√≥n usando bootstrapping\n",
    "- Reduce el overfitting mediante la agregaci√≥n de predicciones\n",
    "- Es naturalmente no lineal y captura relaciones complejas\n",
    "- Maneja bien interacciones entre features\n",
    "- No requiere normalizaci√≥n de datos (aunque usaremos datos normalizados para comparaci√≥n justa)\n",
    "\n",
    "**Ventajas:**\n",
    "- Robusto frente a outliers\n",
    "- Captura relaciones no lineales\n",
    "- Proporciona importancia de features\n",
    "\n",
    "**Desventajas:**\n",
    "- M√°s lento en entrenamiento e inferencia que modelos lineales\n",
    "- Requiere m√°s memoria\n",
    "- Menos interpretable que modelos lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODELO 2: RANDOM FOREST - ENTRENAMIENTO (OPTIMIZADO MEMORIA)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ MODELO: RANDOM FOREST - ENTRENAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "\n",
    "# Crear carpeta para guardar resultados\n",
    "training_folder = Path('03_ML_traditional_models') / 'Random_Forest'\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Carpeta de entrenamiento: {training_folder}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Cargar datos ya procesados desde 02_Data_ML_traditional\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: CARGAR DATOS YA PROCESADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data_ml_folder = root_dir / \"notebook\" / '02_Data_ML_traditional'\n",
    "\n",
    "print(f\"\\nüìÇ Carpeta de datos: {data_ml_folder}\")\n",
    "\n",
    "print(f\"\\n[1/2] Cargando datos de entrenamiento normalizados...\")\n",
    "X_train_norm = joblib.load(data_ml_folder / 'X_train_norm.pkl')\n",
    "y_train_norm = joblib.load(data_ml_folder / 'y_train_norm.pkl')\n",
    "\n",
    "# ‚úÖ OPTIMIZACI√ìN 1: Convertir a float32 (ahorra 50% de memoria)\n",
    "X_train_norm = X_train_norm.astype('float32')\n",
    "y_train_norm = y_train_norm.astype('float32')\n",
    "\n",
    "print(f\"   ‚úÖ X_train_norm: {X_train_norm.shape}\")\n",
    "print(f\"   ‚úÖ y_train_norm: {y_train_norm.shape}\")\n",
    "print(f\"   üíæ Memoria X_train_norm: {X_train_norm.memory_usage(deep=True).sum() / (1024**2):.1f} MB\")\n",
    "print(f\"   üí° Targets: {list(y_train_norm.columns)}\")\n",
    "print(f\"   üí° Normalizado con scalers independientes por target\")\n",
    "\n",
    "print(f\"\\n[2/2] Cargando datos originales de train (para m√©tricas)...\")\n",
    "X_train = joblib.load(data_ml_folder / 'X_train.pkl')\n",
    "y_train = joblib.load(data_ml_folder / 'y_train.pkl')\n",
    "print(f\"   ‚úÖ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚úÖ y_train: {y_train.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) Configurar y entrenar modelo Random Forest (OPTIMIZADO)\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: CONFIGURAR Y ENTRENAR MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/3] Configurando Random Forest (optimizado para memoria)...\")\n",
    "\n",
    "# ‚úÖ OPTIMIZACI√ìN 2: Configuraci√≥n optimizada de Random Forest\n",
    "# rf_model = MultiOutputRegressor(\n",
    "#     RandomForestRegressor(\n",
    "#         n_estimators=50,        # ‚úÖ Reducido de 100 a 50 (menos memoria)\n",
    "#         max_depth=15,           # ‚úÖ Reducido de 20 a 15 (√°rboles m√°s peque√±os)\n",
    "#         min_samples_split=10,   # ‚úÖ Aumentado de 5 a 10 (menos divisiones)\n",
    "#         min_samples_leaf=5,     # ‚úÖ Aumentado de 2 a 5 (hojas m√°s grandes)\n",
    "#         max_features='sqrt',    # ‚úÖ Solo sqrt(n_features) por split (menos memoria)\n",
    "#         max_samples=0.8,        # ‚úÖ Bootstrap con 80% de datos (menos memoria)\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1,              # Usar todos los cores\n",
    "#         verbose=1               # ‚úÖ Mostrar progreso\n",
    "#     )\n",
    "# )\n",
    "\n",
    "rf_model = MultiOutputRegressor(\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=800,       # ‚¨ÜÔ∏è Sube a 100 si puedes (reduce la varianza al promediar m√°s)\n",
    "        max_depth=18,           # ‚¨áÔ∏è B√°jalo un poco m√°s. 10-12 suele bastar para f√≠sica.\n",
    "        min_samples_split=40,  # ‚¨ÜÔ∏è ¬°Dr√°stico! No intentes dividir si hay pocos datos.\n",
    "        min_samples_leaf=20,   # ‚¨ÜÔ∏è ¬°CR√çTICO! Una hoja debe tener ~100 muestras (10 segundos de datos)\n",
    "        max_features='sqrt',    # ‚úÖ Mantenlo, es excelente.\n",
    "        max_samples=0.7,        # ‚¨áÔ∏è Bajar a 0.5 hace cada √°rbol m√°s independiente y reduce overfitting.\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"   ‚úÖ Modelo configurado (OPTIMIZADO PARA MEMORIA):\")\n",
    "print(f\"      ‚Ä¢ Tipo: Random Forest\")\n",
    "print(f\"      ‚Ä¢ N estimators: 50 (reducido para memoria)\")\n",
    "print(f\"      ‚Ä¢ Max depth: 15 (√°rboles menos profundos)\")\n",
    "print(f\"      ‚Ä¢ Min samples split: 10\")\n",
    "print(f\"      ‚Ä¢ Min samples leaf: 5\")\n",
    "print(f\"      ‚Ä¢ Max features: sqrt({X_train_norm.shape[1]}) ‚âà {int(np.sqrt(X_train_norm.shape[1]))}\")\n",
    "print(f\"      ‚Ä¢ Max samples: 80% (bootstrap)\")\n",
    "print(f\"      ‚Ä¢ Outputs: {len(y_train_norm.columns)} targets (M_0, M_1c, M_1s)\")\n",
    "\n",
    "# ‚úÖ OPTIMIZACI√ìN 3 (OPCIONAL): Submuestreo si a√∫n hay problemas de memoria\n",
    "USE_SUBSAMPLING = False  # Cambiar a True si sigue habiendo problemas\n",
    "\n",
    "if USE_SUBSAMPLING:\n",
    "    print(f\"\\n‚ö†Ô∏è  SUBMUESTREO ACTIVADO (para reducir memoria)\")\n",
    "    \n",
    "    # Submuestreo estratificado por series\n",
    "    unique_series = series_id_train.unique()\n",
    "    n_series_to_use = int(len(unique_series) * 0.5)  # 50% de series\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    selected_series = np.random.choice(unique_series, size=n_series_to_use, replace=False)\n",
    "    \n",
    "    mask = series_id_train.isin(selected_series)\n",
    "    X_train_norm_sampled = X_train_norm[mask]\n",
    "    y_train_norm_sampled = y_train_norm[mask]\n",
    "    \n",
    "    print(f\"   Series: {len(unique_series)} ‚Üí {n_series_to_use}\")\n",
    "    print(f\"   Muestras: {len(X_train_norm):,} ‚Üí {len(X_train_norm_sampled):,}\")\n",
    "    \n",
    "    # Usar datos submuestreados\n",
    "    X_train_norm_fit = X_train_norm_sampled\n",
    "    y_train_norm_fit = y_train_norm_sampled\n",
    "else:\n",
    "    X_train_norm_fit = X_train_norm\n",
    "    y_train_norm_fit = y_train_norm\n",
    "\n",
    "print(f\"\\n[2/3] Entrenando modelo con datos normalizados...\")\n",
    "print(f\"   ‚Ä¢ Datos de entrenamiento: {X_train_norm_fit.shape}\")\n",
    "print(f\"   ‚Ä¢ Targets de entrenamiento: {y_train_norm_fit.shape}\")\n",
    "print(f\"   ‚Ä¢ Memoria estimada: {X_train_norm_fit.memory_usage(deep=True).sum() / (1024**2):.1f} MB\")\n",
    "print(f\"   ‚è±Ô∏è  Esto puede tomar varios minutos...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Entrenar el modelo con datos normalizados\n",
    "rf_model.fit(X_train_norm_fit, y_train_norm_fit)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ‚úÖ Modelo entrenado en {training_time:.2f} segundos ({training_time/60:.2f} minutos)\")\n",
    "\n",
    "print(f\"\\n[3/3] Guardando modelo...\")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model_path = training_folder / 'random_forest_model.pkl'\n",
    "joblib.dump(rf_model, model_path)\n",
    "\n",
    "print(f\"   ‚úÖ Modelo guardado: {model_path.name}\")\n",
    "print(f\"      Ubicaci√≥n: {model_path}\")\n",
    "print(f\"      Tama√±o: {model_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# RESUMEN DE ENTRENAMIENTO\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESUMEN:\")\n",
    "print(f\"   ‚Ä¢ Tiempo de entrenamiento: {training_time:.2f} segundos ({training_time/60:.2f} min)\")\n",
    "print(f\"   ‚Ä¢ Muestras entrenamiento: {len(X_train_norm_fit):,}\")\n",
    "print(f\"   ‚Ä¢ Features: {X_train_norm_fit.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Targets: {y_train_norm_fit.shape[1]} (M_0, M_1c, M_1s)\")\n",
    "print(f\"   ‚Ä¢ Normalizaci√≥n: Scalers independientes por target\")\n",
    "print(f\"   ‚Ä¢ Tipo de datos: float32 (optimizado para memoria)\")\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "print(f\"   ‚Ä¢ {model_path.name} ({model_path.stat().st_size / (1024**2):.2f} MB)\")\n",
    "\n",
    "print(f\"\\nüí° Ejecuta la siguiente celda para validar el modelo y generar gr√°ficas.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODELO 2: RANDOM FOREST - VALIDACI√ìN Y VISUALIZACIONES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ MODELO: RANDOM FOREST - VALIDACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Cargar modelo entrenado y datos de test\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: CARGAR MODELO Y DATOS DE TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "training_folder = Path('03_ML_traditional_models') / 'Random_Forest'\n",
    "data_ml_folder = root_dir / 'notebook' / '02_Data_ML_traditional'\n",
    "\n",
    "print(f\"\\n[1/5] Cargando modelo entrenado...\")\n",
    "model_path = training_folder / 'random_forest_model.pkl'\n",
    "rf_model = joblib.load(model_path)\n",
    "print(f\"   ‚úÖ Modelo cargado desde: {model_path.name}\")\n",
    "\n",
    "print(f\"\\n[2/5] Cargando datos de test originales...\")\n",
    "X_test = joblib.load(data_ml_folder / 'X_test.pkl')\n",
    "y_test = joblib.load(data_ml_folder / 'y_test.pkl')\n",
    "print(f\"   ‚úÖ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚úÖ y_test: {y_test.shape}\")\n",
    "print(f\"   üí° Targets: {list(y_test.columns)}\")\n",
    "\n",
    "print(f\"\\n[3/5] Cargando datos de train originales (para m√©tricas)...\")\n",
    "X_train = joblib.load(data_ml_folder / 'X_train.pkl')\n",
    "y_train = joblib.load(data_ml_folder / 'y_train.pkl')\n",
    "X_train_norm = joblib.load(data_ml_folder / 'X_train_norm.pkl')\n",
    "print(f\"   ‚úÖ Datos de train cargados\")\n",
    "\n",
    "print(f\"\\n[4/5] Cargando series_id y Time para gr√°ficas...\")\n",
    "# Intentar cargar series_id y Time (si no existen, las gr√°ficas de time series se omitir√°n)\n",
    "series_id_test_path = data_ml_folder / 'series_id_test.pkl'\n",
    "Time_test_path = data_ml_folder / 'Time_test.pkl'\n",
    "\n",
    "if series_id_test_path.exists() and Time_test_path.exists():\n",
    "    series_id_test = joblib.load(series_id_test_path)\n",
    "    Time_test = joblib.load(Time_test_path)\n",
    "    print(f\"   ‚úÖ series_id_test: {len(series_id_test):,} valores\")\n",
    "    print(f\"   ‚úÖ Time_test: {len(Time_test):,} valores\")\n",
    "else:\n",
    "    series_id_test = None\n",
    "    Time_test = None\n",
    "    print(f\"   ‚ö†Ô∏è  Archivos series_id_test.pkl o Time_test.pkl no encontrados\")\n",
    "    print(f\"   üí° Las gr√°ficas de time series se omitir√°n.\")\n",
    "\n",
    "\n",
    "# ‚úÖ DESPU√âS (scalers independientes)\n",
    "scalers_X = joblib.load(root_dir / 'notebook' / '01_Models_scaler' / 'scalers_X.pkl')\n",
    "scalers_y = joblib.load(root_dir / 'notebook' / '01_Models_scaler' / 'scalers_y.pkl')  # ‚¨ÖÔ∏è A√ëADIR ESTA L√çNEA\n",
    "\n",
    "X_test_norm = pd.DataFrame(index=X_test.index)\n",
    "for col in X_test.columns:\n",
    "    if col not in scalers_X:\n",
    "        raise KeyError(f\"No se encuentra scaler para columna: {col}\")\n",
    "    X_test_norm[col] = scalers_X[col].transform(X_test[[col]]).ravel()\n",
    "print(f\"   ‚úÖ X_test normalizado: {X_test_norm.shape}\")\n",
    "print(f\"   üí° Scalers independientes: {list(scalers_y.keys())}\")  # ‚¨ÖÔ∏è Ahora funcionar√°\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) Realizar predicciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Prediciendo sobre conjunto de entrenamiento...\")\n",
    "\n",
    "y_train_pred_norm = rf_model.predict(X_train_norm)\n",
    "y_train_pred_norm = pd.DataFrame(\n",
    "    y_train_pred_norm,\n",
    "    index=X_train_norm.index,\n",
    "    columns=y_train.columns\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones train: {y_train_pred_norm.shape}\")\n",
    "\n",
    "print(f\"\\n[2/2] Prediciendo sobre conjunto de test...\")\n",
    "\n",
    "y_test_pred_norm = rf_model.predict(X_test_norm)\n",
    "y_test_pred_norm = pd.DataFrame(\n",
    "    y_test_pred_norm,\n",
    "    index=y_test.index,\n",
    "    columns=y_test.columns\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones test: {y_test_pred_norm.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3) Desnormalizar predicciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 3: DESNORMALIZACI√ìN DE PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Desnormalizando predicciones de train con scalers independientes...\")\n",
    "\n",
    "# Usar los scalers independientes para desnormalizar\n",
    "y_train_pred = pd.DataFrame(index=y_train_pred_norm.index)\n",
    "for col in y_train_pred_norm.columns:\n",
    "    y_train_pred[col] = scalers_y[col].inverse_transform(y_train_pred_norm[[col]]).ravel()\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones train desnormalizadas\")\n",
    "\n",
    "print(f\"\\n[2/2] Desnormalizando predicciones de test con scalers independientes...\")\n",
    "\n",
    "# Usar los scalers independientes para desnormalizar\n",
    "y_test_pred = pd.DataFrame(index=y_test_pred_norm.index)\n",
    "for col in y_test_pred_norm.columns:\n",
    "    y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm[[col]]).ravel()\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones test desnormalizadas\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4) Calcular m√©tricas\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 4: C√ÅLCULO DE M√âTRICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Diccionarios para almacenar m√©tricas\n",
    "metrics_train = {}\n",
    "metrics_test = {}\n",
    "\n",
    "print(f\"\\n{'M√âTRICA':20} {'CONJUNTO':10} {'M_0':>15} {'M_1c':>15} {'M_1s':>15}\")\n",
    "print(f\"{'‚îÄ'*20} {'‚îÄ'*10} {'‚îÄ'*15} {'‚îÄ'*15} {'‚îÄ'*15}\")\n",
    "\n",
    "# Calcular m√©tricas para cada target\n",
    "for i, col in enumerate(y_train.columns):\n",
    "    # TRAIN\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train[col], y_train_pred[col]))\n",
    "    r2_train = r2_score(y_train[col], y_train_pred[col])\n",
    "    \n",
    "    # TEST\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test[col], y_test_pred[col]))\n",
    "    r2_test = r2_score(y_test[col], y_test_pred[col])\n",
    "    \n",
    "    # Guardar m√©tricas\n",
    "    metrics_train[col] = {'RMSE': rmse_train, 'R2': r2_train}\n",
    "    metrics_test[col] = {'RMSE': rmse_test, 'R2': r2_test}\n",
    "    \n",
    "    # Mostrar\n",
    "    if i == 0:\n",
    "        print(f\"{'RMSE':20} {'TRAIN':10} {rmse_train:>15.2f} {'-':>15} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {rmse_test:>15.2f} {'-':>15} {'-':>15}\")\n",
    "        print(f\"{'R¬≤':20} {'TRAIN':10} {r2_train:>15.4f} {'-':>15} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {r2_test:>15.4f} {'-':>15} {'-':>15}\")\n",
    "    elif i == 1:\n",
    "        print(f\"{'RMSE':20} {'TRAIN':10} {'-':>15} {rmse_train:>15.2f} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {rmse_test:>15.2f} {'-':>15}\")\n",
    "        print(f\"{'R¬≤':20} {'TRAIN':10} {'-':>15} {r2_train:>15.4f} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {r2_test:>15.4f} {'-':>15}\")\n",
    "    else:\n",
    "        print(f\"{'RMSE':20} {'TRAIN':10} {'-':>15} {'-':>15} {rmse_train:>15.2f}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {'-':>15} {rmse_test:>15.2f}\")\n",
    "        print(f\"{'R¬≤':20} {'TRAIN':10} {'-':>15} {'-':>15} {r2_train:>15.4f}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {'-':>15} {r2_test:>15.4f}\")\n",
    "\n",
    "# Calcular promedios\n",
    "avg_rmse_train = np.mean([m['RMSE'] for m in metrics_train.values()])\n",
    "avg_rmse_test = np.mean([m['RMSE'] for m in metrics_test.values()])\n",
    "avg_r2_train = np.mean([m['R2'] for m in metrics_train.values()])\n",
    "avg_r2_test = np.mean([m['R2'] for m in metrics_test.values()])\n",
    "\n",
    "print(f\"{'‚îÄ'*20} {'‚îÄ'*10} {'‚îÄ'*15} {'‚îÄ'*15} {'‚îÄ'*15}\")\n",
    "print(f\"{'PROMEDIO RMSE':20} {'TRAIN':10} {avg_rmse_train:>15.2f}\")\n",
    "print(f\"{'':20} {'TEST':10} {avg_rmse_test:>15.2f}\")\n",
    "print(f\"{'PROMEDIO R¬≤':20} {'TRAIN':10} {avg_r2_train:>15.4f}\")\n",
    "print(f\"{'':20} {'TEST':10} {avg_r2_test:>15.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5) Visualizaciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 5: GENERACI√ìN DE GR√ÅFICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 5.1) Gr√°fica de m√©tricas por target\n",
    "print(f\"\\n[1/5] Creando gr√°fica de m√©tricas...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "targets = list(metrics_train.keys())\n",
    "rmse_train_vals = [metrics_train[t]['RMSE'] for t in targets]\n",
    "rmse_test_vals = [metrics_test[t]['RMSE'] for t in targets]\n",
    "\n",
    "x = np.arange(len(targets))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, rmse_train_vals, width, label='Train', alpha=0.8)\n",
    "axes[0].bar(x + width/2, rmse_test_vals, width, label='Test', alpha=0.8)\n",
    "axes[0].set_xlabel('Target')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE por Target - Random Forest')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(targets, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤\n",
    "r2_train_vals = [metrics_train[t]['R2'] for t in targets]\n",
    "r2_test_vals = [metrics_test[t]['R2'] for t in targets]\n",
    "\n",
    "axes[1].bar(x - width/2, r2_train_vals, width, label='Train', alpha=0.8)\n",
    "axes[1].bar(x + width/2, r2_test_vals, width, label='Test', alpha=0.8)\n",
    "axes[1].set_xlabel('Target')\n",
    "axes[1].set_ylabel('R¬≤')\n",
    "axes[1].set_title('R¬≤ por Target - Random Forest')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(targets, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "metrics_plot_path = training_folder / 'metrics_comparison.png'\n",
    "plt.savefig(metrics_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {metrics_plot_path.name}\")\n",
    "\n",
    "# 5.2) Gr√°ficas de predicciones vs reales (cambiar a 3x2)\n",
    "print(f\"\\n[2/5] Creando gr√°ficas de predicciones vs reales...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 16))\n",
    "\n",
    "for idx, col in enumerate(y_train.columns):\n",
    "    row = idx\n",
    "    \n",
    "    # TRAIN\n",
    "    axes[row, 0].scatter(y_train[col], y_train_pred[col], alpha=0.3, s=1)\n",
    "    axes[row, 0].plot([y_train[col].min(), y_train[col].max()], \n",
    "                       [y_train[col].min(), y_train[col].max()], \n",
    "                       'r--', lw=2, label='Perfect prediction')\n",
    "    axes[row, 0].set_xlabel('Real')\n",
    "    axes[row, 0].set_ylabel('Predicho')\n",
    "    axes[row, 0].set_title(f'{col} - TRAIN (R¬≤={metrics_train[col][\"R2\"]:.4f})')\n",
    "    axes[row, 0].legend()\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # TEST\n",
    "    axes[row, 1].scatter(y_test[col], y_test_pred[col], alpha=0.3, s=1, color='orange')\n",
    "    axes[row, 1].plot([y_test[col].min(), y_test[col].max()], \n",
    "                       [y_test[col].min(), y_test[col].max()], \n",
    "                       'r--', lw=2, label='Perfect prediction')\n",
    "    axes[row, 1].set_xlabel('Real')\n",
    "    axes[row, 1].set_ylabel('Predicho')\n",
    "    axes[row, 1].set_title(f'{col} - TEST (R¬≤={metrics_test[col][\"R2\"]:.4f})')\n",
    "    axes[row, 1].legend()\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "predictions_plot_path = training_folder / 'predictions_vs_real.png'\n",
    "plt.savefig(predictions_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {predictions_plot_path.name}\")\n",
    "\n",
    "# 5.3) Gr√°fica de residuos (cambiar a 3x2)\n",
    "print(f\"\\n[3/5] Creando gr√°fica de residuos...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 16))\n",
    "\n",
    "for idx, col in enumerate(y_train.columns):\n",
    "    row = idx\n",
    "    \n",
    "    # Calcular residuos\n",
    "    residuals_train = y_train[col] - y_train_pred[col]\n",
    "    residuals_test = y_test[col] - y_test_pred[col]\n",
    "    \n",
    "    # TRAIN\n",
    "    axes[row, 0].scatter(y_train_pred[col], residuals_train, alpha=0.3, s=1)\n",
    "    axes[row, 0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[row, 0].set_xlabel('Predicho')\n",
    "    axes[row, 0].set_ylabel('Residuo (Real - Predicho)')\n",
    "    axes[row, 0].set_title(f'{col} - Residuos TRAIN')\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # TEST\n",
    "    axes[row, 1].scatter(y_test_pred[col], residuals_test, alpha=0.3, s=1, color='orange')\n",
    "    axes[row, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[row, 1].set_xlabel('Predicho')\n",
    "    axes[row, 1].set_ylabel('Residuo (Real - Predicho)')\n",
    "    axes[row, 1].set_title(f'{col} - Residuos TEST')\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "residuals_plot_path = training_folder / 'residuals_analysis.png'\n",
    "plt.savefig(residuals_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {residuals_plot_path.name}\")\n",
    "\n",
    "# 5.4) y 5.5) Gr√°ficas de time series (cambiar a 3x3)\n",
    "if series_id_test is not None and Time_test is not None:\n",
    "    print(f\"\\n[4/5] Creando gr√°ficas de time series (Real vs Predicho)...\")\n",
    "\n",
    "    unique_test_series = series_id_test.unique()\n",
    "    np.random.seed(42)\n",
    "    selected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Series seleccionadas: {selected_series}\")\n",
    "\n",
    "    # Crear figura con 3 filas x 3 columnas (3 series, 3 targets)\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 14))\n",
    "\n",
    "    for plot_idx, series_num in enumerate(selected_series):\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        time_series = Time_test.loc[series_indices]\n",
    "        \n",
    "        for target_idx, col in enumerate(y_test.columns):\n",
    "            ax = axes[plot_idx, target_idx]\n",
    "            \n",
    "            y_real = y_test.loc[series_indices, col]\n",
    "            y_pred = y_test_pred.loc[series_indices, col]\n",
    "            \n",
    "            r2_series = r2_score(y_real, y_pred)\n",
    "            rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n",
    "            \n",
    "            ax.plot(time_series, y_real, label='Real', linewidth=2, alpha=0.8, color='blue')\n",
    "            ax.plot(time_series, y_pred, label='Predicho', linestyle='--', linewidth=2, alpha=0.8, color='red')\n",
    "            \n",
    "            ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "            ax.set_ylabel('Valor', fontsize=10)\n",
    "            ax.set_title(f'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}', \n",
    "                         fontsize=10, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    timeseries_plot_path = training_folder / 'timeseries_comparison.png'\n",
    "    plt.savefig(timeseries_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}\")\n",
    "\n",
    "    # 5.5) Zoom (3x3)\n",
    "    print(f\"\\n[5/5] Creando gr√°ficas de time series con zoom (50s)...\")\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 14))\n",
    "\n",
    "    for plot_idx, series_num in enumerate(selected_series):\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        time_series = Time_test.loc[series_indices]\n",
    "        \n",
    "        time_min = time_series.min()\n",
    "        time_max_zoom = time_min + 50\n",
    "        \n",
    "        zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n",
    "        zoom_indices = time_series[zoom_mask].index\n",
    "        time_zoom = time_series[zoom_mask]\n",
    "        \n",
    "        for target_idx, col in enumerate(y_test.columns):\n",
    "            ax = axes[plot_idx, target_idx]\n",
    "            \n",
    "            y_real_zoom = y_test.loc[zoom_indices, col]\n",
    "            y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n",
    "            \n",
    "            r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n",
    "            rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n",
    "            \n",
    "            ax.plot(time_zoom, y_real_zoom, label='Real', \n",
    "                    linewidth=2.5, alpha=0.8, color='blue', marker='o', markersize=4)\n",
    "            ax.plot(time_zoom, y_pred_zoom, label='Predicho', \n",
    "                    linestyle='--', linewidth=2.5, alpha=0.8, color='red', marker='x', markersize=5)\n",
    "            \n",
    "            ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "            ax.set_ylabel('Valor', fontsize=10)\n",
    "            ax.set_title(f'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}', \n",
    "                         fontsize=10, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            n_points = len(zoom_indices)\n",
    "            ax.text(0.02, 0.02, f'Puntos: {n_points}', transform=ax.transAxes, fontsize=8,\n",
    "                    verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    timeseries_zoom_plot_path = training_folder / 'timeseries_comparison_zoom50s.png'\n",
    "    plt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è Saltando gr√°ficas de time series [4/5 y 5/5]\")\n",
    "    timeseries_plot_path = None\n",
    "    timeseries_zoom_plot_path = None\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6) Guardar m√©tricas en archivo\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 6: GUARDAR RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Crear DataFrame con m√©tricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Target': list(metrics_train.keys()) + ['PROMEDIO'],\n",
    "    'RMSE_Train': list([m['RMSE'] for m in metrics_train.values()]) + [avg_rmse_train],\n",
    "    'RMSE_Test': list([m['RMSE'] for m in metrics_test.values()]) + [avg_rmse_test],\n",
    "    'R2_Train': list([m['R2'] for m in metrics_train.values()]) + [avg_r2_train],\n",
    "    'R2_Test': list([m['R2'] for m in metrics_test.values()]) + [avg_r2_test]\n",
    "})\n",
    "\n",
    "metrics_csv_path = training_folder / 'metrics_results.csv'\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "print(f\"\\n   ‚úÖ M√©tricas guardadas: {metrics_csv_path.name}\")\n",
    "\n",
    "# Crear resumen de validaci√≥n\n",
    "summary = {\n",
    "    'Model': 'Random Forest',\n",
    "    'Train_Samples': len(X_train),\n",
    "    'Test_Samples': len(X_test),\n",
    "    'Features': X_train.shape[1],\n",
    "    'Targets': y_train.shape[1],\n",
    "    'Targets_Names': ', '.join(y_train.columns),\n",
    "    'Avg_RMSE_Train': avg_rmse_train,\n",
    "    'Avg_RMSE_Test': avg_rmse_test,\n",
    "    'Avg_R2_Train': avg_r2_train,\n",
    "    'Avg_R2_Test': avg_r2_test,\n",
    "    'N_Estimators': 100,\n",
    "    'Max_Depth': 20,\n",
    "    'Normalization': 'Independent scalers per target'\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_path = training_folder / 'validation_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(f\"   ‚úÖ Resumen guardado: {summary_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# RESUMEN FINAL\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ VALIDACI√ìN COMPLETADA - RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESULTADOS FINALES:\")\n",
    "print(f\"   ‚Ä¢ Targets: {', '.join(y_train.columns)}\")\n",
    "print(f\"   ‚Ä¢ RMSE promedio (Train): {avg_rmse_train:.2f}\")\n",
    "print(f\"   ‚Ä¢ RMSE promedio (Test):  {avg_rmse_test:.2f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ promedio (Train):   {avg_r2_train:.4f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ promedio (Test):    {avg_r2_test:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "print(f\"   ‚Ä¢ {metrics_csv_path.name}\")\n",
    "print(f\"   ‚Ä¢ {summary_path.name}\")\n",
    "print(f\"   ‚Ä¢ {metrics_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {predictions_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {residuals_plot_path.name}\")\n",
    "if timeseries_plot_path is not None:\n",
    "    print(f\"   ‚Ä¢ {timeseries_plot_path.name}\")\n",
    "    print(f\"   ‚Ä¢ {timeseries_zoom_plot_path.name}\")\n",
    "\n",
    "print(f\"\\nüí° Random Forest captura relaciones no lineales mejor que Ridge.\")\n",
    "print(f\"\\n‚öôÔ∏è  Normalizaci√≥n: Scalers independientes por target para balancear escalas\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "M√≥dulo para generar gr√°ficas adicionales de time series para modelos ML.\n",
    "\n",
    "Este m√≥dulo genera:\n",
    "1. Time series completas: Real vs Predicho (3 series aleatorias)\n",
    "2. Time series con zoom de 50 segundos\n",
    "\n",
    "Adaptado para trabajar con:\n",
    "- Scalers independientes por columna (scalers_X.pkl, scalers_y.pkl)\n",
    "- Targets nuevos: M_0, M_1c, M_1s\n",
    "\n",
    "IMPORTANTE: Este m√≥dulo NO carga predicciones de archivo.\n",
    "Usa y_test_pred que ya est√° en memoria del notebook.\n",
    "\"\"\"\n",
    "\n",
    "def generate_timeseries_plots(root_dir, y_test, y_test_pred, output_folder, \n",
    "                              model_name='Model', n_series=3, zoom_seconds=50, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generar gr√°ficas de time series (Real vs Predicho) con zoom.\n",
    "    \n",
    "    Este c√≥digo replica EXACTAMENTE el flujo de la celda 81 del notebook.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (Path): Directorio ra√≠z del proyecto\n",
    "        y_test (pd.DataFrame): Valores reales de test (ya en memoria)\n",
    "        y_test_pred (pd.DataFrame): Predicciones del modelo (ya en memoria)\n",
    "        output_folder (Path): Carpeta donde guardar las gr√°ficas\n",
    "        model_name (str): Nombre del modelo para t√≠tulos\n",
    "        n_series (int): N√∫mero de series aleatorias a graficar (default: 3)\n",
    "        zoom_seconds (int): Duraci√≥n de la ventana de zoom (default: 50)\n",
    "        random_seed (int): Semilla para selecci√≥n aleatoria de series (default: 42)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Rutas de las gr√°ficas generadas\n",
    "        \n",
    "    Example:\n",
    "        >>> from aditionalPlots import generate_timeseries_plots\n",
    "        >>> \n",
    "        >>> plots = generate_timeseries_plots(\n",
    "        ...     root_dir=root_dir,\n",
    "        ...     y_test=y_test,                    # ‚Üê Variable en memoria\n",
    "        ...     y_test_pred=y_test_pred,          # ‚Üê Variable en memoria\n",
    "        ...     output_folder=training_folder,\n",
    "        ...     model_name='Random Forest'\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üìä {model_name.upper()} - GR√ÅFICAS DE TIME SERIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    root_dir = Path(root_dir)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 1: PREPARAR DATOS PARA TIME SERIES\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PASO 1: PREPARAR DATOS PARA TIME SERIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n[1/3] Cargando Time_test desde archivo pkl...\")\n",
    "    \n",
    "    data_ml_folder = root_dir / 'notebook' / '02_Data_ML_traditional'\n",
    "    Time_test_path = data_ml_folder / 'Time_test.pkl'\n",
    "    \n",
    "    if Time_test_path.exists():\n",
    "        Time_test = joblib.load(Time_test_path)\n",
    "        print(f\"   ‚úÖ Time_test cargado: {len(Time_test):,} valores\")\n",
    "        print(f\"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s\")\n",
    "        print(f\"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}\")\n",
    "        raise FileNotFoundError(f\"Archivo requerido no encontrado: {Time_test_path}\")\n",
    "    \n",
    "    print(f\"\\n[2/3] Generando series_id_test a partir de Time_test...\")\n",
    "    \n",
    "    # Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\n",
    "    series_id_test_values = np.zeros(len(Time_test), dtype=int)\n",
    "    current_series = 0\n",
    "    \n",
    "    Time_test_array = Time_test.values\n",
    "    for i in range(1, len(Time_test_array)):\n",
    "        if Time_test_array[i] < Time_test_array[i-1]:\n",
    "            current_series += 1\n",
    "        series_id_test_values[i] = current_series\n",
    "    \n",
    "    # Convertir a pandas Series con el mismo index que Time_test\n",
    "    series_id_test = pd.Series(series_id_test_values, index=Time_test.index, name='series_id')\n",
    "    \n",
    "    n_test_series = series_id_test.max() + 1\n",
    "    \n",
    "    print(f\"   ‚úÖ Series temporales identificadas en test: {n_test_series}\")\n",
    "    \n",
    "    # Analizar cada serie\n",
    "    print(f\"\\n   üìä Resumen de series en TEST:\")\n",
    "    for sid in range(min(5, n_test_series)):\n",
    "        mask = series_id_test == sid\n",
    "        n_rows = mask.sum()\n",
    "        time_min = Time_test.loc[mask].min()\n",
    "        time_max = Time_test.loc[mask].max()\n",
    "        print(f\"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s\")\n",
    "    \n",
    "    if n_test_series > 5:\n",
    "        print(f\"      ... y {n_test_series - 5} series m√°s\")\n",
    "    \n",
    "    print(f\"\\n[3/3] Verificando que los datos coinciden con predicciones...\")\n",
    "    \n",
    "    # Verificar que los √≠ndices coinciden\n",
    "    if not all(series_id_test.index == y_test.index):\n",
    "        print(f\"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...\")\n",
    "        series_id_test = series_id_test.reindex(y_test.index)\n",
    "        Time_test = Time_test.reindex(y_test.index)\n",
    "    \n",
    "    print(f\"   ‚úÖ √çndices verificados:\")\n",
    "    print(f\"      ‚Ä¢ y_test: {y_test.shape[0]:,} filas\")\n",
    "    print(f\"      ‚Ä¢ y_test_pred: {y_test_pred.shape[0]:,} filas\")\n",
    "    print(f\"      ‚Ä¢ series_id_test: {len(series_id_test):,} valores\")\n",
    "    print(f\"      ‚Ä¢ Time_test: {len(Time_test):,} valores\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 2: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PASO 2: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n[1/1] Creando gr√°ficas de time series (Real vs Predicho)...\")\n",
    "    \n",
    "    # Obtener series √∫nicas del conjunto de test\n",
    "    unique_test_series = series_id_test.unique()\n",
    "    \n",
    "    # Seleccionar series aleatorias\n",
    "    np.random.seed(random_seed)  # Usar semilla configurable\n",
    "    selected_series = np.random.choice(unique_test_series, \n",
    "                                      size=min(n_series, len(unique_test_series)), \n",
    "                                      replace=False)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Total series disponibles: {len(unique_test_series)}\")\n",
    "    print(f\"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}\")\n",
    "    \n",
    "    # Detectar n√∫mero de targets autom√°ticamente\n",
    "    n_targets = len(y_test.columns)\n",
    "    \n",
    "    # Crear figura con n_series filas x n_targets columnas\n",
    "    fig, axes = plt.subplots(n_series, n_targets, figsize=(6*n_targets, 4.5*n_series))\n",
    "    \n",
    "    # Asegurar que axes sea 2D\n",
    "    if n_series == 1 and n_targets == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_series == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif n_targets == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for plot_idx, series_num in enumerate(selected_series):\n",
    "        # Filtrar datos de esta serie\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        \n",
    "        # Obtener tiempo\n",
    "        time_series = Time_test.loc[series_indices]\n",
    "        \n",
    "        # Para cada target, graficar en subplot separado\n",
    "        for target_idx, col in enumerate(y_test.columns):\n",
    "            ax = axes[plot_idx, target_idx]\n",
    "            \n",
    "            # Valores reales\n",
    "            y_real = y_test.loc[series_indices, col]\n",
    "            # Valores predichos\n",
    "            y_pred = y_test_pred.loc[series_indices, col]\n",
    "            \n",
    "            # Calcular m√©tricas para esta serie y target\n",
    "            r2_series = r2_score(y_real, y_pred)\n",
    "            rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n",
    "            \n",
    "            # Graficar\n",
    "            ax.plot(time_series, y_real, label='Real', \n",
    "                    linewidth=2, alpha=0.8, color='blue')\n",
    "            ax.plot(time_series, y_pred, label='Predicho', \n",
    "                    linestyle='--', linewidth=2, alpha=0.8, color='red')\n",
    "            \n",
    "            # Configurar subplot\n",
    "            ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "            ax.set_ylabel('Momento [kNm]', fontsize=10)\n",
    "            ax.set_title(f'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}', \n",
    "                         fontsize=11, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    timeseries_plot_path = output_folder / 'timeseries_comparison.png'\n",
    "    plt.savefig(timeseries_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 3: GR√ÅFICAS DE TIME SERIES - ZOOM\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PASO 3: GR√ÅFICAS DE TIME SERIES - ZOOM {zoom_seconds} SEGUNDOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n[1/1] Creando gr√°ficas de time series con zoom ({zoom_seconds}s)...\")\n",
    "    \n",
    "    # Crear figura con n_series filas x n_targets columnas\n",
    "    fig, axes = plt.subplots(n_series, n_targets, figsize=(6*n_targets, 4.5*n_series))\n",
    "    \n",
    "    # Asegurar que axes sea 2D\n",
    "    if n_series == 1 and n_targets == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_series == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif n_targets == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for plot_idx, series_num in enumerate(selected_series):\n",
    "        # Filtrar datos de esta serie\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        \n",
    "        # Obtener tiempo\n",
    "        time_series = Time_test.loc[series_indices]\n",
    "        \n",
    "        # Definir ventana de zoom desde el inicio\n",
    "        time_min = time_series.min()\n",
    "        time_max_zoom = time_min + zoom_seconds\n",
    "        \n",
    "        # Filtrar por ventana de tiempo\n",
    "        zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n",
    "        zoom_indices = time_series[zoom_mask].index\n",
    "        time_zoom = time_series[zoom_mask]\n",
    "        \n",
    "        # Para cada target, graficar en subplot separado\n",
    "        for target_idx, col in enumerate(y_test.columns):\n",
    "            ax = axes[plot_idx, target_idx]\n",
    "            \n",
    "            # Valores reales y predichos (con zoom)\n",
    "            y_real_zoom = y_test.loc[zoom_indices, col]\n",
    "            y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n",
    "            \n",
    "            # Calcular m√©tricas para esta ventana\n",
    "            r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n",
    "            rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n",
    "            \n",
    "            # Graficar\n",
    "            ax.plot(time_zoom, y_real_zoom, label='Real', \n",
    "                    linewidth=2.5, alpha=0.8, color='blue', marker='o', markersize=4)\n",
    "            ax.plot(time_zoom, y_pred_zoom, label='Predicho', \n",
    "                    linestyle='--', linewidth=2.5, alpha=0.8, color='red', marker='x', markersize=5)\n",
    "            \n",
    "            # Configurar subplot\n",
    "            ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "            ax.set_ylabel('Momento [kNm]', fontsize=10)\n",
    "            ax.set_title(f'Serie {series_num} - {col} (Zoom: 0-{zoom_seconds}s)\\n'\n",
    "                        f'R¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}', \n",
    "                         fontsize=11, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # A√±adir texto con informaci√≥n de puntos\n",
    "            n_points = len(zoom_indices)\n",
    "            ax.text(0.02, 0.02, f'Puntos: {n_points}', transform=ax.transAxes, fontsize=9,\n",
    "                    verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    timeseries_zoom_plot_path = output_folder / f'timeseries_comparison_zoom{zoom_seconds}s.png'\n",
    "    plt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RESUMEN FINAL\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"‚úÖ GR√ÅFICAS DE TIME SERIES COMPLETADAS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä GR√ÅFICAS GENERADAS:\")\n",
    "    print(f\"   ‚Ä¢ {timeseries_plot_path.name}\")\n",
    "    print(f\"   ‚Ä¢ {timeseries_zoom_plot_path.name}\")\n",
    "    \n",
    "    print(f\"\\nüí° Targets graficados: {list(y_test.columns)}\")\n",
    "    print(f\"üí° Series aleatorias: {list(selected_series)}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'timeseries_full': timeseries_plot_path,\n",
    "        'timeseries_zoom': timeseries_zoom_plot_path\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O especificar tu propia semilla\n",
    "plots = generate_timeseries_plots(\n",
    "    root_dir=root_dir,\n",
    "    y_test=y_test,\n",
    "    y_test_pred=y_test_pred,\n",
    "    output_folder=training_folder,\n",
    "    model_name='Random Forest',\n",
    "    zoom_seconds=30,\n",
    "    random_seed=25  # ‚Üê Semilla personalizada\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624ed6f",
   "metadata": {},
   "source": [
    "## üå≥ MODELO 3: XGBOOST\n",
    "\n",
    "**XGBoost (eXtreme Gradient Boosting)** es un algoritmo de gradient boosting optimizado que construye un ensamble de √°rboles de decisi√≥n de forma secuencial. Cada √°rbol nuevo intenta corregir los errores de los anteriores.\n",
    "\n",
    "**Caracter√≠sticas principales:**\n",
    "- **Regularizaci√≥n L1 y L2**: Previene overfitting mejor que otros algoritmos de boosting\n",
    "- **Tree Pruning**: Poda los √°rboles usando un enfoque de profundidad-primera\n",
    "- **Paralelizaci√≥n**: Aunque construye √°rboles secuencialmente, paraleliza operaciones internas\n",
    "- **Manejo de missing values**: Aprende autom√°ticamente la mejor direcci√≥n para valores faltantes\n",
    "- **Built-in Cross-Validation**: Incluye CV durante el entrenamiento\n",
    "\n",
    "**Ventajas:**\n",
    "- Excelente rendimiento en datos tabulares\n",
    "- R√°pido y eficiente (optimizado en C++)\n",
    "- Controla overfitting con m√∫ltiples par√°metros de regularizaci√≥n\n",
    "- Maneja bien datos no balanceados\n",
    "\n",
    "**Hiperpar√°metros clave:**\n",
    "- `n_estimators`: N√∫mero de √°rboles en el ensamble\n",
    "- `max_depth`: Profundidad m√°xima de cada √°rbol\n",
    "- `learning_rate`: Tasa de aprendizaje para reducir overfitting\n",
    "- `subsample`: Fracci√≥n de muestras para entrenar cada √°rbol\n",
    "- `colsample_bytree`: Fracci√≥n de features para cada √°rbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97959a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODELO 3: XGBOOST - ENTRENAMIENTO\n",
    "# =============================================================================\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üå≥ MODELO: XGBOOST - ENTRENAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar disponibilidad de XGBoost\n",
    "if not XGBOOST_AVAILABLE:\n",
    "    print(\"\\n‚ùå ERROR: XGBoost no est√° instalado\")\n",
    "    print(\"   Instalar con: pip install xgboost\")\n",
    "    raise ImportError(\"XGBoost no disponible\")\n",
    "\n",
    "print(\"‚úÖ XGBoost disponible\")\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Preparar carpeta y cargar datos\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: PREPARAR CARPETA Y CARGAR DATOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/4] Creando carpeta para XGBoost...\")\n",
    "\n",
    "training_folder = root_dir / 'notebook' / '03_ML_traditional_models' / 'XGBoost'\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "print(f\"   ‚úÖ Carpeta: {training_folder}\")\n",
    "\n",
    "print(f\"\\n[2/4] Cargando datos de entrenamiento normalizados...\")\n",
    "\n",
    "data_ml_folder = root_dir / 'notebook' / '02_Data_ML_traditional'\n",
    "\n",
    "X_train_norm = joblib.load(data_ml_folder / 'X_train_norm.pkl')\n",
    "y_train_norm = joblib.load(data_ml_folder / 'y_train_norm.pkl')\n",
    "\n",
    "print(f\"   ‚úÖ X_train_norm: {X_train_norm.shape}\")\n",
    "print(f\"   ‚úÖ y_train_norm: {y_train_norm.shape}\")\n",
    "\n",
    "print(f\"\\n[3/4] Verificando datos originales (para m√©tricas posteriores)...\")\n",
    "\n",
    "X_train = joblib.load(data_ml_folder / 'X_train.pkl')\n",
    "y_train = joblib.load(data_ml_folder / 'y_train.pkl')\n",
    "\n",
    "print(f\"   ‚úÖ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚úÖ y_train: {y_train.shape}\")\n",
    "\n",
    "print(f\"\\n[4/4] Verificando n√∫mero de features y targets...\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Features (inputs):  {X_train_norm.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Targets (outputs):  {y_train_norm.shape[1]} (ambas palas)\")\n",
    "print(f\"   ‚Ä¢ Muestras training:  {X_train_norm.shape[0]:,}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) Configurar y entrenar modelo XGBoost\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: CONFIGURACI√ìN Y ENTRENAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/3] Configurando XGBoost con MultiOutputRegressor...\")\n",
    "\n",
    "# PRIMERA PRUEBA - PAR√ÅMETROS POR DEFECTO\n",
    "# Configurar XGBoost con par√°metros optimizados\n",
    "# xgb_base = xgb.XGBRegressor(\n",
    "#     n_estimators=100,          # N√∫mero de √°rboles\n",
    "#     max_depth=6,               # Profundidad m√°xima de cada √°rbol\n",
    "#     learning_rate=0.1,         # Tasa de aprendizaje\n",
    "#     subsample=0.8,             # Fracci√≥n de muestras para cada √°rbol\n",
    "#     colsample_bytree=0.8,      # Fracci√≥n de features para cada √°rbol\n",
    "#     gamma=0,                   # M√≠nima reducci√≥n de loss para split\n",
    "#     min_child_weight=1,        # Peso m√≠nimo en nodo hijo\n",
    "#     reg_alpha=0,               # Regularizaci√≥n L1\n",
    "#     reg_lambda=1,              # Regularizaci√≥n L2\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1,                 # Usar todos los cores\n",
    "#     verbosity=0                # Sin mensajes de XGBoost\n",
    "# )\n",
    "\n",
    "\n",
    "# SEGUNDA PRUEBA - PAR√ÅMETROS OPTIMIZADOS TOP DE MOOMENTO\n",
    "xgb_base = xgb.XGBRegressor(\n",
    "    n_estimators=2000,          # N√∫mero de √°rboles\n",
    "    max_depth=4,               # Profundidad m√°xima de cada √°rbol\n",
    "    learning_rate=0.05,         # Tasa de aprendizaje\n",
    "    subsample=0.7,             # Fracci√≥n de muestras para cada √°rbol\n",
    "    colsample_bytree=0.7,      # Fracci√≥n de features para cada √°rbol\n",
    "    gamma=0.5,                   # M√≠nima reducci√≥n de loss para split\n",
    "    min_child_weight=12,        # Peso m√≠nimo en nodo hijo\n",
    "    reg_alpha=2,               # Regularizaci√≥n L1\n",
    "    reg_lambda=10,              # Regularizaci√≥n L2\n",
    "    random_state=42,\n",
    "    n_jobs=-1,                 # Usar todos los cores\n",
    "    verbosity=0                # Sin mensajes de XGBoost\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Usar MultiOutputRegressor para predecir ambas palas\n",
    "xgb_model = MultiOutputRegressor(xgb_base)\n",
    "\n",
    "print(f\"   ‚úÖ Modelo configurado:\")\n",
    "print(f\"      ‚Ä¢ Tipo: XGBoost (Gradient Boosting)\")\n",
    "print(f\"      ‚Ä¢ N_estimators: 100\")\n",
    "print(f\"      ‚Ä¢ Max_depth: 6\")\n",
    "print(f\"      ‚Ä¢ Learning_rate: 0.1\")\n",
    "print(f\"      ‚Ä¢ Subsample: 0.8\")\n",
    "print(f\"      ‚Ä¢ Colsample_bytree: 0.8\")\n",
    "print(f\"      ‚Ä¢ Outputs: {len(y_train_norm.columns)} (ambas palas)\")\n",
    "\n",
    "print(f\"\\n[2/3] Entrenando modelo con datos normalizados...\")\n",
    "print(f\"   ‚Ä¢ Datos de entrenamiento: {X_train_norm.shape}\")\n",
    "print(f\"   ‚Ä¢ Targets de entrenamiento: {y_train_norm.shape}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Entrenar el modelo\n",
    "xgb_model.fit(X_train_norm, y_train_norm)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ‚úÖ Modelo entrenado en {training_time:.2f} segundos\")\n",
    "\n",
    "print(f\"\\n[3/3] Guardando modelo...\")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model_path = training_folder / 'xgboost_model.pkl'\n",
    "joblib.dump(xgb_model, model_path)\n",
    "\n",
    "print(f\"   ‚úÖ Modelo guardado: {model_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# RESUMEN\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ ENTRENAMIENTO COMPLETADO - XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä INFORMACI√ìN DEL MODELO:\")\n",
    "print(f\"   ‚Ä¢ Algoritmo: XGBoost (Extreme Gradient Boosting)\")\n",
    "print(f\"   ‚Ä¢ Tiempo de entrenamiento: {training_time:.2f} segundos\")\n",
    "print(f\"   ‚Ä¢ Muestras entrenadas: {X_train_norm.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Features utilizadas: {X_train_norm.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Outputs predichos: {y_train_norm.shape[1]}\")\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVO GENERADO:\")\n",
    "print(f\"   ‚Ä¢ {model_path.name}\")\n",
    "\n",
    "print(f\"\\nüí° XGBoost usa gradient boosting optimizado para mejorar iterativamente las predicciones.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a58aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script para validar modelo XGBoost entrenado.\n",
    "\n",
    "Este script:\n",
    "1. Carga modelo entrenado y datos de test\n",
    "2. Normaliza datos de test con scalers independientes\n",
    "3. Realiza predicciones y desnormaliza\n",
    "4. Calcula m√©tricas (RMSE, R¬≤)\n",
    "5. Genera visualizaciones\n",
    "\n",
    "Adaptado para trabajar con:\n",
    "- Scalers independientes por columna (scalers_X.pkl, scalers_y.pkl)\n",
    "- Targets nuevos: M_0, M_1c, M_1s\n",
    "\n",
    "Autor: Adaptado de validaci√≥n XGBoost\n",
    "Fecha: Enero 2026\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# MODELO: XGBOOST - VALIDACI√ìN Y VISUALIZACIONES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ MODELO: XGBOOST - VALIDACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Cargar modelo entrenado y datos de test\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: CARGAR MODELO Y DATOS DE TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "training_folder = root_dir / 'notebook' / '03_ML_traditional_models' / 'XGBoost'\n",
    "data_ml_folder = root_dir / 'notebook' / '02_Data_ML_traditional'\n",
    "\n",
    "print(f\"\\n[1/5] Cargando modelo entrenado...\")\n",
    "model_path = training_folder / 'xgboost_model.pkl'\n",
    "xgb_model = joblib.load(model_path)\n",
    "print(f\"   ‚úÖ Modelo cargado desde: {model_path.name}\")\n",
    "\n",
    "print(f\"\\n[2/5] Cargando datos de test originales...\")\n",
    "X_test = joblib.load(data_ml_folder / 'X_test.pkl')\n",
    "y_test = joblib.load(data_ml_folder / 'y_test.pkl')\n",
    "print(f\"   ‚úÖ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚úÖ y_test: {y_test.shape}\")\n",
    "print(f\"   üí° Targets: {list(y_test.columns)}\")\n",
    "\n",
    "print(f\"\\n[3/5] Cargando datos de train originales (para m√©tricas)...\")\n",
    "X_train = joblib.load(data_ml_folder / 'X_train.pkl')\n",
    "y_train = joblib.load(data_ml_folder / 'y_train.pkl')\n",
    "X_train_norm = joblib.load(data_ml_folder / 'X_train_norm.pkl')\n",
    "print(f\"   ‚úÖ Datos de train cargados\")\n",
    "\n",
    "print(f\"\\n[4/5] Cargando scalers independientes...\")\n",
    "scalers_X = joblib.load(root_dir / 'notebook' / '01_Models_scaler' / 'scalers_X.pkl')\n",
    "scalers_y = joblib.load(root_dir / 'notebook' / '01_Models_scaler' / 'scalers_y.pkl')\n",
    "print(f\"   ‚úÖ Scalers cargados:\")\n",
    "print(f\"      ‚Ä¢ scalers_X: {len(scalers_X)} scalers (uno por feature)\")\n",
    "print(f\"      ‚Ä¢ scalers_y: {len(scalers_y)} scalers (uno por target)\")\n",
    "\n",
    "print(f\"\\n[5/5] Normalizando datos de test con scalers independientes...\")\n",
    "# Normalizar columna por columna usando scalers individuales\n",
    "X_test_norm = pd.DataFrame(index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "for col in X_test.columns:\n",
    "    if col in scalers_X:\n",
    "        X_test_norm[col] = scalers_X[col].transform(X_test[[col]])\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales\")\n",
    "        X_test_norm[col] = X_test[col]\n",
    "\n",
    "# Convertir a float32 para consistencia\n",
    "X_test_norm = X_test_norm.astype('float32')\n",
    "\n",
    "print(f\"   ‚úÖ X_test normalizado: {X_test_norm.shape}\")\n",
    "print(f\"   üí° Normalizaci√≥n columna por columna con scalers independientes\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) Realizar predicciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Prediciendo sobre conjunto de entrenamiento...\")\n",
    "\n",
    "y_train_pred_norm = xgb_model.predict(X_train_norm)\n",
    "y_train_pred_norm = pd.DataFrame(\n",
    "    y_train_pred_norm,\n",
    "    index=X_train_norm.index,\n",
    "    columns=y_train.columns\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones train (normalizadas): {y_train_pred_norm.shape}\")\n",
    "\n",
    "print(f\"\\n[2/2] Prediciendo sobre conjunto de test...\")\n",
    "\n",
    "y_test_pred_norm = xgb_model.predict(X_test_norm)\n",
    "y_test_pred_norm = pd.DataFrame(\n",
    "    y_test_pred_norm,\n",
    "    index=y_test.index,\n",
    "    columns=y_test.columns\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones test (normalizadas): {y_test_pred_norm.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3) Desnormalizar predicciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 3: DESNORMALIZACI√ìN DE PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Desnormalizando predicciones de train...\")\n",
    "\n",
    "# Desnormalizar columna por columna usando scalers independientes\n",
    "y_train_pred = pd.DataFrame(index=y_train_pred_norm.index, columns=y_train_pred_norm.columns)\n",
    "\n",
    "for col in y_train_pred_norm.columns:\n",
    "    if col in scalers_y:\n",
    "        y_train_pred[col] = scalers_y[col].inverse_transform(y_train_pred_norm[[col]])\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores normalizados\")\n",
    "        y_train_pred[col] = y_train_pred_norm[col]\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones train desnormalizadas\")\n",
    "print(f\"   üí° Desnormalizaci√≥n columna por columna con scalers independientes\")\n",
    "\n",
    "print(f\"\\n[2/2] Desnormalizando predicciones de test...\")\n",
    "\n",
    "# Desnormalizar columna por columna usando scalers independientes\n",
    "y_test_pred = pd.DataFrame(index=y_test_pred_norm.index, columns=y_test_pred_norm.columns)\n",
    "\n",
    "for col in y_test_pred_norm.columns:\n",
    "    if col in scalers_y:\n",
    "        y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm[[col]])\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores normalizados\")\n",
    "        y_test_pred[col] = y_test_pred_norm[col]\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones test desnormalizadas\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4) Calcular m√©tricas\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 4: C√ÅLCULO DE M√âTRICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Diccionarios para almacenar m√©tricas\n",
    "metrics_train = {}\n",
    "metrics_test = {}\n",
    "\n",
    "print(f\"\\n{'M√âTRICA':20} {'CONJUNTO':10} {'M_0':>15} {'M_1c':>15} {'M_1s':>15}\")\n",
    "print(f\"{'‚îÄ'*20} {'‚îÄ'*10} {'‚îÄ'*15} {'‚îÄ'*15} {'‚îÄ'*15}\")\n",
    "\n",
    "# Calcular m√©tricas para cada target\n",
    "for i, col in enumerate(y_train.columns):\n",
    "    # TRAIN\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train[col], y_train_pred[col]))\n",
    "    r2_train = r2_score(y_train[col], y_train_pred[col])\n",
    "    \n",
    "    # TEST\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test[col], y_test_pred[col]))\n",
    "    r2_test = r2_score(y_test[col], y_test_pred[col])\n",
    "    \n",
    "    # Guardar m√©tricas\n",
    "    metrics_train[col] = {'RMSE': rmse_train, 'R2': r2_train}\n",
    "    metrics_test[col] = {'RMSE': rmse_test, 'R2': r2_test}\n",
    "    \n",
    "    # Mostrar\n",
    "    if i == 0:\n",
    "        print(f\"{'RMSE':20} {'TRAIN':10} {rmse_train:>15.2f} {'-':>15} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {rmse_test:>15.2f} {'-':>15} {'-':>15}\")\n",
    "        print(f\"{'R¬≤':20} {'TRAIN':10} {r2_train:>15.4f} {'-':>15} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {r2_test:>15.4f} {'-':>15} {'-':>15}\")\n",
    "    elif i == 1:\n",
    "        print(f\"{'RMSE':20} {'TRAIN':10} {'-':>15} {rmse_train:>15.2f} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {rmse_test:>15.2f} {'-':>15}\")\n",
    "        print(f\"{'R¬≤':20} {'TRAIN':10} {'-':>15} {r2_train:>15.4f} {'-':>15}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {r2_test:>15.4f} {'-':>15}\")\n",
    "    else:\n",
    "        print(f\"{'RMSE':20} {'TRAIN':10} {'-':>15} {'-':>15} {rmse_train:>15.2f}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {'-':>15} {rmse_test:>15.2f}\")\n",
    "        print(f\"{'R¬≤':20} {'TRAIN':10} {'-':>15} {'-':>15} {r2_train:>15.4f}\")\n",
    "        print(f\"{'':20} {'TEST':10} {'-':>15} {'-':>15} {r2_test:>15.4f}\")\n",
    "\n",
    "# Calcular promedios\n",
    "avg_rmse_train = np.mean([m['RMSE'] for m in metrics_train.values()])\n",
    "avg_rmse_test = np.mean([m['RMSE'] for m in metrics_test.values()])\n",
    "avg_r2_train = np.mean([m['R2'] for m in metrics_train.values()])\n",
    "avg_r2_test = np.mean([m['R2'] for m in metrics_test.values()])\n",
    "\n",
    "print(f\"{'‚îÄ'*20} {'‚îÄ'*10} {'‚îÄ'*15} {'‚îÄ'*15} {'‚îÄ'*15}\")\n",
    "print(f\"{'PROMEDIO RMSE':20} {'TRAIN':10} {avg_rmse_train:>15.2f}\")\n",
    "print(f\"{'':20} {'TEST':10} {avg_rmse_test:>15.2f}\")\n",
    "print(f\"{'PROMEDIO R¬≤':20} {'TRAIN':10} {avg_r2_train:>15.4f}\")\n",
    "print(f\"{'':20} {'TEST':10} {avg_r2_test:>15.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5) Visualizaciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 5: GENERACI√ìN DE GR√ÅFICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 5.1) Gr√°fica de m√©tricas por target\n",
    "print(f\"\\n[1/3] Creando gr√°fica de m√©tricas...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "targets = list(metrics_train.keys())\n",
    "rmse_train_vals = [metrics_train[t]['RMSE'] for t in targets]\n",
    "rmse_test_vals = [metrics_test[t]['RMSE'] for t in targets]\n",
    "\n",
    "x = np.arange(len(targets))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, rmse_train_vals, width, label='Train', alpha=0.8)\n",
    "axes[0].bar(x + width/2, rmse_test_vals, width, label='Test', alpha=0.8)\n",
    "axes[0].set_xlabel('Target', fontsize=12)\n",
    "axes[0].set_ylabel('RMSE [kNm]', fontsize=12)\n",
    "axes[0].set_title('RMSE por Target - XGBoost', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(targets, fontsize=11)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤\n",
    "r2_train_vals = [metrics_train[t]['R2'] for t in targets]\n",
    "r2_test_vals = [metrics_test[t]['R2'] for t in targets]\n",
    "\n",
    "axes[1].bar(x - width/2, r2_train_vals, width, label='Train', alpha=0.8)\n",
    "axes[1].bar(x + width/2, r2_test_vals, width, label='Test', alpha=0.8)\n",
    "axes[1].set_xlabel('Target', fontsize=12)\n",
    "axes[1].set_ylabel('R¬≤', fontsize=12)\n",
    "axes[1].set_title('R¬≤ por Target - XGBoost', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(targets, fontsize=11)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "metrics_plot_path = training_folder / 'metrics_comparison.png'\n",
    "plt.savefig(metrics_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {metrics_plot_path.name}\")\n",
    "\n",
    "# 5.2) Gr√°ficas de predicciones vs reales\n",
    "print(f\"\\n[2/3] Creando gr√°ficas de predicciones vs reales...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 16))\n",
    "\n",
    "for idx, col in enumerate(y_train.columns):\n",
    "    row = idx\n",
    "    \n",
    "    # TRAIN\n",
    "    axes[row, 0].scatter(y_train[col], y_train_pred[col], alpha=0.3, s=1)\n",
    "    axes[row, 0].plot([y_train[col].min(), y_train[col].max()], \n",
    "                       [y_train[col].min(), y_train[col].max()], \n",
    "                       'r--', lw=2, label='Perfect prediction')\n",
    "    axes[row, 0].set_xlabel('Real [kNm]', fontsize=11)\n",
    "    axes[row, 0].set_ylabel('Predicho [kNm]', fontsize=11)\n",
    "    axes[row, 0].set_title(f'{col} - TRAIN (R¬≤={metrics_train[col][\"R2\"]:.4f})', \n",
    "                          fontsize=12, fontweight='bold')\n",
    "    axes[row, 0].legend(fontsize=10)\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # TEST\n",
    "    axes[row, 1].scatter(y_test[col], y_test_pred[col], alpha=0.3, s=1, color='orange')\n",
    "    axes[row, 1].plot([y_test[col].min(), y_test[col].max()], \n",
    "                       [y_test[col].min(), y_test[col].max()], \n",
    "                       'r--', lw=2, label='Perfect prediction')\n",
    "    axes[row, 1].set_xlabel('Real [kNm]', fontsize=11)\n",
    "    axes[row, 1].set_ylabel('Predicho [kNm]', fontsize=11)\n",
    "    axes[row, 1].set_title(f'{col} - TEST (R¬≤={metrics_test[col][\"R2\"]:.4f})', \n",
    "                          fontsize=12, fontweight='bold')\n",
    "    axes[row, 1].legend(fontsize=10)\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "predictions_plot_path = training_folder / 'predictions_vs_real.png'\n",
    "plt.savefig(predictions_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {predictions_plot_path.name}\")\n",
    "\n",
    "# 5.3) Gr√°fica de residuos\n",
    "print(f\"\\n[3/3] Creando gr√°fica de residuos...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 16))\n",
    "\n",
    "for idx, col in enumerate(y_train.columns):\n",
    "    row = idx\n",
    "    \n",
    "    # Calcular residuos\n",
    "    residuals_train = y_train[col] - y_train_pred[col]\n",
    "    residuals_test = y_test[col] - y_test_pred[col]\n",
    "    \n",
    "    # TRAIN\n",
    "    axes[row, 0].scatter(y_train_pred[col], residuals_train, alpha=0.3, s=1)\n",
    "    axes[row, 0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[row, 0].set_xlabel('Predicho [kNm]', fontsize=11)\n",
    "    axes[row, 0].set_ylabel('Residuo (Real - Predicho) [kNm]', fontsize=11)\n",
    "    axes[row, 0].set_title(f'{col} - Residuos TRAIN', fontsize=12, fontweight='bold')\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # TEST\n",
    "    axes[row, 1].scatter(y_test_pred[col], residuals_test, alpha=0.3, s=1, color='orange')\n",
    "    axes[row, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[row, 1].set_xlabel('Predicho [kNm]', fontsize=11)\n",
    "    axes[row, 1].set_ylabel('Residuo (Real - Predicho) [kNm]', fontsize=11)\n",
    "    axes[row, 1].set_title(f'{col} - Residuos TEST', fontsize=12, fontweight='bold')\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "residuals_plot_path = training_folder / 'residuals_analysis.png'\n",
    "plt.savefig(residuals_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {residuals_plot_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6) Guardar m√©tricas en archivo\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 6: GUARDAR RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Crear DataFrame con m√©tricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Target': list(metrics_train.keys()) + ['PROMEDIO'],\n",
    "    'RMSE_Train': list([m['RMSE'] for m in metrics_train.values()]) + [avg_rmse_train],\n",
    "    'RMSE_Test': list([m['RMSE'] for m in metrics_test.values()]) + [avg_rmse_test],\n",
    "    'R2_Train': list([m['R2'] for m in metrics_train.values()]) + [avg_r2_train],\n",
    "    'R2_Test': list([m['R2'] for m in metrics_test.values()]) + [avg_r2_test]\n",
    "})\n",
    "\n",
    "metrics_csv_path = training_folder / 'metrics_results.csv'\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "print(f\"\\n   ‚úÖ M√©tricas guardadas: {metrics_csv_path.name}\")\n",
    "\n",
    "# Crear resumen de validaci√≥n\n",
    "summary = {\n",
    "    'Model': 'XGBoost',\n",
    "    'Train_Samples': len(X_train),\n",
    "    'Test_Samples': len(X_test),\n",
    "    'Features': X_train.shape[1],\n",
    "    'Targets': y_train.shape[1],\n",
    "    'Avg_RMSE_Train': avg_rmse_train,\n",
    "    'Avg_RMSE_Test': avg_rmse_test,\n",
    "    'Avg_R2_Train': avg_r2_train,\n",
    "    'Avg_R2_Test': avg_r2_test,\n",
    "    'N_Estimators': 100,\n",
    "    'Max_Depth': 6,\n",
    "    'Learning_Rate': 0.1,\n",
    "    'Subsample': 0.8,\n",
    "    'Colsample_Bytree': 0.8,\n",
    "    'Tree_Method': 'hist'\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_path = training_folder / 'validation_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(f\"   ‚úÖ Resumen guardado: {summary_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# RESUMEN FINAL\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ VALIDACI√ìN COMPLETADA - XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESULTADOS FINALES:\")\n",
    "print(f\"   ‚Ä¢ RMSE promedio (Train): {avg_rmse_train:.2f} kNm\")\n",
    "print(f\"   ‚Ä¢ RMSE promedio (Test):  {avg_rmse_test:.2f} kNm\")\n",
    "print(f\"   ‚Ä¢ R¬≤ promedio (Train):   {avg_r2_train:.4f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ promedio (Test):    {avg_r2_test:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "print(f\"   ‚Ä¢ {metrics_csv_path.name}\")\n",
    "print(f\"   ‚Ä¢ {summary_path.name}\")\n",
    "print(f\"   ‚Ä¢ {metrics_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {predictions_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {residuals_plot_path.name}\")\n",
    "\n",
    "print(f\"\\nüí° XGBoost con scalers independientes por columna (X e y).\")\n",
    "print(f\"üí° Targets: {list(y_test.columns)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939eb417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# XGBOOST - GR√ÅFICAS ADICIONALES DE TIME SERIES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä XGBOOST - GR√ÅFICAS DE TIME SERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# NOTA: Este script requiere que y_test y y_test_pred est√©n en memoria\n",
    "# Ejecutar primero XGBOOST_Validation.py\n",
    "\n",
    "# Verificar que las variables existan\n",
    "try:\n",
    "    y_test\n",
    "    y_test_pred\n",
    "    print(f\"\\n‚úÖ Variables encontradas en memoria:\")\n",
    "    print(f\"   ‚Ä¢ y_test: {y_test.shape}\")\n",
    "    print(f\"   ‚Ä¢ y_test_pred: {y_test_pred.shape}\")\n",
    "except NameError:\n",
    "    print(f\"\\n‚ùå ERROR: y_test y/o y_test_pred no est√°n en memoria\")\n",
    "    print(f\"   Por favor, ejecuta primero XGBOOST_Validation.py\")\n",
    "    raise\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Cargar Time_test y generar series_id_test\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: PREPARAR DATOS PARA TIME SERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "training_folder = root_dir / 'notebook' / '03_ML_traditional_models' / 'XGBoost'\n",
    "data_ml_folder = root_dir / 'notebook' / '02_Data_ML_traditional'\n",
    "\n",
    "print(f\"\\n[1/3] Cargando Time_test desde archivo pkl...\")\n",
    "\n",
    "Time_test_path = data_ml_folder / 'Time_test.pkl'\n",
    "\n",
    "if Time_test_path.exists():\n",
    "    Time_test = joblib.load(Time_test_path)\n",
    "    print(f\"   ‚úÖ Time_test cargado: {len(Time_test):,} valores\")\n",
    "    print(f\"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s\")\n",
    "else:\n",
    "    print(f\"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}\")\n",
    "    raise FileNotFoundError(f\"Archivo requerido no encontrado: {Time_test_path}\")\n",
    "\n",
    "print(f\"\\n[2/3] Generando series_id_test a partir de Time_test...\")\n",
    "\n",
    "# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\n",
    "series_id_test_values = np.zeros(len(Time_test), dtype=int)\n",
    "current_series = 0\n",
    "\n",
    "Time_test_array = Time_test.values\n",
    "for i in range(1, len(Time_test_array)):\n",
    "    if Time_test_array[i] < Time_test_array[i-1]:\n",
    "        current_series += 1\n",
    "    series_id_test_values[i] = current_series\n",
    "\n",
    "# Convertir a pandas Series con el mismo index que Time_test\n",
    "series_id_test = pd.Series(series_id_test_values, index=Time_test.index, name='series_id')\n",
    "\n",
    "n_test_series = series_id_test.max() + 1\n",
    "\n",
    "print(f\"   ‚úÖ Series temporales identificadas en test: {n_test_series}\")\n",
    "\n",
    "# Analizar cada serie\n",
    "print(f\"\\n   üìä Resumen de series en TEST:\")\n",
    "for sid in range(min(5, n_test_series)):\n",
    "    mask = series_id_test == sid\n",
    "    n_rows = mask.sum()\n",
    "    time_min = Time_test.loc[mask].min()\n",
    "    time_max = Time_test.loc[mask].max()\n",
    "    print(f\"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s\")\n",
    "\n",
    "if n_test_series > 5:\n",
    "    print(f\"      ... y {n_test_series - 5} series m√°s\")\n",
    "\n",
    "print(f\"\\n[3/3] Verificando que los datos coinciden con predicciones...\")\n",
    "\n",
    "# Verificar que los √≠ndices coinciden\n",
    "if not all(series_id_test.index == y_test.index):\n",
    "    print(f\"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...\")\n",
    "    series_id_test = series_id_test.reindex(y_test.index)\n",
    "    Time_test = Time_test.reindex(y_test.index)\n",
    "\n",
    "print(f\"   ‚úÖ √çndices verificados:\")\n",
    "print(f\"      ‚Ä¢ y_test: {y_test.shape[0]:,} filas\")\n",
    "print(f\"      ‚Ä¢ y_test_pred: {y_test_pred.shape[0]:,} filas\")\n",
    "print(f\"      ‚Ä¢ series_id_test: {len(series_id_test):,} valores\")\n",
    "print(f\"      ‚Ä¢ Time_test: {len(Time_test):,} valores\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) Gr√°fica de time series: Real vs Predicho (3 series aleatorias)\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/1] Creando gr√°ficas de time series (Real vs Predicho)...\")\n",
    "\n",
    "# Obtener series √∫nicas del conjunto de test\n",
    "unique_test_series = series_id_test.unique()\n",
    "\n",
    "# Seleccionar 3 series aleatorias\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "selected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n",
    "\n",
    "print(f\"   ‚Ä¢ Total series disponibles: {len(unique_test_series)}\")\n",
    "print(f\"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}\")\n",
    "\n",
    "# Detectar n√∫mero de targets autom√°ticamente\n",
    "n_targets = len(y_test.columns)\n",
    "print(f\"   ‚Ä¢ Targets a graficar: {list(y_test.columns)}\")\n",
    "\n",
    "# Crear figura con 3 filas x n_targets columnas (3 series, 3 targets)\n",
    "fig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n",
    "\n",
    "# Asegurar que axes sea 2D\n",
    "if n_targets == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Para cada target, graficar en subplot separado\n",
    "    for target_idx, col in enumerate(y_test.columns):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales\n",
    "        y_real = y_test.loc[series_indices, col]\n",
    "        # Valores predichos\n",
    "        y_pred = y_test_pred.loc[series_indices, col]\n",
    "        \n",
    "        # Calcular m√©tricas para esta serie y target\n",
    "        r2_series = r2_score(y_real, y_pred)\n",
    "        rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n",
    "        \n",
    "        # Graficar\n",
    "        ax.plot(time_series, y_real, label='Real', \n",
    "                linewidth=2, alpha=0.8, color='blue')\n",
    "        ax.plot(time_series, y_pred, label='Predicho', \n",
    "                linestyle='--', linewidth=2, alpha=0.8, color='red')\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Momento [kNm]', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "timeseries_plot_path = training_folder / 'timeseries_comparison.png'\n",
    "plt.savefig(timeseries_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3) Gr√°fica de time series con ZOOM (50 segundos)\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 3: GR√ÅFICAS DE TIME SERIES - ZOOM 50 SEGUNDOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/1] Creando gr√°ficas de time series con zoom (50s)...\")\n",
    "\n",
    "# Crear figura con 3 filas x n_targets columnas (3 series, 3 targets)\n",
    "fig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n",
    "\n",
    "# Asegurar que axes sea 2D\n",
    "if n_targets == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Definir ventana de 50 segundos desde el inicio\n",
    "    time_min = time_series.min()\n",
    "    time_max_zoom = time_min + 50\n",
    "    \n",
    "    # Filtrar por ventana de tiempo\n",
    "    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n",
    "    zoom_indices = time_series[zoom_mask].index\n",
    "    time_zoom = time_series[zoom_mask]\n",
    "    \n",
    "    # Para cada target, graficar en subplot separado\n",
    "    for target_idx, col in enumerate(y_test.columns):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales y predichos (con zoom)\n",
    "        y_real_zoom = y_test.loc[zoom_indices, col]\n",
    "        y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n",
    "        \n",
    "        # Calcular m√©tricas para esta ventana\n",
    "        r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n",
    "        rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n",
    "        \n",
    "        # Graficar\n",
    "        ax.plot(time_zoom, y_real_zoom, label='Real', \n",
    "                linewidth=2.5, alpha=0.8, color='blue', marker='o', markersize=4)\n",
    "        ax.plot(time_zoom, y_pred_zoom, label='Predicho', \n",
    "                linestyle='--', linewidth=2.5, alpha=0.8, color='red', marker='x', markersize=5)\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Momento [kNm]', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # A√±adir texto con informaci√≥n de puntos\n",
    "        n_points = len(zoom_indices)\n",
    "        ax.text(0.02, 0.02, f'Puntos: {n_points}', transform=ax.transAxes, fontsize=9,\n",
    "                verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "timeseries_zoom_plot_path = training_folder / 'timeseries_comparison_zoom50s.png'\n",
    "plt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# RESUMEN FINAL\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ GR√ÅFICAS DE TIME SERIES COMPLETADAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä GR√ÅFICAS GENERADAS:\")\n",
    "print(f\"   ‚Ä¢ {timeseries_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {timeseries_zoom_plot_path.name}\")\n",
    "\n",
    "print(f\"\\nüí° Targets graficados: {list(y_test.columns)}\")\n",
    "print(f\"üí° Series aleatorias: {list(selected_series)}\")\n",
    "\n",
    "print(f\"\\nüí° Ahora tienes todas las gr√°ficas para XGBoost:\")\n",
    "print(f\"   1. M√©tricas por target (RMSE y R¬≤)\")\n",
    "print(f\"   2. Predicciones vs Real\")\n",
    "print(f\"   3. An√°lisis de residuos\")\n",
    "print(f\"   4. Time series completas (3 series aleatorias √ó {n_targets} targets)\")\n",
    "print(f\"   5. Time series con zoom de 50s\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e0213",
   "metadata": {},
   "source": [
    "## üß† MULTI-LAYER PERCEPTRON (MLP)\n",
    "\n",
    "**MLP (Multi-Layer Perceptron)** es una red neuronal artificial feedforward que consiste \n",
    "en m√∫ltiples capas de neuronas interconectadas. Cada capa transforma la informaci√≥n usando \n",
    "funciones de activaci√≥n no lineales, permitiendo al modelo aprender relaciones complejas.\n",
    "\n",
    "**Caracter√≠sticas principales:**\n",
    "- **Arquitectura multicapa**: Capas de entrada, ocultas y salida conectadas completamente\n",
    "- **Funciones de activaci√≥n**: ReLU, tanh, sigmoid para introducir no-linealidades\n",
    "- **Backpropagation**: Algoritmo de optimizaci√≥n mediante gradiente descendente\n",
    "- **Regularizaci√≥n L2**: Penalizaci√≥n para evitar overfitting\n",
    "- **Early Stopping**: Detiene el entrenamiento cuando no hay mejora en validaci√≥n\n",
    "\n",
    "**Ventajas:**\n",
    "- Capaz de aprender relaciones no lineales complejas\n",
    "- Flexible en arquitectura (n√∫mero de capas y neuronas)\n",
    "- Funciona bien con datos normalizados y grandes vol√∫menes\n",
    "- Aprende representaciones jer√°rquicas de los datos\n",
    "\n",
    "**Desventajas:**\n",
    "- Requiere normalizaci√≥n de datos\n",
    "- M√°s lento de entrenar que modelos de √°rboles\n",
    "- Puede quedarse en m√≠nimos locales\n",
    "- Menos interpretable que modelos lineales o de √°rboles\n",
    "\n",
    "**Hiperpar√°metros clave:**\n",
    "- `hidden_layer_sizes`: Arquitectura de capas ocultas (ej: (256, 128, 64))\n",
    "- `activation`: Funci√≥n de activaci√≥n ('relu', 'tanh', 'logistic')\n",
    "- `solver`: Optimizador ('adam', 'sgd', 'lbfgs')\n",
    "- `alpha`: Par√°metro de regularizaci√≥n L2\n",
    "- `learning_rate_init`: Tasa de aprendizaje inicial\n",
    "- `batch_size`: Tama√±o del mini-batch para entrenamiento\n",
    "- `max_iter`: N√∫mero m√°ximo de √©pocas\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4dbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "PASO 1: ENTRENAMIENTO DE MODELO MLP (MULTI-LAYER PERCEPTRON)\n",
    "============================================================================\n",
    "\n",
    "Este script entrena un modelo de Red Neuronal MLP para predecir cargas en \n",
    "palas de aerogenerador.\n",
    "\n",
    "Autor: [Tu nombre]\n",
    "Fecha: 2026-01-08\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Deep Learning\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# M√©tricas\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Configuraci√≥n de warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ü§ñ PASO 1: ENTRENAMIENTO DE MODELO MLP\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURACI√ìN DE RUTAS\n",
    "# ============================================================================\n",
    "print(\"\\n[1/7] Configurando rutas...\")\n",
    "\n",
    "# Directorio ra√≠z del proyecto\n",
    "data_ml_folder = root_dir / 'notebook' / '02_Data_ML_traditional'\n",
    "models_folder = root_dir / 'notebook' / '03_ML_traditional_models'\n",
    "\n",
    "# Crear carpeta de modelos si no existe\n",
    "models_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"   ‚úÖ Carpeta de datos: {data_ml_folder}\")\n",
    "print(f\"   ‚úÖ Carpeta de modelos: {models_folder}\")\n",
    "\n",
    "# Verificar que existen los datos\n",
    "if not data_ml_folder.exists():\n",
    "    raise FileNotFoundError(f\"No se encuentra la carpeta: {data_ml_folder}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CARGAR DATOS\n",
    "# ============================================================================\n",
    "print(\"\\n[2/7] Cargando datos de entrenamiento...\")\n",
    "\n",
    "try:\n",
    "    # Cargar datos normalizados (MLP funciona mejor con datos normalizados)\n",
    "    X_train_norm = joblib.load(data_ml_folder / 'X_train_norm.pkl')\n",
    "    y_train_norm = joblib.load(data_ml_folder / 'y_train_norm.pkl')\n",
    "    \n",
    "    print(f\"   ‚úÖ X_train_norm cargado: {X_train_norm.shape}\")\n",
    "    print(f\"   ‚úÖ y_train_norm cargado: {y_train_norm.shape}\")\n",
    "    \n",
    "    # Info de los datos\n",
    "    n_samples, n_features = X_train_norm.shape\n",
    "    n_outputs = y_train_norm.shape[1]\n",
    "    \n",
    "    print(f\"\\n   üìä Informaci√≥n de los datos:\")\n",
    "    print(f\"      ‚Ä¢ Muestras de entrenamiento: {n_samples:,}\")\n",
    "    print(f\"      ‚Ä¢ Features: {n_features}\")\n",
    "    print(f\"      ‚Ä¢ Outputs: {n_outputs}\")\n",
    "    print(f\"      ‚Ä¢ Tama√±o en memoria: {X_train_norm.memory_usage(deep=True).sum() / (1024**2):.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error cargando datos: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CONFIGURACI√ìN DEL MODELO MLP\n",
    "# ============================================================================\n",
    "print(\"\\n[3/7] Configurando modelo MLP...\")\n",
    "\n",
    "# Par√°metros del modelo\n",
    "# mlp_params = {\n",
    "#     # Arquitectura de la red\n",
    "#     'hidden_layer_sizes': (256, 128, 64),  # 3 capas ocultas con 256, 128 y 64 neuronas\n",
    "#     \n",
    "#     # Funci√≥n de activaci√≥n\n",
    "#     'activation': 'relu',  # ReLU es est√°ndar para regresi√≥n\n",
    "#     \n",
    "#     # Solver (optimizador)\n",
    "#     'solver': 'adam',  # Adam es eficiente y robusto\n",
    "#     \n",
    "#     # Par√°metros de regularizaci√≥n\n",
    "#     'alpha': 0.0001,  # L2 penalty (regularizaci√≥n)\n",
    "#     \n",
    "#     # Par√°metros de entrenamiento\n",
    "#     'batch_size': 256,  # Tama√±o de batch para mini-batch gradient descent\n",
    "#     'learning_rate': 'adaptive',  # Ajusta learning rate autom√°ticamente\n",
    "#     'learning_rate_init': 0.001,  # Learning rate inicial\n",
    "#     'max_iter': 500,  # M√°ximo de √©pocas\n",
    "#     \n",
    "#     # Early stopping\n",
    "#     'early_stopping': True,  # Detener si no hay mejora\n",
    "#     'validation_fraction': 0.1,  # 10% para validaci√≥n interna\n",
    "#     'n_iter_no_change': 20,  # Parar despu√©s de 20 √©pocas sin mejora\n",
    "#     'tol': 1e-4,  # Tolerancia para la mejora\n",
    "#     \n",
    "#     # Otros\n",
    "#     'random_state': 42,\n",
    "#     'verbose': True,  # Mostrar progreso\n",
    "#     'warm_start': False,  # No usar warm start\n",
    "# }\n",
    "\n",
    "mlp_params = {\n",
    "    # Arquitectura de la red\n",
    "    'hidden_layer_sizes': (128, 64),  # 2 capas ocultas con 256, 128 y 64 neuronas\n",
    "    \n",
    "    # Funci√≥n de activaci√≥n\n",
    "    'activation': 'relu',  # ReLU es est√°ndar para regresi√≥n\n",
    "    \n",
    "    # Solver (optimizador)\n",
    "    'solver': 'adam',  # Adam es eficiente y robusto\n",
    "    \n",
    "    # Par√°metros de regularizaci√≥n\n",
    "    'alpha': 0.01,  # L2 penalty (regularizaci√≥n)\n",
    "    \n",
    "    # Par√°metros de entrenamiento\n",
    "    'batch_size': 512,  # Tama√±o de batch para mini-batch gradient descent\n",
    "    'learning_rate': 'constant',  # Ajusta learning rate autom√°ticamente\n",
    "    'learning_rate_init': 5e-4,  # Learning rate inicial\n",
    "    'max_iter': 2000,  # M√°ximo de √©pocas\n",
    "    \n",
    "    # Early stopping\n",
    "    'early_stopping': True,  # Detener si no hay mejora\n",
    "    'validation_fraction': 0.1,  # 10% para validaci√≥n interna\n",
    "    'n_iter_no_change': 20,  # Parar despu√©s de 20 √©pocas sin mejora\n",
    "    'tol': 1e-2,  # Tolerancia para la mejora\n",
    "    \n",
    "    # Otros\n",
    "    'random_state': 42,\n",
    "    'verbose': True,  # Mostrar progreso\n",
    "    'warm_start': False,  # No usar warm start\n",
    "}\n",
    "\n",
    "print(f\"   üìã Configuraci√≥n del modelo:\")\n",
    "print(f\"      ‚Ä¢ Arquitectura: {mlp_params['hidden_layer_sizes']}\")\n",
    "print(f\"      ‚Ä¢ Activaci√≥n: {mlp_params['activation']}\")\n",
    "print(f\"      ‚Ä¢ Solver: {mlp_params['solver']}\")\n",
    "print(f\"      ‚Ä¢ Learning rate: {mlp_params['learning_rate_init']}\")\n",
    "print(f\"      ‚Ä¢ Batch size: {mlp_params['batch_size']}\")\n",
    "print(f\"      ‚Ä¢ Max √©pocas: {mlp_params['max_iter']}\")\n",
    "print(f\"      ‚Ä¢ Early stopping: {mlp_params['early_stopping']}\")\n",
    "print(f\"      ‚Ä¢ Regularizaci√≥n (alpha): {mlp_params['alpha']}\")\n",
    "\n",
    "# Crear modelo\n",
    "mlp_model = MLPRegressor(**mlp_params)\n",
    "\n",
    "print(f\"\\n   ‚úÖ Modelo MLP creado\")\n",
    "print(f\"      Total de par√°metros estimados: ~{sum(mlp_params['hidden_layer_sizes']) * n_features + sum(mlp_params['hidden_layer_sizes']) * n_outputs:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ENTRENAR MODELO\n",
    "# ============================================================================\n",
    "print(\"\\n[4/7] Entrenando modelo MLP...\")\n",
    "print(\"   ‚è≥ Esto puede tardar varios minutos...\")\n",
    "print(\"   \" + \"-\"*76)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Entrenar el modelo\n",
    "    mlp_model.fit(X_train_norm, y_train_norm)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(\"   \" + \"-\"*76)\n",
    "    print(f\"\\n   ‚úÖ Entrenamiento completado!\")\n",
    "    print(f\"      ‚Ä¢ Tiempo de entrenamiento: {training_time/60:.2f} minutos ({training_time:.1f} segundos)\")\n",
    "    print(f\"      ‚Ä¢ √âpocas ejecutadas: {mlp_model.n_iter_}\")\n",
    "    print(f\"      ‚Ä¢ Loss final: {mlp_model.loss_:.6f}\")\n",
    "    \n",
    "    if mlp_model.n_iter_ < mlp_params['max_iter']:\n",
    "        print(f\"      ‚Ä¢ Early stopping activado ‚úì\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n   ‚ùå Error durante el entrenamiento: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. EVALUACI√ìN EN TRAIN\n",
    "# ============================================================================\n",
    "print(\"\\n[5/7] Evaluando modelo en conjunto de entrenamiento...\")\n",
    "\n",
    "try:\n",
    "    # Predicciones en train\n",
    "    y_train_pred = mlp_model.predict(X_train_norm)\n",
    "    \n",
    "    # Calcular m√©tricas para cada output\n",
    "    target_names = y_train_norm.columns if isinstance(y_train_norm, pd.DataFrame) else [f'Output_{i}' for i in range(n_outputs)]\n",
    "    \n",
    "    print(f\"\\n   üìä M√©tricas de entrenamiento:\")\n",
    "    print(f\"      {'Target':<30} {'MAE':>12} {'RMSE':>12} {'R¬≤':>10}\")\n",
    "    print(f\"      {'-'*30} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "    \n",
    "    metrics_train = {}\n",
    "    \n",
    "    for i, target_name in enumerate(target_names):\n",
    "        if isinstance(y_train_norm, pd.DataFrame):\n",
    "            y_true = y_train_norm.iloc[:, i]\n",
    "            y_pred = y_train_pred[:, i]\n",
    "        else:\n",
    "            y_true = y_train_norm[:, i]\n",
    "            y_pred = y_train_pred[:, i]\n",
    "        \n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        metrics_train[target_name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "        \n",
    "        print(f\"      {target_name:<30} {mae:12.6f} {rmse:12.6f} {r2:10.6f}\")\n",
    "    \n",
    "    # M√©trica global\n",
    "    mae_global = mean_absolute_error(y_train_norm, y_train_pred)\n",
    "    rmse_global = np.sqrt(mean_squared_error(y_train_norm, y_train_pred))\n",
    "    r2_global = r2_score(y_train_norm, y_train_pred)\n",
    "    \n",
    "    print(f\"      {'-'*30} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "    print(f\"      {'PROMEDIO GLOBAL':<30} {mae_global:12.6f} {rmse_global:12.6f} {r2_global:10.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error calculando m√©tricas: {e}\")\n",
    "    metrics_train = {}\n",
    "\n",
    "# ============================================================================\n",
    "# 6. GUARDAR MODELO\n",
    "# ============================================================================\n",
    "print(\"\\n[6/7] Guardando modelo entrenado...\")\n",
    "\n",
    "try:\n",
    "    # Nombre del archivo del modelo\n",
    "    model_filename = models_folder / 'mlp_model.pkl'\n",
    "    \n",
    "    # Guardar modelo\n",
    "    joblib.dump(mlp_model, model_filename)\n",
    "    \n",
    "    # Tama√±o del archivo\n",
    "    model_size = model_filename.stat().st_size / (1024**2)\n",
    "    \n",
    "    print(f\"   ‚úÖ Modelo guardado exitosamente\")\n",
    "    print(f\"      ‚Ä¢ Archivo: {model_filename}\")\n",
    "    print(f\"      ‚Ä¢ Tama√±o: {model_size:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error guardando modelo: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# 7. GUARDAR INFORMACI√ìN DEL ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "print(\"\\n[7/7] Guardando informaci√≥n del entrenamiento...\")\n",
    "\n",
    "try:\n",
    "    # Crear diccionario con informaci√≥n del entrenamiento\n",
    "    training_info = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_type': 'MLPRegressor',\n",
    "        'sklearn_version': __import__('sklearn').__version__,\n",
    "        \n",
    "        # Datos\n",
    "        'n_samples': n_samples,\n",
    "        'n_features': n_features,\n",
    "        'n_outputs': n_outputs,\n",
    "        \n",
    "        # Par√°metros del modelo\n",
    "        'model_params': mlp_params,\n",
    "        \n",
    "        # Resultados del entrenamiento\n",
    "        'training_time_seconds': training_time,\n",
    "        'n_iterations': mlp_model.n_iter_,\n",
    "        'final_loss': mlp_model.loss_,\n",
    "        \n",
    "        # M√©tricas\n",
    "        'train_metrics': metrics_train,\n",
    "        'train_mae_global': mae_global,\n",
    "        'train_rmse_global': rmse_global,\n",
    "        'train_r2_global': r2_global,\n",
    "    }\n",
    "    \n",
    "    # Guardar informaci√≥n\n",
    "    info_filename = models_folder / 'mlp_training_info.pkl'\n",
    "    joblib.dump(training_info, info_filename)\n",
    "    \n",
    "    print(f\"   ‚úÖ Informaci√≥n guardada en: {info_filename}\")\n",
    "    \n",
    "    # Tambi√©n guardar como JSON legible\n",
    "    import json\n",
    "    \n",
    "    # Convertir a formato serializable\n",
    "    training_info_json = {\n",
    "        'timestamp': training_info['timestamp'],\n",
    "        'model_type': training_info['model_type'],\n",
    "        'sklearn_version': training_info['sklearn_version'],\n",
    "        'n_samples': int(training_info['n_samples']),\n",
    "        'n_features': int(training_info['n_features']),\n",
    "        'n_outputs': int(training_info['n_outputs']),\n",
    "        'training_time_minutes': round(training_info['training_time_seconds'] / 60, 2),\n",
    "        'n_iterations': int(training_info['n_iterations']),\n",
    "        'final_loss': float(training_info['final_loss']),\n",
    "        'train_mae_global': float(training_info['train_mae_global']),\n",
    "        'train_rmse_global': float(training_info['train_rmse_global']),\n",
    "        'train_r2_global': float(training_info['train_r2_global']),\n",
    "    }\n",
    "    \n",
    "    info_json_filename = models_folder / 'mlp_training_info.json'\n",
    "    with open(info_json_filename, 'w') as f:\n",
    "        json.dump(training_info_json, f, indent=4)\n",
    "    \n",
    "    print(f\"   ‚úÖ Resumen guardado en: {info_json_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Advertencia: No se pudo guardar la informaci√≥n completa: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ENTRENAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESUMEN:\")\n",
    "print(f\"   ‚Ä¢ Modelo: MLPRegressor\")\n",
    "print(f\"   ‚Ä¢ Arquitectura: {mlp_params['hidden_layer_sizes']}\")\n",
    "print(f\"   ‚Ä¢ Datos entrenamiento: {n_samples:,} muestras\")\n",
    "print(f\"   ‚Ä¢ Features: {n_features}\")\n",
    "print(f\"   ‚Ä¢ Tiempo de entrenamiento: {training_time/60:.2f} minutos\")\n",
    "print(f\"   ‚Ä¢ √âpocas: {mlp_model.n_iter_}/{mlp_params['max_iter']}\")\n",
    "print(f\"   ‚Ä¢ Loss final: {mlp_model.loss_:.6f}\")\n",
    "\n",
    "print(f\"\\nüìà M√âTRICAS DE ENTRENAMIENTO:\")\n",
    "print(f\"   ‚Ä¢ MAE:  {mae_global:.6f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {rmse_global:.6f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤:   {r2_global:.6f}\")\n",
    "\n",
    "print(f\"\\nüíæ ARCHIVOS GENERADOS:\")\n",
    "print(f\"   ‚Ä¢ {model_filename}\")\n",
    "print(f\"   ‚Ä¢ {info_filename}\")\n",
    "print(f\"   ‚Ä¢ {info_json_filename}\")\n",
    "\n",
    "print(f\"\\nüéØ SIGUIENTE PASO:\")\n",
    "print(f\"   Ejecutar: 02_MLP_validation.py\")\n",
    "print(f\"   Para evaluar el modelo en el conjunto de test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a0bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "PASO 2: VALIDACI√ìN DEL MODELO MLP\n",
    "============================================================================\n",
    "\n",
    "Este script eval√∫a el rendimiento del modelo MLP en el conjunto de test\n",
    "y genera gr√°ficos de an√°lisis.\n",
    "\n",
    "IMPORTANTE: Adaptado para scalers independientes por columna\n",
    "- scalers_X.pkl: Diccionario con un scaler por feature\n",
    "- scalers_y.pkl: Diccionario con un scaler por target\n",
    "\n",
    "Autor: Adaptado para scalers independientes\n",
    "Fecha: Enero 2026\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Configuraci√≥n de matplotlib\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURACI√ìN INICIAL\n",
    "# =============================================================================\n",
    "\n",
    "# Directorio ra√≠z del proyecto (ajustar seg√∫n necesidad)\n",
    "root_dir = Path(r\"C:/Users/aitorredondoruiz/Desktop/2B_energy/__Git/Lidar_My_validation_VLOS\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä PASO 2: VALIDACI√ìN DEL MODELO MLP\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURACI√ìN DE RUTAS\n",
    "# ============================================================================\n",
    "print(\"\\n[1/8] Configurando rutas...\")\n",
    "\n",
    "# Directorios\n",
    "data_ml_folder = root_dir / \"notebook\" / \"02_Data_ML_traditional\"\n",
    "models_folder = root_dir / \"notebook\" / \"03_ML_traditional_models\"\n",
    "scaler_folder = root_dir / \"notebook\" / \"01_Models_scaler\"\n",
    "mlp_folder = root_dir / \"notebook\" / \"03_ML_traditional_models\" / \"MLP\"\n",
    "\n",
    "# Crear carpeta espec√≠fica para MLP si no existe\n",
    "mlp_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"   ‚úÖ Carpeta de datos: {data_ml_folder}\")\n",
    "print(f\"   ‚úÖ Carpeta de scalers: {scaler_folder}\")\n",
    "print(f\"   ‚úÖ Carpeta de modelos: {models_folder}\")\n",
    "print(f\"   ‚úÖ Carpeta MLP: {mlp_folder}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CARGAR MODELO\n",
    "# ============================================================================\n",
    "print(\"\\n[2/8] Cargando modelo MLP entrenado...\")\n",
    "\n",
    "try:\n",
    "    model_path = mlp_folder / 'mlp_model.pkl'\n",
    "    mlp_model = joblib.load(model_path)\n",
    "    \n",
    "    print(f\"   ‚úÖ Modelo cargado desde: {model_path}\")\n",
    "    print(f\"   üìä Arquitectura: {mlp_model.hidden_layer_sizes}\")\n",
    "    print(f\"   üìä √âpocas entrenadas: {mlp_model.n_iter_}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error cargando modelo: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CARGAR DATOS DE TEST Y SCALERS\n",
    "# ============================================================================\n",
    "print(\"\\n[3/8] Cargando datos de test y scalers independientes...\")\n",
    "\n",
    "try:\n",
    "    # Cargar datos de test originales\n",
    "    X_test = joblib.load(data_ml_folder / 'X_test.pkl')\n",
    "    y_test = joblib.load(data_ml_folder / 'y_test.pkl')\n",
    "    \n",
    "    print(f\"   ‚úÖ X_test cargado: {X_test.shape}\")\n",
    "    print(f\"   ‚úÖ y_test cargado: {y_test.shape}\")\n",
    "    print(f\"   üí° Targets: {list(y_test.columns)}\")\n",
    "    \n",
    "    # Cargar scalers independientes (diccionarios)\n",
    "    scalers_X = joblib.load(scaler_folder / 'scalers_X.pkl')\n",
    "    scalers_y = joblib.load(scaler_folder / 'scalers_y.pkl')\n",
    "    \n",
    "    print(f\"   ‚úÖ Scalers independientes cargados:\")\n",
    "    print(f\"      ‚Ä¢ scalers_X: {len(scalers_X)} scalers (uno por feature)\")\n",
    "    print(f\"      ‚Ä¢ scalers_y: {len(scalers_y)} scalers (uno por target)\")\n",
    "    \n",
    "    # Normalizar X_test columna por columna usando scalers independientes\n",
    "    X_test_norm = pd.DataFrame(index=X_test.index, columns=X_test.columns)\n",
    "    \n",
    "    for col in X_test.columns:\n",
    "        if col in scalers_X:\n",
    "            X_test_norm[col] = scalers_X[col].transform(X_test[[col]])\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales\")\n",
    "            X_test_norm[col] = X_test[col]\n",
    "    \n",
    "    X_test_norm = X_test_norm.astype('float32')\n",
    "    \n",
    "    # Normalizar y_test columna por columna usando scalers independientes\n",
    "    y_test_norm = pd.DataFrame(index=y_test.index, columns=y_test.columns)\n",
    "    \n",
    "    for col in y_test.columns:\n",
    "        if col in scalers_y:\n",
    "            y_test_norm[col] = scalers_y[col].transform(y_test[[col]])\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales\")\n",
    "            y_test_norm[col] = y_test[col]\n",
    "    \n",
    "    print(f\"   ‚úÖ Datos de test normalizados columna por columna\")\n",
    "    \n",
    "    n_samples = len(X_test)\n",
    "    target_names = list(y_test.columns)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error cargando datos: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. PREDICCIONES EN TEST\n",
    "# ============================================================================\n",
    "print(\"\\n[4/8] Generando predicciones en test...\")\n",
    "\n",
    "try:\n",
    "    # Predicciones normalizadas\n",
    "    y_test_pred_norm = mlp_model.predict(X_test_norm)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    y_test_pred_norm_df = pd.DataFrame(\n",
    "        y_test_pred_norm,\n",
    "        columns=y_test_norm.columns,\n",
    "        index=y_test_norm.index\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Predicciones generadas (normalizadas): {y_test_pred_norm.shape}\")\n",
    "    \n",
    "    # Desnormalizar predicciones columna por columna usando scalers independientes\n",
    "    y_test_pred = pd.DataFrame(index=y_test_pred_norm_df.index, columns=y_test_pred_norm_df.columns)\n",
    "    \n",
    "    for col in y_test_pred_norm_df.columns:\n",
    "        if col in scalers_y:\n",
    "            y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm_df[[col]])\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores normalizados\")\n",
    "            y_test_pred[col] = y_test_pred_norm_df[col]\n",
    "    \n",
    "    print(f\"   ‚úÖ Predicciones desnormalizadas columna por columna\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error generando predicciones: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CALCULAR M√âTRICAS EN TEST\n",
    "# ============================================================================\n",
    "print(\"\\n[5/8] Calculando m√©tricas en test...\")\n",
    "\n",
    "try:\n",
    "    print(f\"\\n   üìä M√©tricas de TEST (datos normalizados):\")\n",
    "    print(f\"      {'Target':<30} {'MAE':>12} {'RMSE':>12} {'R¬≤':>10}\")\n",
    "    print(f\"      {'-'*30} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "    \n",
    "    metrics_test = {}\n",
    "    \n",
    "    for i, target_name in enumerate(target_names):\n",
    "        y_true = y_test_norm.iloc[:, i]\n",
    "        y_pred = y_test_pred_norm[:, i]\n",
    "        \n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        metrics_test[target_name] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2\n",
    "        }\n",
    "        \n",
    "        print(f\"      {target_name:<30} {mae:12.6f} {rmse:12.6f} {r2:10.6f}\")\n",
    "    \n",
    "    # M√©trica global\n",
    "    mae_global = mean_absolute_error(y_test_norm, y_test_pred_norm)\n",
    "    rmse_global = np.sqrt(mean_squared_error(y_test_norm, y_test_pred_norm))\n",
    "    r2_global = r2_score(y_test_norm, y_test_pred_norm)\n",
    "    \n",
    "    print(f\"      {'-'*30} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "    print(f\"      {'PROMEDIO GLOBAL':<30} {mae_global:12.6f} {rmse_global:12.6f} {r2_global:10.6f}\")\n",
    "    \n",
    "    # Guardar m√©tricas en CSV\n",
    "    metrics_df = pd.DataFrame(metrics_test).T\n",
    "    metrics_csv_path = mlp_folder / 'validation_summary.csv'\n",
    "    metrics_df.to_csv(metrics_csv_path)\n",
    "    print(f\"\\n   üíæ M√©tricas guardadas en: {metrics_csv_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error calculando m√©tricas: {e}\")\n",
    "    metrics_test = {}\n",
    "\n",
    "# ============================================================================\n",
    "# 6. GR√ÅFICO 1: PREDICCIONES VS REALES (SCATTER)\n",
    "# ============================================================================\n",
    "print(\"\\n[6/8] Generando gr√°fico de predicciones vs reales...\")\n",
    "\n",
    "try:\n",
    "    # Detectar n√∫mero de targets autom√°ticamente\n",
    "    n_targets = len(target_names)\n",
    "    \n",
    "    # Crear grid flexible: 1 fila si ‚â§3 targets, 2 filas si >3\n",
    "    if n_targets <= 3:\n",
    "        n_rows, n_cols = 1, n_targets\n",
    "        figsize = (6*n_targets, 6)\n",
    "    else:\n",
    "        n_rows = 2\n",
    "        n_cols = int(np.ceil(n_targets / 2))\n",
    "        figsize = (6*n_cols, 12)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    \n",
    "    # Asegurar que axes sea siempre iterable\n",
    "    if n_targets == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flat\n",
    "    \n",
    "    for i, (ax, target_name) in enumerate(zip(axes, target_names)):\n",
    "        y_true = y_test_norm.iloc[:, i].values\n",
    "        y_pred = y_test_pred_norm[:, i]\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_true, y_pred, alpha=0.3, s=1, label='Predicciones')\n",
    "        \n",
    "        # L√≠nea perfecta (y=x)\n",
    "        min_val = min(y_true.min(), y_pred.min())\n",
    "        max_val = max(y_true.max(), y_pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Predicci√≥n perfecta')\n",
    "        \n",
    "        # M√©tricas en el gr√°fico\n",
    "        mae = metrics_test[target_name]['MAE']\n",
    "        rmse = metrics_test[target_name]['RMSE']\n",
    "        r2 = metrics_test[target_name]['R2']\n",
    "        \n",
    "        textstr = f'MAE = {mae:.4f}\\nRMSE = {rmse:.4f}\\nR¬≤ = {r2:.4f}'\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=props)\n",
    "        \n",
    "        ax.set_xlabel('Valores Reales (Normalizados)', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Predicciones (Normalizadas)', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{target_name}\\nMLP - Test Set', fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    scatter_path = mlp_folder / 'predictions_vs_real.png'\n",
    "    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   ‚úÖ Gr√°fica guardada: {scatter_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error generando gr√°fico de scatter: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. GR√ÅFICO 2: AN√ÅLISIS DE RESIDUOS\n",
    "# ============================================================================\n",
    "print(\"\\n[7/8] Generando an√°lisis de residuos...\")\n",
    "\n",
    "try:\n",
    "    # Grid para residuos: n_targets filas √ó 2 columnas\n",
    "    fig, axes = plt.subplots(n_targets, 2, figsize=(16, 5*n_targets))\n",
    "    \n",
    "    # Asegurar que axes sea 2D\n",
    "    if n_targets == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, target_name in enumerate(target_names):\n",
    "        y_true = y_test_norm.iloc[:, i].values\n",
    "        y_pred = y_test_pred_norm[:, i]\n",
    "        \n",
    "        residuals = y_true - y_pred\n",
    "        \n",
    "        # Subplot 1: Residuos vs Predicciones\n",
    "        ax = axes[i, 0]\n",
    "        ax.scatter(y_pred, residuals, alpha=0.3, s=1, color='steelblue')\n",
    "        ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "        ax.set_xlabel('Predicciones', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Residuos', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{target_name}\\nResiduos vs Predicciones', fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 2: Histograma de residuos\n",
    "        ax = axes[i, 1]\n",
    "        ax.hist(residuals, bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "        ax.set_xlabel('Residuos', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Frecuencia', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{target_name}\\nDistribuci√≥n de Residuos', fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Estad√≠sticas de residuos\n",
    "        mean_res = np.mean(residuals)\n",
    "        std_res = np.std(residuals)\n",
    "        textstr = f'Media = {mean_res:.6f}\\nStd = {std_res:.6f}'\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "        ax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    residuals_path = mlp_folder / 'residuals_analysis.png'\n",
    "    plt.savefig(residuals_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   ‚úÖ Gr√°fica guardada: {residuals_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error generando an√°lisis de residuos: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. TABLA COMPARATIVA DE M√âTRICAS\n",
    "# ============================================================================\n",
    "print(\"\\n[8/8] Generando tabla comparativa de m√©tricas...\")\n",
    "\n",
    "try:\n",
    "    # Cargar informaci√≥n de entrenamiento\n",
    "    training_info_path = mlp_folder / 'mlp_training_info.pkl'\n",
    "    if training_info_path.exists():\n",
    "        training_info = joblib.load(training_info_path)\n",
    "        metrics_train = training_info.get('train_metrics', {})\n",
    "        \n",
    "        # Crear tabla comparativa\n",
    "        comparison_data = []\n",
    "        \n",
    "        for target_name in target_names:\n",
    "            if target_name in metrics_train and target_name in metrics_test:\n",
    "                comparison_data.append({\n",
    "                    'Target': target_name,\n",
    "                    'MAE_Train': metrics_train[target_name]['MAE'],\n",
    "                    'MAE_Test': metrics_test[target_name]['MAE'],\n",
    "                    'RMSE_Train': metrics_train[target_name]['RMSE'],\n",
    "                    'RMSE_Test': metrics_test[target_name]['RMSE'],\n",
    "                    'R2_Train': metrics_train[target_name]['R2'],\n",
    "                    'R2_Test': metrics_test[target_name]['R2'],\n",
    "                })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Guardar tabla\n",
    "        comparison_path = mlp_folder / 'metrics_comparison.csv'\n",
    "        comparison_df.to_csv(comparison_path, index=False)\n",
    "        \n",
    "        print(f\"\\n   üìä Comparaci√≥n Train vs Test:\")\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        print(f\"\\n   üíæ Tabla guardada en: {comparison_path}\")\n",
    "        \n",
    "        # Gr√°fico de comparaci√≥n\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        metrics_to_plot = ['MAE', 'RMSE', 'R2']\n",
    "        \n",
    "        for ax, metric in zip(axes, metrics_to_plot):\n",
    "            x = np.arange(len(target_names))\n",
    "            width = 0.35\n",
    "            \n",
    "            train_vals = [comparison_data[i][f'{metric}_Train'] for i in range(len(target_names))]\n",
    "            test_vals = [comparison_data[i][f'{metric}_Test'] for i in range(len(target_names))]\n",
    "            \n",
    "            ax.bar(x - width/2, train_vals, width, label='Train', color='steelblue', alpha=0.8)\n",
    "            ax.bar(x + width/2, test_vals, width, label='Test', color='orange', alpha=0.8)\n",
    "            \n",
    "            ax.set_xlabel('Target', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'{metric}: Train vs Test', fontsize=12, fontweight='bold')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(target_names, rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        comparison_plot_path = mlp_folder / 'metrics_comparison.png'\n",
    "        plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"   ‚úÖ Gr√°fica guardada: {comparison_plot_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  No se encontr√≥ informaci√≥n de entrenamiento\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error generando comparaci√≥n: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ VALIDACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESUMEN DE RESULTADOS:\")\n",
    "print(f\"   ‚Ä¢ Muestras de test: {n_samples:,}\")\n",
    "print(f\"   ‚Ä¢ Targets: {target_names}\")\n",
    "print(f\"   ‚Ä¢ MAE Global:  {mae_global:.6f}\")\n",
    "print(f\"   ‚Ä¢ RMSE Global: {rmse_global:.6f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Global:   {r2_global:.6f}\")\n",
    "\n",
    "print(f\"\\nüíæ ARCHIVOS GENERADOS EN: {mlp_folder}\")\n",
    "print(f\"   ‚Ä¢ validation_summary.csv\")\n",
    "print(f\"   ‚Ä¢ predictions_vs_real.png\")\n",
    "print(f\"   ‚Ä¢ residuals_analysis.png\")\n",
    "print(f\"   ‚Ä¢ metrics_comparison.csv\")\n",
    "print(f\"   ‚Ä¢ metrics_comparison.png\")\n",
    "\n",
    "print(f\"\\nüéØ SIGUIENTE PASO:\")\n",
    "print(f\"   Ejecutar: MLP_Validation_timeseries.py\")\n",
    "print(f\"   Para an√°lisis de series temporales\")\n",
    "\n",
    "print(f\"\\nüí° Scalers independientes utilizados correctamente\")\n",
    "print(f\"   ‚Ä¢ X: Normalizaci√≥n columna por columna ({len(scalers_X)} scalers)\")\n",
    "print(f\"   ‚Ä¢ y: Normalizaci√≥n columna por columna ({len(scalers_y)} scalers)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51605f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "PASO 3: AN√ÅLISIS DE SERIES TEMPORALES - MODELO MLP\n",
    "============================================================================\n",
    "\n",
    "Este script genera gr√°ficos de series temporales comparando las predicciones\n",
    "del modelo MLP con los valores reales en el conjunto de test.\n",
    "\n",
    "IMPORTANTE: Adaptado para scalers independientes por columna\n",
    "- scalers_X.pkl: Diccionario con un scaler por feature\n",
    "- scalers_y.pkl: Diccionario con un scaler por target\n",
    "\n",
    "Autor: Adaptado para scalers independientes\n",
    "Fecha: Enero 2026\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Configuraci√≥n de matplotlib\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURACI√ìN INICIAL\n",
    "# =============================================================================\n",
    "\n",
    "# Directorio ra√≠z del proyecto (ajustar seg√∫n necesidad)\n",
    "root_dir = Path(r\"C:/Users/aitorredondoruiz/Desktop/2B_energy/__Git/Lidar_My_validation_VLOS\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MLP - GR√ÅFICAS DE TIME SERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Cargar Time_test y generar series_id_test\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: PREPARAR DATOS PARA TIME SERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Directorios\n",
    "data_ml_folder = root_dir / \"notebook\" / \"02_Data_ML_traditional\"\n",
    "models_folder = root_dir / \"notebook\" / \"03_ML_traditional_models\"\n",
    "scaler_folder = root_dir / \"notebook\" / \"01_Models_scaler\"\n",
    "mlp_folder = models_folder / \"MLP\"\n",
    "\n",
    "print(f\"\\n[1/3] Cargando Time_test desde archivo pkl...\")\n",
    "\n",
    "Time_test_path = data_ml_folder / 'Time_test.pkl'\n",
    "\n",
    "if Time_test_path.exists():\n",
    "    Time_test = joblib.load(Time_test_path)\n",
    "    print(f\"   ‚úÖ Time_test cargado: {len(Time_test):,} valores\")\n",
    "    print(f\"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s\")\n",
    "else:\n",
    "    print(f\"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}\")\n",
    "    raise FileNotFoundError(f\"Archivo requerido no encontrado: {Time_test_path}\")\n",
    "\n",
    "print(f\"\\n[2/3] Generando series_id_test a partir de Time_test...\")\n",
    "\n",
    "# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\n",
    "series_id_test_values = np.zeros(len(Time_test), dtype=int)\n",
    "current_series = 0\n",
    "\n",
    "Time_test_array = Time_test.values\n",
    "for i in range(1, len(Time_test_array)):\n",
    "    if Time_test_array[i] < Time_test_array[i-1]:\n",
    "        current_series += 1\n",
    "    series_id_test_values[i] = current_series\n",
    "\n",
    "# Convertir a pandas Series con el mismo index que Time_test\n",
    "series_id_test = pd.Series(series_id_test_values, index=Time_test.index, name='series_id')\n",
    "\n",
    "n_test_series = series_id_test.max() + 1\n",
    "\n",
    "print(f\"   ‚úÖ Series temporales identificadas en test: {n_test_series}\")\n",
    "\n",
    "# Analizar cada serie\n",
    "print(f\"\\n   üìä Resumen de series en TEST:\")\n",
    "for sid in range(min(5, n_test_series)):\n",
    "    mask = series_id_test == sid\n",
    "    n_rows = mask.sum()\n",
    "    time_min = Time_test.loc[mask].min()\n",
    "    time_max = Time_test.loc[mask].max()\n",
    "    print(f\"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s\")\n",
    "\n",
    "if n_test_series > 5:\n",
    "    print(f\"      ... y {n_test_series - 5} series m√°s\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) Cargar modelo, datos y generar predicciones\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: CARGAR MODELO Y GENERAR PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/6] Cargando modelo entrenado...\")\n",
    "model_path = mlp_folder / 'mlp_model.pkl'\n",
    "mlp_model = joblib.load(model_path)\n",
    "print(f\"   ‚úÖ Modelo cargado desde: {model_path.name}\")\n",
    "\n",
    "print(f\"\\n[2/6] Cargando datos de test originales...\")\n",
    "X_test = joblib.load(data_ml_folder / 'X_test.pkl')\n",
    "y_test = joblib.load(data_ml_folder / 'y_test.pkl')\n",
    "print(f\"   ‚úÖ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚úÖ y_test: {y_test.shape}\")\n",
    "print(f\"   üí° Targets: {list(y_test.columns)}\")\n",
    "\n",
    "print(f\"\\n[3/6] Cargando scalers independientes...\")\n",
    "scalers_X = joblib.load(scaler_folder / 'scalers_X.pkl')\n",
    "scalers_y = joblib.load(scaler_folder / 'scalers_y.pkl')\n",
    "print(f\"   ‚úÖ Scalers independientes cargados:\")\n",
    "print(f\"      ‚Ä¢ scalers_X: {len(scalers_X)} scalers (uno por feature)\")\n",
    "print(f\"      ‚Ä¢ scalers_y: {len(scalers_y)} scalers (uno por target)\")\n",
    "\n",
    "print(f\"\\n[4/6] Normalizando datos de test con scalers independientes...\")\n",
    "# Normalizar columna por columna\n",
    "X_test_norm = pd.DataFrame(index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "for col in X_test.columns:\n",
    "    if col in scalers_X:\n",
    "        X_test_norm[col] = scalers_X[col].transform(X_test[[col]])\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales\")\n",
    "        X_test_norm[col] = X_test[col]\n",
    "\n",
    "X_test_norm = X_test_norm.astype('float32')\n",
    "print(f\"   ‚úÖ X_test normalizado columna por columna: {X_test_norm.shape}\")\n",
    "\n",
    "print(f\"\\n[5/6] Generando predicciones...\")\n",
    "y_test_pred_norm = mlp_model.predict(X_test_norm)\n",
    "y_test_pred_norm = pd.DataFrame(\n",
    "    y_test_pred_norm,\n",
    "    index=y_test.index,\n",
    "    columns=y_test.columns\n",
    ")\n",
    "print(f\"   ‚úÖ Predicciones generadas (normalizadas): {y_test_pred_norm.shape}\")\n",
    "\n",
    "print(f\"\\n[6/6] Desnormalizando predicciones con scalers independientes...\")\n",
    "# Desnormalizar columna por columna\n",
    "y_test_pred = pd.DataFrame(index=y_test_pred_norm.index, columns=y_test_pred_norm.columns)\n",
    "\n",
    "for col in y_test_pred_norm.columns:\n",
    "    if col in scalers_y:\n",
    "        y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm[[col]])\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores normalizados\")\n",
    "        y_test_pred[col] = y_test_pred_norm[col]\n",
    "\n",
    "print(f\"   ‚úÖ Predicciones desnormalizadas columna por columna: {y_test_pred.shape}\")\n",
    "\n",
    "print(f\"\\n[3/3] Verificando que los datos coinciden con predicciones...\")\n",
    "\n",
    "# Verificar que los √≠ndices coinciden\n",
    "if not all(series_id_test.index == y_test.index):\n",
    "    print(f\"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...\")\n",
    "    series_id_test = series_id_test.reindex(y_test.index)\n",
    "    Time_test = Time_test.reindex(y_test.index)\n",
    "\n",
    "print(f\"   ‚úÖ √çndices verificados:\")\n",
    "print(f\"      ‚Ä¢ y_test: {y_test.shape[0]:,} filas\")\n",
    "print(f\"      ‚Ä¢ y_test_pred: {y_test_pred.shape[0]:,} filas\")\n",
    "print(f\"      ‚Ä¢ series_id_test: {len(series_id_test):,} valores\")\n",
    "print(f\"      ‚Ä¢ Time_test: {len(Time_test):,} valores\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3) Gr√°fica de time series: Real vs Predicho (3 series aleatorias)\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 3: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/1] Creando gr√°ficas de time series (Real vs Predicho)...\")\n",
    "\n",
    "# Obtener series √∫nicas del conjunto de test\n",
    "unique_test_series = series_id_test.unique()\n",
    "\n",
    "# Seleccionar 3 series aleatorias\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "selected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n",
    "\n",
    "print(f\"   ‚Ä¢ Total series disponibles: {len(unique_test_series)}\")\n",
    "print(f\"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}\")\n",
    "\n",
    "# Detectar n√∫mero de targets autom√°ticamente\n",
    "n_targets = len(y_test.columns)\n",
    "print(f\"   ‚Ä¢ Targets a graficar: {list(y_test.columns)}\")\n",
    "\n",
    "# Crear figura con 3 filas x n_targets columnas (3 series, 3 targets)\n",
    "fig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n",
    "\n",
    "# Asegurar que axes sea 2D\n",
    "if n_targets == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Para cada target, graficar en subplot separado\n",
    "    for target_idx, col in enumerate(y_test.columns):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales\n",
    "        y_real = y_test.loc[series_indices, col]\n",
    "        # Valores predichos\n",
    "        y_pred = y_test_pred.loc[series_indices, col]\n",
    "        \n",
    "        # Calcular m√©tricas para esta serie y target\n",
    "        r2_series = r2_score(y_real, y_pred)\n",
    "        rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n",
    "        \n",
    "        # Graficar\n",
    "        ax.plot(time_series, y_real, label='Real', \n",
    "                linewidth=2, alpha=0.8, color='blue')\n",
    "        ax.plot(time_series, y_pred, label='Predicho', \n",
    "                linestyle='--', linewidth=2, alpha=0.8, color='red')\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Momento [kNm]', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "timeseries_plot_path = mlp_folder / 'timeseries_comparison.png'\n",
    "plt.savefig(timeseries_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4) Gr√°fica de time series con ZOOM (50 segundos)\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 4: GR√ÅFICAS DE TIME SERIES - ZOOM 50 SEGUNDOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/1] Creando gr√°ficas de time series con zoom (50s)...\")\n",
    "\n",
    "# Crear figura con 3 filas x n_targets columnas (3 series, 3 targets)\n",
    "fig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n",
    "\n",
    "# Asegurar que axes sea 2D\n",
    "if n_targets == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Definir ventana de 50 segundos desde el inicio\n",
    "    time_min = time_series.min()\n",
    "    time_max_zoom = time_min + 50\n",
    "    \n",
    "    # Filtrar por ventana de tiempo\n",
    "    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n",
    "    zoom_indices = time_series[zoom_mask].index\n",
    "    time_zoom = time_series[zoom_mask]\n",
    "    \n",
    "    # Para cada target, graficar en subplot separado\n",
    "    for target_idx, col in enumerate(y_test.columns):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales y predichos (con zoom)\n",
    "        y_real_zoom = y_test.loc[zoom_indices, col]\n",
    "        y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n",
    "        \n",
    "        # Calcular m√©tricas para esta ventana\n",
    "        r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n",
    "        rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n",
    "        \n",
    "        # Graficar\n",
    "        ax.plot(time_zoom, y_real_zoom, label='Real', \n",
    "                linewidth=2.5, alpha=0.8, color='blue', marker='o', markersize=4)\n",
    "        ax.plot(time_zoom, y_pred_zoom, label='Predicho', \n",
    "                linestyle='--', linewidth=2.5, alpha=0.8, color='red', marker='x', markersize=5)\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Momento [kNm]', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # A√±adir texto con informaci√≥n de puntos\n",
    "        n_points = len(zoom_indices)\n",
    "        ax.text(0.02, 0.02, f'Puntos: {n_points}', transform=ax.transAxes, fontsize=9,\n",
    "                verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "timeseries_zoom_plot_path = mlp_folder / 'timeseries_comparison_zoom50s.png'\n",
    "plt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# RESUMEN FINAL\n",
    "# ------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ GR√ÅFICAS DE TIME SERIES COMPLETADAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä GR√ÅFICAS GENERADAS:\")\n",
    "print(f\"   ‚Ä¢ {timeseries_plot_path.name}\")\n",
    "print(f\"   ‚Ä¢ {timeseries_zoom_plot_path.name}\")\n",
    "\n",
    "print(f\"\\nüí° Targets graficados: {list(y_test.columns)}\")\n",
    "print(f\"üí° Series aleatorias: {list(selected_series)}\")\n",
    "\n",
    "print(f\"\\nüí° Ahora tienes todas las gr√°ficas para MLP:\")\n",
    "print(f\"   1. M√©tricas por target (RMSE y R¬≤)\")\n",
    "print(f\"   2. Predicciones vs Real\")\n",
    "print(f\"   3. An√°lisis de residuos\")\n",
    "print(f\"   4. Time series completas (3 series aleatorias √ó {n_targets} targets)\")\n",
    "print(f\"   5. Time series con zoom de 50s\")\n",
    "\n",
    "print(f\"\\nüí° Scalers independientes utilizados correctamente\")\n",
    "print(f\"   ‚Ä¢ X: Normalizaci√≥n columna por columna ({len(scalers_X)} scalers)\")\n",
    "print(f\"   ‚Ä¢ y: Desnormalizaci√≥n columna por columna ({len(scalers_y)} scalers)\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7852726",
   "metadata": {},
   "source": [
    "## üîÑ ESTRUCTURA DEL PASO 5 (DIVIDIDO EN SUB-PASOS)\n",
    "\n",
    "### **PASO 5.1: VALIDACI√ìN EN CONJUNTO DE TEST** (Este script)\n",
    "Genera gr√°ficos individuales para cada modelo evaluando su desempe√±o en test:\n",
    "\n",
    "1. **Scatter Plots** (Predicciones vs Reales)\n",
    "   - Visualiza qu√© tan cerca est√°n las predicciones de los valores reales\n",
    "   - L√≠nea de predicci√≥n perfecta (y=x) como referencia\n",
    "   - Un gr√°fico por modelo con 2 subplots (uno por cada target/pala)\n",
    "   - M√©tricas: R¬≤, RMSE, MAE\n",
    "\n",
    "2. **Gr√°ficos de Residuos** (Predicho vs Residuo)\n",
    "   - Analiza la distribuci√≥n de errores del modelo\n",
    "   - Detecta patrones sistem√°ticos (sesgo, heterocedasticidad)\n",
    "   - L√≠neas de referencia: residuo=0 y ¬±2œÉ\n",
    "   - Idealmente: residuos centrados en 0 sin patrones\n",
    "\n",
    "3. **Histogramas de Residuos**\n",
    "   - Eval√∫a si los residuos siguen una distribuci√≥n normal\n",
    "   - Compara con curva normal te√≥rica\n",
    "   - Detecta asimetr√≠as o colas pesadas\n",
    "   - Validaci√≥n de supuestos del modelo\n",
    "\n",
    "---\n",
    "\n",
    "### **PASO 5.2: COMPARACI√ìN VISUAL CONJUNTA** (Pr√≥ximo script)\n",
    "Gr√°ficos que muestran todos los modelos juntos para comparaci√≥n directa:\n",
    "- Scatter plots lado a lado (grid de comparaci√≥n)\n",
    "- Gr√°ficos de barras comparando m√©tricas (RMSE, R¬≤, MAE)\n",
    "- Box plots de residuos por modelo\n",
    "- Ranking visual de modelos\n",
    "\n",
    "---\n",
    "\n",
    "### **PASO 5.3: COMPARACI√ìN EN SERIES TEMPORALES** (Pr√≥ximo script)\n",
    "An√°lisis del comportamiento temporal de cada modelo:\n",
    "- Time series: Real vs Predicciones de todos los modelos\n",
    "- Comparaci√≥n en ventanas de 50 segundos (zoom)\n",
    "- Identificaci√≥n de modelos m√°s estables temporalmente\n",
    "\n",
    "---\n",
    "\n",
    "### **PASO 5.4: COMPARACI√ìN CON BASELINE 1P** (Pr√≥ximo script)\n",
    "Evaluaci√≥n de todos los modelos vs baseline f√≠sico:\n",
    "- Comparaci√≥n de m√©tricas: Modelos ML vs Baseline 1P\n",
    "- An√°lisis de mejora porcentual respecto al baseline\n",
    "- Identificaci√≥n del modelo con mejor mejora\n",
    "\n",
    "---\n",
    "\n",
    "### **PASO 5.5: RESUMEN Y RECOMENDACIONES** (Pr√≥ximo script)\n",
    "Tabla final y recomendaciones:\n",
    "- Tabla comparativa consolidada con todas las m√©tricas\n",
    "- An√°lisis de trade-offs (precisi√≥n vs complejidad vs tiempo)\n",
    "- Recomendaci√≥n del mejor modelo seg√∫n caso de uso\n",
    "- Exportaci√≥n de resultados a CSV/Excel\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ ESTRUCTURA DE SALIDA\n",
    "\n",
    "Todos los gr√°ficos y resultados se almacenan en:\n",
    "```\n",
    "notebook/04_ML_all_models/\n",
    "‚îú‚îÄ‚îÄ scatter_ridge.png\n",
    "‚îú‚îÄ‚îÄ scatter_randomforest.png\n",
    "‚îú‚îÄ‚îÄ scatter_xgboost.png\n",
    "‚îú‚îÄ‚îÄ scatter_mlp.png\n",
    "‚îú‚îÄ‚îÄ residuals_ridge.png\n",
    "‚îú‚îÄ‚îÄ residuals_randomforest.png\n",
    "‚îú‚îÄ‚îÄ residuals_xgboost.png\n",
    "‚îú‚îÄ‚îÄ residuals_mlp.png\n",
    "‚îú‚îÄ‚îÄ histogram_residuals_ridge.png\n",
    "‚îú‚îÄ‚îÄ histogram_residuals_randomforest.png\n",
    "‚îú‚îÄ‚îÄ histogram_residuals_xgboost.png\n",
    "‚îú‚îÄ‚îÄ histogram_residuals_mlp.png\n",
    "‚îî‚îÄ‚îÄ (m√°s gr√°ficos en pasos siguientes...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### PASO 5.1: COMPARACI√ìN DE TODOS LOS MODELOS - VALIDACI√ìN EN TEST ####################\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Configuraci√≥n\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de matplotlib\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARACI√ìN DE TODOS LOS MODELOS - VALIDACI√ìN EN TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DETECTAR MODELOS DISPONIBLES\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: DETECTAR MODELOS DISPONIBLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Directorio ra√≠z del proyecto\n",
    "models_folder = root_dir / \"notebook\" / \"03_ML_traditional_models\"\n",
    "data_ml_folder = root_dir / \"notebook\" / \"02_Data_ML_traditional\"\n",
    "scaler_folder = root_dir / \"notebook\" / \"01_Models_scaler\"\n",
    "\n",
    "# Crear carpeta para comparaci√≥n de todos los modelos\n",
    "comparison_folder = root_dir / \"notebook\" / \"04_ML_all_models\"\n",
    "comparison_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n[1/3] Detectando carpetas de modelos en: {models_folder}\")\n",
    "\n",
    "# Detectar todas las carpetas (excluir archivos .pkl sueltos)\n",
    "model_folders = [f for f in models_folder.iterdir() if f.is_dir()]\n",
    "model_names = [f.name for f in model_folders]\n",
    "\n",
    "print(f\"   ‚úÖ Carpetas detectadas: {len(model_folders)}\")\n",
    "for name in model_names:\n",
    "    print(f\"      ‚Ä¢ {name}\")\n",
    "\n",
    "print(f\"\\n[2/3] Mapeando nombres de carpetas a archivos de modelos...\")\n",
    "\n",
    "# Mapeo de carpetas a archivos de modelo\n",
    "model_mapping = {\n",
    "    'Linear_Ridge': 'ridge_model.pkl',\n",
    "    'Random_Forest': 'random_forest_model.pkl',\n",
    "    'XGBoost': 'xgboost_model.pkl',\n",
    "    'MLP': 'mlp_model.pkl'\n",
    "}\n",
    "\n",
    "# Detectar qu√© modelos existen\n",
    "available_models = {}\n",
    "for folder_name, model_file in model_mapping.items():\n",
    "    model_path = models_folder / folder_name / model_file\n",
    "    if model_path.exists():\n",
    "        available_models[folder_name] = model_path\n",
    "        print(f\"   ‚úÖ {folder_name}: {model_file}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  {folder_name}: {model_file} NO encontrado\")\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    print(f\"\\n   ‚ùå ERROR: No se encontraron modelos entrenados\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\n[3/3] Resumen de modelos disponibles:\")\n",
    "print(f\"   ‚Ä¢ Total modelos a comparar: {len(available_models)}\")\n",
    "print(f\"   ‚Ä¢ Carpeta de resultados: {comparison_folder}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CARGAR DATOS DE TEST Y SCALERS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: CARGAR DATOS DE TEST Y SCALERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/3] Cargando datos de test originales...\")\n",
    "X_test = joblib.load(data_ml_folder / 'X_test.pkl')\n",
    "y_test = joblib.load(data_ml_folder / 'y_test.pkl')\n",
    "print(f\"   ‚úÖ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚úÖ y_test: {y_test.shape}\")\n",
    "print(f\"   ‚úÖ Targets: {list(y_test.columns)}\")\n",
    "\n",
    "print(f\"\\n[2/3] Cargando scalers...\")\n",
    "scaler_X = joblib.load(scaler_folder / 'scaler_X.pkl')\n",
    "scaler_y = joblib.load(scaler_folder / 'scaler_y.pkl')\n",
    "print(f\"   ‚úÖ Scalers cargados\")\n",
    "\n",
    "print(f\"\\n[3/3] Normalizando datos de test...\")\n",
    "X_test_norm = pd.DataFrame(\n",
    "    scaler_X.transform(X_test),\n",
    "    index=X_test.index,\n",
    "    columns=X_test.columns\n",
    ")\n",
    "print(f\"   ‚úÖ X_test normalizado: {X_test_norm.shape}\")\n",
    "\n",
    "target_names = y_test.columns\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CARGAR MODELOS Y GENERAR PREDICCIONES\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 3: CARGAR MODELOS Y GENERAR PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Diccionario para almacenar predicciones y m√©tricas\n",
    "predictions = {}\n",
    "metrics = {}\n",
    "\n",
    "for idx, (model_name, model_path) in enumerate(available_models.items(), 1):\n",
    "    print(f\"\\n[{idx}/{len(available_models)}] Procesando modelo: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar modelo\n",
    "        model = joblib.load(model_path)\n",
    "        print(f\"   ‚úÖ Modelo cargado desde: {model_path.name}\")\n",
    "        \n",
    "        # Generar predicciones normalizadas\n",
    "        y_test_pred_norm = model.predict(X_test_norm)\n",
    "        \n",
    "        # Convertir a DataFrame si es necesario\n",
    "        if not isinstance(y_test_pred_norm, pd.DataFrame):\n",
    "            y_test_pred_norm = pd.DataFrame(\n",
    "                y_test_pred_norm,\n",
    "                index=y_test.index,\n",
    "                columns=y_test.columns\n",
    "            )\n",
    "        \n",
    "        # Desnormalizar predicciones\n",
    "        y_test_pred = pd.DataFrame(\n",
    "            scaler_y.inverse_transform(y_test_pred_norm),\n",
    "            index=y_test_pred_norm.index,\n",
    "            columns=y_test_pred_norm.columns\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úÖ Predicciones generadas: {y_test_pred.shape}\")\n",
    "        \n",
    "        # Guardar predicciones\n",
    "        predictions[model_name] = y_test_pred\n",
    "        \n",
    "        # Calcular m√©tricas para cada target\n",
    "        model_metrics = {}\n",
    "        for col in target_names:\n",
    "            rmse = np.sqrt(mean_squared_error(y_test[col], y_test_pred[col]))\n",
    "            r2 = r2_score(y_test[col], y_test_pred[col])\n",
    "            mae = np.mean(np.abs(y_test[col] - y_test_pred[col]))\n",
    "            model_metrics[col] = {'RMSE': rmse, 'R2': r2, 'MAE': mae}\n",
    "            \n",
    "        metrics[model_name] = model_metrics\n",
    "        \n",
    "        # Mostrar m√©tricas\n",
    "        print(f\"   üìä M√©tricas en TEST:\")\n",
    "        for col in target_names:\n",
    "            print(f\"      {col}:\")\n",
    "            print(f\"         ‚Ä¢ RMSE: {model_metrics[col]['RMSE']:.2f}\")\n",
    "            print(f\"         ‚Ä¢ R¬≤:   {model_metrics[col]['R2']:.4f}\")\n",
    "            print(f\"         ‚Ä¢ MAE:  {model_metrics[col]['MAE']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error procesando {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Predicciones completadas para {len(predictions)} modelos\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINIR COLORES CONSISTENTES PARA CADA MODELO\n",
    "# ============================================================================\n",
    "# Cada modelo tiene un color √∫nico y bien diferenciado\n",
    "model_colors = {\n",
    "    'Ridge': '#3498db',           # Azul brillante\n",
    "    'RandomForest': \"#28df10\",    # Verde esmeralda\n",
    "    'XGBoost': '#eef11a',         # Naranja\n",
    "    'MLP': '#e74c3c'              # Rojo\n",
    "}\n",
    "\n",
    "# Asegurar que todos los modelos tienen un color asignado\n",
    "available_colors = ['#3498db', '#28df10', \"#eef11a\", '#e74c3c']\n",
    "color_idx = 0\n",
    "\n",
    "for model_name in predictions.keys():\n",
    "    if model_name not in model_colors:\n",
    "        # Asignar color √∫nico si no est√° en el diccionario\n",
    "        model_colors[model_name] = available_colors[color_idx % len(available_colors)]\n",
    "        color_idx += 1\n",
    "\n",
    "print(f\"\\nüìä Colores asignados a cada modelo (colores √∫nicos y diferenciados):\")\n",
    "for model_name, color in model_colors.items():\n",
    "    if model_name in predictions:\n",
    "        print(f\"   ‚Ä¢ {model_name}: {color}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. GR√ÅFICO 1: SCATTER PLOTS (Predicciones vs Reales)\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 4: SCATTER PLOTS - PREDICCIONES VS REALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando scatter plots para cada modelo...\")\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"\\n   [{model_name}] Generando scatter plot...\")\n",
    "    \n",
    "    # Crear figura con 1 fila x 2 columnas (2 targets)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    for idx, col in enumerate(target_names):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Valores reales vs predichos\n",
    "        y_real = y_test[col]\n",
    "        y_pred_col = y_pred[col]\n",
    "        \n",
    "        # M√©tricas\n",
    "        r2 = metrics[model_name][col]['R2']\n",
    "        rmse = metrics[model_name][col]['RMSE']\n",
    "        mae = metrics[model_name][col]['MAE']\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_real, y_pred_col, alpha=0.3, s=2, color=model_colors[model_name], edgecolors='none')\n",
    "        \n",
    "        # L√≠nea de predicci√≥n perfecta (y=x)\n",
    "        min_val = min(y_real.min(), y_pred_col.min())\n",
    "        max_val = max(y_real.max(), y_pred_col.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], \n",
    "                'r--', linewidth=2, alpha=0.7, label='Predicci√≥n perfecta (y=x)')\n",
    "        \n",
    "        # Configuraci√≥n\n",
    "        ax.set_xlabel('Valores Reales [kNm]', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Valores Predichos [kNm]', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{col}\\nR¬≤={r2:.4f} | RMSE={rmse:.2f} | MAE={mae:.2f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # T√≠tulo general\n",
    "    fig.suptitle(f'SCATTER PLOTS - {model_name.upper()}', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    scatter_path = comparison_folder / f'scatter_{model_name.lower()}.png'\n",
    "    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"      ‚úÖ Guardado: {scatter_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. GR√ÅFICO 2: RESIDUOS (Predicho vs Residuo)\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 5: GR√ÅFICOS DE RESIDUOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando gr√°ficos de residuos para cada modelo...\")\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"\\n   [{model_name}] Generando gr√°fico de residuos...\")\n",
    "    \n",
    "    # Crear figura con 1 fila x 2 columnas (2 targets)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    for idx, col in enumerate(target_names):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Calcular residuos\n",
    "        residuals = y_test[col] - y_pred[col]\n",
    "        \n",
    "        # Estad√≠sticas de residuos\n",
    "        mean_res = residuals.mean()\n",
    "        std_res = residuals.std()\n",
    "        \n",
    "        # Scatter plot de residuos\n",
    "        ax.scatter(y_pred[col], residuals, alpha=0.3, s=2, color=model_colors[model_name], edgecolors='none')\n",
    "        \n",
    "        # L√≠nea horizontal en y=0\n",
    "        ax.axhline(y=0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Residuo = 0')\n",
    "        \n",
    "        # L√≠neas de ¬±2œÉ\n",
    "        ax.axhline(y=2*std_res, color='gray', linestyle=':', linewidth=1.5, alpha=0.5, label=f'¬±2œÉ')\n",
    "        ax.axhline(y=-2*std_res, color='gray', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "        \n",
    "        # Configuraci√≥n\n",
    "        ax.set_xlabel('Valores Predichos [kNm]', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Residuos (Real - Predicho) [kNm]', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{col}\\nMedia={mean_res:.2f} | œÉ={std_res:.2f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # T√≠tulo general\n",
    "    fig.suptitle(f'AN√ÅLISIS DE RESIDUOS - {model_name.upper()}', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    residuals_path = comparison_folder / f'residuals_{model_name.lower()}.png'\n",
    "    plt.savefig(residuals_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"      ‚úÖ Guardado: {residuals_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. GR√ÅFICO 3: HISTOGRAMAS DE RESIDUOS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 6: HISTOGRAMAS DE RESIDUOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando histogramas de residuos para cada modelo...\")\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"\\n   [{model_name}] Generando histograma de residuos...\")\n",
    "    \n",
    "    # Crear figura con 1 fila x 2 columnas (2 targets)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    for idx, col in enumerate(target_names):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Calcular residuos\n",
    "        residuals = y_test[col] - y_pred[col]\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        mean_res = residuals.mean()\n",
    "        std_res = residuals.std()\n",
    "        \n",
    "        # Histograma\n",
    "        n, bins, patches = ax.hist(residuals, bins=50, alpha=0.7, color=model_colors[model_name], \n",
    "                                     edgecolor='black', density=True, label=model_name)\n",
    "        \n",
    "        # Curva normal te√≥rica\n",
    "        from scipy import stats\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        p = stats.norm.pdf(x, mean_res, std_res)\n",
    "        ax.plot(x, p, 'r-', linewidth=2, label=f'Normal(Œº={mean_res:.2f}, œÉ={std_res:.2f})')\n",
    "        \n",
    "        # L√≠nea vertical en la media\n",
    "        ax.axvline(mean_res, color='green', linestyle='--', linewidth=2, \n",
    "                   alpha=0.7, label=f'Media: {mean_res:.2f}')\n",
    "        \n",
    "        # Configuraci√≥n\n",
    "        ax.set_xlabel('Residuos (Real - Predicho) [kNm]', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Densidad', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{col}\\nŒº={mean_res:.2f} | œÉ={std_res:.2f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # T√≠tulo general\n",
    "    fig.suptitle(f'HISTOGRAMA DE RESIDUOS - {model_name.upper()}', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    hist_path = comparison_folder / f'histogram_residuals_{model_name.lower()}.png'\n",
    "    plt.savefig(hist_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"      ‚úÖ Guardado: {hist_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. GR√ÅFICO CONJUNTO 1: SCATTER PLOTS - TODOS LOS MODELOS JUNTOS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 7: SCATTER PLOTS CONJUNTOS - COMPARACI√ìN DIRECTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando scatter plots con todos los modelos en un solo gr√°fico...\")\n",
    "\n",
    "# Crear figura con 1 fila x 2 columnas (2 targets)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for idx, col in enumerate(target_names):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # L√≠nea de predicci√≥n perfecta (y=x) primero\n",
    "    all_real = y_test[col]\n",
    "    min_val = all_real.min()\n",
    "    max_val = all_real.max()\n",
    "    \n",
    "    # Extender un poco los l√≠mites para mejor visualizaci√≥n\n",
    "    for model_name, y_pred in predictions.items():\n",
    "        min_val = min(min_val, y_pred[col].min())\n",
    "        max_val = max(max_val, y_pred[col].max())\n",
    "    \n",
    "    ax.plot([min_val, max_val], [min_val, max_val], \n",
    "            'k--', linewidth=2.5, alpha=0.7, label='Predicci√≥n perfecta (y=x)', zorder=10)\n",
    "    \n",
    "    # Graficar cada modelo\n",
    "    for model_name, y_pred in predictions.items():\n",
    "        y_real = y_test[col]\n",
    "        y_pred_col = y_pred[col]\n",
    "        \n",
    "        # M√©tricas\n",
    "        r2 = metrics[model_name][col]['R2']\n",
    "        \n",
    "        # Scatter plot con transparencia\n",
    "        ax.scatter(y_real, y_pred_col, alpha=0.3, s=3, \n",
    "                  color=model_colors[model_name], edgecolors='none',\n",
    "                  label=f'{model_name} (R¬≤={r2:.4f})')\n",
    "    \n",
    "    # Configuraci√≥n\n",
    "    ax.set_xlabel('Valores Reales [kNm]', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Valores Predichos [kNm]', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{col}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# T√≠tulo general\n",
    "fig.suptitle('SCATTER PLOTS - COMPARACI√ìN DE TODOS LOS MODELOS', \n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "scatter_all_path = comparison_folder / 'scatter_all_models_combined.png'\n",
    "plt.savefig(scatter_all_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úÖ Guardado: {scatter_all_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. GR√ÅFICO CONJUNTO 2: RESIDUOS - TODOS LOS MODELOS JUNTOS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 8: GR√ÅFICOS DE RESIDUOS CONJUNTOS - COMPARACI√ìN DIRECTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando gr√°ficos de residuos con todos los modelos en un solo gr√°fico...\")\n",
    "\n",
    "# Crear figura con 1 fila x 2 columnas (2 targets)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for idx, col in enumerate(target_names):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # L√≠nea horizontal en y=0\n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=2.5, alpha=0.7, \n",
    "               label='Residuo = 0', zorder=10)\n",
    "    \n",
    "    # Graficar cada modelo\n",
    "    for model_name, y_pred in predictions.items():\n",
    "        # Calcular residuos\n",
    "        residuals = y_test[col] - y_pred[col]\n",
    "        \n",
    "        # Scatter plot con transparencia\n",
    "        ax.scatter(y_pred[col], residuals, alpha=0.3, s=3, \n",
    "                  color=model_colors[model_name], edgecolors='none',\n",
    "                  label=f'{model_name} (œÉ={residuals.std():.2f})')\n",
    "    \n",
    "    # Configuraci√≥n\n",
    "    ax.set_xlabel('Valores Predichos [kNm]', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Residuos (Real - Predicho) [kNm]', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{col}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# T√≠tulo general\n",
    "fig.suptitle('AN√ÅLISIS DE RESIDUOS - COMPARACI√ìN DE TODOS LOS MODELOS', \n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "residuals_all_path = comparison_folder / 'residuals_all_models_combined.png'\n",
    "plt.savefig(residuals_all_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úÖ Guardado: {residuals_all_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. GR√ÅFICO CONJUNTO 3: HISTOGRAMAS - TODOS LOS MODELOS JUNTOS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 9: HISTOGRAMAS DE RESIDUOS CONJUNTOS - COMPARACI√ìN DIRECTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando histogramas con todos los modelos en un solo gr√°fico...\")\n",
    "\n",
    "# Crear figura con 1 fila x 2 columnas (2 targets)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for idx, col in enumerate(target_names):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Graficar histograma de cada modelo\n",
    "    for model_name, y_pred in predictions.items():\n",
    "        # Calcular residuos\n",
    "        residuals = y_test[col] - y_pred[col]\n",
    "        \n",
    "        # Histograma con transparencia para que se vean todos\n",
    "        ax.hist(residuals, bins=50, alpha=0.5, color=model_colors[model_name], \n",
    "                edgecolor='black', density=True, label=f'{model_name} (Œº={residuals.mean():.2f})')\n",
    "    \n",
    "    # L√≠nea vertical en x=0\n",
    "    ax.axvline(0, color='black', linestyle='--', linewidth=2, \n",
    "               alpha=0.7, label='Media ideal = 0')\n",
    "    \n",
    "    # Configuraci√≥n\n",
    "    ax.set_xlabel('Residuos (Real - Predicho) [kNm]', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Densidad', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{col}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# T√≠tulo general\n",
    "fig.suptitle('HISTOGRAMAS DE RESIDUOS - COMPARACI√ìN DE TODOS LOS MODELOS', \n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "hist_all_path = comparison_folder / 'histogram_residuals_all_models_combined.png'\n",
    "plt.savefig(hist_all_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úÖ Guardado: {hist_all_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ VALIDACI√ìN DE TODOS LOS MODELOS COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä MODELOS COMPARADOS: {len(predictions)}\")\n",
    "for model_name in predictions.keys():\n",
    "    print(f\"   ‚Ä¢ {model_name}\")\n",
    "\n",
    "print(f\"\\nüìà RESUMEN DE M√âTRICAS (PROMEDIO POR MODELO):\")\n",
    "print(f\"\\n{'MODELO':20} {'RMSE PROMEDIO':>20} {'R¬≤ PROMEDIO':>15} {'MAE PROMEDIO':>15}\")\n",
    "print(f\"{'‚îÄ'*20} {'‚îÄ'*20} {'‚îÄ'*15} {'‚îÄ'*15}\")\n",
    "\n",
    "for model_name in sorted(predictions.keys()):\n",
    "    avg_rmse = np.mean([metrics[model_name][col]['RMSE'] for col in target_names])\n",
    "    avg_r2 = np.mean([metrics[model_name][col]['R2'] for col in target_names])\n",
    "    avg_mae = np.mean([metrics[model_name][col]['MAE'] for col in target_names])\n",
    "    print(f\"{model_name:20} {avg_rmse:>20.2f} {avg_r2:>15.4f} {avg_mae:>15.2f}\")\n",
    "\n",
    "print(f\"\\nüíæ GR√ÅFICOS GENERADOS EN: {comparison_folder}\")\n",
    "print(f\"   ‚Ä¢ Scatter plots individuales: {len(predictions)} archivos\")\n",
    "print(f\"   ‚Ä¢ Gr√°ficos de residuos individuales: {len(predictions)} archivos\")\n",
    "print(f\"   ‚Ä¢ Histogramas de residuos individuales: {len(predictions)} archivos\")\n",
    "print(f\"   ‚Ä¢ Scatter plot conjunto: 1 archivo\")\n",
    "print(f\"   ‚Ä¢ Gr√°fico de residuos conjunto: 1 archivo\")\n",
    "print(f\"   ‚Ä¢ Histograma conjunto: 1 archivo\")\n",
    "print(f\"   ‚Ä¢ TOTAL: {len(predictions) * 3 + 3} gr√°ficos\")\n",
    "\n",
    "print(f\"\\nüéØ SIGUIENTE PASO:\")\n",
    "print(f\"   Paso 5.2: Comparaci√≥n visual conjunta de todos los modelos\")\n",
    "print(f\"   (Scatter plots lado a lado, m√©tricas comparativas, etc.)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c86c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "PASO 5.2: COMPARACI√ìN DE TODOS LOS MODELOS - SERIES TEMPORALES\n",
    "============================================================================\n",
    "\n",
    "Este script compara el comportamiento temporal de TODOS los modelos de ML:\n",
    "- Ridge\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- MLP\n",
    "\n",
    "Genera gr√°ficos de series temporales mostrando:\n",
    "- Series completas: Real vs Predicciones de cada modelo\n",
    "- Zoom 50 segundos: Detalle de los primeros 50 segundos\n",
    "- Gr√°ficos individuales por modelo\n",
    "- Gr√°ficos conjuntos con todos los modelos superpuestos\n",
    "\n",
    "Autor: [Tu nombre]\n",
    "Fecha: 2026-01-08\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Configuraci√≥n\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de matplotlib\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARACI√ìN DE TODOS LOS MODELOS - SERIES TEMPORALES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURACI√ìN Y DETECCI√ìN DE MODELOS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: CONFIGURACI√ìN Y DETECCI√ìN DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Directorio ra√≠z del proyecto\n",
    "models_folder = root_dir / \"notebook\" / \"03_ML_traditional_models\"\n",
    "data_ml_folder = root_dir / \"notebook\" / \"02_Data_ML_traditional\"\n",
    "scaler_folder = root_dir / \"notebook\" / \"01_Models_scaler\"\n",
    "comparison_folder = root_dir / \"notebook\" / \"04_ML_all_models\"\n",
    "\n",
    "print(f\"\\n[1/2] Detectando modelos disponibles...\")\n",
    "\n",
    "# Mapeo de carpetas a archivos de modelo\n",
    "model_mapping = {\n",
    "    'Linear_Ridge': 'ridge_model.pkl',\n",
    "    'Random_Forest': 'random_forest_model.pkl',\n",
    "    'XGBoost': 'xgboost_model.pkl',\n",
    "    'MLP': 'mlp_model.pkl'\n",
    "}\n",
    "\n",
    "# Detectar qu√© modelos existen\n",
    "available_models = {}\n",
    "for folder_name, model_file in model_mapping.items():\n",
    "    model_path = models_folder / folder_name / model_file\n",
    "    if model_path.exists():\n",
    "        available_models[folder_name] = model_path\n",
    "        print(f\"   ‚úÖ {folder_name}: {model_file}\")\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    print(f\"\\n   ‚ùå ERROR: No se encontraron modelos entrenados\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\n[2/2] Definiendo colores √∫nicos para cada modelo...\")\n",
    "\n",
    "# Colores consistentes (iguales que en validaci√≥n)\n",
    "model_colors = {\n",
    "    'Linear_Ridge': '#3498db',           # Azul brillante\n",
    "    'Random_Forest': \"#28df10\",    # Verde esmeralda\n",
    "    'XGBoost': '#eef11a',         # Naranja\n",
    "    'MLP': '#e74c3c'              # Rojo\n",
    "}\n",
    "\n",
    "# Asegurar que todos los modelos tienen un color asignado\n",
    "available_colors = ['#3498db', '#28df10', \"#eef11a\", '#e74c3c']\n",
    "\n",
    "for model_name in available_models.keys():\n",
    "    print(f\"   ‚Ä¢ {model_name}: {model_colors[model_name]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CARGAR TIME_TEST Y GENERAR SERIES_ID_TEST\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: PREPARAR DATOS PARA TIME SERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/3] Cargando Time_test desde archivo pkl...\")\n",
    "\n",
    "Time_test_path = data_ml_folder / 'Time_test.pkl'\n",
    "\n",
    "if Time_test_path.exists():\n",
    "    Time_test = joblib.load(Time_test_path)\n",
    "    print(f\"   ‚úÖ Time_test cargado: {len(Time_test):,} valores\")\n",
    "    print(f\"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s\")\n",
    "else:\n",
    "    print(f\"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}\")\n",
    "    raise FileNotFoundError(f\"Archivo requerido no encontrado: {Time_test_path}\")\n",
    "\n",
    "print(f\"\\n[2/3] Generando series_id_test a partir de Time_test...\")\n",
    "\n",
    "# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\n",
    "series_id_test_values = np.zeros(len(Time_test), dtype=int)\n",
    "current_series = 0\n",
    "\n",
    "Time_test_array = Time_test.values\n",
    "for i in range(1, len(Time_test_array)):\n",
    "    if Time_test_array[i] < Time_test_array[i-1]:\n",
    "        current_series += 1\n",
    "    series_id_test_values[i] = current_series\n",
    "\n",
    "# Convertir a pandas Series con el mismo index que Time_test\n",
    "series_id_test = pd.Series(series_id_test_values, index=Time_test.index, name='series_id')\n",
    "\n",
    "n_test_series = series_id_test.max() + 1\n",
    "\n",
    "print(f\"   ‚úÖ Series temporales identificadas en test: {n_test_series}\")\n",
    "\n",
    "# Analizar cada serie\n",
    "print(f\"\\n   üìä Resumen de series en TEST:\")\n",
    "for sid in range(min(5, n_test_series)):\n",
    "    mask = series_id_test == sid\n",
    "    n_rows = mask.sum()\n",
    "    time_min = Time_test.loc[mask].min()\n",
    "    time_max = Time_test.loc[mask].max()\n",
    "    print(f\"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s\")\n",
    "\n",
    "if n_test_series > 5:\n",
    "    print(f\"      ... y {n_test_series - 5} series m√°s\")\n",
    "\n",
    "print(f\"\\n[3/3] Cargando datos de test...\")\n",
    "\n",
    "X_test = joblib.load(data_ml_folder / 'X_test.pkl')\n",
    "y_test = joblib.load(data_ml_folder / 'y_test.pkl')\n",
    "print(f\"   ‚úÖ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚úÖ y_test: {y_test.shape}\")\n",
    "\n",
    "# Verificar √≠ndices\n",
    "if not all(series_id_test.index == y_test.index):\n",
    "    print(f\"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...\")\n",
    "    series_id_test = series_id_test.reindex(y_test.index)\n",
    "    Time_test = Time_test.reindex(y_test.index)\n",
    "\n",
    "print(f\"   ‚úÖ √çndices verificados\")\n",
    "\n",
    "target_names = y_test.columns\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CARGAR SCALERS Y GENERAR PREDICCIONES PARA TODOS LOS MODELOS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 3: CARGAR MODELOS Y GENERAR PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Cargando scalers...\")\n",
    "scaler_X = joblib.load(scaler_folder / 'scaler_X.pkl')\n",
    "scaler_y = joblib.load(scaler_folder / 'scaler_y.pkl')\n",
    "print(f\"   ‚úÖ Scalers cargados\")\n",
    "\n",
    "print(f\"\\n[2/2] Normalizando datos de test...\")\n",
    "X_test_norm = pd.DataFrame(\n",
    "    scaler_X.transform(X_test),\n",
    "    index=X_test.index,\n",
    "    columns=X_test.columns\n",
    ")\n",
    "print(f\"   ‚úÖ X_test normalizado: {X_test_norm.shape}\")\n",
    "\n",
    "# Diccionario para almacenar predicciones\n",
    "predictions = {}\n",
    "\n",
    "for idx, (model_name, model_path) in enumerate(available_models.items(), 1):\n",
    "    print(f\"\\n   [{idx}/{len(available_models)}] Procesando modelo: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar modelo\n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        # Generar predicciones normalizadas\n",
    "        y_test_pred_norm = model.predict(X_test_norm)\n",
    "        \n",
    "        # Convertir a DataFrame si es necesario\n",
    "        if not isinstance(y_test_pred_norm, pd.DataFrame):\n",
    "            y_test_pred_norm = pd.DataFrame(\n",
    "                y_test_pred_norm,\n",
    "                index=y_test.index,\n",
    "                columns=y_test.columns\n",
    "            )\n",
    "        \n",
    "        # Desnormalizar predicciones\n",
    "        y_test_pred = pd.DataFrame(\n",
    "            scaler_y.inverse_transform(y_test_pred_norm),\n",
    "            index=y_test_pred_norm.index,\n",
    "            columns=y_test_pred_norm.columns\n",
    "        )\n",
    "        \n",
    "        predictions[model_name] = y_test_pred\n",
    "        print(f\"      ‚úÖ Predicciones generadas: {y_test_pred.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SELECCIONAR SERIES PARA VISUALIZAR\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 4: SELECCIONAR SERIES PARA VISUALIZAR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Obtener series √∫nicas del conjunto de test\n",
    "unique_test_series = series_id_test.unique()\n",
    "\n",
    "# Seleccionar 3 series aleatorias\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "selected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Total series disponibles: {len(unique_test_series)}\")\n",
    "print(f\"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. GR√ÅFICOS INDIVIDUALES: SERIES COMPLETAS POR MODELO\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 5: GR√ÅFICOS INDIVIDUALES - SERIES COMPLETAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"\\n   [{model_name}] Generando gr√°fico de series completas...\")\n",
    "    \n",
    "    # Crear figura con 3 filas x 2 columnas (3 series, 2 targets)\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "    \n",
    "    for plot_idx, series_num in enumerate(selected_series):\n",
    "        # Filtrar datos de esta serie\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        \n",
    "        # Obtener tiempo\n",
    "        time_series = Time_test.loc[series_indices]\n",
    "        \n",
    "        # Para cada target, graficar en subplot separado\n",
    "        for target_idx, col in enumerate(target_names):\n",
    "            ax = axes[plot_idx, target_idx]\n",
    "            \n",
    "            # Valores reales y predichos\n",
    "            y_real = y_test.loc[series_indices, col]\n",
    "            y_pred_col = y_pred.loc[series_indices, col]\n",
    "            \n",
    "            # Calcular m√©tricas para esta serie y target\n",
    "            r2_series = r2_score(y_real, y_pred_col)\n",
    "            rmse_series = np.sqrt(mean_squared_error(y_real, y_pred_col))\n",
    "            \n",
    "            # Graficar\n",
    "            ax.plot(time_series, y_real, label='Real', \n",
    "                    linewidth=2, alpha=0.8, color='blue')\n",
    "            ax.plot(time_series, y_pred_col, label=f'Predicho ({model_name})', \n",
    "                    linestyle='--', linewidth=2, alpha=0.8, color=model_colors[model_name])\n",
    "            \n",
    "            # Configurar subplot\n",
    "            ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "            ax.set_ylabel('Carga [kNm]', fontsize=10)\n",
    "            ax.set_title(f'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}', \n",
    "                         fontsize=11, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # T√≠tulo general\n",
    "    fig.suptitle(f'TIME SERIES COMPLETAS - {model_name.upper()}', \n",
    "                 fontsize=14, fontweight='bold', y=1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    timeseries_path = comparison_folder / f'timeseries_{model_name.lower()}.png'\n",
    "    plt.savefig(timeseries_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"      ‚úÖ Guardado: {timeseries_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. GR√ÅFICOS INDIVIDUALES: ZOOM 50 SEGUNDOS POR MODELO\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 6: GR√ÅFICOS INDIVIDUALES - ZOOM 50 SEGUNDOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"\\n   [{model_name}] Generando gr√°fico con zoom...\")\n",
    "    \n",
    "    # Crear figura con 3 filas x 2 columnas (3 series, 2 targets)\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "    \n",
    "    for plot_idx, series_num in enumerate(selected_series):\n",
    "        # Filtrar datos de esta serie\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        \n",
    "        # Obtener tiempo\n",
    "        time_series = Time_test.loc[series_indices]\n",
    "        \n",
    "        # Definir ventana de 50 segundos desde el inicio\n",
    "        time_min = time_series.min()\n",
    "        time_max_zoom = time_min + 50\n",
    "        \n",
    "        # Filtrar por ventana de tiempo\n",
    "        zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n",
    "        zoom_indices = time_series[zoom_mask].index\n",
    "        time_zoom = time_series[zoom_mask]\n",
    "        \n",
    "        # Para cada target, graficar en subplot separado\n",
    "        for target_idx, col in enumerate(target_names):\n",
    "            ax = axes[plot_idx, target_idx]\n",
    "            \n",
    "            # Valores reales y predichos (con zoom)\n",
    "            y_real_zoom = y_test.loc[zoom_indices, col]\n",
    "            y_pred_zoom = y_pred.loc[zoom_indices, col]\n",
    "            \n",
    "            # Calcular m√©tricas para esta ventana\n",
    "            r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n",
    "            rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n",
    "            \n",
    "            # Graficar\n",
    "            ax.plot(time_zoom, y_real_zoom, label='Real', \n",
    "                    linewidth=2.5, alpha=0.8, color='blue', marker='o', markersize=4)\n",
    "            ax.plot(time_zoom, y_pred_zoom, label=f'Predicho ({model_name})', \n",
    "                    linestyle='--', linewidth=2.5, alpha=0.8, color=model_colors[model_name], \n",
    "                    marker='x', markersize=5)\n",
    "            \n",
    "            # Configurar subplot\n",
    "            ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "            ax.set_ylabel('Carga [kNm]', fontsize=10)\n",
    "            ax.set_title(f'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}', \n",
    "                         fontsize=11, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # A√±adir texto con informaci√≥n de puntos\n",
    "            n_points = len(zoom_indices)\n",
    "            ax.text(0.02, 0.02, f'Puntos: {n_points}', transform=ax.transAxes, fontsize=9,\n",
    "                    verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # T√≠tulo general\n",
    "    fig.suptitle(f'TIME SERIES ZOOM 50s - {model_name.upper()}', \n",
    "                 fontsize=14, fontweight='bold', y=1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    timeseries_zoom_path = comparison_folder / f'timeseries_zoom50s_{model_name.lower()}.png'\n",
    "    plt.savefig(timeseries_zoom_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"      ‚úÖ Guardado: {timeseries_zoom_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. GR√ÅFICO CONJUNTO: SERIES COMPLETAS - TODOS LOS MODELOS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 7: GR√ÅFICO CONJUNTO - SERIES COMPLETAS CON TODOS LOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando gr√°fico con todos los modelos superpuestos...\")\n",
    "\n",
    "# Crear figura con 3 filas x 2 columnas (3 series, 2 targets)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Para cada target, graficar en subplot separado\n",
    "    for target_idx, col in enumerate(target_names):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales (solo una vez)\n",
    "        y_real = y_test.loc[series_indices, col]\n",
    "        ax.plot(time_series, y_real, label='Real', \n",
    "                linewidth=2.5, alpha=0.9, color='black', zorder=10)\n",
    "        \n",
    "        # Graficar predicciones de cada modelo\n",
    "        for model_name, y_pred in predictions.items():\n",
    "            y_pred_col = y_pred.loc[series_indices, col]\n",
    "            \n",
    "            # Calcular R¬≤ para la leyenda\n",
    "            r2_series = r2_score(y_real, y_pred_col)\n",
    "            \n",
    "            ax.plot(time_series, y_pred_col, \n",
    "                    label=f'{model_name} (R¬≤={r2_series:.3f})', \n",
    "                    linestyle='--', linewidth=2, alpha=0.7, \n",
    "                    color=model_colors[model_name])\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Carga [kNm]', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=8, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "# T√≠tulo general\n",
    "fig.suptitle('TIME SERIES COMPLETAS - COMPARACI√ìN DE TODOS LOS MODELOS', \n",
    "             fontsize=15, fontweight='bold', y=1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "timeseries_all_path = comparison_folder / 'timeseries_all_models_combined.png'\n",
    "plt.savefig(timeseries_all_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úÖ Guardado: {timeseries_all_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. GR√ÅFICO CONJUNTO: ZOOM 50s - TODOS LOS MODELOS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 8: GR√ÅFICO CONJUNTO - ZOOM 50s CON TODOS LOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando gr√°fico con zoom y todos los modelos superpuestos...\")\n",
    "\n",
    "# Crear figura con 3 filas x 2 columnas (3 series, 2 targets)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Definir ventana de 50 segundos desde el inicio\n",
    "    time_min = time_series.min()\n",
    "    time_max_zoom = time_min + 50\n",
    "    \n",
    "    # Filtrar por ventana de tiempo\n",
    "    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n",
    "    zoom_indices = time_series[zoom_mask].index\n",
    "    time_zoom = time_series[zoom_mask]\n",
    "    \n",
    "    # Para cada target, graficar en subplot separado\n",
    "    for target_idx, col in enumerate(target_names):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales (solo una vez)\n",
    "        y_real_zoom = y_test.loc[zoom_indices, col]\n",
    "        ax.plot(time_zoom, y_real_zoom, label='Real', \n",
    "                linewidth=2.5, alpha=0.9, color='black', \n",
    "                marker='o', markersize=3, zorder=10)\n",
    "        \n",
    "        # Graficar predicciones de cada modelo\n",
    "        for model_name, y_pred in predictions.items():\n",
    "            y_pred_zoom = y_pred.loc[zoom_indices, col]\n",
    "            \n",
    "            # Calcular R¬≤ para la leyenda\n",
    "            r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n",
    "            \n",
    "            ax.plot(time_zoom, y_pred_zoom, \n",
    "                    label=f'{model_name} (R¬≤={r2_zoom:.3f})', \n",
    "                    linestyle='--', linewidth=2.5, alpha=0.7, \n",
    "                    color=model_colors[model_name],\n",
    "                    marker='x', markersize=4)\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Carga [kNm]', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col} (Zoom: 0-50s)', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=8, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # A√±adir texto con informaci√≥n de puntos\n",
    "        n_points = len(zoom_indices)\n",
    "        ax.text(0.02, 0.02, f'Puntos: {n_points}', transform=ax.transAxes, fontsize=9,\n",
    "                verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# T√≠tulo general\n",
    "fig.suptitle('TIME SERIES ZOOM 50s - COMPARACI√ìN DE TODOS LOS MODELOS', \n",
    "             fontsize=15, fontweight='bold', y=1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "timeseries_zoom_all_path = comparison_folder / 'timeseries_zoom50s_all_models_combined.png'\n",
    "plt.savefig(timeseries_zoom_all_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úÖ Guardado: {timeseries_zoom_all_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ COMPARACI√ìN DE SERIES TEMPORALES COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä MODELOS COMPARADOS: {len(predictions)}\")\n",
    "for model_name in predictions.keys():\n",
    "    print(f\"   ‚Ä¢ {model_name}\")\n",
    "\n",
    "print(f\"\\nüíæ GR√ÅFICOS GENERADOS EN: {comparison_folder}\")\n",
    "print(f\"   ‚Ä¢ Time series individuales (completas): {len(predictions)} archivos\")\n",
    "print(f\"   ‚Ä¢ Time series individuales (zoom 50s): {len(predictions)} archivos\")\n",
    "print(f\"   ‚Ä¢ Time series conjunto (completas): 1 archivo\")\n",
    "print(f\"   ‚Ä¢ Time series conjunto (zoom 50s): 1 archivo\")\n",
    "print(f\"   ‚Ä¢ TOTAL: {len(predictions) * 2 + 2} gr√°ficos\")\n",
    "\n",
    "print(f\"\\nüéØ SIGUIENTE PASO:\")\n",
    "print(f\"   Paso 5.3: Comparaci√≥n con baseline 1P para todos los modelos\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "PASO 5.3: COMPARACI√ìN DE TODOS LOS MODELOS VS BASELINE 1P\n",
    "============================================================================\n",
    "\n",
    "Este script compara las predicciones de TODOS los modelos de ML con el \n",
    "baseline 1P (componente de frecuencia del rotor extra√≠da con filtro Butterworth):\n",
    "- Ridge\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- MLP\n",
    "\n",
    "Genera dos tipos de gr√°ficos:\n",
    "1. INDIVIDUALES: Cada modelo vs baseline (series completas y zoom)\n",
    "2. CONJUNTOS: Todos los modelos + baseline en el mismo gr√°fico\n",
    "\n",
    "Autor: [Tu nombre]\n",
    "Fecha: 2026-01-08\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "\n",
    "# Configuraci√≥n\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de matplotlib\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARACI√ìN DE TODOS LOS MODELOS VS BASELINE 1P\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURACI√ìN Y DETECCI√ìN DE MODELOS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 1: CONFIGURACI√ìN Y DETECCI√ìN DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Directorio ra√≠z del proyecto\n",
    "models_folder = root_dir / \"notebook\" / \"03_ML_traditional_models\"\n",
    "data_ml_folder = root_dir / \"notebook\" / \"02_Data_ML_traditional\"\n",
    "scaler_folder = root_dir / \"notebook\" / \"01_Models_scaler\"\n",
    "comparison_folder = root_dir / \"notebook\" / \"04_ML_all_models\"\n",
    "\n",
    "print(f\"\\n[1/2] Detectando modelos disponibles...\")\n",
    "\n",
    "# Mapeo de carpetas a archivos de modelo\n",
    "model_mapping = {\n",
    "    'Linear_Ridge': 'ridge_model.pkl',\n",
    "    'Random_Forest': 'random_forest_model.pkl',\n",
    "    'XGBoost': 'xgboost_model.pkl',\n",
    "    'MLP': 'mlp_model.pkl'\n",
    "}\n",
    "\n",
    "# Detectar qu√© modelos existen\n",
    "available_models = {}\n",
    "for folder_name, model_file in model_mapping.items():\n",
    "    model_path = models_folder / folder_name / model_file\n",
    "    if model_path.exists():\n",
    "        available_models[folder_name] = model_path\n",
    "        print(f\"   ‚úÖ {folder_name}: {model_file}\")\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    print(f\"\\n   ‚ùå ERROR: No se encontraron modelos entrenados\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\n[2/2] Definiendo colores √∫nicos para cada modelo...\")\n",
    "\n",
    "# Colores consistentes\n",
    "model_colors = {\n",
    "    'Linear_Ridge': '#3498db',           # Azul brillante\n",
    "    'Random_Forest': \"#28df10\",    # Verde esmeralda\n",
    "    'XGBoost': '#eef11a',         # Naranja\n",
    "    'MLP': '#e74c3c'    ,          # Rojo\n",
    "    'Baseline_1P': \"#EB0EBB\"\n",
    "}\n",
    "\n",
    "# Asegurar que todos los modelos tienen un color asignado\n",
    "available_colors = ['#3498db', '#28df10', \"#eef11a\", '#e74c3c',  \"#EB0EBB\"]\n",
    "\n",
    "for model_name in available_models.keys():\n",
    "    print(f\"   ‚Ä¢ {model_name}: {model_colors[model_name]}\")\n",
    "print(f\"   ‚Ä¢ Baseline 1P: {model_colors['Baseline_1P']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CARGAR DATOS Y GENERAR SERIES_ID_TEST\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 2: CARGAR DATOS Y GENERAR SERIES_ID_TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/4] Cargando Time_test...\")\n",
    "\n",
    "Time_test_path = data_ml_folder / 'Time_test.pkl'\n",
    "Time_test = joblib.load(Time_test_path)\n",
    "print(f\"   ‚úÖ Time_test cargado: {len(Time_test):,} valores\")\n",
    "print(f\"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s\")\n",
    "print(f\"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s\")\n",
    "\n",
    "print(f\"\\n[2/4] Generando series_id_test a partir de Time_test...\")\n",
    "\n",
    "# Detectar inicio de cada serie (cuando Time reinicia)\n",
    "series_id_test_values = np.zeros(len(Time_test), dtype=int)\n",
    "current_series = 0\n",
    "\n",
    "Time_test_array = Time_test.values\n",
    "for i in range(1, len(Time_test_array)):\n",
    "    if Time_test_array[i] < Time_test_array[i-1]:\n",
    "        current_series += 1\n",
    "    series_id_test_values[i] = current_series\n",
    "\n",
    "series_id_test = pd.Series(series_id_test_values, index=Time_test.index, name='series_id')\n",
    "n_test_series = series_id_test.max() + 1\n",
    "\n",
    "print(f\"   ‚úÖ Series temporales identificadas: {n_test_series}\")\n",
    "\n",
    "print(f\"\\n[3/4] Cargando datos de test...\")\n",
    "\n",
    "X_test = joblib.load(data_ml_folder / 'X_test.pkl')\n",
    "y_test = joblib.load(data_ml_folder / 'y_test.pkl')\n",
    "print(f\"   ‚úÖ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚úÖ y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\n[4/4] Verificando disponibilidad de Rotor speed...\")\n",
    "\n",
    "if 'Rotor speed' not in X_test.columns:\n",
    "    print(f\"   ‚ùå ERROR: 'Rotor speed' no est√° en las features\")\n",
    "    raise ValueError(\"Rotor speed no disponible\")\n",
    "\n",
    "rotor_speed_test = X_test['Rotor speed']\n",
    "print(f\"   ‚úÖ Rotor speed extra√≠do: {len(rotor_speed_test):,} valores\")\n",
    "print(f\"   ‚Ä¢ Velocidad m√≠nima: {rotor_speed_test.min():.3f} rad/s\")\n",
    "print(f\"   ‚Ä¢ Velocidad m√°xima: {rotor_speed_test.max():.3f} rad/s\")\n",
    "print(f\"   ‚Ä¢ Velocidad promedio: {rotor_speed_test.mean():.3f} rad/s\")\n",
    "\n",
    "# Verificar √≠ndices\n",
    "if not all(series_id_test.index == y_test.index):\n",
    "    series_id_test = series_id_test.reindex(y_test.index)\n",
    "    Time_test = Time_test.reindex(y_test.index)\n",
    "\n",
    "if not all(rotor_speed_test.index == y_test.index):\n",
    "    rotor_speed_test = rotor_speed_test.reindex(y_test.index)\n",
    "\n",
    "print(f\"   ‚úÖ √çndices verificados\")\n",
    "\n",
    "target_names = y_test.columns\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CARGAR MODELOS Y GENERAR PREDICCIONES\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 3: CARGAR MODELOS Y GENERAR PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[1/2] Cargando scalers...\")\n",
    "scaler_X = joblib.load(scaler_folder / 'scaler_X.pkl')\n",
    "scaler_y = joblib.load(scaler_folder / 'scaler_y.pkl')\n",
    "print(f\"   ‚úÖ Scalers cargados\")\n",
    "\n",
    "print(f\"\\n[2/2] Normalizando datos de test...\")\n",
    "X_test_norm = pd.DataFrame(\n",
    "    scaler_X.transform(X_test),\n",
    "    index=X_test.index,\n",
    "    columns=X_test.columns\n",
    ")\n",
    "print(f\"   ‚úÖ X_test normalizado: {X_test_norm.shape}\")\n",
    "\n",
    "# Diccionario para almacenar predicciones\n",
    "predictions = {}\n",
    "\n",
    "for idx, (model_name, model_path) in enumerate(available_models.items(), 1):\n",
    "    print(f\"\\n   [{idx}/{len(available_models)}] Procesando modelo: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar modelo\n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        # Generar predicciones normalizadas\n",
    "        y_test_pred_norm = model.predict(X_test_norm)\n",
    "        \n",
    "        # Convertir a DataFrame si es necesario\n",
    "        if not isinstance(y_test_pred_norm, pd.DataFrame):\n",
    "            y_test_pred_norm = pd.DataFrame(\n",
    "                y_test_pred_norm,\n",
    "                index=y_test.index,\n",
    "                columns=y_test.columns\n",
    "            )\n",
    "        \n",
    "        # Desnormalizar predicciones\n",
    "        y_test_pred = pd.DataFrame(\n",
    "            scaler_y.inverse_transform(y_test_pred_norm),\n",
    "            index=y_test_pred_norm.index,\n",
    "            columns=y_test_pred_norm.columns\n",
    "        )\n",
    "        \n",
    "        predictions[model_name] = y_test_pred\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        avg_rmse = np.mean([np.sqrt(mean_squared_error(y_test[col], y_test_pred[col])) \n",
    "                           for col in y_test.columns])\n",
    "        avg_r2 = np.mean([r2_score(y_test[col], y_test_pred[col]) \n",
    "                         for col in y_test.columns])\n",
    "        \n",
    "        print(f\"      ‚úÖ Predicciones generadas: {y_test_pred.shape}\")\n",
    "        print(f\"      ‚Ä¢ RMSE promedio: {avg_rmse:.2f}\")\n",
    "        print(f\"      ‚Ä¢ R¬≤ promedio: {avg_r2:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# ============================================================================\n",
    "# 4. EXTRAER COMPONENTE 1P USANDO FILTRO BUTTERWORTH\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 4: EXTRAER COMPONENTE 1P (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   üí° La se√±al baseline 1P es la componente de la se√±al real que oscila\")\n",
    "print(f\"      a la frecuencia instant√°nea del rotor (1P).\")\n",
    "print(f\"      Se extrae usando un filtro Butterworth pasa-bajos con fc = frecuencia 1P.\")\n",
    "\n",
    "print(f\"\\n[1/2] Extrayendo componente 1P para cada pala...\")\n",
    "\n",
    "# Frecuencia de muestreo\n",
    "fs = 20.0  # Hz\n",
    "\n",
    "# Inicializar diccionarios para almacenar baseline por pala\n",
    "baseline_1P_dict = {}\n",
    "unique_test_series = series_id_test.unique()\n",
    "\n",
    "# Extraer componente 1P para cada pala\n",
    "for col in y_test.columns:\n",
    "    baseline_series_dict = {}\n",
    "    \n",
    "    # Procesar cada serie por separado\n",
    "    for series_num in unique_test_series:\n",
    "        # Filtrar datos de esta serie\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        \n",
    "        # Se√±al real de esta serie y pala\n",
    "        signal_real = y_test.loc[series_indices, col].values\n",
    "        \n",
    "        # Velocidad angular de esta serie (rad/s)\n",
    "        omega_series = rotor_speed_test.loc[series_indices].values\n",
    "        \n",
    "        # Calcular frecuencia 1P promedio: freq_1P = omega / (2œÄ)\n",
    "        freq_1P_mean = omega_series.mean() / (2 * np.pi)\n",
    "        \n",
    "        # Dise√±ar filtro Butterworth pasa-bajos de orden 2\n",
    "        fc = freq_1P_mean  # Hz\n",
    "        sos = butter(2, fc, btype='lowpass', fs=fs, output='sos')\n",
    "        \n",
    "        # Aplicar filtro con fase cero\n",
    "        signal_1P = sosfiltfilt(sos, signal_real)\n",
    "        \n",
    "        # Guardar con mismo √≠ndice\n",
    "        baseline_series_dict[series_num] = pd.Series(signal_1P, index=series_indices)\n",
    "    \n",
    "    # Concatenar todas las series para esta pala\n",
    "    baseline_1P_dict[col] = pd.concat([baseline_series_dict[s] for s in unique_test_series]).sort_index()\n",
    "\n",
    "print(f\"   ‚úÖ Componente 1P extra√≠da para {len(y_test.columns)} palas y {len(unique_test_series)} series\")\n",
    "print(f\"   ‚Ä¢ M√©todo: Filtro Butterworth pasa-bajos (orden 2)\")\n",
    "print(f\"   ‚Ä¢ Frecuencia de corte: frecuencia 1P de cada serie\")\n",
    "print(f\"   ‚Ä¢ Frecuencia de muestreo: {fs:.0f} Hz\")\n",
    "\n",
    "print(f\"\\n[2/2] Calculando m√©tricas del baseline...\")\n",
    "\n",
    "# Calcular RMSE y R¬≤ del baseline para cada pala\n",
    "baseline_metrics = {}\n",
    "\n",
    "for col in y_test.columns:\n",
    "    baseline_signal = baseline_1P_dict[col]\n",
    "    rmse_baseline = np.sqrt(mean_squared_error(y_test[col], baseline_signal))\n",
    "    r2_baseline = r2_score(y_test[col], baseline_signal)\n",
    "    baseline_metrics[col] = {'RMSE': rmse_baseline, 'R2': r2_baseline}\n",
    "\n",
    "avg_rmse_baseline = np.mean([baseline_metrics[col]['RMSE'] for col in y_test.columns])\n",
    "avg_r2_baseline = np.mean([baseline_metrics[col]['R2'] for col in y_test.columns])\n",
    "\n",
    "print(f\"   ‚úÖ Baseline 1P:\")\n",
    "print(f\"      ‚Ä¢ RMSE promedio: {avg_rmse_baseline:.2f}\")\n",
    "print(f\"      ‚Ä¢ R¬≤ promedio: {avg_r2_baseline:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SELECCIONAR SERIES PARA VISUALIZAR\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 5: SELECCIONAR SERIES PARA VISUALIZAR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Seleccionar 3 series aleatorias\n",
    "np.random.seed(42) #42\n",
    "selected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Total series disponibles: {len(unique_test_series)}\")\n",
    "print(f\"   ‚Ä¢ Series seleccionadas: {selected_series}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. GR√ÅFICOS INDIVIDUALES: CADA MODELO VS BASELINE 1P (SERIES COMPLETAS)\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 6: GR√ÅFICOS INDIVIDUALES - MODELO VS BASELINE (SERIES COMPLETAS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"\\n   [{model_name}] Generando gr√°fico vs baseline...\")\n",
    "    \n",
    "    # Crear figura con 3 filas x 2 columnas\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "    \n",
    "    for plot_idx, series_num in enumerate(selected_series):\n",
    "        # Filtrar datos de esta serie\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        \n",
    "        # Obtener tiempo\n",
    "        time_series = Time_test.loc[series_indices]\n",
    "        \n",
    "        # Para cada target\n",
    "        for target_idx, col in enumerate(target_names):\n",
    "            ax = axes[plot_idx, target_idx]\n",
    "            \n",
    "            # Valores predichos y baseline\n",
    "            y_real = y_test.loc[series_indices, col]\n",
    "            y_pred_col = y_pred.loc[series_indices, col]\n",
    "            y_baseline = baseline_1P_dict[col].loc[series_indices]\n",
    "            \n",
    "            # Calcular m√©tricas\n",
    "            r2_pred = r2_score(y_real, y_pred_col)\n",
    "            rmse_pred = np.sqrt(mean_squared_error(y_real, y_pred_col))\n",
    "            r2_base = r2_score(y_real, y_baseline)\n",
    "            rmse_base = np.sqrt(mean_squared_error(y_real, y_baseline))\n",
    "            \n",
    "            # Graficar (SIN l√≠nea Real)\n",
    "            ax.plot(time_series, y_pred_col, label=f'Predicho ({model_name})', \n",
    "                    linestyle='--', linewidth=2.5, alpha=0.9, color=model_colors[model_name])\n",
    "            ax.plot(time_series, y_baseline, label='Baseline 1P', \n",
    "                    linestyle='-', linewidth=2, alpha=0.8, color=model_colors['Baseline_1P'])\n",
    "            \n",
    "            # Configurar subplot\n",
    "            ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "            ax.set_ylabel('Carga [kNm]', fontsize=10)\n",
    "            ax.set_title(f'Serie {series_num} - {col}\\n' + \n",
    "                         f'{model_name}: R¬≤={r2_pred:.4f}, RMSE={rmse_pred:.1f} | ' +\n",
    "                         f'Baseline: R¬≤={r2_base:.4f}, RMSE={rmse_base:.1f}', \n",
    "                         fontsize=10, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # T√≠tulo general\n",
    "    fig.suptitle(f'{model_name.upper()} VS BASELINE 1P - SERIES COMPLETAS', \n",
    "                 fontsize=14, fontweight='bold', y=1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    timeseries_baseline_path = comparison_folder / f'timeseries_1P_vs_{model_name.lower()}.png'\n",
    "    plt.savefig(timeseries_baseline_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"      ‚úÖ Guardado: {timeseries_baseline_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. GR√ÅFICOS INDIVIDUALES: CADA MODELO VS BASELINE 1P (ZOOM 50s)\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 7: GR√ÅFICOS INDIVIDUALES - MODELO VS BASELINE (ZOOM 50s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"\\n   [{model_name}] Generando gr√°fico con zoom vs baseline...\")\n",
    "    \n",
    "    # Crear figura con 3 filas x 2 columnas\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "    \n",
    "    for plot_idx, series_num in enumerate(selected_series):\n",
    "        # Filtrar datos de esta serie\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        \n",
    "        # Obtener tiempo\n",
    "        time_series = Time_test.loc[series_indices]\n",
    "        \n",
    "        # Definir ventana de 50 segundos\n",
    "        time_min = time_series.min()\n",
    "        time_max_zoom = time_min + 50\n",
    "        \n",
    "        # Filtrar por ventana de tiempo\n",
    "        zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n",
    "        zoom_indices = time_series[zoom_mask].index\n",
    "        time_zoom = time_series[zoom_mask]\n",
    "        \n",
    "        # Para cada target\n",
    "        for target_idx, col in enumerate(target_names):\n",
    "            ax = axes[plot_idx, target_idx]\n",
    "            \n",
    "            # Valores con zoom\n",
    "            y_real_zoom = y_test.loc[zoom_indices, col]\n",
    "            y_pred_zoom = y_pred.loc[zoom_indices, col]\n",
    "            y_baseline_zoom = baseline_1P_dict[col].loc[zoom_indices]\n",
    "            \n",
    "            # Calcular m√©tricas\n",
    "            r2_pred_zoom = r2_score(y_real_zoom, y_pred_zoom)\n",
    "            rmse_pred_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n",
    "            r2_base_zoom = r2_score(y_real_zoom, y_baseline_zoom)\n",
    "            rmse_base_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_baseline_zoom))\n",
    "            \n",
    "            # Graficar (SIN l√≠nea Real)\n",
    "            ax.plot(time_zoom, y_pred_zoom, label=f'Predicho ({model_name})', \n",
    "                    linestyle='--', linewidth=2.5, alpha=0.9, color=model_colors[model_name],\n",
    "                    marker='x', markersize=4)\n",
    "            ax.plot(time_zoom, y_baseline_zoom, label='Baseline 1P', \n",
    "                    linestyle='-', linewidth=2.5, alpha=0.8, color=model_colors['Baseline_1P'],\n",
    "                    marker='o', markersize=3)\n",
    "            \n",
    "            # Configurar subplot\n",
    "            ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "            ax.set_ylabel('Carga [kNm]', fontsize=10)\n",
    "            ax.set_title(f'Serie {series_num} - {col} (Zoom: 0-50s)\\n' + \n",
    "                         f'{model_name}: R¬≤={r2_pred_zoom:.4f} | Baseline: R¬≤={r2_base_zoom:.4f}', \n",
    "                         fontsize=10, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # A√±adir texto con informaci√≥n de puntos\n",
    "            n_points = len(zoom_indices)\n",
    "            ax.text(0.02, 0.02, f'Puntos: {n_points}', transform=ax.transAxes, fontsize=9,\n",
    "                    verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # T√≠tulo general\n",
    "    fig.suptitle(f'{model_name.upper()} VS BASELINE 1P - ZOOM 50s', \n",
    "                 fontsize=14, fontweight='bold', y=1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    timeseries_zoom_baseline_path = comparison_folder / f'timeseries_1P_vs_{model_name.lower()}_zoom50s.png'\n",
    "    plt.savefig(timeseries_zoom_baseline_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"      ‚úÖ Guardado: {timeseries_zoom_baseline_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. GR√ÅFICO CONJUNTO: TODOS LOS MODELOS VS BASELINE 1P (SERIES COMPLETAS)\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 8: GR√ÅFICO CONJUNTO - TODOS LOS MODELOS VS BASELINE (SERIES COMPLETAS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando gr√°fico con todos los modelos + baseline...\")\n",
    "\n",
    "# Crear figura con 3 filas x 2 columnas\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Para cada target\n",
    "    for target_idx, col in enumerate(target_names):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales (para c√°lculo de m√©tricas)\n",
    "        y_real = y_test.loc[series_indices, col]\n",
    "        \n",
    "        # Baseline 1P\n",
    "        y_baseline = baseline_1P_dict[col].loc[series_indices]\n",
    "        r2_base = r2_score(y_real, y_baseline)\n",
    "        ax.plot(time_series, y_baseline, \n",
    "                label=f'Baseline 1P (R¬≤={r2_base:.3f})', \n",
    "                linestyle='-', linewidth=2.5, alpha=0.9, \n",
    "                color=model_colors['Baseline_1P'], zorder=10)\n",
    "        \n",
    "        # Graficar predicciones de cada modelo\n",
    "        for model_name, y_pred in predictions.items():\n",
    "            y_pred_col = y_pred.loc[series_indices, col]\n",
    "            r2_pred = r2_score(y_real, y_pred_col)\n",
    "            \n",
    "            ax.plot(time_series, y_pred_col, \n",
    "                    label=f'{model_name} (R¬≤={r2_pred:.3f})', \n",
    "                    linestyle='--', linewidth=2, alpha=0.7, \n",
    "                    color=model_colors[model_name])\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Carga [kNm]', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=8, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "# T√≠tulo general\n",
    "fig.suptitle('TODOS LOS MODELOS VS BASELINE 1P - SERIES COMPLETAS', \n",
    "             fontsize=15, fontweight='bold', y=1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "timeseries_all_baseline_path = comparison_folder / 'timeseries_1P_all_models_combined.png'\n",
    "plt.savefig(timeseries_all_baseline_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úÖ Guardado: {timeseries_all_baseline_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. GR√ÅFICO CONJUNTO: TODOS LOS MODELOS VS BASELINE 1P (ZOOM 50s)\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 9: GR√ÅFICO CONJUNTO - TODOS LOS MODELOS VS BASELINE (ZOOM 50s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando gr√°fico con zoom y todos los modelos + baseline...\")\n",
    "\n",
    "# Crear figura con 3 filas x 2 columnas\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Obtener tiempo\n",
    "    time_series = Time_test.loc[series_indices]\n",
    "    \n",
    "    # Definir ventana de 50 segundos\n",
    "    time_min = time_series.min()\n",
    "    time_max_zoom = time_min + 50\n",
    "    \n",
    "    # Filtrar por ventana de tiempo\n",
    "    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n",
    "    zoom_indices = time_series[zoom_mask].index\n",
    "    time_zoom = time_series[zoom_mask]\n",
    "    \n",
    "    # Para cada target\n",
    "    for target_idx, col in enumerate(target_names):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Valores reales (para c√°lculo de m√©tricas)\n",
    "        y_real_zoom = y_test.loc[zoom_indices, col]\n",
    "        \n",
    "        # Baseline 1P\n",
    "        y_baseline_zoom = baseline_1P_dict[col].loc[zoom_indices]\n",
    "        r2_base_zoom = r2_score(y_real_zoom, y_baseline_zoom)\n",
    "        ax.plot(time_zoom, y_baseline_zoom, \n",
    "                label=f'Baseline 1P (R¬≤={r2_base_zoom:.3f})', \n",
    "                linestyle='-', linewidth=2.5, alpha=0.9, \n",
    "                color=model_colors['Baseline_1P'],\n",
    "                marker='o', markersize=3, zorder=10)\n",
    "        \n",
    "        # Graficar predicciones de cada modelo\n",
    "        for model_name, y_pred in predictions.items():\n",
    "            y_pred_zoom = y_pred.loc[zoom_indices, col]\n",
    "            r2_pred_zoom = r2_score(y_real_zoom, y_pred_zoom)\n",
    "            \n",
    "            ax.plot(time_zoom, y_pred_zoom, \n",
    "                    label=f'{model_name} (R¬≤={r2_pred_zoom:.3f})', \n",
    "                    linestyle='--', linewidth=2.5, alpha=0.7, \n",
    "                    color=model_colors[model_name],\n",
    "                    marker='x', markersize=4)\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Tiempo [s]', fontsize=10)\n",
    "        ax.set_ylabel('Carga [kNm]', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col} (Zoom: 0-50s)', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=8, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # A√±adir texto con informaci√≥n de puntos\n",
    "        n_points = len(zoom_indices)\n",
    "        ax.text(0.02, 0.02, f'Puntos: {n_points}', transform=ax.transAxes, fontsize=9,\n",
    "                verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# T√≠tulo general\n",
    "fig.suptitle('TODOS LOS MODELOS VS BASELINE 1P - ZOOM 50s', \n",
    "             fontsize=15, fontweight='bold', y=1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "timeseries_zoom_all_baseline_path = comparison_folder / 'timeseries_1P_all_models_zoom50s_combined.png'\n",
    "plt.savefig(timeseries_zoom_all_baseline_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úÖ Guardado: {timeseries_zoom_all_baseline_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. GR√ÅFICOS SCATTER INDIVIDUALES: BASELINE 1P (X) VS PREDICCIONES (Y)\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 10: SCATTER PLOTS INDIVIDUALES - BASELINE 1P VS PREDICCIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"\\n   [{model_name}] Generando scatter plot vs baseline 1P...\")\n",
    "    \n",
    "    # Crear figura con 3 filas x 2 columnas (3 series, 2 targets)\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "    \n",
    "    for plot_idx, series_num in enumerate(selected_series):\n",
    "        # Filtrar datos de esta serie\n",
    "        series_mask = series_id_test == series_num\n",
    "        series_indices = series_mask[series_mask].index\n",
    "        \n",
    "        # Para cada target\n",
    "        for target_idx, col in enumerate(target_names):\n",
    "            ax = axes[plot_idx, target_idx]\n",
    "            \n",
    "            # Baseline 1P (eje X) y Predicciones (eje Y)\n",
    "            baseline_vals = baseline_1P_dict[col].loc[series_indices]\n",
    "            pred_vals = y_pred.loc[series_indices, col]\n",
    "            \n",
    "            # Calcular m√©tricas de correlaci√≥n\n",
    "            r2_corr = r2_score(baseline_vals, pred_vals)\n",
    "            rmse_corr = np.sqrt(mean_squared_error(baseline_vals, pred_vals))\n",
    "            correlation = np.corrcoef(baseline_vals, pred_vals)[0, 1]\n",
    "            \n",
    "            # Scatter plot\n",
    "            ax.scatter(baseline_vals, pred_vals, alpha=0.5, s=15, \n",
    "                      color=model_colors[model_name], edgecolors='none')\n",
    "            \n",
    "            # L√≠nea de referencia (y=x) en negro\n",
    "            min_val = min(baseline_vals.min(), pred_vals.min())\n",
    "            max_val = max(baseline_vals.max(), pred_vals.max())\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "                   linewidth=2, alpha=0.7, label='y=x (ideal)')\n",
    "            \n",
    "            # Configurar subplot\n",
    "            ax.set_xlabel('Baseline 1P [kNm]', fontsize=10)\n",
    "            ax.set_ylabel(f'Predicciones {model_name} [kNm]', fontsize=10)\n",
    "            ax.set_title(f'Serie {series_num} - {col}\\n' + \n",
    "                        f'Correlaci√≥n: {correlation:.4f} | R¬≤={r2_corr:.4f} | RMSE={rmse_corr:.1f}', \n",
    "                        fontsize=10, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # T√≠tulo general\n",
    "    fig.suptitle(f'SCATTER: BASELINE 1P VS {model_name.upper()}', \n",
    "                 fontsize=14, fontweight='bold', y=1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    scatter_path = comparison_folder / f'scatter_1P_vs_{model_name.lower()}.png'\n",
    "    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"      ‚úÖ Guardado: {scatter_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 11. GR√ÅFICO SCATTER CONJUNTO: BASELINE 1P (X) VS TODAS LAS PREDICCIONES (Y)\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PASO 11: SCATTER PLOT CONJUNTO - BASELINE 1P VS TODOS LOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n   Creando scatter plot con todos los modelos vs baseline 1P...\")\n",
    "\n",
    "# Crear figura con 3 filas x 2 columnas (3 series, 2 targets)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "\n",
    "for plot_idx, series_num in enumerate(selected_series):\n",
    "    # Filtrar datos de esta serie\n",
    "    series_mask = series_id_test == series_num\n",
    "    series_indices = series_mask[series_mask].index\n",
    "    \n",
    "    # Para cada target\n",
    "    for target_idx, col in enumerate(target_names):\n",
    "        ax = axes[plot_idx, target_idx]\n",
    "        \n",
    "        # Baseline 1P (eje X - igual para todos los modelos)\n",
    "        baseline_vals = baseline_1P_dict[col].loc[series_indices]\n",
    "        \n",
    "        # L√≠nea de referencia (y=x) primero\n",
    "        min_val = baseline_vals.min()\n",
    "        max_val = baseline_vals.max()\n",
    "        \n",
    "        # Graficar predicciones de cada modelo\n",
    "        for model_name, y_pred in predictions.items():\n",
    "            pred_vals = y_pred.loc[series_indices, col]\n",
    "            \n",
    "            # Actualizar rango para l√≠nea de referencia\n",
    "            min_val = min(min_val, pred_vals.min())\n",
    "            max_val = max(max_val, pred_vals.max())\n",
    "            \n",
    "            # Calcular correlaci√≥n\n",
    "            correlation = np.corrcoef(baseline_vals, pred_vals)[0, 1]\n",
    "            \n",
    "            # Scatter plot\n",
    "            ax.scatter(baseline_vals, pred_vals, alpha=0.6, s=12, \n",
    "                      color=model_colors[model_name], edgecolors='none',\n",
    "                      label=f'{model_name} (œÅ={correlation:.3f})')\n",
    "        \n",
    "        # L√≠nea de referencia (y=x)\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "               linewidth=2.5, alpha=0.8, label='y=x (ideal)', zorder=10)\n",
    "        \n",
    "        # Configurar subplot\n",
    "        ax.set_xlabel('Baseline 1P [kNm]', fontsize=10)\n",
    "        ax.set_ylabel('Predicciones de modelos [kNm]', fontsize=10)\n",
    "        ax.set_title(f'Serie {series_num} - {col}', \n",
    "                    fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=8, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# T√≠tulo general\n",
    "fig.suptitle('SCATTER: BASELINE 1P VS TODOS LOS MODELOS', \n",
    "             fontsize=15, fontweight='bold', y=1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "scatter_all_path = comparison_folder / 'scatter_1P_all_models_combined.png'\n",
    "plt.savefig(scatter_all_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úÖ Guardado: {scatter_all_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMEN FINAL Y COMPARACI√ìN DE M√âTRICAS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ COMPARACI√ìN CON BASELINE 1P COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä MODELOS COMPARADOS: {len(predictions) + 1} (incluyendo baseline)\")\n",
    "for model_name in predictions.keys():\n",
    "    print(f\"   ‚Ä¢ {model_name}\")\n",
    "print(f\"   ‚Ä¢ Baseline 1P\")\n",
    "\n",
    "print(f\"\\nüíæ GR√ÅFICOS GENERADOS EN: {comparison_folder}\")\n",
    "print(f\"   ‚Ä¢ Time series individuales (series completas): {len(predictions)} archivos\")\n",
    "print(f\"   ‚Ä¢ Time series individuales (zoom 50s): {len(predictions)} archivos\")\n",
    "print(f\"   ‚Ä¢ Time series conjuntos (series completas): 1 archivo\")\n",
    "print(f\"   ‚Ä¢ Time series conjuntos (zoom 50s): 1 archivo\")\n",
    "print(f\"   ‚Ä¢ Scatter plots individuales: {len(predictions)} archivos\")\n",
    "print(f\"   ‚Ä¢ Scatter plot conjunto: 1 archivo\")\n",
    "print(f\"   ‚Ä¢ TOTAL: {len(predictions) * 3 + 3} gr√°ficos\")\n",
    "\n",
    "print(f\"\\nüìà COMPARACI√ìN DE M√âTRICAS (PROMEDIO EN TEST):\")\n",
    "print(f\"\\n{'MODELO':20} {'RMSE PROMEDIO':>20} {'R¬≤ PROMEDIO':>15}\")\n",
    "print(f\"{'‚îÄ'*20} {'‚îÄ'*20} {'‚îÄ'*15}\")\n",
    "\n",
    "# Baseline 1P\n",
    "print(f\"{'Baseline 1P':20} {avg_rmse_baseline:>20.2f} {avg_r2_baseline:>15.4f}\")\n",
    "\n",
    "# Modelos ML\n",
    "model_metrics = {}\n",
    "for model_name, y_pred in predictions.items():\n",
    "    avg_rmse = np.mean([np.sqrt(mean_squared_error(y_test[col], y_pred[col])) \n",
    "                       for col in y_test.columns])\n",
    "    avg_r2 = np.mean([r2_score(y_test[col], y_pred[col]) \n",
    "                     for col in y_test.columns])\n",
    "    model_metrics[model_name] = {'RMSE': avg_rmse, 'R2': avg_r2}\n",
    "    print(f\"{model_name:20} {avg_rmse:>20.2f} {avg_r2:>15.4f}\")\n",
    "\n",
    "# Mejoras relativas\n",
    "print(f\"\\nüí° MEJORA RELATIVA DE CADA MODELO RESPECTO AL BASELINE 1P:\")\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    mejora_rmse = ((avg_rmse_baseline - metrics['RMSE']) / avg_rmse_baseline) * 100\n",
    "    mejora_r2 = ((metrics['R2'] - avg_r2_baseline) / abs(avg_r2_baseline)) * 100 if avg_r2_baseline != 0 else 0\n",
    "    print(f\"   {model_name:20} ‚Üí RMSE: {mejora_rmse:+6.1f}% | R¬≤: {mejora_r2:+6.1f}%\")\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_model = min(model_metrics.items(), key=lambda x: x[1]['RMSE'])\n",
    "print(f\"\\nüèÜ MEJOR MODELO (menor RMSE): {best_model[0]}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {best_model[1]['RMSE']:.2f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤: {best_model[1]['R2']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Mejora vs Baseline: {((avg_rmse_baseline - best_model[1]['RMSE']) / avg_rmse_baseline) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ SIGUIENTE PASO:\")\n",
    "print(f\"   Paso 5.4: An√°lisis comparativo con m√©tricas detalladas y rankings\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
