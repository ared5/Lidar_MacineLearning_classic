# -*- coding: utf-8 -*-
"""
Script autogenerado desde ML_workflow_newTargets_PRETIME_HubLoads.ipynb.
Permite ejecutar cada secci√≥n de forma interactiva.
"""

from __future__ import annotations

import traceback

def ask_yes_no(question: str, default: bool = True) -> bool:
    suffix = " [Y/n]: " if default else " [y/N]: "
    while True:
        reply = input(question + suffix).strip().lower()
        if not reply:
            return default
        if reply in {"y", "yes", "s", "si", "s√≠"}:
            return True
        if reply in {"n", "no"}:
            return False
        print("Respuesta no v√°lida. Usa y/n.")

def is_plotting_cell(code: str) -> bool:
    markers = ("plt.", "sns.", ".plot(", "savefig(", "hist(", "imshow(", "scatter(", "boxplot(")
    return any(m in code for m in markers)


def sanitize_code(code: str) -> str:
    """Limpia sintaxis exclusiva de notebook para que el c√≥digo corra como script."""
    cleaned_lines = []
    for line in code.splitlines():
        stripped = line.strip()
        if stripped.startswith('%') or stripped.startswith('!'):
            print(f"    ‚Ü≥ L√≠nea omitida (magia/shell de notebook): {stripped}")
            continue
        cleaned_lines.append(line)
    return "\n".join(cleaned_lines) + "\n"


def execute_cell(code: str, title: str, idx: int, global_ns: dict) -> None:
    prepared_code = sanitize_code(code)
    exec(compile(prepared_code, f"<section:{title}:cell:{idx}>", "exec"), global_ns, global_ns)


def build_section_functions(sections: list[dict]) -> list[tuple[str, callable]]:
    """Crea una funci√≥n ejecutora por secci√≥n (step) para facilitar trazabilidad."""
    section_functions = []
    for section in sections:
        title = section["title"]
        cells = section["cells"]

        def _runner(global_ns: dict, title=title, cells=cells):
            for idx, code in enumerate(cells, start=1):
                is_plot = is_plotting_cell(code)
                if is_plot and not ask_yes_no(f"  - Celda {idx} detectada como visualizaci√≥n. ¬øEjecutarla?", default=False):
                    print(f"  - Celda {idx} omitida (visualizaci√≥n).")
                    continue
                if (not is_plot) and not ask_yes_no(f"  - Ejecutar celda {idx} de esta secci√≥n?", default=True):
                    print(f"  - Celda {idx} omitida.")
                    continue
                try:
                    execute_cell(code, title, idx, global_ns)
                    print(f"  - Celda {idx} ejecutada correctamente.")
                except Exception as exc:
                    print(f"  - Error en celda {idx}: {exc}")
                    traceback.print_exc()
                    if not ask_yes_no("¬øContinuar con la siguiente celda/secci√≥n?", default=False):
                        raise

        section_functions.append((title, _runner))
    return section_functions

SECTIONS = [
    {
        "title": 'üîß STEP 1: Configuraci√≥n Inicial del Entorno',
        "cells": [
            '# ============================================================================\n# PASO 1.1: Importar librer√≠as est√°ndar de Python\n# ============================================================================\n\n# Manejo del sistema operativo y archivos\nimport os           # Operaciones del sistema operativo (crear carpetas, paths, etc.)\nimport sys          # Configuraci√≥n del sistema Python (paths, argumentos)\nfrom pathlib import Path  # Manejo moderno de rutas de archivos\n\n# Manejo de advertencias\nimport warnings\nwarnings.filterwarnings(\'ignore\')  # Ignorar warnings para output m√°s limpio\n\nprint("‚úÖ Librer√≠as est√°ndar de Python importadas")\n\n# ============================================================================\n# PASO 1.2: Importar librer√≠as de an√°lisis de datos\n# ============================================================================\n\n# pandas: Librer√≠a principal para manipulaci√≥n de datos tabulares\nimport pandas as pd\n\nimport re\n\n# numpy: Operaciones num√©ricas y arrays eficientes\nimport numpy as np\n\n# joblib: Guardar y cargar modelos y scalers\nimport joblib\n\nprint("‚úÖ Librer√≠as de an√°lisis de datos importadas")\n\n# ============================================================================\n# PASO 1.3: Importar librer√≠as de visualizaci√≥n\n# ============================================================================\n\n# matplotlib: Librer√≠a base para gr√°ficos en Python\nimport matplotlib.pyplot as plt\n\n# seaborn: Visualizaciones estad√≠sticas de alto nivel\nimport seaborn as sns\n\n# Configuraci√≥n de estilo de gr√°ficos\nplt.style.use(\'seaborn-v0_8-darkgrid\')  # Estilo con grid oscuro\nsns.set_palette(\'husl\')  # Paleta de colores HUSL (vibrante y distinguible)\n\n# Mostrar gr√°ficos directamente en el notebook\n%matplotlib inline\n\n# PASO 1.4: Importar librer√≠as de Machine Learning\n\n# ============================================================================\n# Scikit-learn: Modelos y m√©tricas\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\n\nprint("‚úÖ Librer√≠as de Machine Learning importadas")\n\n# ============================================================================\n# PASO 1.5: Configurar pandas para mejor visualizaci√≥n\n# ============================================================================\n\n\nprint("="*70)\nprint("CONFIGURACION BASICA COMPLETADA")\nprint("\\n" + "="*70)\nprint("‚úÖ Configuraci√≥n de pandas aplicada")\npd.set_option(\'display.precision\', 4)# Precisi√≥n de 4 decimales para n√∫meros\npd.set_option(\'display.max_rows\', 100)# Mostrar hasta 100 filas\npd.set_option(\'display.max_columns\', None)\nprint("\\n" + "="*70)\nprint("CONFIGURACION BASICA COMPLETADA")\nprint("="*70)',
            '# ============================================================================\n# PASO 1.5: Importar herramientas de Machine Learning - Scikit-learn\n# ============================================================================\n\n# --- VALIDACI√ìN Y SEPARACI√ìN DE DATOS ---\nfrom sklearn.model_selection import (\n    train_test_split,    # Dividir datos en train/test\n    cross_val_score,     # Validaci√≥n cruzada\n    KFold                # K-Fold para validaci√≥n cruzada robusta\n)\n\n# --- PREPROCESAMIENTO ---\nfrom sklearn.preprocessing import (\n    StandardScaler,      # Normalizaci√≥n Z-score (media=0, std=1)\n    MinMaxScaler         # Escalado a rango [0,1]\n)\n\n# --- M√âTRICAS DE EVALUACI√ìN ---\nfrom sklearn.metrics import (\n    mean_squared_error,  # MSE: Error cuadr√°tico medio\n    r2_score,            # R¬≤: Coeficiente de determinaci√≥n\n    mean_absolute_error  # MAE: Error absoluto medio\n)\n\nprint("‚úÖ Herramientas de ML (sklearn) importadas")\n\n# ============================================================================\n# PASO 1.6: Importar modelos de Machine Learning\n# ============================================================================\n\n# --- MODELOS LINEALES ---\nfrom sklearn.linear_model import (\n    LinearRegression,    # Regresi√≥n lineal simple\n    Ridge,               # Regresi√≥n Ridge (regularizaci√≥n L2)\n    Lasso,               # Regresi√≥n Lasso (regularizaci√≥n L1)\n    ElasticNet           # ElasticNet (combinaci√≥n L1 + L2)\n)\n\n# --- MODELOS BASADOS EN √ÅRBOLES ---\nfrom sklearn.ensemble import (\n    RandomForestRegressor,      # Random Forest: ensemble de √°rboles\n    ExtraTreesRegressor,        # Extra Trees: √°rboles con splits aleatorios\n    GradientBoostingRegressor   # Gradient Boosting: boosting secuencial\n)\n\nfrom sklearn.tree import DecisionTreeRegressor  # √Årbol de decisi√≥n simple\n\nprint("‚úÖ Modelos de ML (sklearn) importados")\n\n# ============================================================================\n# PASO 1.7: Importar algoritmos avanzados (XGBoost y LightGBM)\n# ============================================================================\n\n# XGBoost: Extreme Gradient Boosting (muy potente para competiciones)\ntry:\n    import xgboost as xgb\n    print("‚úÖ XGBoost disponible (versi√≥n: {})".format(xgb.__version__))\n    XGBOOST_AVAILABLE = True\nexcept ImportError:\n    print("‚ö†Ô∏è  XGBoost NO disponible - instalar con: pip install xgboost")\n    xgb = None\n    XGBOOST_AVAILABLE = False\n\n# LightGBM: Light Gradient Boosting Machine (r√°pido y eficiente)\ntry:\n    import lightgbm as lgb\n    print("‚úÖ LightGBM disponible (versi√≥n: {})".format(lgb.__version__))\n    LIGHTGBM_AVAILABLE = True\nexcept ImportError:\n    print("‚ö†Ô∏è  LightGBM NO disponible - instalar con: pip install lightgbm")\n    lgb = None\n    LIGHTGBM_AVAILABLE = False\n\nprint("\\n" + "="*70)\nprint("üéâ LIBRER√çAS DE MACHINE LEARNING COMPLETADAS")\nprint("="*70)',
            '# ============================================================================\n# PASO 1.8: Configurar estructura de directorios del proyecto\n# ============================================================================\n\n# Directorio ra√≠z del proyecto (carpeta padre del notebook)\nroot_dir = Path.cwd().parent\n\n# Directorios principales del proyecto\ndata_folder = root_dir / "data_train"      # Datos de entrenamiento (CSVs)\ndata_folder_ml = root_dir / "data_train_traditional_ML"\nreport_folder = root_dir / "_report"       # Reportes y visualizaciones\nmodels_folder = root_dir / "notebook" / "03_ML_traditional_models"      # Modelos entrenados guardados\nresults_folder = root_dir / "_results"     # Resultados de predicciones\neda_folder = Path.cwd() / "00_EDA_traditional_ML"\ncomplete_dataset_path = data_folder_ml / "0000_Complete_dataset.csv"\ntraining_folder = root_dir / \'notebook\' / \'03_Models_training\'\nscaler_folder = root_dir / \'notebook\' / \'01_Models_scaler\'\n\n\n# Crear carpetas si no existen\nmodels_folder.mkdir(exist_ok=True)\nresults_folder.mkdir(exist_ok=True)\n\n# Verificar existencia de carpetas cr√≠ticas\ndata_exists = data_folder.exists()\nreport_exists = report_folder.exists()\n\nprint("="*70)\nprint("ESTRUCTURA DE DIRECTORIOS")\nprint("="*70)\nprint(f"Directorio raiz:     {root_dir}")\nprint(f"Datos entrenamiento: {data_folder} {\'OK\' if data_exists else \'NO EXISTE\'}")\nprint(f"Reportes:            {report_folder} {\'OK\' if report_exists else \'NO EXISTE\'}")\nprint(f"Modelos guardados:   {models_folder} OK")\nprint(f"Resultados:          {results_folder} OK")\nprint(f"Eda folder:          {eda_folder} OK")\nprint("="*70)\n\n# Advertencias si faltan directorios cr√≠ticos\nif not data_exists:\n    print("\\nADVERTENCIA: Carpeta \'data_train\' no encontrada")\n    print("   -> Usando directorio raiz para buscar datos")\n    data_folder = root_dir\n    \nif not report_exists:\n    print("\\nADVERTENCIA: Carpeta \'_report\' no encontrada")\n    print("   -> Algunos analisis previos pueden no estar disponibles")',
        ],
    },
    {
        "title": '‚úÖ Datos guardados (normalizados y no normalizados, de test y train)',
        "cells": [
            "# Cargar datos originales\nX_train = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\X_train.pkl')\ny_train = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\y_train.pkl')\nX_test = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\X_test.pkl')\ny_test = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\y_test.pkl')\n\n# Cargar datos normalizados\nX_train_norm = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\X_train_norm.pkl')\ny_train_norm = joblib.load(r'c:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\notebook\\02_Data_ML_traditional\\y_train_norm.pkl')",
        ],
    },
    {
        "title": 'üìä STEP 2: Importar Se√±ales desde Simulaciones Bladed',
        "cells": [
            '# ============================================================================\n# PASO 2.1: Importar postprocessbladed (libreria para leer simulaciones Bladed)\n# ============================================================================\n\n# A√±adir ruta de postprocessbladed al path\npostprocessbladed_parent_path = r"C:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git"\nif postprocessbladed_parent_path not in sys.path:\n    sys.path.append(postprocessbladed_parent_path)\n\n# Importar postprocessbladed\nimport postprocessbladed as pp\n\nprint("OK - postprocessbladed importado correctamente")\nprint(f"   Ruta: {postprocessbladed_parent_path}")',
            '# ============================================================================\n# PASO 2.2: Configurar rutas y parametros de carga\n# ============================================================================\n\n# Ruta donde estan las simulaciones de Bladed (archivos .$TE)\nloadpath = r"U:\\Studies\\437_lidar_VLOS_IPC\\Outputs\\OPT3_PassiveYaw\\DLC13"\n\n# Lista de nombres de archivos de simulacion a procesar\n# PUEDES A√ëADIR VARIOS ARCHIVOS Y SE GENERARA UN CSV POR CADA UNO\nfile_names = [\n    #"0007_DLC13_050_000",\n    #"0008_DLC13_050_000",\n    #"0009_DLC13_050_000",\n    #"0010_DLC13_050_000",\n    #"0011_DLC13_050_000",\n    #"0012_DLC13_050_000",\n    #"0013_DLC13_070_000",\n    #"0014_DLC13_070_000",\n    #"0015_DLC13_070_000",\n    #"0016_DLC13_070_000",\n    #"0017_DLC13_070_000",\n    #"0018_DLC13_070_000",\n    #"0019_DLC13_090_000",\n    "0020_DLC13_090_000",\n    "0021_DLC13_090_000",\n    "0022_DLC13_090_000",\n    "0023_DLC13_090_000",\n    "0024_DLC13_090_000",\n    "0025_DLC13_110_000",\n    "0026_DLC13_110_000",\n    "0027_DLC13_110_000",\n    "0028_DLC13_110_000",\n    "0029_DLC13_110_000",\n    "0030_DLC13_110_000",\n    "0031_DLC13_130_000",\n    "0032_DLC13_130_000",\n    "0033_DLC13_130_000",\n    "0034_DLC13_130_000",\n    "0035_DLC13_130_000",\n    "0036_DLC13_130_000",\n    "0037_DLC13_150_000",\n    "0038_DLC13_150_000",\n    "0039_DLC13_150_000",\n    "0040_DLC13_150_000",\n    "0041_DLC13_150_000",\n    "0042_DLC13_150_000",\n    "0043_DLC13_170_000",\n    "0044_DLC13_170_000",\n    "0045_DLC13_170_000",\n    "0046_DLC13_170_000",\n    "0047_DLC13_170_000",\n    "0048_DLC13_170_000",\n    "0049_DLC13_190_000",\n    "0050_DLC13_190_000",\n    "0051_DLC13_190_000",\n    "0052_DLC13_190_000",\n    "0053_DLC13_190_000",\n    "0054_DLC13_190_000",\n    "0055_DLC13_210_000",\n    "0056_DLC13_210_000",\n    "0057_DLC13_210_000",\n    "0058_DLC13_210_000",\n    "0059_DLC13_210_000",\n    "0060_DLC13_210_000",\n    "0061_DLC13_230_000",\n    "0062_DLC13_230_000",\n    "0063_DLC13_230_000",\n    "0064_DLC13_230_000",\n    "0065_DLC13_230_000",\n    "0066_DLC13_230_000",\n    "0067_DLC13_250_000",\n    "0068_DLC13_250_000",\n    "0069_DLC13_250_000",\n    "0070_DLC13_250_000",\n    "0071_DLC13_250_000",\n    "0072_DLC13_250_000",\n]\n\n# Ruta donde guardar los resultados CSV\nresultspath = str(data_folder)  # Usar data_folder definida anteriormente\n\n# Opcion para a√±adir unidades en el CSV (True/False)\nadd_units = False\n\n# Crear carpeta de resultados si no existe\nif not os.path.exists(resultspath):\n    os.makedirs(resultspath)\n\nprint("="*70)\nprint("CONFIGURACION DE RUTAS")\nprint("="*70)\nprint(f"Simulaciones Bladed: {loadpath}")\nprint(f"Archivos a procesar: {len(file_names)}")\nfor i, fname in enumerate(file_names, 1):\n    print(f"  {i}. {fname}")\nprint(f"\\nGuardar CSV en:      {resultspath}")\nprint(f"Anadir unidades:     {add_units}")\nprint("="*70)',
            '# ============================================================================\n# PASO 2.3: Definir diccionarios de variables a extraer\n# ============================================================================\n# Aqui defines QUE SE√ëALES quieres extraer de las simulaciones\n# Organizadas por categoria (se√±al de Bladed)\n\n# Inicializar diccionario de variables\nvar_dicts = {}\n\n# EJEMPLO 1: Variables de velocidad del viento en la pala 1 (Aero_B1)\n# Estas son las velocidades del viento incidente a diferentes posiciones radiales\n# var_dicts["Aero_B1"] = {\n#     "Aero_B1": [\n#         "Blade 1 Incident axial wind speed at 0m",\n#         "Blade 1 Incident axial wind speed at 6m",\n#         "Blade 1 Incident axial wind speed at 18m",\n#         "Blade 1 Incident axial wind speed at 30m",\n#         "Blade 1 Incident axial wind speed at 46m",\n#         "Blade 1 Incident axial wind speed at 59m",\n#         "Blade 1 Incident axial wind speed at 68.25m",\n#     ]\n# }\n\n# EJEMPLO 2: Variables de velocidad del viento en la pala 2 (Aero_B2)\n# var_dicts["Aero_B2"] = {\n#     "Aero_B2": [\n#         "Blade 2 Incident axial wind speed at 0m",\n#         "Blade 2 Incident axial wind speed at 6m",\n#         "Blade 2 Incident axial wind speed at 18m",\n#         "Blade 2 Incident axial wind speed at 30m",\n#         "Blade 2 Incident axial wind speed at 46m",\n#         "Blade 2 Incident axial wind speed at 59m",\n#         "Blade 2 Incident axial wind speed at 68.25m",\n#     ]\n# }\n\n# Posiciones radiales (m) para las se√±ales Aero (deben coincidir con las variables)\n#AERO_POSITIONS = [0.0, 6.0, 18.0, 30.0, 46.0, 59.0, 68.25]\n\n# EJEMPLO 3: Informacion ambiental\n#var_dicts["Environmental_information"] = {\n#    "Environmental_information": ["Rotor average longitudinal wind speed"]\n#}\n\n# EJEMPLO 4: Variables en el hub rotante (TARGETS - lo que queremos predecir)\nvar_dicts["Hub_rotating"] = {\n    "Hub_rotating": [\n        "Blade root 1 My",  # Momento flector pala 1 (TARGET)\n        "Blade root 2 My"   # Momento flector pala 2 (TARGET)\n    ]\n}\n\n# EJEMPLO 5: Actuadores de pitch (control de palas)\nvar_dicts["Pitch_actuator"] = {\n    "Pitch_actuator": [\n        "Blade 1 pitch angle",  # Angulo de pala 1\n        "Blade 2 pitch angle",  # Angulo de pala 2\n        #"Blade 1 pitch rate",   # Velocidad angular pala 1\n        #"Blade 2 pitch rate"    # Velocidad angular pala 2\n    ]\n}\n\n# EJEMPLO 6: Drivetrain (tren de potencia)\nvar_dicts["Drive_train"] = {\n    "Drive_train": ["Rotor azimuth angle"]  # Angulo azimutal del rotor\n}\n\n# EJEMPLO 7: Rotor speed \nvar_dicts["Summary"] = {\n    "Summary": ["Rotor speed"]  # Rotor speed\n}\n\n# EJEMPLO 8: VLOS \nvar_dicts["External_controller"] = {\n    "External_controller": [\n        "LAC_VLOS_BEAM0_RANGE5",\n        "LAC_VLOS_BEAM1_RANGE5",\n        "LAC_VLOS_BEAM2_RANGE5",\n        "LAC_VLOS_BEAM3_RANGE5",\n        "LAC_VLOS_BEAM4_RANGE5",\n        "LAC_VLOS_BEAM5_RANGE5",\n        "LAC_VLOS_BEAM6_RANGE5",\n        "LAC_VLOS_BEAM7_RANGE5",\n        #"LAC_VLOS_BEAM8_RANGE5",\n        #"LAC_VLOS_BEAM9_RANGE5",\n        "OPER_MEAS_YAWERROR",\n        ]  # VLOS\n}\n\n# Resumen de variables definidas\nprint("="*70)\nprint("VARIABLES A EXTRAER")\nprint("="*70)\ntotal_vars = 0\nfor dict_name, var_dict in var_dicts.items():\n    for signal, variables in var_dict.items():\n        print(f"\\n{signal}: {len(variables)} variables")\n        for var in variables:\n            print(f"  - {var}")\n            total_vars += 1\nprint(f"\\n{\'=\'*70}")\nprint(f"TOTAL: {total_vars} variables")\nprint("="*70)',
            '# ============================================================================\n# PASO 2.4: Funciones de ayuda para procesar archivos Bladed\n# ============================================================================\n\n# FUNCION 1: Crear CSV de series temporales desde Bladed (IGUAL QUE timeseries_script.py)\ndef create_timeseries_csv(bin_series, header_series, filenames, var_dict, output_path, output_filename, add_units=False):\n    """\n    Crea un CSV con series temporales desde archivos Bladed binarios.\n    Esta funcion replica exactamente el comportamiento de timeseries_script.py\n    \n    Args:\n        bin_series: Diccionario con datos binarios {archivo: {se√±al: datos}}\n        header_series: Headers leidos con pp.read_hdr_files()\n        filenames: Lista de archivos procesados\n        var_dict: Diccionario con variables a extraer {se√±al: [variables]}\n        output_path: Ruta donde guardar el CSV\n        output_filename: Nombre del archivo (sin extension)\n        add_units: Si True, a√±ade fila de unidades\n    \n    Returns:\n        Ruta del CSV creado\n    """\n    import csv\n    import re\n    \n    print(f\'Creando CSV para variables: {list(var_dict.keys())}\')\n    \n    # Diccionario de unidades en SI\n    variable_units = {\n        "Time": "s",\n        "Rotor average longitudinal wind speed": "m/s",\n        "Blade 1 Incident axial wind speed": "m/s",\n        "Blade 2 Incident axial wind speed": "m/s",\n        "Blade root 1 My": "kNm",\n        "Blade root 2 My": "kNm",\n        "Blade 1 pitch angle": "deg",\n        "Blade 2 pitch angle": "deg",\n        "Blade 1 pitch rate": "deg/s",\n        "Blade 2 pitch rate": "deg/s",\n        "Rotor azimuth angle": "deg",\n        "LAC_VLOS_BEAM0_RANGE5": "m/s",\n        "LAC_VLOS_BEAM1_RANGE5": "m/s",\n        "LAC_VLOS_BEAM2_RANGE5": "m/s",\n        "LAC_VLOS_BEAM3_RANGE5": "m/s",\n        "LAC_VLOS_BEAM4_RANGE5": "m/s",\n        "LAC_VLOS_BEAM5_RANGE5": "m/s",\n        "LAC_VLOS_BEAM6_RANGE5": "m/s",\n        "LAC_VLOS_BEAM7_RANGE5": "m/s",\n        "LAC_VLOS_BEAM8_RANGE5": "m/s",\n        "LAC_VLOS_BEAM9_RANGE5": "m/s"\n    }\n    \n    # Obtener todos los nombres de variables para el header\n    all_variables = []\n    for signal in var_dict.keys():\n        all_variables.extend(var_dict[signal])\n    \n    # Crear fila de header\n    header = [\'Time\'] + all_variables\n    \n    # Inicializar estructura de datos\n    csv_data = [header]\n    \n    # A√±adir fila de unidades si se solicita\n    if add_units:\n        units_row = []\n        for var in header:\n            if var in variable_units:\n                units_row.append(variable_units[var])\n            else:\n                units_row.append("")\n        csv_data.append(units_row)\n    \n    # Procesar cada archivo\n    for file in filenames:\n        print(f\'Procesando archivo: {os.path.basename(file)}\')\n        \n        # Obtener vector de tiempo\n        time_data = None\n        first_signal = list(var_dict.keys())[0]\n        first_variable = var_dict[first_signal][0]\n        \n        try:\n            if first_signal in bin_series[file]:\n                if isinstance(bin_series[file][first_signal], np.ndarray):\n                    signal_data = bin_series[file][first_signal]\n                    \n                    # Manejo especial para se√±ales Aero\n                    if first_signal in ("Aero_B1", "Aero_B2") and hasattr(signal_data.dtype, \'names\') and signal_data.dtype.names:\n                        base_name = signal_data.dtype.names[0]\n                        arr = signal_data[base_name]\n                        if arr.ndim == 2:\n                            time_length = arr.shape[-1] if arr.shape[-1] >= arr.shape[0] else arr.shape[0]\n                        else:\n                            time_length = len(arr)\n                        try:\n                            dt = header_series[file][first_signal][\'dtime\']\n                        except KeyError:\n                            dt = 0.02\n                        time_data = [i * dt for i in range(time_length)]\n                    else:\n                        # Manejo generico\n                        if hasattr(signal_data.dtype, \'names\') and signal_data.dtype.names:\n                            if first_variable in signal_data.dtype.names:\n                                time_length = len(signal_data[first_variable])\n                                try:\n                                    dt = header_series[file][first_signal][\'dtime\']\n                                except KeyError:\n                                    dt = 0.02\n                                time_data = [i * dt for i in range(time_length)]\n                        else:\n                            time_length = len(signal_data)\n                            try:\n                                dt = header_series[file][first_signal][\'dtime\']\n                            except KeyError:\n                                dt = 0.02\n                            time_data = [i * dt for i in range(time_length)]\n        except Exception as e:\n            print(f\'Error obteniendo datos de tiempo: {e}\')\n            continue\n        \n        if time_data is None:\n            print(f"Advertencia: No se pudo determinar datos de tiempo para {file}")\n            continue\n        \n        print(f\'Procesando {len(time_data)} pasos de tiempo\')\n        \n        # Procesar cada paso de tiempo\n        for i in range(len(time_data)):\n            row = [time_data[i]]\n            \n            # A√±adir datos para cada variable\n            for signal in var_dict.keys():\n                for variable in var_dict[signal]:\n                    try:\n                        if signal in bin_series[file]:\n                            signal_data = bin_series[file][signal]\n                            \n                            if isinstance(signal_data, np.ndarray):\n                                # Manejo especial para se√±ales Aero\n                                if signal in ("Aero_B1", "Aero_B2"):\n                                    m = re.search(r"at\\s+([0-9]+(?:\\.[0-9]+)?)m", variable)\n                                    if m:\n                                        try:\n                                            pos = float(m.group(1))\n                                            idx = None\n                                            for j, p in enumerate(AERO_POSITIONS):\n                                                if abs(p - pos) < 1e-6:\n                                                    idx = j\n                                                    break\n                                            if idx is None:\n                                                idx = int(np.argmin([abs(p - pos) for p in AERO_POSITIONS]))\n                                            \n                                            if hasattr(signal_data.dtype, \'names\') and signal_data.dtype.names:\n                                                base_name = signal_data.dtype.names[0]\n                                                arr = signal_data[base_name]\n                                            else:\n                                                arr = signal_data\n                                            \n                                            if arr.ndim == 2:\n                                                val = arr[idx, i]\n                                            elif arr.ndim == 1:\n                                                val = arr[i] if i < arr.shape[0] else np.nan\n                                            else:\n                                                val = np.nan\n                                            row.append(float(val) if np.isfinite(val) else 0.0)\n                                        except Exception as ex:\n                                            row.append(0.0)\n                                    else:\n                                        row.append(0.0)\n                                else:\n                                    # Arrays no-Aero\n                                    if hasattr(signal_data.dtype, \'names\') and signal_data.dtype.names:\n                                        if variable in signal_data.dtype.names and i < len(signal_data[variable]):\n                                            row.append(float(signal_data[variable][i]))\n                                        else:\n                                            row.append(0.0)\n                                    else:\n                                        if signal_data.ndim == 1 and i < signal_data.shape[0]:\n                                            row.append(float(signal_data[i]))\n                                        else:\n                                            row.append(0.0)\n                            else:\n                                row.append(0.0)\n                        else:\n                            row.append(0.0)\n                    except Exception as e:\n                        row.append(0.0)\n            \n            csv_data.append(row)\n    \n    # Escribir a CSV\n    output_file = os.path.join(output_path, f"{output_filename}.csv")\n    with open(output_file, \'w\', newline=\'\', encoding=\'utf-8\') as f:\n        writer = csv.writer(f)\n        writer.writerows(csv_data)\n    \n    print(f\'Archivo CSV creado: {output_file}\')\n    return output_file\n\nprint("OK - Funcion create_timeseries_csv definida correctamente")\nprint("  Esta funcion replica exactamente timeseries_script.py")',
            '# ============================================================================\n# PASO 2.5: Buscar archivos de simulacion usando pp.read_dlcnames\n# ============================================================================\n# Usar la misma funcion que timeseries_script.py para buscar archivos\n\nprint("="*70)\nprint("BUSCANDO ARCHIVOS DE SIMULACION")\nprint("="*70)\nprint(f"Directorio: {loadpath}\\n")\n\n# Usar pp.read_dlcnames para buscar todos los archivos .$TE\n# El filtro {"": 1} busca todos los archivos\nall_dlcnames = pp.read_dlcnames(loadpath, file_filter={"": 1})\n\nprint(f"Total de archivos .$TE encontrados: {len(all_dlcnames)}")\n\n# Diccionario para almacenar archivos filtrados por cada file_name\nall_matching_files = {}\n\n# Filtrar archivos para cada patron en file_names\nfor file_name in file_names:\n    # Filtrar archivos que contengan file_name en el nombre\n    matching = [f for f in all_dlcnames if file_name in os.path.basename(f)]\n    all_matching_files[file_name] = matching\n    \n    print(f"\\nPatron \'{file_name}\':")\n    if matching:\n        for file in matching:\n            print(f"  -> {os.path.basename(file)}")\n    else:\n        print(f"  -> ADVERTENCIA: No se encontraron archivos")\n\n# Resumen\ntotal_files = sum(len(files) for files in all_matching_files.values())\nprint(f"\\n{\'=\'*70}")\nprint(f"TOTAL: {total_files} archivos encontrados para {len(file_names)} patrones")\nprint("="*70)',
            '# ============================================================================\n# PASO 2.6: Combinar todos los diccionarios de variables y preparar signal_list\n# ============================================================================\n# Unir todos los diccionarios en uno solo para pasarlo a la funcion\n\n# Combinar todos los diccionarios de variables en uno\ncombined_var_dict = {}\nfor dict_name, var_dict in var_dicts.items():\n    combined_var_dict.update(var_dict)\n\n# Lista de se√±ales (keys) para pp.read_hdr_files y pp.read_bin_files\nsignal_list = list(combined_var_dict.keys())\n\nprint("="*70)\nprint("DICCIONARIO COMBINADO DE VARIABLES")\nprint("="*70)\nprint(f"\\nTotal de grupos de se√±ales: {len(combined_var_dict)}")\nprint(f"Signal list: {signal_list}\\n")\nfor signal, variables in combined_var_dict.items():\n    print(f"{signal}: {len(variables)} variables")\nprint("="*70)',
            '# ============================================================================\n# PASO 2.7: PROCESAR CADA FILE_NAME POR SEPARADO\n# ============================================================================\n# Iterar sobre cada file_name y procesarlo independientemente\n# Cada file_name generara su propio CSV\n\nprint("="*70)\nprint("PROCESANDO ARCHIVOS POR SEPARADO")\nprint("="*70)\n\n# Contadores globales\ntotal_processed = 0\ntotal_failed = 0\nall_dataframes = {}\n\n# ITERAR SOBRE CADA FILE_NAME\nfor file_name_pattern in file_names:\n    print(f"\\n{\'=\'*70}")\n    print(f"PROCESANDO: {file_name_pattern}")\n    print("="*70)\n    \n    # Obtener archivos para este file_name\n    current_files = all_matching_files.get(file_name_pattern, [])\n    \n    if not current_files:\n        print(f"SKIP - No se encontraron archivos para \'{file_name_pattern}\'")\n        continue\n    \n    print(f"Archivos encontrados: {len(current_files)}")\n    for f in current_files:\n        print(f"  - {os.path.basename(f)}")\n    \n    try:\n        # PASO A: Leer headers para este grupo de archivos\n        print(f"\\n[1/3] Leyendo headers...")\n        header_series = pp.read_hdr_files(current_files, signal_list)\n        print(f"      OK - Headers leidos")\n        \n        # PASO B: Leer datos binarios\n        print(f"\\n[2/3] Leyendo datos binarios...")\n        \n        # Separar se√±ales Aero de no-Aero\n        non_aero_signals = [s for s in signal_list if "Aero" not in s]\n        aero_signals = [s for s in signal_list if "Aero" in s]\n        \n        # Estructura para datos binarios\n        bin_series = {f: {} for f in current_files}\n        \n        # Leer se√±ales NO Aero\n        if non_aero_signals:\n            print(f"      Leyendo {len(non_aero_signals)} se√±ales NO Aero...")\n            non_aero_bin = pp.read_bin_files(current_files, header_series, non_aero_signals)\n            for f in current_files:\n                if f in non_aero_bin:\n                    bin_series[f].update(non_aero_bin[f])\n        \n        # Leer se√±ales Aero con posiciones especificas\n        if aero_signals:\n            print(f"      Leyendo {len(aero_signals)} se√±ales Aero...")\n            for aero_sig in aero_signals:\n                # Detectar variable base del header\n                first_file = current_files[0]\n                hdr_vars = header_series[first_file][aero_sig].get(\'variab\', [])\n                \n                base_var = None\n                for v in hdr_vars:\n                    if \'incident\' in v.lower() and \'axial\' in v.lower() and \'wind speed\' in v.lower():\n                        base_var = v\n                        break\n                \n                if base_var is None and hdr_vars:\n                    base_var = hdr_vars[0]\n                \n                aero_bin = pp.read_bin_files(\n                    current_files,\n                    header_series,\n                    [aero_sig],\n                    var_list=[base_var],\n                    reduced_mbr_list=AERO_POSITIONS\n                )\n                \n                for f in current_files:\n                    if f in aero_bin:\n                        bin_series[f].update(aero_bin[f])\n        \n        print(f"      OK - Datos binarios leidos")\n        \n        # PASO C: Crear CSV usando la funcion create_timeseries_csv\n        print(f"\\n[3/3] Generando CSV...")\n        \n        output_filename = file_name_pattern\n        create_timeseries_csv(\n            bin_series, \n            header_series, \n            current_files, \n            combined_var_dict, \n            resultspath, \n            output_filename, \n            add_units\n        )\n        \n        print(f"\\n      OK - CSV generado: {output_filename}.csv")\n        total_processed += 1\n        \n        # Cargar el CSV recien creado como DataFrame para verificacion\n        csv_path = os.path.join(resultspath, f"{output_filename}.csv")\n        if os.path.exists(csv_path):\n            df_check = pd.read_csv(csv_path)\n            all_dataframes[file_name_pattern] = df_check\n            print(f"      Shape: {df_check.shape}")\n        \n    except Exception as e:\n        print(f"\\nERROR procesando \'{file_name_pattern}\': {str(e)}")\n        import traceback\n        traceback.print_exc()\n        total_failed += 1\n\n# RESUMEN FINAL\nprint(f"\\n{\'=\'*70}")\nprint("RESUMEN FINAL DEL PROCESAMIENTO")\nprint("="*70)\nprint(f"Archivos procesados correctamente: {total_processed}")\nprint(f"Archivos con errores:              {total_failed}")\nprint(f"Total intentados:                  {total_processed + total_failed}")\n\nif total_processed > 0:\n    print(f"\\nCSVs generados en: {resultspath}")\n    print("\\nDataFrames cargados en memoria:")\n    for fname, df in all_dataframes.items():\n        print(f"  - {fname}: {df.shape}")\n\nprint("\\n" + "="*70)\nprint("STEP 2 COMPLETADO")\nprint("="*70)',
        ],
    },
    {
        "title": 'üîß STEP 3: Feature Engineering para ML Tradicional',
        "cells": [
            '# ============================================================================\n# PASO 3.1: Configurar carpeta de datos para ML tradicional\n# ============================================================================\n\n# Carpeta con los CSVs para ML tradicional\ndata_folder_ml = root_dir / "data_train_traditional_ML"\n\n# Verificar que existe\nif not data_folder_ml.exists():\n    print("ERROR: La carpeta data_train_traditional_ML no existe")\n    print("Por favor, ejecuta primero el comando para copiar data_train")\nelse:\n    # Listar archivos CSV\n    csv_files = list(data_folder_ml.glob("*.csv"))\n    \n    print("="*70)\n    print("CARPETA DE DATOS PARA ML TRADICIONAL")\n    print("="*70)\n    print(f"Ruta: {data_folder_ml}")\n    print(f"\\nArchivos CSV encontrados: {len(csv_files)}")\n    \n    if csv_files:\n        print("\\nPrimeros 10 archivos:")\n        for i, csv_file in enumerate(csv_files[:10], 1):\n            print(f"  {i}. {csv_file.name}")\n        if len(csv_files) > 10:\n            print(f"  ... y {len(csv_files) - 10} archivos m√°s")\n    \n    print("="*70)',
            'def create_vlos_lags(\n    df,\n    lag_seconds_list=[2, 5, 8, 11, 14, 17, 20, 23, 26],\n    range_values=None,\n    range_min=None,\n    range_max=None,\n    include_all_ranges=False,\n):\n    """\n    Crea features de lag para las variables de velocidad del viento (VLOS).\n\n    Args:\n        df: DataFrame con los datos\n        lag_seconds_list: Lista de lags en segundos a crear\n        range_values: Lista/tupla de rangos espec√≠ficos (por ejemplo [5, 7, 9]).\n        range_min: Rango m√≠nimo (inclusive) si se usa filtro por rango\n        range_max: Rango m√°ximo (inclusive) si se usa filtro por rango\n        include_all_ranges: Si True, ignora filtros y usa todas las columnas VLOS\n\n    Returns:\n        DataFrame con las nuevas columnas de lag a√±adidas\n    """\n    # Identificar columnas de velocidad del viento (VLOS)\n    vlos_columns = [col for col in df.columns if col.startswith(\'LAC_VLOS\')]\n\n    # Filtrar por rango si se solicita\n    if not include_all_ranges:\n        filtered_columns = []\n        for col in vlos_columns:\n            parts = col.split(\'_\')\n            range_part = next((p for p in parts if p.startswith(\'RANGE\')), None)\n            if not range_part:\n                continue\n            try:\n                range_value = int(range_part.replace(\'RANGE\', \'\'))\n            except ValueError:\n                continue\n\n            if range_values is not None:\n                if range_value in range_values:\n                    filtered_columns.append(col)\n            elif range_min is not None or range_max is not None:\n                min_ok = range_min is None or range_value >= range_min\n                max_ok = range_max is None or range_value <= range_max\n                if min_ok and max_ok:\n                    filtered_columns.append(col)\n\n        vlos_columns = filtered_columns\n\n    print(f"Variables VLOS encontradas: {len(vlos_columns)}")\n    for col in vlos_columns:\n        print(f"  - {col}")\n\n    # Calcular tiempo de muestreo (dt) asumiendo columna Time\n    if \'Time\' in df.columns and len(df) > 1:\n        dt = df[\'Time\'].iloc[1] - df[\'Time\'].iloc[0]  # Segundos entre muestras\n        print(f"\\nTiempo de muestreo detectado: {dt:.4f} segundos")\n    else:\n        dt = 0.02  # Default 50Hz\n        print(f"\\nTiempo de muestreo por defecto: {dt} segundos")\n\n    # Crear lags para cada variable VLOS\n    print(f"\\nCreando {len(lag_seconds_list)} lags para cada variable VLOS...")\n\n    total_created = 0\n    for vlos_col in vlos_columns:\n        for lag_sec in lag_seconds_list:\n            # Calcular numero de muestras para el lag\n            lag_samples = int(round(lag_sec / dt))\n\n            # Crear nombre de la nueva columna\n            new_col_name = f"{vlos_col}_lag{lag_sec}s"\n\n            # Crear la columna con shift\n            df[new_col_name] = df[vlos_col].shift(lag_samples)\n\n            total_created += 1\n\n    print(f"Total de features de lag creadas: {total_created}")\n    print(f"Shape del DataFrame: {df.shape}")\n\n    return df\n',
            '# ============================================================================\n# PASO 3.2: Definir funcion para crear features con lags de VLOS\n# ============================================================================\n\ndef create_vlos_lags(df, lag_seconds_list=[2, 5, 8, 11, 14, 17, 20, 23, 26]):\n    """\n    Crea features de lag para las variables de velocidad del viento (VLOS).\n    \n    Args:\n        df: DataFrame con los datos\n        lag_seconds_list: Lista de lags en segundos a crear\n    \n    Returns:\n        DataFrame con las nuevas columnas de lag a√±adidas\n    """\n    # Identificar columnas de velocidad del viento (VLOS)\n    vlos_columns = [col for col in df.columns if \'LAC_VLOS\' in col]\n    \n    print(f"Variables VLOS encontradas: {len(vlos_columns)}")\n    for col in vlos_columns:\n        print(f"  - {col}")\n    \n    # Calcular tiempo de muestreo (dt) asumiendo columna Time\n    if \'Time\' in df.columns:\n        dt = df[\'Time\'].iloc[1] - df[\'Time\'].iloc[0]  # Segundos entre muestras\n        print(f"\\nTiempo de muestreo detectado: {dt:.4f} segundos")\n    else:\n        dt = 0.02  # Default 50Hz\n        print(f"\\nTiempo de muestreo por defecto: {dt} segundos")\n    \n    # Crear lags para cada variable VLOS\n    print(f"\\nCreando {len(lag_seconds_list)} lags para cada variable VLOS...")\n    \n    total_created = 0\n    for vlos_col in vlos_columns:\n        for lag_sec in lag_seconds_list:\n            # Calcular numero de muestras para el lag\n            lag_samples = int(round(lag_sec / dt))\n            \n            # Crear nombre de la nueva columna\n            new_col_name = f"{vlos_col}_lag{lag_sec}s"\n            \n            # Crear la columna con shift\n            df[new_col_name] = df[vlos_col].shift(lag_samples)\n            \n            total_created += 1\n    \n    print(f"Total de features de lag creadas: {total_created}")\n    print(f"Shape del DataFrame: {df.shape}")\n    \n    return df\n\nprint("OK - Funcion create_vlos_lags definida correctamente")',
            '# ============================================================================\n# PASO 3.3b: Definir funci√≥n para crear componentes seno/coseno del yaw error\n# ============================================================================\n\ndef create_yawerror_components(df):\n    """\n    Crea componentes seno y coseno del √°ngulo de yaw error.\n    Esto evita discontinuidades en los l√≠mites angulares y mejora el aprendizaje del modelo.\n    \n    El yaw error es el √°ngulo de desalineaci√≥n entre la g√≥ndola y el viento.\n    Al convertirlo a sin/cos, el modelo puede aprender mejor patrones c√≠clicos.\n    \n    Args:\n        df: DataFrame con los datos\n    \n    Returns:\n        DataFrame con las nuevas columnas sin_yawerror y cos_yawerror\n    """\n    yawerror_col = \'OPER_MEAS_YAWERROR\'\n    \n    if yawerror_col not in df.columns:\n        print(f"ADVERTENCIA: Columna \'{yawerror_col}\' no encontrada")\n        return df\n    \n    print(f"Creando componentes trigonom√©tricas de \'{yawerror_col}\'...")\n    \n    # Verificar rango de valores para determinar unidades\n    max_val = df[yawerror_col].abs().max()\n    min_val = df[yawerror_col].min()\n    \n    # El yaw error t√≠picamente est√° en radianes en el rango [-œÄ, œÄ] o [-180¬∞, 180¬∞]\n    if max_val > 6.5:  # Si es > 2*pi, probablemente en grados\n        print(f"   Rango detectado: [{min_val:.1f}, {max_val:.1f}] (grados)")\n        # Convertir de grados a radianes\n        yawerror_rad = np.deg2rad(df[yawerror_col])\n    else:\n        print(f"   Rango detectado: [{min_val:.3f}, {max_val:.3f}] (radianes)")\n        yawerror_rad = df[yawerror_col]\n    \n    # Crear componentes trigonom√©tricas\n    df[\'sin_yawerror\'] = np.sin(yawerror_rad)\n    df[\'cos_yawerror\'] = np.cos(yawerror_rad)\n    \n    print(f"   ‚úÖ Creadas 2 nuevas columnas: sin_yawerror, cos_yawerror")\n    print(f"   üí° Beneficio: El modelo puede aprender mejor patrones c√≠clicos del yaw error")\n    print(f"   Shape del DataFrame: {df.shape}")\n    \n    return df\n\nprint("‚úÖ Funci√≥n create_yawerror_components definida correctamente")',
            '# ============================================================================\n# PASO 3.3: Definir funcion para crear componentes seno/coseno del azimuth\n# ============================================================================\n\ndef create_azimuth_components(df):\n    """\n    Crea componentes seno y coseno del angulo de azimuth del rotor.\n    Esto evita discontinuidades en 0-360 grados.\n    \n    Args:\n        df: DataFrame con los datos\n    \n    Returns:\n        DataFrame con las nuevas columnas sin(azimuth) y cos(azimuth)\n    """\n    azimuth_col = \'Rotor azimuth angle\'\n    \n    if azimuth_col not in df.columns:\n        print(f"ADVERTENCIA: Columna \'{azimuth_col}\' no encontrada")\n        return df\n    \n    print(f"Creando componentes trigonometricas de \'{azimuth_col}\'...")\n    \n    # Crear componentes (asumiendo que el angulo esta en grados)\n    # Si esta en radianes, no hace falta convertir\n    # Verificar rango de valores para determinar unidades\n    max_val = df[azimuth_col].max()\n    \n    if max_val > 6.5:  # Si es > 2*pi, probablemente en grados\n        print(f"   Rango detectado: 0-{max_val:.1f} (grados)")\n        # Convertir de grados a radianes\n        azimuth_rad = np.deg2rad(df[azimuth_col])\n    else:\n        print(f"   Rango detectado: 0-{max_val:.1f} (radianes)")\n        azimuth_rad = df[azimuth_col]\n    \n    # Crear componentes\n    df[\'sin_rotor_azimuth\'] = np.sin(azimuth_rad)\n    df[\'cos_rotor_azimuth\'] = np.cos(azimuth_rad)\n    \n    print(f"   OK - Creadas 2 nuevas columnas: sin_rotor_azimuth, cos_rotor_azimuth")\n    print(f"   Shape del DataFrame: {df.shape}")\n    \n    return df\n\nprint("OK - Funcion create_azimuth_components definida correctamente")',
            'def lowpass_filter(signal_data, cutoff, fs, order=2):\n    """\n    Aplica un filtro pasa-bajo Butterworth a la se√±al.\n    \n    Args:\n        signal_data (np.array): Se√±al de entrada\n        cutoff (float): Frecuencia de corte en Hz\n        fs (float): Frecuencia de muestreo en Hz\n        order (int): Orden del filtro\n    \n    Returns:\n        np.array: Se√±al filtrada\n    """\n    from scipy import signal as sp_signal\n    \n    nyq = 0.5 * fs\n    normal_cutoff = cutoff / nyq\n    \n    # Asegurar que el valor est√° en el rango v√°lido (0, 1)\n    normal_cutoff = max(0.001, min(normal_cutoff, 0.999))\n    \n    sos = sp_signal.butter(order, normal_cutoff, btype=\'low\', output=\'sos\')\n    filtered_signal = sp_signal.sosfilt(sos, signal_data)\n    \n    return filtered_signal\n\n\ndef lowpass_filter_safe(signal_data, cutoff, fs, order=2):\n    """\n    Versi√≥n segura del filtro pasa-bajo que maneja errores.\n    \n    Args:\n        signal_data (np.array): Se√±al de entrada\n        cutoff (float): Frecuencia de corte en Hz\n        fs (float): Frecuencia de muestreo en Hz\n        order (int): Orden del filtro\n    \n    Returns:\n        np.array: Se√±al filtrada, o se√±al original si falla el filtrado\n    """\n    try:\n        return lowpass_filter(signal_data, cutoff, fs, order)\n    except Exception as e:\n        print(f"      ADVERTENCIA: Fallo en filtrado pasa-bajo ({e}). Usando se√±al sin filtrar.")\n        return signal_data\n\n\ndef create_frequency_components_1P_2P(df, apply_filtering=True):\n    """\n    Crea componentes de frecuencia 0P, 1P y 2P de los momentos flectores.\n    \n    DESCRIPCI√ìN:\n    Esta funci√≥n crea los siguientes targets a partir de los momentos flectores M1(t) y M2(t):\n    \n    1. Se√±ales suma y diferencia:\n       - M_Œ£(t) = (M1(t) + M2(t)) / 2  ‚Üí contiene componentes pares (2P, 4P, ...)\n       - M_Œî(t) = (M1(t) - M2(t)) / 2  ‚Üí contiene componentes impares (1P, 3P, ...)\n    \n    2. Componente 0P (lento/promedio):\n       - M_0(t) = M_Œ£(t)  ‚Üí componente lento\n    \n    3. Componente 1P (proyectado en ejes fijos):\n       - M_1c(t) = M_Œî(t) * cos(œà(t))\n       - M_1s(t) = M_Œî(t) * sin(œà(t))\n       donde œà(t) es el √°ngulo de azimut de la pala 1\n    \n    4. Componente 2P (proyectado en ejes fijos):\n       - M_2c(t) = M_Œ£(t) * cos(2œà(t))\n       - M_2s(t) = M_Œ£(t) * sin(2œà(t))\n    \n    IMPORTANTE: Para que 1P y 2P sean "limpios", se recomienda filtrar:\n       - M_Œî alrededor de 1P antes de proyectar (band-pass)\n       - M_Œ£ alrededor de 2P antes de proyectar (band-pass)\n    \n    Targets de salida: [M_0, M_1c, M_1s, M_2c, M_2s]\n    \n    Args:\n        df (pd.DataFrame): DataFrame con los datos de la simulaci√≥n.\n                          Debe contener al menos:\n                          - \'Time\': tiempo en segundos\n                          - \'Rotor speed\': velocidad del rotor en rpm\n                          - \'Rotor azimuth angle\': √°ngulo de azimut del rotor (pala 1)\n                          - \'Blade root 1 My\': momento flector pala 1\n                          - \'Blade root 2 My\': momento flector pala 2\n        apply_filtering (bool): Si True, aplica filtrado pasa-banda antes de proyectar.\n                               Default: True\n    \n    Returns:\n        pd.DataFrame: DataFrame con las nuevas columnas:\n                     - \'M_0\' (0P): componente lento\n                     - \'M_1c\' (1P coseno): componente 1P en fase\n                     - \'M_1s\' (1P seno): componente 1P en cuadratura\n                     - \'M_2c\' (2P coseno): componente 2P en fase\n                     - \'M_2s\' (2P seno): componente 2P en cuadratura\n    \n    Raises:\n        ValueError: Si faltan columnas requeridas en el DataFrame.\n    """\n    \n    # Validar columnas requeridas\n    required_cols = [\'Time\', \'Rotor speed\', \'Rotor azimuth angle\', \n                     \'Blade root 1 My\', \'Blade root 2 My\']\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    \n    if missing_cols:\n        raise ValueError(f"Columnas faltantes en el DataFrame: {missing_cols}")\n    \n    print("=" * 70)\n    print("Creando componentes de frecuencia 0P, 1P y 2P...")\n    print("=" * 70)\n    \n    # =========================================================================\n    # PASO 1: Obtener par√°metros b√°sicos\n    # =========================================================================\n    M1 = df[\'Blade root 1 My\'].values\n    M2 = df[\'Blade root 2 My\'].values\n    time = df[\'Time\'].values\n    azimuth = df[\'Rotor azimuth angle\'].values\n    rotor_speed_rpm = df[\'Rotor speed\'].values\n    \n    # Convertir azimut a radianes si est√° en grados\n    if azimuth.max() > 6.5:\n        azimuth_rad = np.deg2rad(azimuth)\n        print("   Azimut convertido de grados a radianes")\n    else:\n        azimuth_rad = azimuth\n        print("   Azimut ya est√° en radianes")\n    \n    # Calcular frecuencias\n    freq_1P_Hz = rotor_speed_rpm / (2 * np.pi)  # Convertir rad/s a Hz\n    freq_2P_Hz = 2 * freq_1P_Hz\n    freq_1P_mean = freq_1P_Hz.mean()\n    freq_2P_mean = freq_2P_Hz.mean()\n    \n    # Calcular frecuencia de muestreo\n    if len(df) > 1:\n        dt = time[1] - time[0]\n        fs = 1.0 / dt\n    else:\n        dt = 0.02\n        fs = 50.0\n    \n    print(f"\\n   Par√°metros:")\n    print(f"   - Rotor Speed promedio: {rotor_speed_rpm.mean():.2f} rpm")\n    print(f"   - Frecuencia 1P promedio: {freq_1P_mean:.3f} Hz")\n    print(f"   - Frecuencia 2P promedio: {freq_2P_mean:.3f} Hz")\n    print(f"   - Frecuencia de muestreo: {fs:.1f} Hz")\n    print(f"   - N√∫mero de muestras: {len(df)}")\n    \n    # =========================================================================\n    # PASO 2: Calcular se√±ales suma (Œ£) y diferencia (Œî)\n    # =========================================================================\n    print(f"\\n   Calculando M_Œ£ y M_Œî...")\n    \n    M_sum = (M1 + M2) / 2.0  # M_Œ£: contiene componentes pares (2P, 4P, ...)\n    M_diff = (M1 - M2) / 2.0  # M_Œî: contiene componentes impares (1P, 3P, ...)\n    \n    print(f"   - M_Œ£ (suma) calculada: rango [{M_sum.min():.2f}, {M_sum.max():.2f}]")\n    print(f"   - M_Œî (diferencia) calculada: rango [{M_diff.min():.2f}, {M_diff.max():.2f}]")\n    \n    # =========================================================================\n    # PASO 3: Aplicar filtrado pasa-banda (opcional pero recomendado)\n    # =========================================================================\n    if apply_filtering:\n        print(f"\\n   Aplicando filtrado pasa-banda...")\n        \n        # Filtrar M_Œî alrededor de 1P\n        bandwidth_1P = 0.3  # Ancho de banda en Hz alrededor de 1P\n        lowcut_1P = max(0.01, freq_1P_mean - bandwidth_1P)\n        highcut_1P = min(fs/2 - 0.1, freq_1P_mean + bandwidth_1P)\n        \n        print(f"   - Filtrando M_Œî alrededor de 1P: [{lowcut_1P:.3f}, {highcut_1P:.3f}] Hz")\n        M_diff_filtered = bandpass_filter_safe(M_diff, lowcut_1P, highcut_1P, fs, order=2)\n        \n        # Filtrar M_Œ£ alrededor de 2P\n        bandwidth_2P = 0.5  # Ancho de banda en Hz alrededor de 2P\n        lowcut_2P = max(0.01, freq_2P_mean - bandwidth_2P)\n        highcut_2P = min(fs/2 - 0.1, freq_2P_mean + bandwidth_2P)\n        \n        print(f"   - Filtrando M_Œ£ alrededor de 2P: [{lowcut_2P:.3f}, {highcut_2P:.3f}] Hz")\n        M_sum_filtered = bandpass_filter_safe(M_sum, lowcut_2P, highcut_2P, fs, order=2)\n    else:\n        print(f"\\n   Sin filtrado (apply_filtering=False)")\n        M_diff_filtered = M_diff\n        M_sum_filtered = M_sum\n    \n    # =========================================================================\n    # PASO 4: Crear componentes 0P, 1P y 2P\n    # =========================================================================\n    print(f"\\n   Creando componentes de frecuencia...")\n    \n    # 0P: Componente DC (eliminar frecuencias pares 2P, 4P, ...)\n    if apply_filtering:\n        # Filtro pasa-bajo para quedarse solo con componente DC\n        # Corte por debajo de 1P para eliminar 2P, 4P, etc.\n        cutoff_0P = freq_1P_mean * 5 # Cortar a la mitad de 1P\n        print(f"   - Filtrando M_0 (pasa-bajo) con corte en {cutoff_0P:.3f} Hz")\n        M_0 = lowpass_filter_safe(M_sum, cutoff_0P, fs, order=2)\n        print(f"   - M_0 (0P): componente DC creado (sin 2P, 4P, ...)")\n    else:\n        M_0 = M_sum  # Sin filtrar\n        print(f"   - M_0 (0P): componente lento creado (sin filtrar)")\n    \n    # 1P: Proyecci√≥n de M_Œî en ejes fijos usando azimut\n    M_1c = M_diff_filtered * np.cos(azimuth_rad)  # Componente 1P en fase (coseno)\n    M_1s = M_diff_filtered * np.sin(azimuth_rad)  # Componente 1P en cuadratura (seno)\n    print(f"   - M_1c, M_1s (1P): componentes creadas con proyecci√≥n en ejes fijos")\n    \n    # 2P: Proyecci√≥n de M_Œ£ en ejes fijos usando 2*azimut\n    M_2c = M_sum_filtered * np.cos(2 * azimuth_rad)  # Componente 2P en fase (coseno)\n    M_2s = M_sum_filtered * np.sin(2 * azimuth_rad)  # Componente 2P en cuadratura (seno)\n    print(f"   - M_2c, M_2s (2P): componentes creadas con proyecci√≥n en ejes fijos")\n    \n    # =========================================================================\n    # PASO 5: Agregar al DataFrame\n    # =========================================================================\n    print(f"\\n   Agregando columnas al DataFrame...")\n    \n    df[\'M_0\'] = M_0      # 0P\n    df[\'M_1c\'] = M_1c    # 1P coseno\n    df[\'M_1s\'] = M_1s    # 1P seno\n    df[\'M_2c\'] = M_2c    # 2P coseno\n    df[\'M_2s\'] = M_2s    # 2P seno\n    \n    new_columns = [\'M_0\', \'M_1c\', \'M_1s\', \'M_2c\', \'M_2s\']\n    print(f"   - Columnas creadas: {new_columns}")\n    \n    # =========================================================================\n    # PASO 6: Resumen final\n    # =========================================================================\n    print(f"\\n" + "=" * 70)\n    print(f"RESUMEN:")\n    print(f"=" * 70)\n    print(f"   Vector de salida: y(t) = [M_0, M_1c, M_1s, M_2c, M_2s]")\n    print(f"   - M_0:  componente 0P (lento)")\n    print(f"   - M_1c: componente 1P en fase (coseno)")\n    print(f"   - M_1s: componente 1P en cuadratura (seno)")\n    print(f"   - M_2c: componente 2P en fase (coseno)")\n    print(f"   - M_2s: componente 2P en cuadratura (seno)")\n    print(f"\\n   Shape final del DataFrame: {df.shape}")\n    print(f"=" * 70)\n    \n    return df\n\n\ndef bandpass_filter(signal_data, lowcut, highcut, fs, order=2):\n    """\n    Aplica un filtro pasa-banda Butterworth a la se√±al.\n    \n    Args:\n        signal_data (np.array): Se√±al de entrada\n        lowcut (float): Frecuencia de corte inferior en Hz\n        highcut (float): Frecuencia de corte superior en Hz\n        fs (float): Frecuencia de muestreo en Hz\n        order (int): Orden del filtro\n    \n    Returns:\n        np.array: Se√±al filtrada\n    """\n    from scipy import signal as sp_signal\n    \n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    \n    # Asegurar que los valores est√°n en el rango v√°lido (0, 1)\n    low = max(0.001, min(low, 0.999))\n    high = max(low + 0.001, min(high, 0.999))\n    \n    sos = sp_signal.butter(order, [low, high], btype=\'band\', output=\'sos\')\n    filtered_signal = sp_signal.sosfilt(sos, signal_data)\n    \n    return filtered_signal\n\n\ndef bandpass_filter_safe(signal_data, lowcut, highcut, fs, order=2):\n    """\n    Versi√≥n segura del filtro pasa-banda que maneja errores.\n    \n    Args:\n        signal_data (np.array): Se√±al de entrada\n        lowcut (float): Frecuencia de corte inferior en Hz\n        highcut (float): Frecuencia de corte superior en Hz\n        fs (float): Frecuencia de muestreo en Hz\n        order (int): Orden del filtro\n    \n    Returns:\n        np.array: Se√±al filtrada, o se√±al original si falla el filtrado\n    """\n    try:\n        return bandpass_filter(signal_data, lowcut, highcut, fs, order=2)\n    except Exception as e:\n        print(f"      ADVERTENCIA: Fallo en filtrado ({e}). Usando se√±al sin filtrar.")\n        return signal_data',
            'def create_stationary_hub_components(df, apply_filtering=True):\n    """\n    Crea componentes 0P, 1P y 2P a partir de se√±ales Stationary hub My y Mz.\n    Se√±ales ya est√°n en ejes fijos ‚Üí no se requiere proyecci√≥n con azimut.\n    """\n\n    required_cols = [\'Time\', \'Rotor speed\', \'Stationary hub My\', \'Stationary hub Mz\']\n    missing = [c for c in required_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f"Faltan columnas: {missing}")\n\n    My = df[\'Stationary hub My\'].values\n    Mz = df[\'Stationary hub Mz\'].values\n    time = df[\'Time\'].values\n    rotor_speed_rpm = df[\'Rotor speed\'].values\n\n    # Frecuencias\n    freq_1P = (rotor_speed_rpm / 60.0).mean()\n    freq_2P = 2 * freq_1P\n\n    # Frecuencia de muestreo\n    if len(df) > 1:\n        dt = time[1] - time[0]\n        fs = 1.0 / dt\n    else:\n        fs = 50.0\n\n    # --- 0P (pasa-bajo) ---\n    if apply_filtering:\n        cutoff_0P = freq_1P * 0.5   # corte por debajo de 1P\n        My_0P = lowpass_filter_safe(My, cutoff_0P, fs, order=2)\n        Mz_0P = lowpass_filter_safe(Mz, cutoff_0P, fs, order=2)\n    else:\n        My_0P = My\n        Mz_0P = Mz\n\n    # --- 1P (pasa-banda alrededor de 1P) ---\n    bandwidth_1P = 0.3\n    low_1P = max(0.01, freq_1P - bandwidth_1P)\n    high_1P = min(fs/2 - 0.1, freq_1P + bandwidth_1P)\n\n    My_1P = bandpass_filter_safe(My, low_1P, high_1P, fs, order=2)\n    Mz_1P = bandpass_filter_safe(Mz, low_1P, high_1P, fs, order=2)\n\n    # --- 2P (pasa-banda alrededor de 2P) ---\n    bandwidth_2P = 0.5\n    low_2P = max(0.01, freq_2P - bandwidth_2P)\n    high_2P = min(fs/2 - 0.1, freq_2P + bandwidth_2P)\n\n    My_2P = bandpass_filter_safe(My, low_2P, high_2P, fs, order=2)\n    Mz_2P = bandpass_filter_safe(Mz, low_2P, high_2P, fs, order=2)\n\n    # Guardar en df\n    df[\'HubMy_0P\'] = My_0P\n    df[\'HubMy_1P\'] = My_1P\n    df[\'HubMy_2P\'] = My_2P\n\n    df[\'HubMz_0P\'] = Mz_0P\n    df[\'HubMz_1P\'] = Mz_1P\n    df[\'HubMz_2P\'] = Mz_2P\n\n    return df\n',
            'def create_pitch_coleman_features(df):\n    """\n    Crea features de pitch en marco coherente (Coleman transformation).\n    \n    DESCRIPCI√ìN:\n    Esta funci√≥n transforma los √°ngulos de pitch de las palas individuales a un marco\n    de referencia fijo (no rotatorio) usando la transformaci√≥n de Coleman:\n    \n    1. Componentes colectivo y diferencial:\n       - Œ∏_0(t) = (Œ∏_1(t) + Œ∏_2(t)) / 2  ‚Üí colectivo (promedio)\n       - Œ∏_Œî(t) = (Œ∏_1(t) - Œ∏_2(t)) / 2  ‚Üí diferencial\n    \n    2. Proyecci√≥n del diferencial a ejes fijos (1P):\n       - Œ∏_1c(t) = Œ∏_Œî(t) * cos(œà(t))\n       - Œ∏_1s(t) = Œ∏_Œî(t) * sin(œà(t))\n       donde œà(t) es el √°ngulo de azimut\n    \n    3. Rates (derivadas temporales):\n       - Œ∏Ãá_0(t) ‚âà (Œ∏_0(t) - Œ∏_0(t-Œît)) / Œît\n       - Œ∏Ãá_1c(t), Œ∏Ãá_1s(t) de forma similar\n    \n    4. Rotor speed rate:\n       - Œ©Ãá(t) ‚âà (Œ©(t) - Œ©(t-Œît)) / Œît\n    \n    IMPORTANTE: Estos features son coherentes con targets en Coleman (M_0, M_1c, M_1s).\n    \n    Args:\n        df (pd.DataFrame): DataFrame con los datos. Debe contener:\n                          - \'Time\': tiempo en segundos\n                          - \'Blade 1 pitch angle\': pitch pala 1 (grados)\n                          - \'Blade 2 pitch angle\': pitch pala 2 (grados)\n                          - \'Rotor azimuth angle\': √°ngulo de azimut\n                          - \'Rotor speed\': velocidad del rotor (rpm)\n    \n    Returns:\n        pd.DataFrame: DataFrame con las nuevas columnas:\n                     - \'pitch_0\': componente colectivo Œ∏_0\n                     - \'pitch_1c\': componente 1P coseno Œ∏_1c\n                     - \'pitch_1s\': componente 1P seno Œ∏_1s\n                     - \'pitch_0_rate\': derivada temporal de Œ∏_0\n                     - \'pitch_1c_rate\': derivada temporal de Œ∏_1c\n                     - \'pitch_1s_rate\': derivada temporal de Œ∏_1s\n                     - \'rotor_speed_rate\': derivada temporal de Œ©\n    \n    Raises:\n        ValueError: Si faltan columnas requeridas.\n    """\n    \n    # Validar columnas requeridas\n    required_cols = [\'Time\', \'Blade 1 pitch angle\', \'Blade 2 pitch angle\', \n                     \'Rotor azimuth angle\', \'Rotor speed\']\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    \n    if missing_cols:\n        raise ValueError(f"Columnas faltantes para pitch Coleman: {missing_cols}")\n    \n    print("=" * 70)\n    print("Creando features de pitch en marco Coleman...")\n    print("=" * 70)\n    \n    # =========================================================================\n    # PASO 1: Obtener datos b√°sicos\n    # =========================================================================\n    theta_1 = df[\'Blade 1 pitch angle\'].values\n    theta_2 = df[\'Blade 2 pitch angle\'].values\n    time = df[\'Time\'].values\n    azimuth = df[\'Rotor azimuth angle\'].values\n    rotor_speed = df[\'Rotor speed\'].values\n    \n    # Convertir azimut a radianes si est√° en grados\n    if azimuth.max() > 6.5:\n        azimuth_rad = np.deg2rad(azimuth)\n        print("   Azimut convertido de grados a radianes")\n    else:\n        azimuth_rad = azimuth\n        print("   Azimut ya est√° en radianes")\n    \n    # Calcular dt (paso de tiempo)\n    if len(time) > 1:\n        dt = time[1] - time[0]\n    else:\n        dt = 0.1  # default\n    \n    print(f"\\n   Par√°metros:")\n    print(f"   - N√∫mero de muestras: {len(df)}")\n    print(f"   - Œît (paso temporal): {dt:.4f} s")\n    print(f"   - Frecuencia de muestreo: {1.0/dt:.1f} Hz")\n    \n    # =========================================================================\n    # PASO 2: Transformaci√≥n Coleman - Colectivo y Diferencial\n    # =========================================================================\n    print(f"\\n   [1/3] Calculando componentes colectivo y diferencial...")\n    \n    # Œ∏_0: componente colectivo (promedio)\n    theta_0 = (theta_1 + theta_2) / 2.0\n    \n    # Œ∏_Œî: componente diferencial\n    theta_delta = (theta_1 - theta_2) / 2.0\n    \n    print(f"   - Œ∏_0 (colectivo): rango [{theta_0.min():.2f}, {theta_0.max():.2f}]¬∞")\n    print(f"   - Œ∏_Œî (diferencial): rango [{theta_delta.min():.2f}, {theta_delta.max():.2f}]¬∞")\n    \n    # =========================================================================\n    # PASO 3: Proyecci√≥n del diferencial a ejes fijos (1P)\n    # =========================================================================\n    print(f"\\n   [2/3] Proyectando Œ∏_Œî a ejes fijos (1P)...")\n    \n    # Œ∏_1c: componente 1P en fase (coseno)\n    theta_1c = theta_delta * np.cos(azimuth_rad)\n    \n    # Œ∏_1s: componente 1P en cuadratura (seno)\n    theta_1s = theta_delta * np.sin(azimuth_rad)\n    \n    print(f"   - Œ∏_1c (1P coseno): rango [{theta_1c.min():.2f}, {theta_1c.max():.2f}]¬∞")\n    print(f"   - Œ∏_1s (1P seno): rango [{theta_1s.min():.2f}, {theta_1s.max():.2f}]¬∞")\n    \n    # =========================================================================\n    # PASO 4: Calcular rates (derivadas temporales)\n    # =========================================================================\n    print(f"\\n   [3/3] Calculando rates (derivadas temporales)...")\n    \n    # Derivadas usando diferencias finitas hacia atr√°s\n    # rate(t) ‚âà (value(t) - value(t-Œît)) / Œît\n    \n    # Œ∏Ãá_0: rate del colectivo\n    theta_0_rate = np.zeros_like(theta_0)\n    theta_0_rate[1:] = (theta_0[1:] - theta_0[:-1]) / dt\n    theta_0_rate[0] = theta_0_rate[1]  # primera muestra = segunda\n    \n    # Œ∏Ãá_1c: rate de 1P coseno\n    theta_1c_rate = np.zeros_like(theta_1c)\n    theta_1c_rate[1:] = (theta_1c[1:] - theta_1c[:-1]) / dt\n    theta_1c_rate[0] = theta_1c_rate[1]\n    \n    # Œ∏Ãá_1s: rate de 1P seno\n    theta_1s_rate = np.zeros_like(theta_1s)\n    theta_1s_rate[1:] = (theta_1s[1:] - theta_1s[:-1]) / dt\n    theta_1s_rate[0] = theta_1s_rate[1]\n    \n    # Œ©Ãá: rate de rotor speed\n    rotor_speed_rate = np.zeros_like(rotor_speed)\n    rotor_speed_rate[1:] = (rotor_speed[1:] - rotor_speed[:-1]) / dt\n    rotor_speed_rate[0] = rotor_speed_rate[1]\n    \n    print(f"   - Œ∏Ãá_0 rate: rango [{theta_0_rate.min():.2f}, {theta_0_rate.max():.2f}] ¬∞/s")\n    print(f"   - Œ∏Ãá_1c rate: rango [{theta_1c_rate.min():.2f}, {theta_1c_rate.max():.2f}] ¬∞/s")\n    print(f"   - Œ∏Ãá_1s rate: rango [{theta_1s_rate.min():.2f}, {theta_1s_rate.max():.2f}] ¬∞/s")\n    print(f"   - Œ©Ãá rate: rango [{rotor_speed_rate.min():.2f}, {rotor_speed_rate.max():.2f}] rpm/s")\n    \n    # =========================================================================\n    # PASO 5: Agregar al DataFrame\n    # =========================================================================\n    print(f"\\n   Agregando columnas al DataFrame...")\n    \n    df[\'pitch_0\'] = theta_0\n    df[\'pitch_1c\'] = theta_1c\n    df[\'pitch_1s\'] = theta_1s\n    df[\'pitch_0_rate\'] = theta_0_rate\n    df[\'pitch_1c_rate\'] = theta_1c_rate\n    df[\'pitch_1s_rate\'] = theta_1s_rate\n    df[\'rotor_speed_rate\'] = rotor_speed_rate\n    \n    new_columns = [\'pitch_0\', \'pitch_1c\', \'pitch_1s\', \n                   \'pitch_0_rate\', \'pitch_1c_rate\', \'pitch_1s_rate\',\n                   \'rotor_speed_rate\']\n    \n    print(f"   - Columnas creadas: {len(new_columns)}")\n    \n    # =========================================================================\n    # PASO 6: Resumen final\n    # =========================================================================\n    print(f"\\n" + "=" * 70)\n    print(f"RESUMEN:")\n    print(f"=" * 70)\n    print(f"   Features Coleman de pitch creados:")\n    print(f"   - pitch_0:  colectivo Œ∏_0 = (Œ∏_1 + Œ∏_2)/2")\n    print(f"   - pitch_1c: 1P coseno Œ∏_1c = Œ∏_Œî¬∑cos(œà)")\n    print(f"   - pitch_1s: 1P seno Œ∏_1s = Œ∏_Œî¬∑sin(œà)")\n    print(f"\\n   Rates (derivadas temporales):")\n    print(f"   - pitch_0_rate:  Œ∏Ãá_0")\n    print(f"   - pitch_1c_rate: Œ∏Ãá_1c")\n    print(f"   - pitch_1s_rate: Œ∏Ãá_1s")\n    print(f"   - rotor_speed_rate: Œ©Ãá")\n    print(f"\\n   üí° Estos features son coherentes con targets Coleman (M_0, M_1c, M_1s)")\n    print(f"   üí° Los rates capturan din√°mica ‚Üí mejoran predicci√≥n de componentes 1P")\n    print(f"\\n   Shape final del DataFrame: {df.shape}")\n    print(f"=" * 70)\n    \n    return df\n\nprint("OK - Funcion create_pitch_coleman_features definida correctamente")',
            '"""\nM√≥dulo para procesamiento de estad√≠sticas del campo de viento LIDAR con ROTACI√ìN configurable.\n\nEste m√≥dulo permite ROTAR la configuraci√≥n de beams para alinear correctamente\nel sistema de coordenadas del LIDAR con el sistema del rotor.\n\nC√ìMO USAR:\n- Cambia el par√°metro ROTATION_OFFSET para rotar todos los beams\n- ROTATION_OFFSET = 0  ‚Üí Sin rotaci√≥n (configuraci√≥n original)\n- ROTATION_OFFSET = 1  ‚Üí Rota 45¬∞ en sentido horario (beam 1 pasa a ser "arriba")\n- ROTATION_OFFSET = 2  ‚Üí Rota 90¬∞ en sentido horario (beam 2 pasa a ser "arriba")\n- ROTATION_OFFSET = -1 ‚Üí Rota 45¬∞ en sentido antihorario (beam 7 pasa a ser "arriba")\n"""\n\n\n# =============================================================================\n# PAR√ÅMETRO DE ROTACI√ìN - CAMBIA ESTE VALOR PARA TESTEAR\n# =============================================================================\n# ROTATION_OFFSET = 2  # üëà CAMBIA ESTE VALOR para rotar los beams\n                     # Cada unidad = 45¬∞ de rotaci√≥n\n\n\ndef create_wind_field_statistics(df, rotation_offset=None):\n    """\n    Crea estad√≠sticas del campo de viento LIDAR con configuraci√≥n ROTABLE de beams.\n    \n    PAR√ÅMETROS DE ROTACI√ìN:\n    - rotation_offset: N√∫mero de posiciones a rotar (None usa ROTATION_OFFSET global)\n                      +1 = rotar 45¬∞ CW, -1 = rotar 45¬∞ CCW\n    \n    DESCRIPCI√ìN:\n    Esta funci√≥n calcula caracter√≠sticas agregadas del campo de viento medido por el LIDAR\n    que capturan:\n    1. Intensidad del viento (media)\n    2. Turbulencia/heterogeneidad (desviaci√≥n est√°ndar)\n    3. Shear vertical (gradiente arriba-abajo)\n    4. Gradiente horizontal (diferencia izquierda-derecha, relacionado con yaw misalignment)\n    \n    F√ìRMULAS:\n    - U_mean = mean(VLOS de todos los BEAMs v√°lidos)  ‚Üí ayuda a predecir M_0\n    - U_std = std(VLOS de todos los BEAMs v√°lidos)    ‚Üí captura turbulencia/heterogeneidad\n    - U_shear_vert = mean(BEAMs arriba) - mean(BEAMs abajo)  ‚Üí shear vertical\n    - U_shear_horiz = mean(BEAMs izquierda) - mean(BEAMs derecha)  ‚Üí gradiente lateral\n    \n    CONFIGURACI√ìN BASE (antes de rotar):\n                    0¬∞ (‚Üë)\n                    BEAM 0\n                     |\n        315¬∞ BEAM 7  |  45¬∞ BEAM 1\n               ‚ï≤     |     ‚ï±\n                ‚ï≤    |    ‚ï±\n        270¬∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ+‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 90¬∞\n        BEAM 6       |       BEAM 2\n                ‚ï±    |    ‚ï≤\n               ‚ï±     |     ‚ï≤\n        225¬∞ BEAM 5  |  135¬∞ BEAM 3\n                     |\n                  BEAM 4\n                 180¬∞ (‚Üì)\n    \n    Args:\n        df (pd.DataFrame): DataFrame con columnas LAC_VLOS de diferentes BEAMs.\n        rotation_offset (int): Offset de rotaci√≥n (None usa valor global)\n    \n    Returns:\n        pd.DataFrame: DataFrame con las nuevas columnas:\n                     - \'U_mean\': velocidad media del campo de viento\n                     - \'U_std\': desviaci√≥n est√°ndar (heterogeneidad)\n                     - \'U_shear_vert\': shear vertical (arriba - abajo)\n                     - \'U_shear_horiz\': gradiente horizontal (izquierda - derecha)\n    \n    Raises:\n        ValueError: Si no se encuentran columnas VLOS en el DataFrame.\n    """\n    \n    # Usar valor global si no se especifica\n    if rotation_offset is None:\n        rotation_offset = ROTATION_OFFSET\n    \n    print("=" * 70)\n    print("Creando estad√≠sticas del campo de viento LIDAR...")\n    print("=" * 70)\n    print(f"üîÑ ROTACI√ìN APLICADA: {rotation_offset} posiciones ({rotation_offset * 45}¬∞)")\n    \n    # =========================================================================\n    # PASO 1: Identificar columnas VLOS (sin lag) y FILTRAR BEAMS VAC√çOS\n    # =========================================================================\n    print("\\n   [1/4] Identificando y filtrando columnas VLOS...")\n    \n    # Buscar todas las columnas que contengan \'LAC_VLOS\' pero NO \'lag\'\n    vlos_cols = [col for col in df.columns if \'LAC_VLOS\' in col and \'lag\' not in col.lower()]\n    \n    if len(vlos_cols) == 0:\n        raise ValueError("No se encontraron columnas LAC_VLOS en el DataFrame")\n    \n    print(f"   - Columnas VLOS encontradas: {len(vlos_cols)}")\n    \n    # Extraer n√∫meros de BEAM y FILTRAR por porcentaje de NaNs\n    beam_numbers = []\n    beam_to_col = {}\n    \n    for col in vlos_cols:\n        match = re.search(r\'BEAM(\\d+)\', col)\n        if match:\n            beam_num = int(match.group(1))\n            \n            # FILTRADO: Calcular porcentaje de NaNs\n            nan_percentage = df[col].isna().sum() / len(df) * 100\n            \n            if nan_percentage > 90:\n                # Beam vac√≠o ‚Üí ignorar\n                print(f"   ‚ö†Ô∏è  BEAM {beam_num} ignorado ({nan_percentage:.1f}% NaNs)")\n                continue\n            else:\n                # Beam v√°lido ‚Üí incluir\n                beam_numbers.append(beam_num)\n                beam_to_col[beam_num] = col\n                print(f"   ‚úì  BEAM {beam_num} v√°lido ({nan_percentage:.1f}% NaNs)")\n    \n    beam_numbers = sorted(set(beam_numbers))\n    print(f"\\n   ‚Üí BEAMs v√°lidos detectados: {beam_numbers}")\n    \n    # =========================================================================\n    # PASO 2: Configuraci√≥n de posiciones de BEAMs CON ROTACI√ìN\n    # =========================================================================\n    print("\\n   [2/4] Configurando posiciones de BEAMs...")\n    \n    num_beams = len(beam_numbers)\n    \n    # CONFIGURACI√ìN ESPEC√çFICA PARA 8 BEAMS (0-7) CON ROTACI√ìN\n    if num_beams == 8:\n        print(f"   ‚Üí Configuraci√≥n de 8 BEAMs (distribuidos cada 45¬∞):")\n        \n        # CONFIGURACI√ìN BASE (sin rotar)\n        # Arriba: 0¬∞, 45¬∞, 315¬∞ ‚Üí beams [0, 1, 7]\n        # Abajo: 135¬∞, 180¬∞, 225¬∞ ‚Üí beams [3, 4, 5]\n        # Izquierda: 225¬∞, 270¬∞, 315¬∞ ‚Üí beams [5, 6, 7]\n        # Derecha: 45¬∞, 90¬∞, 135¬∞ ‚Üí beams [1, 2, 3]\n        \n        # APLICAR ROTACI√ìN: cada beam se mueve rotation_offset posiciones\n        def rotate_beam(beam_num, offset):\n            """Rota un beam aplicando m√≥dulo 8"""\n            return (beam_num + offset) % 8\n        \n        # Rotar cada grupo\n        beams_up_base = [0, 1, 7]\n        beams_down_base = [3, 4, 5]\n        beams_left_base = [5, 6, 7]\n        beams_right_base = [1, 2, 3]\n        \n        beams_up = [rotate_beam(b, rotation_offset) for b in beams_up_base]\n        beams_down = [rotate_beam(b, rotation_offset) for b in beams_down_base]\n        beams_left = [rotate_beam(b, rotation_offset) for b in beams_left_base]\n        beams_right = [rotate_beam(b, rotation_offset) for b in beams_right_base]\n        \n        # Calcular √°ngulos correspondientes\n        angles_up = [(b * 45) % 360 for b in beams_up]\n        angles_down = [(b * 45) % 360 for b in beams_down]\n        angles_left = [(b * 45) % 360 for b in beams_left]\n        angles_right = [(b * 45) % 360 for b in beams_right]\n        \n        print(f"      Arriba (‚Üë):    {beams_up}    ‚Üí √°ngulos: {angles_up}")\n        print(f"      Abajo (‚Üì):     {beams_down}    ‚Üí √°ngulos: {angles_down}")\n        print(f"      Izquierda (‚Üê): {beams_left}    ‚Üí √°ngulos: {angles_left}")\n        print(f"      Derecha (‚Üí):   {beams_right}    ‚Üí √°ngulos: {angles_right}")\n        \n    else:\n        # CONFIGURACI√ìN GEN√âRICA para otros n√∫meros de beams\n        print(f"   ‚Üí Configuraci√≥n gen√©rica para {num_beams} BEAMs:")\n        \n        if num_beams >= 4:\n            # Dividir en cuadrantes aproximadamente\n            quarter = num_beams // 4\n            \n            beams_up = beam_numbers[:quarter + 1]\n            beams_right = beam_numbers[quarter:2*quarter + 1]\n            beams_down = beam_numbers[2*quarter:3*quarter + 1]\n            beams_left = beam_numbers[3*quarter:] + beam_numbers[:1]\n            \n        else:\n            # Si hay muy pocos BEAMs, usar todos para cada c√°lculo\n            print("   ‚ö†Ô∏è  Pocos BEAMs detectados. Usando configuraci√≥n simplificada.")\n            beams_up = beam_numbers[:len(beam_numbers)//2]\n            beams_down = beam_numbers[len(beam_numbers)//2:]\n            beams_left = beam_numbers[:len(beam_numbers)//2]\n            beams_right = beam_numbers[len(beam_numbers)//2:]\n        \n        print(f"      Arriba (‚Üë):    {beams_up}")\n        print(f"      Derecha (‚Üí):   {beams_right}")\n        print(f"      Abajo (‚Üì):     {beams_down}")\n        print(f"      Izquierda (‚Üê): {beams_left}")\n    \n    # =========================================================================\n    # PASO 3: Calcular estad√≠sticas del campo de viento\n    # =========================================================================\n    print("\\n   [3/4] Calculando estad√≠sticas...")\n    \n    # Crear DataFrame solo con columnas VLOS v√°lidas para c√°lculos eficientes\n    vlos_data = df[[beam_to_col[b] for b in beam_numbers]]\n    \n    # --- 3.1: U_mean (media del campo de viento) ---\n    U_mean = vlos_data.mean(axis=1)\n    print(f"   - U_mean: rango [{U_mean.min():.2f}, {U_mean.max():.2f}] m/s")\n    \n    # --- 3.2: U_std (heterogeneidad/turbulencia) ---\n    U_std = vlos_data.std(axis=1)\n    print(f"   - U_std: rango [{U_std.min():.2f}, {U_std.max():.2f}] m/s")\n    \n    # --- 3.3: U_shear_vert (shear vertical: arriba - abajo) ---\n    if len(beams_up) > 0 and len(beams_down) > 0:\n        cols_up = [beam_to_col[b] for b in beams_up if b in beam_to_col]\n        cols_down = [beam_to_col[b] for b in beams_down if b in beam_to_col]\n        \n        U_up = df[cols_up].mean(axis=1)\n        U_down = df[cols_down].mean(axis=1)\n        U_shear_vert = U_up - U_down\n        \n        print(f"   - U_shear_vert: rango [{U_shear_vert.min():.2f}, {U_shear_vert.max():.2f}] m/s")\n        print(f"                   media: {U_shear_vert.mean():.3f} m/s")\n    else:\n        U_shear_vert = pd.Series(0.0, index=df.index)\n        print(f"   - U_shear_vert: no se pudo calcular (BEAMs insuficientes)")\n    \n    # --- 3.4: U_shear_horiz (gradiente horizontal: izquierda - derecha) ---\n    if len(beams_left) > 0 and len(beams_right) > 0:\n        cols_left = [beam_to_col[b] for b in beams_left if b in beam_to_col]\n        cols_right = [beam_to_col[b] for b in beams_right if b in beam_to_col]\n        \n        U_left = df[cols_left].mean(axis=1)\n        U_right = df[cols_right].mean(axis=1)\n        U_shear_horiz = U_left - U_right\n        \n        print(f"   - U_shear_horiz: rango [{U_shear_horiz.min():.2f}, {U_shear_horiz.max():.2f}] m/s")\n        print(f"                    media: {U_shear_horiz.mean():.3f} m/s")\n    else:\n        U_shear_horiz = pd.Series(0.0, index=df.index)\n        print(f"   - U_shear_horiz: no se pudo calcular (BEAMs insuficientes)")\n    \n    # =========================================================================\n    # PASO 4: Agregar al DataFrame\n    # =========================================================================\n    print("\\n   [4/4] Agregando columnas al DataFrame...")\n    \n    df[\'U_mean\'] = U_mean\n    df[\'U_std\'] = U_std\n    df[\'U_shear_vert\'] = U_shear_vert\n    df[\'U_shear_horiz\'] = U_shear_horiz\n    \n    new_columns = [\'U_mean\', \'U_std\', \'U_shear_vert\', \'U_shear_horiz\']\n    \n    # =========================================================================\n    # PASO 5: Resumen final\n    # =========================================================================\n    print(f"\\n" + "=" * 70)\n    print(f"RESUMEN:")\n    print(f"=" * 70)\n    print(f"   Estad√≠sticas del campo de viento creadas:")\n    print(f"   - U_mean:        velocidad media ‚Üí predice M_0")\n    print(f"   - U_std:         heterogeneidad/turbulencia")\n    print(f"   - U_shear_vert:  shear vertical (‚Üë - ‚Üì)")\n    print(f"   - U_shear_horiz: gradiente lateral (‚Üê - ‚Üí)")\n    print(f"\\n   üí° Estas variables capturan la estructura espacial del viento")\n    print(f"   üí° U_shear_vert y U_shear_horiz ayudan a predecir componentes 1P")\n    print(f"   üí° BEAMs vac√≠os (>90% NaN) fueron filtrados autom√°ticamente")\n    print(f"   üîÑ Rotaci√≥n aplicada: {rotation_offset} √ó 45¬∞ = {rotation_offset * 45}¬∞")\n    print(f"\\n   Shape final del DataFrame: {df.shape}")\n    print(f"=" * 70)\n    \n    return df\n\n\ndef create_wind_statistics_lags(df, lag_times=[2, 5, 8, 11, 14, 17, 20, 23, 26]):\n    """\n    Crea lags de las estad√≠sticas del campo de viento (U_mean, U_std, U_shear_vert, U_shear_horiz).\n    \n    DESCRIPCI√ìN:\n    Esta funci√≥n crea versiones desplazadas temporalmente de las estad√≠sticas del viento,\n    permitiendo al modelo capturar c√≥mo las condiciones de viento pasadas afectan las\n    cargas actuales en las palas.\n    \n    IMPORTANTE: Ejecutar despu√©s de create_wind_field_statistics().\n    \n    Args:\n        df (pd.DataFrame): DataFrame con las columnas U_mean, U_std, U_shear_vert, U_shear_horiz\n        lag_times (list): Lista de tiempos de lag en segundos\n    \n    Returns:\n        pd.DataFrame: DataFrame con columnas adicionales:\n                     - \'U_mean_lag{X}s\', \'U_std_lag{X}s\', etc. para cada lag\n    \n    Raises:\n        ValueError: Si faltan columnas de estad√≠sticas de viento.\n    """\n    \n    # Validar que existen las columnas base\n    required_cols = [\'U_mean\', \'U_std\', \'U_shear_vert\', \'U_shear_horiz\', \'Time\']\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    \n    if missing_cols:\n        raise ValueError(f"Faltan columnas de estad√≠sticas de viento: {missing_cols}. "\n                        f"Ejecuta create_wind_field_statistics() primero.")\n    \n    print("=" * 70)\n    print("Creando lags de estad√≠sticas del campo de viento...")\n    print("=" * 70)\n    \n    # Calcular sampling rate\n    time_values = df[\'Time\'].values\n    if len(time_values) > 1:\n        dt = time_values[1] - time_values[0]\n        fs = 1.0 / dt\n    else:\n        dt = 0.1\n        fs = 10.0\n    \n    print(f"\\n   Par√°metros:")\n    print(f"   - Sampling rate: {fs:.1f} Hz (dt = {dt:.3f} s)")\n    print(f"   - Lags a crear: {lag_times[0]}s - {lag_times[-1]}s ({len(lag_times)} lags)")\n    \n    # Variables base\n    base_vars = [\'U_mean\', \'U_std\', \'U_shear_vert\', \'U_shear_horiz\']\n    \n    print(f"   - Variables base: {len(base_vars)}")\n    \n    # Contador de columnas creadas\n    created_cols = 0\n    \n    # Crear lags para cada variable\n    for var in base_vars:\n        for lag_sec in lag_times:\n            # Calcular lag en muestras\n            lag_samples = int(round(lag_sec * fs))\n            \n            # Nombre de la nueva columna\n            col_name = f"{var}_lag{lag_sec}s"\n            \n            # Crear lag usando shift\n            df[col_name] = df[var].shift(lag_samples)\n            \n            created_cols += 1\n    \n    print(f"\\n   ‚úÖ Columnas de lag creadas: {created_cols}")\n    print(f"      ({len(base_vars)} variables √ó {len(lag_times)} lags)")\n    \n    # Resumen\n    print(f"\\n" + "=" * 70)\n    print(f"RESUMEN:")\n    print(f"=" * 70)\n    print(f"   Lags de estad√≠sticas de viento creados:")\n    print(f"   - U_mean_lag{lag_times[0]}s ... U_mean_lag{lag_times[-1]}s")\n    print(f"   - U_std_lag{lag_times[0]}s ... U_std_lag{lag_times[-1]}s")\n    print(f"   - U_shear_vert_lag{lag_times[0]}s ... U_shear_vert_lag{lag_times[-1]}s")\n    print(f"   - U_shear_horiz_lag{lag_times[0]}s ... U_shear_horiz_lag{lag_times[-1]}s")\n    print(f"\\n   üí° Total de nuevas features: {created_cols}")\n    print(f"   üí° Estas capturas temporales del viento son cruciales para la predicci√≥n")\n    print(f"\\n   Shape final del DataFrame: {df.shape}")\n    print(f"=" * 70)\n    \n    return df\n',
            'def add_name_dlc_column(df: pd.DataFrame, csv_name: str) -> pd.DataFrame:\n    """\n    A√±ade columna identificadora Name_DLC al DataFrame.\n    \n    Args:\n        df: DataFrame a procesar\n        csv_name: Nombre del archivo CSV (ej: \'0001_DLC12a_030_000.csv\')\n        \n    Returns:\n        DataFrame con columna Name_DLC a√±adida\n    """\n    # Extraer nombre sin extensi√≥n\n    name_without_ext = csv_name.replace(\'.csv\', \'\')\n    df[\'Name_DLC\'] = name_without_ext\n    return df',
            '# ============================================================================\n# PASO 3.3: Definir funcion para crear features de Intensidad de Turbulencia\n# ============================================================================\n\ndef create_turbulence_intensity(df):\n    """\n    Crea features de intensidad de turbulencia (TI) para cada punto de medici√≥n.\n    TI = U_std / U_mean\n    \n    Args:\n        df: DataFrame con los datos\n    \n    Returns:\n        DataFrame con las nuevas columnas de TI a√±adidas\n    """\n    # Identificar columnas de U_mean y U_std\n    u_mean_columns = [col for col in df.columns if \'U_mean\' in col]\n    u_std_columns = [col for col in df.columns if \'U_std\' in col]\n    \n    print(f"Variables U_mean encontradas: {len(u_mean_columns)}")\n    print(f"Variables U_std encontradas: {len(u_std_columns)}")\n    \n    # Verificar que hay el mismo n√∫mero de columnas mean y std\n    if len(u_mean_columns) != len(u_std_columns):\n        print("ADVERTENCIA: N√∫mero diferente de columnas U_mean y U_std")\n    \n    # Crear TI para cada par de U_mean y U_std\n    print(f"\\nCreando features de Intensidad de Turbulencia...")\n    \n    total_created = 0\n    for u_mean_col in u_mean_columns:\n        # Obtener el identificador del punto (ej: "LAC_001_030" de "LAC_001_030_U_mean")\n        point_id = u_mean_col.replace(\'U_mean\', \'\').rstrip(\'_\')\n        \n        # Buscar la columna U_std correspondiente\n        u_std_col = f"{point_id}U_std"\n        \n        if u_std_col in u_std_columns:\n            # Crear nombre de la nueva columna\n            ti_col_name = f"{point_id}TI"\n            \n            # Calcular TI = U_std / U_mean\n            # Evitar divisi√≥n por cero\n            df[ti_col_name] = df[u_std_col] / df[u_mean_col].replace(0, np.nan)\n            \n            total_created += 1\n            print(f"  - {ti_col_name} creada desde {u_mean_col} y {u_std_col}")\n        else:\n            print(f"ADVERTENCIA: No se encontr√≥ {u_std_col} para {u_mean_col}")\n    \n    print(f"\\nTotal de features de TI creadas: {total_created}")\n    print(f"Shape del DataFrame: {df.shape}")\n    \n    return df\n\nprint("OK - Funcion create_turbulence_intensity definida correctamente")',
            'import numpy as np\nfrom pathlib import Path\nfrom scipy.interpolate import interp1d\n\ndef add_extreme_condition_flag(df, dlc_type=\'DLC12a\'):\n    """\n    A√±ade una columna \'Extreme_condition\' al DataFrame bas√°ndose en si TI supera el P95.\n    \n    Args:\n        df: DataFrame con columnas \'U_mean\' y \'TI\'\n        dlc_type: Tipo de DLC para cargar los archivos (\'DLC12a\' o \'DLC13\')\n    \n    Returns:\n        DataFrame con la nueva columna \'Extreme_condition\' a√±adida\n    """\n    # Ruta a la carpeta con los archivos de referencia\n    TI_data = Path(\'C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub\')\n    \n    # Cargar los archivos de texto\n    umean_file = TI_data / f\'Umean_{dlc_type}.txt\'\n    tip95_file = TI_data / f\'TI_P95_{dlc_type}.txt\'\n    \n    print(f"Cargando archivos de referencia para {dlc_type}...")\n    \n    # Leer los archivos SALTANDO LA PRIMERA L√çNEA (header)\n    umean_reference = np.loadtxt(umean_file, skiprows=1)\n    tip95_reference = np.loadtxt(tip95_file, skiprows=1)\n    \n    print(f"  - {len(umean_reference)} valores de Umean cargados")\n    print(f"  - {len(tip95_reference)} valores de TI_P95 cargados")\n    print(f"  - Rango Umean: [{umean_reference.min():.2f}, {umean_reference.max():.2f}] m/s")\n    \n    # Verificar que ambos arrays tienen el mismo tama√±o\n    if len(umean_reference) != len(tip95_reference):\n        raise ValueError(f"Los archivos tienen diferente n√∫mero de valores: {len(umean_reference)} vs {len(tip95_reference)}")\n    \n    # Crear funci√≥n de interpolaci√≥n\n    # kind=\'linear\' para interpolaci√≥n lineal\n    # fill_value=\'extrapolate\' para valores fuera del rango\n    interp_func = interp1d(umean_reference, tip95_reference, \n                           kind=\'linear\', \n                           fill_value=\'extrapolate\', \n                           bounds_error=False)\n    \n    print(f"\\nProcesando {len(df)} filas del DataFrame...")\n    \n    # Verificar que las columnas existen\n    if \'U_mean\' not in df.columns:\n        raise ValueError("El DataFrame no contiene la columna \'U_mean\'")\n    if \'TI\' not in df.columns:\n        raise ValueError("El DataFrame no contiene la columna \'TI\'")\n    \n    # Interpolar TI_P95 para cada valor de U_mean en el DataFrame\n    ti_p95_interpolated = interp_func(df[\'U_mean\'].values)\n    \n    # Crear la columna Extreme_condition\n    # 0 si TI < TI_P95, 1 si TI >= TI_P95\n    df[\'Extreme_condition\'] = (df[\'TI\'] >= ti_p95_interpolated).astype(int)\n    \n    # Estad√≠sticas\n    n_extreme = df[\'Extreme_condition\'].sum()\n    n_normal = len(df) - n_extreme\n    pct_extreme = (n_extreme / len(df)) * 100\n    \n    print(f"\\nResultados:")\n    print(f"  - Condiciones normales (TI < P95):  {n_normal} ({100-pct_extreme:.2f}%)")\n    print(f"  - Condiciones extremas (TI >= P95): {n_extreme} ({pct_extreme:.2f}%)")\n    print(f"  - Nueva columna \'Extreme_condition\' a√±adida")\n    \n    return df\n\nprint("OK - Funci√≥n add_extreme_condition_flag definida correctamente")',
            '# ============================================================================\n# PASO 3.4: Definir funcion para crear features de Yaw Rate y Yaw Acceleration\n# ============================================================================\n\ndef create_yaw_derivatives(df):\n    """\n    Crea features de yaw rate (velocidad angular) y yaw acceleration (aceleraci√≥n angular).\n    \n    Yaw Rate: Derivada temporal del √°ngulo de yaw (rad/s)\n    Yaw Acceleration: Derivada temporal del yaw rate (rad/s¬≤)\n    \n    Calcula dt din√°micamente desde la columna Time para cada registro.\n    \n    Args:\n        df: DataFrame con columnas \'Time\' y \'OPER_MEAS_YAWERROR\'\n    \n    Returns:\n        DataFrame con las nuevas columnas a√±adidas:\n        - \'Yaw_rate_rad_s\': Velocidad angular del yaw (rad/s)\n        - \'Yaw_acc_rad_s2\': Aceleraci√≥n angular del yaw (rad/s¬≤)\n    """\n    print("Creando features de Yaw Rate y Yaw Acceleration...")\n    \n    # Verificar que existan las columnas necesarias\n    if \'Time\' not in df.columns:\n        print("ERROR: No se encontr√≥ la columna \'Time\'")\n        return df\n    \n    if \'OPER_MEAS_YAWERROR\' not in df.columns:\n        print("ERROR: No se encontr√≥ la columna \'OPER_MEAS_YAWERROR\'")\n        return df\n    \n    # Convertir yaw error de grados a radianes\n    psi_rad = np.deg2rad(df[\'OPER_MEAS_YAWERROR\'].values)\n    \n    # Calcular dt din√°micamente\n    time_array = df[\'Time\'].values\n    dt = np.diff(time_array)\n    dt_mean = np.mean(dt)\n    \n    print(f"  - Time step promedio (dt): {dt_mean:.4f} s")\n    print(f"  - Time step m√≠nimo: {np.min(dt):.4f} s")\n    print(f"  - Time step m√°ximo: {np.max(dt):.4f} s")\n    \n    # Calcular yaw rate usando np.gradient con el time step promedio\n    yaw_rate_rad_s = np.gradient(psi_rad, dt_mean)\n    \n    # Calcular yaw acceleration como derivada del yaw rate\n    yaw_acc_rad_s2 = np.gradient(yaw_rate_rad_s, dt_mean)\n    \n    # A√±adir las nuevas columnas al DataFrame\n    df[\'Yaw_rate_rad_s\'] = yaw_rate_rad_s\n    df[\'Yaw_acc_rad_s2\'] = yaw_acc_rad_s2\n    \n    print(f"  ‚úì Yaw_rate_rad_s creada (min: {yaw_rate_rad_s.min():.4f}, max: {yaw_rate_rad_s.max():.4f})")\n    print(f"  ‚úì Yaw_acc_rad_s2 creada (min: {yaw_acc_rad_s2.min():.4f}, max: {yaw_acc_rad_s2.max():.4f})")\n    print(f"\\nShape del DataFrame: {df.shape}")\n    \n    return df\n\nprint("OK - Funcion create_yaw_derivatives definida correctamente")',
            'import pandas as pd\nimport numpy as np\nimport re\n\ndef compute_shear_physical_multi_range(\n    df,\n    ranges,\n    range_distance_map,         # dict: {range_num: distancia_m}\n    cone_half_angle_deg=15.0,\n    exclude_beams=(8, 9),\n):\n    """\n    Calcula U0, dU/dy, dU/dz para m√∫ltiples ranges.\n    Cada range usa su propia distancia f√≠sica.\n    """\n\n    beam_angle_deg = {\n        0:   0,\n        1:  45,\n        2:  90,\n        3: 135,\n        4: 180,\n        5: 225,\n        6: 270,\n        7: 315\n    }\n    needed_beams = sorted(beam_angle_deg.keys())\n    phi = np.deg2rad(cone_half_angle_deg)\n    proj = np.cos(phi)\n\n    out = df.copy()\n\n    for range_target in ranges:\n        if range_target not in range_distance_map:\n            raise ValueError(f"No hay distancia definida para RANGE{range_target}")\n\n        range_distance_m = float(range_distance_map[range_target])\n        rho = range_distance_m * np.sin(phi)  # radio anillo para ESTE range\n\n        # 1) mapear beam->columna para este range\n        beam_col = {}\n        for col in df.columns:\n            if \'LAC_VLOS\' in col and \'lag\' not in col.lower():\n                bm = re.search(r\'BEAM(\\d+)\', col)\n                rm = re.search(r\'RANGE(\\d+)\', col)\n                if not bm or not rm:\n                    continue\n                b = int(bm.group(1))\n                r = int(rm.group(1))\n                if b in exclude_beams:\n                    continue\n                if r == range_target and b in beam_angle_deg:\n                    beam_col[b] = col\n\n        missing = [b for b in needed_beams if b not in beam_col]\n        if missing:\n            # si prefieres no romper, puedes hacer continue\n            raise ValueError(f"Faltan beams {missing} en RANGE{range_target}")\n\n        # 2) coordenadas de beams para este range\n        y_b, z_b = {}, {}\n        for b, ang_deg in beam_angle_deg.items():\n            th = np.deg2rad(ang_deg)\n            y_b[b] = rho * np.sin(th)\n            z_b[b] = rho * np.cos(th)\n\n        n = len(df)\n        U0 = np.full(n, np.nan)\n        Gy = np.full(n, np.nan)   # dU/dy\n        Gz = np.full(n, np.nan)   # dU/dz\n\n        # 3) ajuste por fila\n        for k in range(n):\n            vals, Y, Z = [], [], []\n            for b in needed_beams:\n                v = df.iloc[k][beam_col[b]]\n                if pd.notna(v):\n                    vals.append(v / proj)\n                    Y.append(y_b[b])\n                    Z.append(z_b[b])\n\n            if len(vals) < 3:\n                continue\n\n            A = np.column_stack([np.ones(len(vals)), np.array(Y), np.array(Z)])\n            bvec = np.array(vals)\n            coef, *_ = np.linalg.lstsq(A, bvec, rcond=None)\n            U0[k], Gy[k], Gz[k] = coef\n\n        # 4) guardar columnas por range\n        out[f\'U0_RANGE{range_target}\'] = U0\n        out[f\'dU_dy_RANGE{range_target}\'] = Gy\n        out[f\'dU_dz_RANGE{range_target}\'] = Gz\n        out[f\'deltaU_y_diam_RANGE{range_target}\'] = Gy * (2 * rho)\n        out[f\'deltaU_z_diam_RANGE{range_target}\'] = Gz * (2 * rho)\n\n    return out\n',
            'def create_physical_shear_lags(df, lag_times=[2, 5, 8, 11, 14, 17, 20, 23, 26], ranges=None):\n    """\n    Crea lags para variables de cortadura f√≠sica por RANGE.\n\n    DESCRIPCI√ìN:\n    Esta funci√≥n crea versiones desplazadas temporalmente de las variables de shear f√≠sico\n    calculadas previamente (por ejemplo con compute_shear_physical_multi_range), para capturar\n    c√≥mo la estructura espacial del viento pasada afecta las cargas actuales.\n\n    Variables objetivo por rango:\n      - dU_dy_RANGE{r}\n      - dU_dz_RANGE{r}\n      - deltaU_y_diam_RANGE{r}\n      - deltaU_z_diam_RANGE{r}\n      - U0_RANGE{r}   (opcional pero √∫til)\n\n    Args:\n        df (pd.DataFrame): DataFrame con columnas f√≠sicas de shear y columna Time.\n        lag_times (list): Lista de tiempos de lag en segundos.\n        ranges (list or None): Lista de rangos a procesar (ej. [3,5,7]).\n                               Si None, detecta autom√°ticamente los ranges a partir de dU_dy_RANGE*.\n\n    Returns:\n        pd.DataFrame: DataFrame con columnas de lag a√±adidas.\n\n    Raises:\n        ValueError: Si falta Time o no se encuentran columnas base de shear.\n    """\n\n    import re\n\n    # Validaci√≥n Time\n    if \'Time\' not in df.columns:\n        raise ValueError("Falta la columna \'Time\' para calcular los lags.")\n\n    print("=" * 70)\n    print("Creando lags de variables de cortadura f√≠sica por RANGE...")\n    print("=" * 70)\n\n    # Calcular sampling rate\n    time_values = df[\'Time\'].values\n    if len(time_values) > 1:\n        dt = time_values[1] - time_values[0]\n        fs = 1.0 / dt\n    else:\n        dt = 0.1\n        fs = 10.0\n\n    # Detectar ranges autom√°ticamente si no se pasan\n    if ranges is None:\n        detected_ranges = []\n        for col in df.columns:\n            m = re.match(r\'dU_dy_RANGE(\\d+)$\', col)\n            if m:\n                detected_ranges.append(int(m.group(1)))\n        ranges = sorted(list(set(detected_ranges)))\n\n    if not ranges:\n        raise ValueError("No se detectaron ranges. Aseg√∫rate de haber creado columnas tipo dU_dy_RANGE{r}.")\n\n    print(f"\\n   Par√°metros:")\n    print(f"   - Sampling rate: {fs:.1f} Hz (dt = {dt:.3f} s)")\n    print(f"   - Lags a crear: {lag_times[0]}s - {lag_times[-1]}s ({len(lag_times)} lags)")\n    print(f"   - Ranges a procesar: {ranges}")\n\n    # Variables base esperadas por rango\n    var_templates = [\n        \'U0_RANGE{r}\',\n        \'dU_dy_RANGE{r}\',\n        \'dU_dz_RANGE{r}\',\n        \'deltaU_y_diam_RANGE{r}\',\n        \'deltaU_z_diam_RANGE{r}\'\n    ]\n\n    created_cols = 0\n    total_base_vars = 0\n\n    for r in ranges:\n        # Columnas base que realmente existen en df para este rango\n        base_vars_r = []\n        for tpl in var_templates:\n            c = tpl.format(r=r)\n            if c in df.columns:\n                base_vars_r.append(c)\n\n        if len(base_vars_r) == 0:\n            print(f"   ‚ö†Ô∏è RANGE{r}: no se encontraron columnas base, se omite.")\n            continue\n\n        total_base_vars += len(base_vars_r)\n        print(f"\\n   RANGE{r}: {len(base_vars_r)} variables base encontradas")\n\n        # Crear lags para cada variable de este rango\n        for var in base_vars_r:\n            for lag_sec in lag_times:\n                lag_samples = int(round(lag_sec * fs))\n                col_name = f"{var}_lag{lag_sec}s"\n                df[col_name] = df[var].shift(lag_samples)\n                created_cols += 1\n\n    print(f"\\n   ‚úÖ Columnas de lag creadas: {created_cols}")\n    print(f"      (variables base detectadas: {total_base_vars} √ó {len(lag_times)} lags, seg√∫n disponibilidad)")\n\n    # Resumen final\n    print(f"\\n" + "=" * 70)\n    print("RESUMEN:")\n    print("=" * 70)\n    print(f"   Ranges procesados: {ranges}")\n    print(f"   Total de nuevas features lag: {created_cols}")\n    print(f"\\n   Ejemplos de nombres creados:")\n    if len(ranges) > 0 and len(lag_times) > 0:\n        r0 = ranges[0]\n        l0 = lag_times[0]\n        print(f"   - dU_dz_RANGE{r0}_lag{l0}s")\n        print(f"   - dU_dy_RANGE{r0}_lag{l0}s")\n        print(f"   - deltaU_z_diam_RANGE{r0}_lag{l0}s")\n        print(f"   - deltaU_y_diam_RANGE{r0}_lag{l0}s")\n        print(f"   - U0_RANGE{r0}_lag{l0}s")\n    print(f"\\n   Shape final del DataFrame: {df.shape}")\n    print("=" * 70)\n\n    return df\n',
            '# ============================================================================\n# PASO 3.5: PROCESAR TODOS LOS ARCHIVOS CSV Y CREAR FEATURES\n# ============================================================================\n\nprint("="*70)\nprint("PROCESANDO ARCHIVOS CSV - FEATURE ENGINEERING")\nprint("="*70)\n\n# Contadores\nprocessed_count = 0\nfailed_count = 0\nskipped_count = 0\n\n# Verificar que la carpeta existe\nif not data_folder_ml.exists():\n    print("ERROR: La carpeta data_train_traditional_ML no existe")\nelse:\n    csv_files = list(data_folder_ml.glob("*.csv"))\n    \n    print(f"\\nTotal de archivos CSV a procesar: {len(csv_files)}\\n")\n    \n    # Procesar cada archivo CSV\n    for csv_file in csv_files:\n        try:\n            print(f"{\'=\'*70}")\n            print(f"Procesando: {csv_file.name}")\n            print(f"{\'=\'*70}")\n            \n            # Cargar CSV\n            print("[1/5] Cargando CSV...")\n            df = pd.read_csv(csv_file)\n            print(f"      Shape original: {df.shape}")\n            print(f"      Columnas originales: {len(df.columns)}")\n            \n            # Guardar numero de filas original\n            original_rows = len(df)\n            \n            # FEATURE 1: Lags de VLOS\n            print("\\n[2/5] Creando lags de VLOS (5-25 segundos)...")\n            # df = create_vlos_lags(df)\n            df = create_vlos_lags(df, range_values=[5], include_all_ranges=False)\n\n\n            \n            # FEATURE 2: Componentes trigonometricas del azimuth\n            print("\\n[3/5] Creando componentes sin/cos del azimuth...")\n            df = create_azimuth_components(df)\n\n            # FEATURE 2: Componentes trigonometricas del yawerror\n            print("\\n[3/5] Creando componentes sin/cos del yawerror...")\n            df = create_yawerror_components(df)\n            \n            # FEATURE 3: Componentes 1P y 2P de momentos flectores\n            print("\\n[4/5] Creando componentes 1P y 2P de momentos...")\n            df = create_frequency_components_1P_2P(df)\n\n            # FEATURE 4: Componentes de pitch en marco Coleman\n            print("\\n[4.5/5] Creando componentes de pitch en marco Coleman...")\n            df = create_pitch_coleman_features(df)\n\n            # FEATURE 5: Estad√≠sticas del campo de viento LIDAR\n            print("\\n[5/5] Creando estad√≠sticas del campo de viento LIDAR...")\n            df = create_wind_field_statistics(df, rotation_offset=0)\n            df = create_wind_statistics_lags(df)\n            df = create_turbulence_intensity(df)\n            df = add_extreme_condition_flag(df, dlc_type=\'DLC12a\') \n            df = create_yaw_derivatives(df)\n            \n            # FEATURE 7: A√±adir columna identificadora Name_DLC\n            print("\\n[6/6] A√±adiendo columna identificadora Name_DLC...")\n            df = add_name_dlc_column(df, csv_file.name)\n            print(f"      ‚úì Name_DLC = \'{csv_file.stem}\' asignado a {len(df):,} filas")\n\n            ranges = [5]\n            range_distance_map = {\n                5: 150.0,\n            }\n            \n            df = compute_shear_physical_multi_range(\n                df,\n                ranges=ranges,\n                range_distance_map=range_distance_map,\n                cone_half_angle_deg=20.0\n            )\n\n            # O expl√≠cito:\n            df = create_physical_shear_lags(df, ranges=[5], lag_times=[2,5,8,11,14,17,20,23,26])\n\n\n            # Eliminar filas con NaN generadas por los lags\n            print("\\n[5/5] Limpiando datos...")\n            df_cleaned = df.dropna()\n            rows_removed = original_rows - len(df_cleaned)\n            print(f"      Filas con NaN eliminadas: {rows_removed}")\n            print(f"      Shape final: {df_cleaned.shape}")\n            print(f"      Columnas finales: {len(df_cleaned.columns)}")\n            \n            # Guardar CSV actualizado\n            df_cleaned.to_csv(csv_file, index=False)\n            print(f"\\n      OK - CSV actualizado guardado")\n            \n            processed_count += 1\n            \n        except Exception as e:\n            print(f"\\nERROR procesando {csv_file.name}: {str(e)}")\n            import traceback\n            traceback.print_exc()\n            failed_count += 1\n        \n        print()  # Linea en blanco entre archivos\n    \n    # Resumen final\n    print(f"{\'=\'*70}")\n    print("RESUMEN DEL PROCESAMIENTO")\n    print("="*70)\n    print(f"Archivos procesados exitosamente: {processed_count}")\n    print(f"Archivos con errores:              {failed_count}")\n    print(f"Total intentados:                  {len(csv_files)}")\n    print("="*70)\n    \n    if processed_count > 0:\n        print(f"\\nCSVs actualizados en: {data_folder_ml}")\n        print("\\nNuevas features a√±adidas a cada CSV:")\n        print("  1. Lags de VLOS: 21 lags x N variables VLOS")\n        print("  2. sin_rotor_azimuth y cos_rotor_azimuth")\n        print("  3. Blade root 1 My 1P y Blade root 1 My 2P")\n        print("  4. Blade root 2 My 1P y Blade root 2 My 2P")\n    \n    print("\\n" + "="*70)\n    print("STEP 3 COMPLETADO - FEATURE ENGINEERING")\n    print("="*70)',
            'from pathlib import Path\nimport pandas as pd\n\ndef shift_targets_forward_two_seconds(\n    data_folder="data_train_traditional_ML",\n    columns_to_shift=None,\n    shift_seconds=2.0\n):\n    """\n    Lee todos los CSV en data_folder, desplaza hacia adelante (futuro)\n    las columnas indicadas y recorta las √∫ltimas filas para mantener\n    la misma longitud.\n\n    Desplazar hacia adelante significa:\n    - En el tiempo t, la columna tendr√° el valor de t + shift_seconds.\n    - Se consigue con shift negativo (df[col].shift(-n_samples)).\n\n    Args:\n        data_folder (str): Carpeta con los CSV.\n        columns_to_shift (list): Columnas a desplazar.\n        shift_seconds (float): Segundos a desplazar hacia adelante.\n\n    Returns:\n        None (sobrescribe los CSV con los datos modificados).\n    """\n    if columns_to_shift is None:\n        columns_to_shift = [\n            \'Stationary hub My\',\n            \'Stationary hub Mz\',\n            \'Blade root 1 My\',\n            \'Blade root 2 My\',\n            \'M_0\', \'M_1c\', \'M_1s\', \'M_2c\', \'M_2s\'\n        ]\n\n    data_path = Path(data_folder)\n    csv_files = list(data_path.glob("*.csv"))\n\n    print(f"CSV encontrados: {len(csv_files)}")\n\n    for csv_file in csv_files:\n        df = pd.read_csv(csv_file)\n\n        # Calcular dt a partir de la columna Time\n        if \'Time\' in df.columns and len(df) > 1:\n            dt = df[\'Time\'].iloc[1] - df[\'Time\'].iloc[0]\n        else:\n            dt = 0.02  # fallback 50 Hz\n\n        # N√∫mero de muestras a desplazar\n        shift_samples = int(round(shift_seconds / dt))\n\n        # Desplazar columnas hacia adelante (t -> t + shift_seconds)\n        for col in columns_to_shift:\n            if col in df.columns:\n                df[col] = df[col].shift(-shift_samples)\n\n        # Recortar las √∫ltimas filas para eliminar NaNs del shift\n        if shift_samples > 0:\n            df = df.iloc[:-shift_samples].reset_index(drop=True)\n\n        # Guardar CSV modificado\n        df.to_csv(csv_file, index=False)\n\n        print(f"{csv_file.name} procesado | dt={dt:.4f}s | shift={shift_samples} filas | filas finales={len(df)}")\n',
            'shift_targets_forward_two_seconds(\n    data_folder="C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub\\\\data_train_traditional_ML",\n    shift_seconds=2.0\n)\n',
            '# ============================================================================\n# VISUALIZACI√ìN: ANALISIS DE TI\n# ============================================================================\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom pathlib import Path\n\n# ============================================================================\n# Cargar datos DLC12a y DLC13 - Crear gr√°ficos TI vs U_mean con bandas\n# ============================================================================\n\n# Ruta a la carpeta con los datos\ndata_folder = Path(\'C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub\\\\data_train_traditional_ML\')\n\n# Buscar archivos DLC12a y DLC13\ndlc12a_files = sorted([f for f in os.listdir(data_folder) if \'DLC12a\' in f and f.endswith(\'.csv\')])\ndlc13_files = sorted([f for f in os.listdir(data_folder) if \'DLC13\' in f and f.endswith(\'.csv\')])\n\nprint(f"Archivos DLC12a encontrados: {len(dlc12a_files)}")\nprint(f"Archivos DLC13 encontrados: {len(dlc13_files)}")\n\n# Frecuencia de muestreo: cada fila es 0.02s, queremos 1 punto cada 1s\n# 1s / 0.02s = 50 filas por segundo\nsampling_interval = 50  # Tomar 1 de cada 50 filas\n\ndef load_data(file_list, dlc_name):\n    """Funci√≥n para cargar datos de una lista de archivos"""\n    all_u_mean = []\n    all_ti = []\n    \n    for i, file in enumerate(file_list, 1):\n        file_path = data_folder / file\n        \n        # Leer el CSV\n        df = pd.read_csv(file_path)\n        \n        # Verificar que existen las columnas necesarias\n        u_mean_cols = [col for col in df.columns if \'U_mean\' in col]\n        ti_cols = [col for col in df.columns if \'TI\' in col and \'U_mean\' not in col]\n        \n        if len(u_mean_cols) > 0 and len(ti_cols) > 0:\n            # Tomar una muestra cada 1 segundo (cada 50 filas)\n            df_sampled = df.iloc[::sampling_interval]\n            \n            # Para cada columna de U_mean y TI correspondiente\n            for u_mean_col in u_mean_cols:\n                # Buscar la columna TI correspondiente\n                point_id = u_mean_col.replace(\'U_mean\', \'\').rstrip(\'_\')\n                ti_col = f"{point_id}TI"\n                \n                if ti_col in ti_cols:\n                    # Extraer valores v√°lidos (sin NaN)\n                    mask = df_sampled[u_mean_col].notna() & df_sampled[ti_col].notna()\n                    u_mean_values = df_sampled.loc[mask, u_mean_col].values\n                    ti_values = df_sampled.loc[mask, ti_col].values\n                    \n                    all_u_mean.extend(u_mean_values)\n                    all_ti.extend(ti_values)\n        \n        if i % 10 == 0:\n            print(f"  {dlc_name}: Procesados {i}/{len(file_list)} archivos...")\n    \n    return np.array(all_u_mean), np.array(all_ti)\n\ndef calculate_percentile_bands(u_mean_array, ti_array, u_min=3):\n    """Calcula las bandas de percentiles desde u_min hasta el m√°ximo"""\n    # Filtrar datos desde u_min\n    mask = u_mean_array >= u_min\n    u_mean_filtered = u_mean_array[mask]\n    ti_filtered = ti_array[mask]\n    \n    # Dividir en bins de velocidad\n    u_bins = np.arange(u_min, u_mean_filtered.max() + 1, 1)  # Bins de 1 m/s\n    u_bin_centers = []\n    ti_p5 = []\n    ti_p50 = []\n    ti_p95 = []\n    \n    for i in range(len(u_bins) - 1):\n        mask_bin = (u_mean_filtered >= u_bins[i]) & (u_mean_filtered < u_bins[i + 1])\n        if mask_bin.sum() > 10:  # Al menos 10 puntos en el bin\n            u_bin_centers.append((u_bins[i] + u_bins[i + 1]) / 2)\n            ti_values_in_bin = ti_filtered[mask_bin]\n            ti_p5.append(np.percentile(ti_values_in_bin, 5))\n            ti_p50.append(np.percentile(ti_values_in_bin, 50))\n            ti_p95.append(np.percentile(ti_values_in_bin, 95))\n    \n    return np.array(u_bin_centers), np.array(ti_p5), np.array(ti_p50), np.array(ti_p95)\n\n# ============================================================================\n# CARGAR DATOS DLC12a\n# ============================================================================\nprint("\\nCargando datos DLC12a...")\nu_mean_12a, ti_12a = load_data(dlc12a_files, "DLC12a")\nprint(f"Total de puntos DLC12a: {len(u_mean_12a)}")\n\n# ============================================================================\n# CARGAR DATOS DLC13\n# ============================================================================\nprint("\\nCargando datos DLC13...")\n#u_mean_13, ti_13 = load_data(dlc13_files, "DLC13")\nprint(f"Total de puntos DLC13: {len(u_mean_13)}")\n\n# ============================================================================\n# CALCULAR BANDAS (desde 3 m/s hasta el m√°ximo)\n# ============================================================================\nprint("\\nCalculando bandas de percentiles...")\nUmean_DLC12a, ti_p5_12a, ti_p50_12a, TI_P95_DLC12a = calculate_percentile_bands(u_mean_12a, ti_12a, u_min=3)\n#Umean_DLC13, ti_p5_13, ti_p50_13, TI_P95_DLC13 = calculate_percentile_bands(u_mean_13, ti_13, u_min=3)\n\n# ============================================================================\n# GR√ÅFICO 1: DLC12a\n# ============================================================================\nplt.figure(figsize=(14, 9))\nplt.scatter(u_mean_12a, ti_12a, alpha=0.2, s=8, c=\'steelblue\', edgecolors=\'none\', label=\'Datos DLC12a\')\nplt.plot(Umean_DLC12a, ti_p50_12a, \'r-\', linewidth=2.5, label=\'Mediana (P50)\', zorder=5)\nplt.plot(Umean_DLC12a, TI_P95_DLC12a, \'r--\', linewidth=2, label=\'P95 (l√≠mite superior)\', zorder=5)\nplt.plot(Umean_DLC12a, ti_p5_12a, \'r--\', linewidth=2, label=\'P5 (l√≠mite inferior)\', zorder=5)\nplt.fill_between(Umean_DLC12a, ti_p5_12a, TI_P95_DLC12a, alpha=0.15, color=\'red\', label=\'Banda P5-P95\')\nplt.xlabel(\'U_mean [m/s]\', fontsize=13)\nplt.ylabel(\'Intensidad de Turbulencia (TI) [-]\', fontsize=13)\nplt.title(\'Intensidad de Turbulencia vs Velocidad Media - DLC12a\', fontsize=14, fontweight=\'bold\')\nplt.ylim(0, 1)\nplt.grid(True, alpha=0.3)\nplt.legend(loc=\'upper right\', fontsize=11)\nplt.tight_layout()\nplt.show()\n\nprint(f"\\n{\'=\'*60}")\nprint(f"ARRAYS DE SALIDA DLC12a (desde 3 m/s):")\nprint(f"{\'=\'*60}")\nprint(f"Umean_DLC12a: array con {len(Umean_DLC12a)} valores")\nprint(f"  - Rango: [{Umean_DLC12a.min():.2f}, {Umean_DLC12a.max():.2f}] m/s")\nprint(f"TI_P95_DLC12a: array con {len(TI_P95_DLC12a)} valores")\nprint(f"  - Rango: [{TI_P95_DLC12a.min():.4f}, {TI_P95_DLC12a.max():.4f}]")\nprint(f"Primeros 10 valores:")\nprint(f"  Umean:  {Umean_DLC12a[:10]}")\nprint(f"  TI_P95: {TI_P95_DLC12a[:10]}")\n\n# ============================================================================\n# GR√ÅFICO 2: DLC13\n# ============================================================================\nplt.figure(figsize=(14, 9))\nplt.scatter(u_mean_13, ti_13, alpha=0.2, s=8, c=\'steelblue\', edgecolors=\'none\', label=\'Datos DLC13\')\nplt.plot(Umean_DLC13, ti_p50_13, \'r-\', linewidth=2.5, label=\'Mediana (P50)\', zorder=5)\nplt.plot(Umean_DLC13, TI_P95_DLC13, \'r--\', linewidth=2, label=\'P95 (l√≠mite superior)\', zorder=5)\nplt.plot(Umean_DLC13, ti_p5_13, \'r--\', linewidth=2, label=\'P5 (l√≠mite inferior)\', zorder=5)\nplt.fill_between(Umean_DLC13, ti_p5_13, TI_P95_DLC13, alpha=0.15, color=\'red\', label=\'Banda P5-P95\')\nplt.xlabel(\'U_mean [m/s]\', fontsize=13)\nplt.ylabel(\'Intensidad de Turbulencia (TI) [-]\', fontsize=13)\nplt.title(\'Intensidad de Turbulencia vs Velocidad Media - DLC13\', fontsize=14, fontweight=\'bold\')\nplt.ylim(0, 1)\nplt.grid(True, alpha=0.3)\nplt.legend(loc=\'upper right\', fontsize=11)\nplt.tight_layout()\nplt.show()\n\nprint(f"\\n{\'=\'*60}")\nprint(f"ARRAYS DE SALIDA DLC13 (desde 3 m/s):")\nprint(f"{\'=\'*60}")\nprint(f"Umean_DLC13: array con {len(Umean_DLC13)} valores")\nprint(f"  - Rango: [{Umean_DLC13.min():.2f}, {Umean_DLC13.max():.2f}] m/s")\nprint(f"TI_P95_DLC13: array con {len(TI_P95_DLC13)} valores")\nprint(f"  - Rango: [{TI_P95_DLC13.min():.4f}, {TI_P95_DLC13.max():.4f}]")\nprint(f"Primeros 10 valores:")\nprint(f"  Umean:  {Umean_DLC13[:10]}")\nprint(f"  TI_P95: {TI_P95_DLC13[:10]}")\n\n# ============================================================================\n# GR√ÅFICO 3: DLC12a + DLC13 COMBINADOS\n# ============================================================================\nplt.figure(figsize=(14, 9))\n\n# Scatter plots\nplt.scatter(u_mean_12a, ti_12a, alpha=0.15, s=8, c=\'steelblue\', edgecolors=\'none\', label=\'Datos DLC12a\')\nplt.scatter(u_mean_13, ti_13, alpha=0.15, s=8, c=\'orange\', edgecolors=\'none\', label=\'Datos DLC13\')\n\n# Bandas DLC12a (azul)\nplt.plot(Umean_DLC12a, ti_p50_12a, \'b-\', linewidth=2.5, label=\'DLC12a - Mediana (P50)\', zorder=5)\nplt.plot(Umean_DLC12a, TI_P95_DLC12a, \'b--\', linewidth=2, label=\'DLC12a - P95\', zorder=5)\nplt.plot(Umean_DLC12a, ti_p5_12a, \'b--\', linewidth=2, label=\'DLC12a - P5\', zorder=5)\nplt.fill_between(Umean_DLC12a, ti_p5_12a, TI_P95_DLC12a, alpha=0.1, color=\'blue\')\n\n# Bandas DLC13 (naranja)\nplt.plot(Umean_DLC13, ti_p50_13, color=\'darkorange\', linewidth=2.5, label=\'DLC13 - Mediana (P50)\', zorder=5)\nplt.plot(Umean_DLC13, TI_P95_DLC13, \'--\', color=\'darkorange\', linewidth=2, label=\'DLC13 - P95\', zorder=5)\nplt.plot(Umean_DLC13, ti_p5_13, \'--\', color=\'darkorange\', linewidth=2, label=\'DLC13 - P5\', zorder=5)\nplt.fill_between(Umean_DLC13, ti_p5_13, TI_P95_DLC13, alpha=0.1, color=\'orange\')\n\nplt.xlabel(\'U_mean [m/s]\', fontsize=13)\nplt.ylabel(\'Intensidad de Turbulencia (TI) [-]\', fontsize=13)\nplt.title(\'Intensidad de Turbulencia vs Velocidad Media - DLC12a y DLC13\', fontsize=14, fontweight=\'bold\')\nplt.ylim(0, 1)\nplt.grid(True, alpha=0.3)\nplt.legend(loc=\'upper right\', fontsize=10, ncol=2)\nplt.tight_layout()\nplt.show()\n\nprint(f"\\n{\'=\'*60}")\nprint(f"RESUMEN COMPARATIVO:")\nprint(f"{\'=\'*60}")\nprint(f"DLC12a: {len(u_mean_12a)} puntos totales, {len(Umean_DLC12a)} bins desde 3 m/s")\nprint(f"DLC13:  {len(u_mean_13)} puntos totales, {len(Umean_DLC13)} bins desde 3 m/s")\n\n# ============================================================================\n# EXPORTAR ARRAYS A ARCHIVOS .TXT\n# ============================================================================\nprint(f"\\n{\'=\'*60}")\nprint(f"EXPORTANDO ARRAYS A ARCHIVOS .TXT:")\nprint(f"{\'=\'*60}")\n\n# Exportar DLC12a\noutput_file_12a_umean = data_folder / \'Umean_DLC12a.txt\'\noutput_file_12a_tip95 = data_folder / \'TI_P95_DLC12a.txt\'\n\nnp.savetxt(output_file_12a_umean, Umean_DLC12a, fmt=\'%.6f\', header=\'Umean_DLC12a [m/s]\', comments=\'\')\nnp.savetxt(output_file_12a_tip95, TI_P95_DLC12a, fmt=\'%.8f\', header=\'TI_P95_DLC12a [-]\', comments=\'\')\n\nprint(f"‚úì Exportado: {output_file_12a_umean.name} ({len(Umean_DLC12a)} valores)")\nprint(f"‚úì Exportado: {output_file_12a_tip95.name} ({len(TI_P95_DLC12a)} valores)")\n\n# Exportar DLC13\noutput_file_13_umean = data_folder / \'Umean_DLC13.txt\'\noutput_file_13_tip95 = data_folder / \'TI_P95_DLC13.txt\'\n\nnp.savetxt(output_file_13_umean, Umean_DLC13, fmt=\'%.6f\', header=\'Umean_DLC13 [m/s]\', comments=\'\')\nnp.savetxt(output_file_13_tip95, TI_P95_DLC13, fmt=\'%.8f\', header=\'TI_P95_DLC13 [-]\', comments=\'\')\n\nprint(f"‚úì Exportado: {output_file_13_umean.name} ({len(Umean_DLC13)} valores)")\nprint(f"‚úì Exportado: {output_file_13_tip95.name} ({len(TI_P95_DLC13)} valores)")\n\nprint(f"\\nTodos los archivos exportados en: {data_folder}")',
            '# ============================================================================\n# VISUALIZACI√ìN: Componentes 0P, 1P, 2P y Reconstrucci√≥n de se√±ales\n# ============================================================================\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n# Cargar el CSV\ncsv_path = Path(r"C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub\\\\data_train_traditional_ML\\\\0040_DLC12a_150_000.csv")\ndf = pd.read_csv(csv_path)\n\nprint(f"CSV cargado: {csv_path.name}")\nprint(f"Shape: {df.shape}")\nprint(f"\\nColumnas disponibles que contienen \'M_\':")\nprint([col for col in df.columns if \'M_\' in col or \'Blade root\' in col])\n\n# ============================================================================\n# PARTE 1: Plot de las 5 componentes de frecuencia (M_0, M_1c, M_1s, M_2c, M_2s)\n# ============================================================================\n\nfig, axes = plt.subplots(5, 1, figsize=(14, 12))\nfig.suptitle(\'Componentes de Frecuencia 0P, 1P y 2P\', fontsize=16, fontweight=\'bold\')\n\ntime = df[\'Time\'].values\n\n# M_0 (0P - componente lento)\naxes[0].plot(time, df[\'M_0\'].values, \'b-\', linewidth=1)\naxes[0].set_ylabel(\'M_0 (0P)\\n[kNm]\', fontsize=10, fontweight=\'bold\')\naxes[0].set_title(\'Componente 0P (lento/promedio)\', fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\n# M_1c (1P coseno)\naxes[1].plot(time, df[\'M_1c\'].values, \'r-\', linewidth=1)\naxes[1].set_ylabel(\'M_1c (1P cos)\\n[kNm]\', fontsize=10, fontweight=\'bold\')\naxes[1].set_title(\'Componente 1P en fase (coseno)\', fontsize=11)\naxes[1].grid(True, alpha=0.3)\n\n# M_1s (1P seno)\naxes[2].plot(time, df[\'M_1s\'].values, \'g-\', linewidth=1)\naxes[2].set_ylabel(\'M_1s (1P sin)\\n[kNm]\', fontsize=10, fontweight=\'bold\')\naxes[2].set_title(\'Componente 1P en cuadratura (seno)\', fontsize=11)\naxes[2].grid(True, alpha=0.3)\n\n# M_2c (2P coseno)\naxes[3].plot(time, df[\'M_2c\'].values, \'m-\', linewidth=1)\naxes[3].set_ylabel(\'M_2c (2P cos)\\n[kNm]\', fontsize=10, fontweight=\'bold\')\naxes[3].set_title(\'Componente 2P en fase (coseno)\', fontsize=11)\naxes[3].grid(True, alpha=0.3)\n\n# M_2s (2P seno)\naxes[4].plot(time, df[\'M_2s\'].values, \'c-\', linewidth=1)\naxes[4].set_ylabel(\'M_2s (2P sin)\\n[kNm]\', fontsize=10, fontweight=\'bold\')\naxes[4].set_title(\'Componente 2P en cuadratura (seno)\', fontsize=11)\naxes[4].set_xlabel(\'Tiempo [s]\', fontsize=11, fontweight=\'bold\')\naxes[4].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# PARTE 2: Reconstrucci√≥n de Blade root 1 My y Blade root 2 My\n# ============================================================================\n\nprint("\\n" + "="*70)\nprint("RECONSTRUYENDO SE√ëALES ORIGINALES")\nprint("="*70)\n\n# Obtener datos necesarios\nM_0 = df[\'M_0\'].values\nM_1c = df[\'M_1c\'].values\nM_1s = df[\'M_1s\'].values\nM_2c = df[\'M_2c\'].values\nM_2s = df[\'M_2s\'].values\nazimuth = df[\'Rotor azimuth angle\'].values\n\n# Convertir azimut a radianes si est√° en grados\nif azimuth.max() > 6.5:\n    azimuth_rad = np.deg2rad(azimuth)\n    print("Azimut convertido de grados a radianes")\nelse:\n    azimuth_rad = azimuth\n    print("Azimut ya est√° en radianes")\n\n# Reconstruir componentes diferenciales y de suma\nprint("\\nReconstruyendo componentes intermedias...")\n\n# Recuperar M_diff (1P) desde M_1c y M_1s\nM_diff_reconstructed = M_1c * np.cos(azimuth_rad) + M_1s * np.sin(azimuth_rad)\nprint("   ‚úì M_Œî (1P) reconstruido")\n\n# Recuperar M_sum (2P) desde M_2c y M_2s\nM_sum_reconstructed = M_2c * np.cos(2 * azimuth_rad) + M_2s * np.sin(2 * azimuth_rad)\nprint("   ‚úì M_Œ£ (2P) reconstruido")\n\n# Reconstruir las se√±ales originales\n# Opci√≥n 1: Solo con 1P (sin 2P)\nM1_reconstructed_1P = M_0 + M_diff_reconstructed \nM2_reconstructed_1P = M_0 - M_diff_reconstructed \n\n\n# Opci√≥n 2: Con 1P y 2P (m√°s completa)\nM1_reconstructed_full = M_0 + M_diff_reconstructed \nM2_reconstructed_full = M_0 - M_diff_reconstructed \n\nprint("   ‚úì Blade root 1 My reconstruido")\nprint("   ‚úì Blade root 2 My reconstruido")\n\n# Se√±ales originales\nM1_original = df[\'Blade root 1 My\'].values\nM2_original = df[\'Blade root 2 My\'].values\n\n# Calcular errores de reconstrucci√≥n\nerror_M1_1P = np.sqrt(np.mean((M1_original - M1_reconstructed_1P)**2))\nerror_M2_1P = np.sqrt(np.mean((M2_original - M2_reconstructed_1P)**2))\nerror_M1_full = np.sqrt(np.mean((M1_original - M1_reconstructed_full)**2))\nerror_M2_full = np.sqrt(np.mean((M2_original - M2_reconstructed_full)**2))\n\nprint(f"\\nErrores de reconstrucci√≥n (RMSE):")\nprint(f"   Blade root 1 My (solo 1P):    {error_M1_1P:.2f} kNm")\nprint(f"   Blade root 2 My (solo 1P):    {error_M2_1P:.2f} kNm")\nprint(f"   Blade root 1 My (1P + 2P):    {error_M1_full:.2f} kNm")\nprint(f"   Blade root 2 My (1P + 2P):    {error_M2_full:.2f} kNm")\n\n# ============================================================================\n# PLOT: Comparaci√≥n Original vs Reconstrucci√≥n\n# ============================================================================\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 10))\nfig.suptitle(\'Reconstrucci√≥n de Momentos Flectores desde Componentes 0P, 1P y 2P\', \n             fontsize=16, fontweight=\'bold\')\n\n# SUBPLOT 1: Blade root 1 My\naxes[0].plot(time, M1_original, \'b-\', linewidth=2, label=\'Original\', alpha=0.7)\naxes[0].plot(time, M1_reconstructed_1P, \'r--\', linewidth=1.5, \n             label=f\'Reconstrucci√≥n (0P + 1P) - RMSE={error_M1_1P:.2f}\', alpha=0.8)\naxes[0].plot(time, M1_reconstructed_full, \'g:\', linewidth=1.5, \n             label=f\'Reconstrucci√≥n (0P + 1P + 2P) - RMSE={error_M1_full:.2f}\', alpha=0.8)\naxes[0].set_ylabel(\'Blade root 1 My [kNm]\', fontsize=11, fontweight=\'bold\')\naxes[0].set_title(\'Blade root 1 My - Original vs Reconstrucci√≥n\', fontsize=12)\naxes[0].legend(loc=\'upper right\', fontsize=9)\naxes[0].grid(True, alpha=0.3)\n\n# SUBPLOT 2: Blade root 2 My\naxes[1].plot(time, M2_original, \'b-\', linewidth=2, label=\'Original\', alpha=0.7)\naxes[1].plot(time, M2_reconstructed_1P, \'r--\', linewidth=1.5, \n             label=f\'Reconstrucci√≥n (0P + 1P) - RMSE={error_M2_1P:.2f}\', alpha=0.8)\naxes[1].plot(time, M2_reconstructed_full, \'g:\', linewidth=1.5, \n             label=f\'Reconstrucci√≥n (0P + 1P + 2P) - RMSE={error_M2_full:.2f}\', alpha=0.8)\naxes[1].set_ylabel(\'Blade root 2 My [kNm]\', fontsize=11, fontweight=\'bold\')\naxes[1].set_xlabel(\'Tiempo [s]\', fontsize=11, fontweight=\'bold\')\naxes[1].set_title(\'Blade root 2 My - Original vs Reconstrucci√≥n\', fontsize=12)\naxes[1].legend(loc=\'upper right\', fontsize=9)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# PLOT ADICIONAL: Error de reconstrucci√≥n en el tiempo\n# ============================================================================\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8))\nfig.suptitle(\'Error de Reconstrucci√≥n en el Tiempo\', fontsize=16, fontweight=\'bold\')\n\n# Error Blade root 1 My\nerror_1P_M1 = M1_original - M1_reconstructed_1P\nerror_full_M1 = M1_original - M1_reconstructed_full\n\naxes[0].plot(time, error_1P_M1, \'r-\', linewidth=1, label=\'Error (0P + 1P)\', alpha=0.7)\naxes[0].plot(time, error_full_M1, \'g-\', linewidth=1, label=\'Error (0P + 1P + 2P)\', alpha=0.7)\naxes[0].axhline(y=0, color=\'k\', linestyle=\'--\', linewidth=0.5)\naxes[0].set_ylabel(\'Error [kNm]\', fontsize=11, fontweight=\'bold\')\naxes[0].set_title(\'Blade root 1 My - Error de Reconstrucci√≥n\', fontsize=12)\naxes[0].legend(loc=\'upper right\', fontsize=9)\naxes[0].grid(True, alpha=0.3)\n\n# Error Blade root 2 My\nerror_1P_M2 = M2_original - M2_reconstructed_1P\nerror_full_M2 = M2_original - M2_reconstructed_full\n\naxes[1].plot(time, error_1P_M2, \'r-\', linewidth=1, label=\'Error (0P + 1P)\', alpha=0.7)\naxes[1].plot(time, error_full_M2, \'g-\', linewidth=1, label=\'Error (0P + 1P + 2P)\', alpha=0.7)\naxes[1].axhline(y=0, color=\'k\', linestyle=\'--\', linewidth=0.5)\naxes[1].set_ylabel(\'Error [kNm]\', fontsize=11, fontweight=\'bold\')\naxes[1].set_xlabel(\'Tiempo [s]\', fontsize=11, fontweight=\'bold\')\naxes[1].set_title(\'Blade root 2 My - Error de Reconstrucci√≥n\', fontsize=12)\naxes[1].legend(loc=\'upper right\', fontsize=9)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# PLOT ZOOM: Original vs Reconstrucci√≥n 1P (200-260s)\n# ============================================================================\n\n# Crear m√°scara para el rango de tiempo deseado\ntime_mask = (time >= 200) & (time <= 260)\ntime_zoom = time[time_mask]\n\nif len(time_zoom) > 0:\n    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n    fig.suptitle(\'Reconstrucci√≥n 1P - ZOOM 200-260s\', \n                 fontsize=16, fontweight=\'bold\')\n    \n    # SUBPLOT 1: Blade root 1 My (zoom)\n    axes[0].plot(time_zoom, M1_original[time_mask], \'b-\', linewidth=2, \n                 label=\'M1 Original\', alpha=0.8)\n    axes[0].plot(time_zoom, M1_reconstructed_1P[time_mask], \'r--\', linewidth=2, \n                 label=f\'M1 Reconstrucci√≥n 1P - RMSE={error_M1_1P:.2f}\', alpha=0.8)\n    axes[0].set_ylabel(\'Blade root 1 My [kNm]\', fontsize=11, fontweight=\'bold\')\n    axes[0].set_title(\'Blade root 1 My - Original vs Reconstrucci√≥n 1P (200-260s)\', fontsize=12)\n    axes[0].legend(loc=\'upper right\', fontsize=10)\n    axes[0].grid(True, alpha=0.3)\n    \n    # SUBPLOT 2: Blade root 2 My (zoom)\n    axes[1].plot(time_zoom, M2_original[time_mask], \'b-\', linewidth=2, \n                 label=\'M2 Original\', alpha=0.8)\n    axes[1].plot(time_zoom, M2_reconstructed_1P[time_mask], \'r--\', linewidth=2, \n                 label=f\'M2 Reconstrucci√≥n 1P - RMSE={error_M2_1P:.2f}\', alpha=0.8)\n    axes[1].set_ylabel(\'Blade root 2 My [kNm]\', fontsize=11, fontweight=\'bold\')\n    axes[1].set_xlabel(\'Tiempo [s]\', fontsize=11, fontweight=\'bold\')\n    axes[1].set_title(\'Blade root 2 My - Original vs Reconstrucci√≥n 1P (200-260s)\', fontsize=12)\n    axes[1].legend(loc=\'upper right\', fontsize=10)\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f"\\n‚úì Plot con zoom 200-260s generado ({len(time_zoom)} puntos)")\nelse:\n    print("\\n‚ö†Ô∏è No hay datos en el rango 200-260s")\n\nprint("\\n" + "="*70)\nprint("VISUALIZACI√ìN COMPLETADA")\nprint("="*70)',
            '    # ========================================================================\n    # AN√ÅLISIS: TURBULENCIA POR BEAM Y RANGO\n    # ========================================================================\n\n    print(f"\\n{\'=\'*70}")\n    print("AN√ÅLISIS: TURBULENCIA POR BEAM Y RANGO")\n    print("="*70)\n\n    turbulence_rows = []\n    for col in df.columns:\n        if \'LAC_VLOS\' in col and \'lag\' not in col.lower():\n            beam_match = re.search(r\'BEAM(\\d+)\', col)\n            range_match = re.search(r\'RANGE(\\d+)\', col)\n            if not beam_match or not range_match:\n                continue\n            beam_num = int(beam_match.group(1))\n            if beam_num in [8, 9]:\n                continue\n            range_num = int(range_match.group(1))\n            series = df[col]\n            mean_val = series.mean()\n            std_val = series.std()\n            ti_val = std_val / abs(mean_val) if mean_val != 0 else np.nan\n            turbulence_rows.append({\n                \'Beam\': beam_num,\n                \'Range\': range_num,\n                \'Media\': mean_val,\n                \'Std\': std_val,\n                \'TI\': ti_val\n            })\n\n    if len(turbulence_rows) == 0:\n        print("‚ùå No se encontraron columnas LAC_VLOS para calcular turbulencia")\n    else:\n        turbulence_df = pd.DataFrame(turbulence_rows)\n        print(turbulence_df.sort_values([\'Range\', \'Beam\']).to_string(index=False))\n\n        pivot_ti = turbulence_df.pivot(index=\'Range\', columns=\'Beam\', values=\'TI\')\n        fig, ax = plt.subplots(figsize=(10, 6))\n        im = ax.imshow(pivot_ti.values, cmap=\'viridis\', aspect=\'auto\')\n        ax.set_title(\'Turbulencia (TI = Std/Media) por Range y Beam\', fontsize=12, fontweight=\'bold\')\n        ax.set_xlabel(\'Beam\', fontsize=10, fontweight=\'bold\')\n        ax.set_ylabel(\'Range\', fontsize=10, fontweight=\'bold\')\n        ax.set_xticks(range(len(pivot_ti.columns)))\n        ax.set_xticklabels(pivot_ti.columns)\n        ax.set_yticks(range(len(pivot_ti.index)))\n        ax.set_yticklabels(pivot_ti.index)\n        cbar = fig.colorbar(im, ax=ax)\n        cbar.set_label(\'TI (Std/Media)\')\n\n        # Anotar valores en el heatmap\n        for i, range_val in enumerate(pivot_ti.index):\n            for j, beam_val in enumerate(pivot_ti.columns):\n                val = pivot_ti.iloc[i, j]\n                if pd.notna(val):\n                    ax.text(j, i, f"{val:.2f}", ha=\'center\', va=\'center\', color=\'white\', fontsize=8)\n\n        plt.tight_layout()\n        plt.show()\n        print("‚úÖ Gr√°fico de turbulencia por beam y rango generado")\n',
            '# ========================================================================\n# AN√ÅLISIS: MEDIA VLOS POR BEAM Y RANGO\n# ========================================================================\n\nprint(f"\\n{\'=\'*70}")\nprint("AN√ÅLISIS: MEDIA VLOS POR BEAM Y RANGO")\nprint("="*70)\n\nmean_rows = []\nfor col in df.columns:\n    if \'LAC_VLOS\' in col and \'lag\' not in col.lower():\n        beam_match = re.search(r\'BEAM(\\d+)\', col)\n        range_match = re.search(r\'RANGE(\\d+)\', col)\n        if not beam_match or not range_match:\n            continue\n\n        beam_num = int(beam_match.group(1))\n        if beam_num in [8, 9]:   # opcional: mantener exclusi√≥n como en tu bloque TI\n            continue\n\n        range_num = int(range_match.group(1))\n        mean_val = df[col].mean()\n        std_val = df[col].std()\n\n        mean_rows.append({\n            \'Beam\': beam_num,\n            \'Range\': range_num,\n            \'Media\': mean_val,\n            \'Std\': std_val\n        })\n\nif len(mean_rows) == 0:\n    print("‚ùå No se encontraron columnas LAC_VLOS para calcular medias")\nelse:\n    mean_df = pd.DataFrame(mean_rows)\n    print(mean_df.sort_values([\'Range\', \'Beam\']).to_string(index=False))\n\n    # Heatmap Range x Beam con la MEDIA\n    pivot_mean = mean_df.pivot(index=\'Range\', columns=\'Beam\', values=\'Media\')\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    im = ax.imshow(pivot_mean.values, cmap=\'plasma\', aspect=\'auto\')\n\n    ax.set_title(\'Media VLOS por Range y Beam\', fontsize=12, fontweight=\'bold\')\n    ax.set_xlabel(\'Beam\', fontsize=10, fontweight=\'bold\')\n    ax.set_ylabel(\'Range\', fontsize=10, fontweight=\'bold\')\n\n    ax.set_xticks(range(len(pivot_mean.columns)))\n    ax.set_xticklabels(pivot_mean.columns)\n    ax.set_yticks(range(len(pivot_mean.index)))\n    ax.set_yticklabels(pivot_mean.index)\n\n    cbar = fig.colorbar(im, ax=ax)\n    cbar.set_label(\'Media VLOS (m/s)\')\n\n    # Anotar valores en el heatmap\n    for i, range_val in enumerate(pivot_mean.index):\n        for j, beam_val in enumerate(pivot_mean.columns):\n            val = pivot_mean.iloc[i, j]\n            if pd.notna(val):\n                ax.text(j, i, f"{val:.2f}", ha=\'center\', va=\'center\',\n                        color=\'white\', fontsize=8)\n\n    plt.tight_layout()\n    plt.show()\n    print("‚úÖ Gr√°fico de media por beam y rango generado")\n',
            '# ============================================================================\n# GR√ÅFICO ESPACIAL: VIENTO MEDIO POR BEAM EN UN RANGO ESPEC√çFICO\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom pathlib import Path\n\n# ---------------------------\n# CONFIGURACI√ìN\n# ---------------------------\ncsv_path = Path(r"C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train_traditional_ML\\0040_DLC12a_150_000.csv")\nrange_objetivo = 5          # <-- cambia aqu√≠ el rango que quieras (ej: 5)\nexcluir_beams = [8, 9]      # mantenemos misma l√≥gica de exclusi√≥n\nradio = 1.0                 # radio de colocaci√≥n de beams\n\nprint("=" * 70)\nprint("GR√ÅFICO ESPACIAL: VIENTO MEDIO POR BEAM")\nprint("=" * 70)\nprint(f"Archivo: {csv_path.name}")\nprint(f"Rango objetivo: RANGE{range_objetivo}\\n")\n\n# ---------------------------\n# CARGA DE DATOS\n# ---------------------------\ndf = pd.read_csv(csv_path)\nprint(f"‚úÖ Datos cargados: {df.shape[0]:,} filas, {df.shape[1]} columnas")\n\n# ---------------------------\n# EXTRAER MEDIAS POR BEAM EN RANGE OBJETIVO\n# ---------------------------\nbeam_mean = {}   # {beam_num: mean_val}\n\nfor col in df.columns:\n    if \'LAC_VLOS\' in col and \'lag\' not in col.lower():\n        beam_match = re.search(r\'BEAM(\\d+)\', col)\n        range_match = re.search(r\'RANGE(\\d+)\', col)\n\n        if not beam_match or not range_match:\n            continue\n\n        b = int(beam_match.group(1))\n        r = int(range_match.group(1))\n\n        if b in excluir_beams:\n            continue\n\n        if r == range_objetivo:\n            beam_mean[b] = df[col].mean()\n\nif len(beam_mean) == 0:\n    print(f"‚ùå No se encontraron columnas LAC_VLOS para RANGE{range_objetivo}")\nelse:\n    # ---------------------------\n    # DEFINIR POSICIONES ANGULARES (seg√∫n tu esquema)\n    # 0¬∞ arriba, 90¬∞ derecha, 180¬∞ abajo, 270¬∞ izquierda\n    # ---------------------------\n    beam_angle_deg = {\n        0:   0,\n        1:  45,\n        2:  90,\n        3: 135,\n        4: 180,\n        5: 225,\n        6: 270,\n        7: 315\n    }\n\n    # Solo beams existentes en datos\n    beams_disponibles = sorted([b for b in beam_mean.keys() if b in beam_angle_deg])\n\n    # Coordenadas cartesianas con convenci√≥n 0¬∞ hacia arriba:\n    # x = sin(theta), y = cos(theta)\n    xs, ys, vals = [], [], []\n    for b in beams_disponibles:\n        theta = np.deg2rad(beam_angle_deg[b])\n        x = radio * np.sin(theta)\n        y = radio * np.cos(theta)\n\n        xs.append(x)\n        ys.append(y)\n        vals.append(beam_mean[b])\n\n    vals = np.array(vals)\n\n    # ---------------------------\n    # PLOT\n    # ---------------------------\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # c√≠rculo gu√≠a\n    t = np.linspace(0, 2*np.pi, 300)\n    ax.plot(np.cos(t), np.sin(t), \'--\', color=\'gray\', alpha=0.4, linewidth=1)\n\n    # ejes gu√≠a\n    ax.axhline(0, color=\'gray\', alpha=0.3, linewidth=1)\n    ax.axvline(0, color=\'gray\', alpha=0.3, linewidth=1)\n\n    # l√≠neas del centro a cada beam\n    for x, y in zip(xs, ys):\n        ax.plot([0, x], [0, y], color=\'gray\', alpha=0.35, linewidth=1)\n\n    # scatter coloreado por viento medio\n    sc = ax.scatter(xs, ys, c=vals, cmap=\'viridis\', s=800, edgecolor=\'black\', linewidth=1.2)\n\n    # anotaciones beam + valor\n    for b, x, y, v in zip(beams_disponibles, xs, ys, vals):\n        ax.text(x, y + 0.06, f"BEAM {b}", ha=\'center\', va=\'bottom\', fontsize=10, fontweight=\'bold\')\n        ax.text(x, y - 0.02, f"{v:.2f} m/s", ha=\'center\', va=\'top\', fontsize=9)\n\n    # etiquetas cardinales/angulares (opcionales)\n    ax.text(0, 1.18, "0¬∞ (‚Üë)", ha=\'center\', va=\'bottom\', fontsize=10)\n    ax.text(1.18, 0, "90¬∞", ha=\'left\', va=\'center\', fontsize=10)\n    ax.text(0, -1.20, "180¬∞ (‚Üì)", ha=\'center\', va=\'top\', fontsize=10)\n    ax.text(-1.18, 0, "270¬∞", ha=\'right\', va=\'center\', fontsize=10)\n\n    # formato\n    ax.set_title(f"Viento medio LAC_VLOS por BEAM - RANGE{range_objetivo}\\n{csv_path.name}",\n                 fontsize=12, fontweight=\'bold\')\n    ax.set_aspect(\'equal\', adjustable=\'box\')\n    ax.set_xlim(-1.35, 1.35)\n    ax.set_ylim(-1.35, 1.35)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    cbar = plt.colorbar(sc, ax=ax, shrink=0.85)\n    cbar.set_label("Viento medio (m/s)", fontsize=10)\n\n    plt.tight_layout()\n    plt.show()\n\n    print("‚úÖ Gr√°fico generado correctamente")\n    print("\\nResumen (Beam, Mean):")\n    for b in beams_disponibles:\n        print(f"  BEAM {b}: {beam_mean[b]:.3f} m/s")\n',
            '# ============================================================================\n# AN√ÅLISIS VISUAL DE CORTADURA F√çSICA EN UN CSV (dU_dz/dU_dy o _diam)\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\ndef plot_shear_analysis_csv(\n    csv_path,\n    range_target=5,\n    mode="grad",          # "grad" -> dU_dy/dU_dz ; "diam" -> deltaU_y_diam/deltaU_z_diam\n    aggregate_1s=True,    # True: agrega por 1 segundo para visual m√°s limpia\n    rolling_seconds=5,    # suavizado opcional\n    max_points_quiver=2000\n):\n    """\n    Visualiza cortadura f√≠sica en un CSV concreto.\n\n    mode:\n      - "grad": usa dU_dy_RANGE{r}, dU_dz_RANGE{r}\n      - "diam": usa deltaU_y_diam_RANGE{r}, deltaU_z_diam_RANGE{r}\n    """\n\n    csv_path = Path(csv_path)\n    df = pd.read_csv(csv_path)\n\n    if "Time" not in df.columns:\n        raise ValueError("El CSV debe tener columna \'Time\'.")\n\n    if mode == "grad":\n        col_y = f"dU_dy_RANGE{range_target}"   # horizontal\n        col_z = f"dU_dz_RANGE{range_target}"   # vertical\n        y_label = "dU/dy"\n        z_label = "dU/dz"\n    elif mode == "diam":\n        col_y = f"deltaU_y_diam_RANGE{range_target}"   # horizontal (m/s en di√°metro)\n        col_z = f"deltaU_z_diam_RANGE{range_target}"   # vertical   (m/s en di√°metro)\n        y_label = "ŒîU_y (diam)"\n        z_label = "ŒîU_z (diam)"\n    else:\n        raise ValueError("mode debe ser \'grad\' o \'diam\'.")\n\n    required = [col_y, col_z]\n    missing = [c for c in required if c not in df.columns]\n    if missing:\n        raise ValueError(f"Faltan columnas en CSV: {missing}")\n\n    # Seleccionar columnas y limpiar NaNs\n    data = df[["Time", col_y, col_z]].copy().dropna()\n    if data.empty:\n        raise ValueError("No hay datos v√°lidos (todo NaN) para esas columnas.")\n\n    # dt / fs estimado\n    if len(data) > 1:\n        dt = np.median(np.diff(data["Time"].values))\n        fs = 1.0 / dt if dt > 0 else np.nan\n    else:\n        dt, fs = np.nan, np.nan\n\n    # Agregaci√≥n por 1 segundo (opcional)\n    if aggregate_1s:\n        data["t_sec"] = np.floor(data["Time"]).astype(int)\n        data_plot = data.groupby("t_sec", as_index=False)[[col_y, col_z]].mean()\n        data_plot.rename(columns={"t_sec": "Time"}, inplace=True)\n    else:\n        data_plot = data.copy()\n\n    # Rolling para tendencia\n    if rolling_seconds is not None and rolling_seconds > 1 and len(data_plot) > 3:\n        # si est√° agregado a 1s, la ventana = rolling_seconds muestras\n        # si no, aproximamos por fs\n        if aggregate_1s:\n            win = int(rolling_seconds)\n        else:\n            win = int(max(2, round(rolling_seconds * fs))) if np.isfinite(fs) else 10\n\n        data_plot[f"{col_y}_roll"] = data_plot[col_y].rolling(win, min_periods=1).mean()\n        data_plot[f"{col_z}_roll"] = data_plot[col_z].rolling(win, min_periods=1).mean()\n    else:\n        data_plot[f"{col_y}_roll"] = data_plot[col_y]\n        data_plot[f"{col_z}_roll"] = data_plot[col_z]\n\n    # Magnitud combinada de cortadura\n    data_plot["shear_mag"] = np.sqrt(data_plot[col_y]**2 + data_plot[col_z]**2)\n\n    # --------------------------\n    # FIGURA 1: SERIES TEMPORALES\n    # --------------------------\n    fig, axs = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n    fig.suptitle(\n        f"Cortadura f√≠sica - RANGE{range_target} - mode={mode}\\n{csv_path.name}",\n        fontsize=14, fontweight="bold"\n    )\n\n    axs[0].plot(data_plot["Time"], data_plot[col_y], alpha=0.35, lw=1.0, label=y_label)\n    axs[0].plot(data_plot["Time"], data_plot[f"{col_y}_roll"], lw=2.0, label=f"{y_label} (rolling)")\n    axs[0].axhline(0, color="black", ls="--", lw=0.8, alpha=0.5)\n    axs[0].set_ylabel(y_label)\n    axs[0].grid(True, alpha=0.3, ls="--")\n    axs[0].legend()\n\n    axs[1].plot(data_plot["Time"], data_plot[col_z], alpha=0.35, lw=1.0, label=z_label, color="tab:orange")\n    axs[1].plot(data_plot["Time"], data_plot[f"{col_z}_roll"], lw=2.0, label=f"{z_label} (rolling)", color="tab:red")\n    axs[1].axhline(0, color="black", ls="--", lw=0.8, alpha=0.5)\n    axs[1].set_ylabel(z_label)\n    axs[1].grid(True, alpha=0.3, ls="--")\n    axs[1].legend()\n\n    axs[2].plot(data_plot["Time"], data_plot["shear_mag"], color="tab:green", lw=1.5)\n    axs[2].set_ylabel("||shear||")\n    axs[2].set_xlabel("Time (s)")\n    axs[2].grid(True, alpha=0.3, ls="--")\n\n    plt.tight_layout()\n    plt.show()\n\n    # --------------------------\n    # FIGURA 2: "ROSA" DE CORTADURA (horizontal vs vertical)\n    # --------------------------\n    d = data_plot.copy()\n    if len(d) > max_points_quiver:\n        idx = np.linspace(0, len(d)-1, max_points_quiver).astype(int)\n        d = d.iloc[idx].reset_index(drop=True)\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n    sc = ax.scatter(d[col_y], d[col_z], c=d["Time"], cmap="viridis", s=18, alpha=0.7, edgecolors="none")\n    ax.axhline(0, color="black", ls="--", lw=0.8, alpha=0.5)\n    ax.axvline(0, color="black", ls="--", lw=0.8, alpha=0.5)\n    ax.set_xlabel(y_label)\n    ax.set_ylabel(z_label)\n    ax.set_title(f"Plano de cortadura ({y_label} vs {z_label}) - RANGE{range_target}")\n    ax.grid(True, alpha=0.25, ls="--")\n    cbar = plt.colorbar(sc, ax=ax)\n    cbar.set_label("Time (s)")\n    ax.set_aspect("equal", adjustable="box")\n    plt.tight_layout()\n    plt.show()\n\n    # --------------------------\n    # FIGURA 3: RESUMEN ESTAD√çSTICO\n    # --------------------------\n    summary = data_plot[[col_y, col_z, "shear_mag"]].describe(percentiles=[0.05, 0.5, 0.95]).T\n    print("\\n" + "="*70)\n    print("RESUMEN ESTAD√çSTICO")\n    print("="*70)\n    print(summary)\n\n    fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n    axs[0].hist(data_plot[col_y], bins=50, alpha=0.8)\n    axs[0].set_title(f"Distribuci√≥n {y_label}")\n    axs[0].grid(True, alpha=0.25, ls="--")\n\n    axs[1].hist(data_plot[col_z], bins=50, alpha=0.8, color="tab:orange")\n    axs[1].set_title(f"Distribuci√≥n {z_label}")\n    axs[1].grid(True, alpha=0.25, ls="--")\n\n    axs[2].hist(data_plot["shear_mag"], bins=50, alpha=0.8, color="tab:green")\n    axs[2].set_title("Distribuci√≥n ||shear||")\n    axs[2].grid(True, alpha=0.25, ls="--")\n\n    plt.tight_layout()\n    plt.show()\n\n    return data_plot, summary\n',
            '# 1) Gradientes f√≠sicos (recomendado para comparar ranges)\nsummary_grad = plot_shear_analysis_csv(\n    csv_path=r"C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train_traditional_ML\\0040_DLC12a_150_000.csv",\n    range_target=5,\n    mode="grad",\n    aggregate_1s=True\n)\n\n## 2) Versi√≥n en di√°metro (m√°s intuitiva en m/s)\n#_ , summary_diam = plot_shear_analysis_csv(\n#    csv_path=r"C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train_traditional_ML\\0040_DLC12a_150_000.csv",\n#    range_target=5,\n#    mode="diam",\n#    aggregate_1s=True\n#)\n',
            'import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom pathlib import Path\n\ndef plot_beam_map_with_shear(\n    csv_path,\n    range_target=5,\n    mode="grad",             # "grad" => dU_dy/dU_dz ; "diam" => deltaU_y_diam/deltaU_z_diam\n    use_time_mean=True,      # True: media de toda la serie; False: usar un instante\n    time_index=None,         # √≠ndice si use_time_mean=False\n    scale_arrow=0.35         # escala visual de flechas\n):\n    """\n    Mapa espacial de BEAMs + cortadura en el mismo gr√°fico.\n    """\n\n    csv_path = Path(csv_path)\n    df = pd.read_csv(csv_path)\n\n    # -------------------------\n    # 1) localizar columnas VLOS para el range\n    # -------------------------\n    beam_cols = {}\n    for col in df.columns:\n        if \'LAC_VLOS\' in col and \'lag\' not in col.lower():\n            bm = re.search(r\'BEAM(\\d+)\', col)\n            rm = re.search(r\'RANGE(\\d+)\', col)\n            if bm and rm:\n                b = int(bm.group(1))\n                r = int(rm.group(1))\n                if r == range_target and b in range(0, 8):   # beams 0..7\n                    beam_cols[b] = col\n\n    missing_beams = [b for b in range(8) if b not in beam_cols]\n    if missing_beams:\n        raise ValueError(f"Faltan beams para RANGE{range_target}: {missing_beams}")\n\n    # -------------------------\n    # 2) seleccionar shear vars\n    # -------------------------\n    if mode == "grad":\n        col_h = f"dU_dy_RANGE{range_target}"         # horizontal\n        col_v = f"dU_dz_RANGE{range_target}"         # vertical\n        shear_label_h = "dU/dy"\n        shear_label_v = "dU/dz"\n    elif mode == "diam":\n        col_h = f"deltaU_y_diam_RANGE{range_target}" # horizontal\n        col_v = f"deltaU_z_diam_RANGE{range_target}" # vertical\n        shear_label_h = "ŒîU_y(diam)"\n        shear_label_v = "ŒîU_z(diam)"\n    else:\n        raise ValueError("mode debe ser \'grad\' o \'diam\'")\n\n    for c in [col_h, col_v]:\n        if c not in df.columns:\n            raise ValueError(f"No existe la columna \'{c}\' en el CSV.")\n\n    # -------------------------\n    # 3) valor medio por beam + shear\n    # -------------------------\n    if use_time_mean:\n        beam_vals = {b: df[col].mean() for b, col in beam_cols.items()}\n        shear_h = df[col_h].mean()\n        shear_v = df[col_v].mean()\n        title_time = "MEDIA TEMPORAL"\n    else:\n        if time_index is None:\n            time_index = len(df) // 2\n        beam_vals = {b: df[col].iloc[time_index] for b, col in beam_cols.items()}\n        shear_h = df[col_h].iloc[time_index]\n        shear_v = df[col_v].iloc[time_index]\n        t_val = df[\'Time\'].iloc[time_index] if \'Time\' in df.columns else time_index\n        title_time = f"INSTANTE t={t_val:.2f}s"\n\n    # -------------------------\n    # 4) geometr√≠a beams (0¬∞ arriba)\n    # -------------------------\n    beam_angle_deg = {0:0, 1:45, 2:90, 3:135, 4:180, 5:225, 6:270, 7:315}\n    R = 1.0\n    xy = {}\n    for b, ang in beam_angle_deg.items():\n        th = np.deg2rad(ang)\n        x = R * np.sin(th)  # 90¬∞ derecha\n        y = R * np.cos(th)  # 0¬∞ arriba\n        xy[b] = (x, y)\n\n    xs = [xy[b][0] for b in range(8)]\n    ys = [xy[b][1] for b in range(8)]\n    cs = [beam_vals[b] for b in range(8)]\n\n    # -------------------------\n    # 5) plot\n    # -------------------------\n    fig, ax = plt.subplots(figsize=(9, 9))\n\n    # c√≠rculo gu√≠a + ejes\n    t = np.linspace(0, 2*np.pi, 360)\n    ax.plot(np.cos(t), np.sin(t), \'--\', color=\'gray\', alpha=0.35)\n    ax.axhline(0, color=\'gray\', lw=1, alpha=0.3)\n    ax.axvline(0, color=\'gray\', lw=1, alpha=0.3)\n\n    # radios\n    for b in range(8):\n        ax.plot([0, xy[b][0]], [0, xy[b][1]], color=\'gray\', lw=0.8, alpha=0.25)\n\n    # beams coloreados por viento medio\n    sc = ax.scatter(xs, ys, c=cs, cmap=\'viridis\', s=900, edgecolor=\'black\', linewidth=1.2, zorder=3)\n\n    for b in range(8):\n        x, y = xy[b]\n        ax.text(x, y+0.055, f"B{b}", ha=\'center\', va=\'bottom\', fontsize=10, fontweight=\'bold\')\n        ax.text(x, y-0.01, f"{beam_vals[b]:.2f}", ha=\'center\', va=\'top\', fontsize=8)\n\n    cbar = plt.colorbar(sc, ax=ax, shrink=0.85)\n    cbar.set_label(f"Viento medio VLOS (RANGE{range_target})", fontsize=10)\n\n    # -------------------------\n    # 6) flechas shear (dos componentes)\n    # -------------------------\n    # horizontal: eje x ; vertical: eje y\n    # normalizaci√≥n solo visual para que no explote por escala\n    norm = max(abs(shear_h), abs(shear_v), 1e-12)\n    u = scale_arrow * (shear_h / norm)\n    v = scale_arrow * (shear_v / norm)\n\n    # flecha resultante\n    ax.quiver(0, 0, u, v, angles=\'xy\', scale_units=\'xy\', scale=1,\n              color=\'red\', width=0.012, zorder=4, label=\'Shear resultante\')\n\n    # componentes separadas (opcional visual)\n    ax.quiver(0, 0, u, 0, angles=\'xy\', scale_units=\'xy\', scale=1,\n              color=\'orange\', width=0.008, alpha=0.9, zorder=4)\n    ax.quiver(0, 0, 0, v, angles=\'xy\', scale_units=\'xy\', scale=1,\n              color=\'deepskyblue\', width=0.008, alpha=0.9, zorder=4)\n\n    txt = (\n        f"{shear_label_h} = {shear_h:+.4f}\\n"\n        f"{shear_label_v} = {shear_v:+.4f}"\n    )\n    ax.text(0.03, 0.97, txt, transform=ax.transAxes, va=\'top\', ha=\'left\',\n            bbox=dict(boxstyle=\'round\', facecolor=\'white\', alpha=0.85), fontsize=10)\n\n    ax.set_title(\n        f"RANGE{range_target} | {title_time}\\n"\n        f"Mapa de BEAMs + cortadura ({mode})",\n        fontsize=13, fontweight=\'bold\'\n    )\n    ax.set_aspect(\'equal\')\n    ax.set_xlim(-1.35, 1.35)\n    ax.set_ylim(-1.35, 1.35)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n',
            '# 1) media temporal, usando gradientes (recomendado)\nplot_beam_map_with_shear(\n    csv_path=r"C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train_traditional_ML\\0040_DLC12a_150_000.csv",\n    range_target=5,\n    mode="grad",\n    use_time_mean=True\n)\n\n# 2) media temporal, usando _diam\nplot_beam_map_with_shear(\n    csv_path=r"C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train_traditional_ML\\0040_DLC12a_150_000.csv",\n    range_target=5,\n    mode="diam",\n    use_time_mean=True\n)\n\n# 3) un instante concreto\nplot_beam_map_with_shear(\n    csv_path=r"C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train_traditional_ML\\0040_DLC12a_150_000.csv",\n    range_target=5,\n    mode="grad",\n    use_time_mean=False,\n    time_index=6000\n)\n',
            '# ============================================================================\n# VISUALIZACI√ìN: SE√ëALES LAC_VLOS DE BEAMS INDIVIDUALES (0-7)\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom pathlib import Path\n\n# Archivo a analizar\ncsv_path = Path(r"C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub\\\\data_train_traditional_ML\\\\0040_DLC12a_150_000.csv")\n\nprint("="*70)\nprint("VISUALIZACI√ìN: SE√ëALES LAC_VLOS POR BEAM")\nprint("="*70)\nprint(f"Archivo: {csv_path.name}\\n")\n\n# Cargar datos\ndf = pd.read_csv(csv_path)\nprint(f"‚úÖ Datos cargados: {df.shape[0]:,} filas, {df.shape[1]} columnas\\n")\n\n# Buscar columnas LAC_VLOS (sin lag, sin beam8/beam9)\nvlos_cols = []\nfor col in df.columns:\n    if \'LAC_VLOS\' in col and \'lag\' not in col.lower():\n        # Extraer n√∫mero de beam\n        match = re.search(r\'BEAM(\\d+)\', col)\n        if match:\n            beam_num = int(match.group(1))\n            # Excluir beams 8 y 9\n            if beam_num not in [8, 9]:\n                vlos_cols.append((beam_num, col))\n\n# Ordenar por n√∫mero de beam\nvlos_cols.sort(key=lambda x: x[0])\n\nprint(f"Columnas LAC_VLOS encontradas (beams 0-7):")\nfor beam_num, col_name in vlos_cols:\n    nan_pct = df[col_name].isna().sum() / len(df) * 100\n    mean_val = df[col_name].mean()\n    print(f"  BEAM {beam_num}: {col_name}")\n    print(f"    - Media: {mean_val:.3f} m/s | NaN: {nan_pct:.1f}%")\n\nif len(vlos_cols) == 0:\n    print("\\n‚ùå No se encontraron columnas LAC_VLOS v√°lidas")\nelse:\n    print(f"\\n{\'=\'*70}")\n    print(f"GR√ÅFICO: SERIES TEMPORALES DE LAC_VLOS (BEAMS 0-7)")\n    print("="*70)\n    \n    # Crear figura con subplots (uno por beam)\n    n_beams = len(vlos_cols)\n    fig, axes = plt.subplots(n_beams, 1, figsize=(16, 2.5 * n_beams))\n    fig.suptitle(f\'LAC_VLOS - Series Temporales por BEAM\\n{csv_path.name}\', \n                 fontsize=14, fontweight=\'bold\', y=0.995)\n    \n    # Asegurar que axes sea una lista\n    if n_beams == 1:\n        axes = [axes]\n    \n    # Tiempo\n    time = df[\'Time\'].values if \'Time\' in df.columns else np.arange(len(df))\n    \n    # Colores por beam (usar colormap)\n    colors = plt.cm.tab10(np.linspace(0, 1, 8))\n    \n    # Plotear cada beam\n    for idx, (beam_num, col_name) in enumerate(vlos_cols):\n        ax = axes[idx]\n        \n        # Datos\n        vlos = df[col_name].values\n        \n        # Plot\n        ax.plot(time, vlos, linewidth=0.8, color=colors[beam_num], alpha=0.9)\n        ax.axhline(y=0, color=\'black\', linestyle=\'--\', linewidth=0.5, alpha=0.3)\n        \n        # Estad√≠sticas\n        mean_val = np.nanmean(vlos)\n        std_val = np.nanstd(vlos)\n        min_val = np.nanmin(vlos)\n        max_val = np.nanmax(vlos)\n        \n        # Labels\n        ax.set_ylabel(f\'BEAM {beam_num}\\n(m/s)\', fontsize=10, fontweight=\'bold\')\n        ax.grid(True, alpha=0.3, linestyle=\'--\')\n        \n        # T√≠tulo con √°ngulo\n        angle = beam_num * 45\n        ax.set_title(f\'BEAM {beam_num} - √Ångulo: {angle}¬∞ | Media: {mean_val:.2f} m/s | Std: {std_val:.2f} m/s\', \n                    fontsize=10, fontweight=\'bold\', loc=\'left\')\n        \n        # Estad√≠sticas en cuadro\n        stats_text = f\'Rango: [{min_val:.2f}, {max_val:.2f}]\'\n        ax.text(0.98, 0.95, stats_text, transform=ax.transAxes, \n               verticalalignment=\'top\', horizontalalignment=\'right\',\n               fontsize=8, bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.7))\n    \n    # X label solo en el √∫ltimo\n    axes[-1].set_xlabel(\'Time (s)\', fontsize=11, fontweight=\'bold\')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print("‚úÖ Gr√°fico de series temporales generado")\n    \n    # ========================================================================\n    # AN√ÅLISIS COMPARATIVO: MEDIAS POR BEAM\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("AN√ÅLISIS: COMPARACI√ìN DE MEDIAS POR BEAM")\n    print("="*70)\n    \n    # Extraer medias\n    beam_stats = []\n    for beam_num, col_name in vlos_cols:\n        mean_val = df[col_name].mean()\n        std_val = df[col_name].std()\n        beam_stats.append({\n            \'Beam\': beam_num,\n            \'√Ångulo\': beam_num * 45,\n            \'Media\': mean_val,\n            \'Std\': std_val\n        })\n    \n    stats_df = pd.DataFrame(beam_stats)\n    print(stats_df.to_string(index=False))\n    \n    # Gr√°fico de barras con medias\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    fig.suptitle(\'Comparaci√≥n de Estad√≠sticas por BEAM\', fontsize=14, fontweight=\'bold\')\n    \n    # Subplot 1: Medias\n    ax1.bar(stats_df[\'Beam\'], stats_df[\'Media\'], color=colors[:len(stats_df)], \n           edgecolor=\'black\', alpha=0.8)\n    ax1.set_xlabel(\'BEAM\', fontsize=11, fontweight=\'bold\')\n    ax1.set_ylabel(\'Media VLOS (m/s)\', fontsize=11, fontweight=\'bold\')\n    ax1.set_title(\'Media de Velocidad por BEAM\', fontsize=11, fontweight=\'bold\')\n    ax1.grid(True, alpha=0.3, axis=\'y\', linestyle=\'--\')\n    ax1.set_xticks(stats_df[\'Beam\'])\n    \n    # A√±adir √°ngulos como segundo eje X\n    ax1_top = ax1.twiny()\n    ax1_top.set_xlim(ax1.get_xlim())\n    ax1_top.set_xticks(stats_df[\'Beam\'])\n    ax1_top.set_xticklabels([f"{a}¬∞" for a in stats_df[\'√Ångulo\']])\n    ax1_top.set_xlabel(\'√Ångulo\', fontsize=10)\n    \n    # Subplot 2: Desviaciones est√°ndar\n    ax2.bar(stats_df[\'Beam\'], stats_df[\'Std\'], color=colors[:len(stats_df)], \n           edgecolor=\'black\', alpha=0.8)\n    ax2.set_xlabel(\'BEAM\', fontsize=11, fontweight=\'bold\')\n    ax2.set_ylabel(\'Std VLOS (m/s)\', fontsize=11, fontweight=\'bold\')\n    ax2.set_title(\'Desviaci√≥n Est√°ndar por BEAM\', fontsize=11, fontweight=\'bold\')\n    ax2.grid(True, alpha=0.3, axis=\'y\', linestyle=\'--\')\n    ax2.set_xticks(stats_df[\'Beam\'])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print("‚úÖ Gr√°fico de comparaci√≥n generado")\n    \n    # ========================================================================\n    # DIAGN√ìSTICO: SHEAR CALCULADO A MANO\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("DIAGN√ìSTICO: C√ÅLCULO MANUAL DE SHEAR")\n    print("="*70)\n    \n    # Configuraci√≥n actual\n    beams_up = [0, 1, 7]\n    beams_down = [3, 4, 5]\n    beams_left = [5, 6, 7]\n    beams_right = [1, 2, 3]\n    \n    # Extraer medias por grupo\n    mean_up = np.mean([stats_df[stats_df[\'Beam\'] == b][\'Media\'].values[0] \n                       for b in beams_up if b in stats_df[\'Beam\'].values])\n    mean_down = np.mean([stats_df[stats_df[\'Beam\'] == b][\'Media\'].values[0] \n                         for b in beams_down if b in stats_df[\'Beam\'].values])\n    mean_left = np.mean([stats_df[stats_df[\'Beam\'] == b][\'Media\'].values[0] \n                         for b in beams_left if b in stats_df[\'Beam\'].values])\n    mean_right = np.mean([stats_df[stats_df[\'Beam\'] == b][\'Media\'].values[0] \n                          for b in beams_right if b in stats_df[\'Beam\'].values])\n    \n    shear_vert = mean_up - mean_down\n    shear_horiz = mean_left - mean_right\n    \n    print(f"Configuraci√≥n actual:")\n    print(f"  Arriba (‚Üë):     beams {beams_up}     ‚Üí media: {mean_up:.3f} m/s")\n    print(f"  Abajo (‚Üì):      beams {beams_down}   ‚Üí media: {mean_down:.3f} m/s")\n    print(f"  Izquierda (‚Üê):  beams {beams_left}   ‚Üí media: {mean_left:.3f} m/s")\n    print(f"  Derecha (‚Üí):    beams {beams_right}  ‚Üí media: {mean_right:.3f} m/s")\n    print(f"\\nüîç SHEAR CALCULADO:")\n    print(f"  U_shear_vert  = {mean_up:.3f} - {mean_down:.3f} = {shear_vert:+.3f} m/s")\n    print(f"  U_shear_horiz = {mean_left:.3f} - {mean_right:.3f} = {shear_horiz:+.3f} m/s")\n    \n    # Comparar con valores guardados en el CSV\n    if \'U_shear_vert\' in df.columns:\n        csv_shear_vert = df[\'U_shear_vert\'].mean()\n        csv_shear_horiz = df[\'U_shear_horiz\'].mean()\n        print(f"\\nüìä VALORES EN EL CSV:")\n        print(f"  U_shear_vert  (CSV): {csv_shear_vert:+.3f} m/s")\n        print(f"  U_shear_horiz (CSV): {csv_shear_horiz:+.3f} m/s")\n        print(f"\\n‚úì Diferencia vertical:   {abs(shear_vert - csv_shear_vert):.4f} m/s")\n        print(f"‚úì Diferencia horizontal: {abs(shear_horiz - csv_shear_horiz):.4f} m/s")\n    \n    print("="*70)',
            '# ============================================================================\n# VISUALIZACI√ìN DE NUEVAS FEATURES: PITCH COLEMAN Y ESTAD√çSTICAS DE VIENTO\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Archivo a analizar\ncsv_path = Path(r"C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub\\\\data_train_traditional_ML\\\\0040_DLC12a_150_000.csv")\n\nprint("="*70)\nprint("VISUALIZACI√ìN DE NUEVAS FEATURES")\nprint("="*70)\nprint(f"Archivo: {csv_path.name}\\n")\n\n# Cargar datos\ndf = pd.read_csv(csv_path)\nprint(f"‚úÖ Datos cargados: {df.shape[0]:,} filas, {df.shape[1]} columnas\\n")\n\n# Variables a plotear\npitch_vars = [\'pitch_0\', \'pitch_1c\', \'pitch_1s\', \n              \'pitch_0_rate\', \'pitch_1c_rate\', \'pitch_1s_rate\', \'rotor_speed_rate\']\n\nwind_vars = [\'U_mean\', \'U_std\', \'U_shear_vert\', \'U_shear_horiz\']\n\n# Verificar qu√© variables existen\npitch_available = [v for v in pitch_vars if v in df.columns]\nwind_available = [v for v in wind_vars if v in df.columns]\n\nprint(f"Variables de pitch Coleman encontradas: {len(pitch_available)}/{len(pitch_vars)}")\nprint(f"Variables de estad√≠sticas de viento encontradas: {len(wind_available)}/{len(wind_vars)}")\n\nif len(pitch_available) == 0 and len(wind_available) == 0:\n    print("\\n‚ùå ERROR: No se encontraron las variables. Ejecuta create_pitch_coleman_features() "\n          "y create_wind_field_statistics() primero.")\nelse:\n    # ========================================================================\n    # GR√ÅFICO 1: PITCH COLEMAN - TIME SERIES\n    # ========================================================================\n    \n    if len(pitch_available) >= 3:\n        print(f"\\n{\'=\'*70}")\n        print("GR√ÅFICO 1: PITCH COLEMAN - COMPONENTES")\n        print("="*70)\n        \n        fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n        fig.suptitle(f\'Pitch Coleman Transformation - {csv_path.name}\', \n                     fontsize=14, fontweight=\'bold\', y=0.995)\n        \n        time = df[\'Time\'].values if \'Time\' in df.columns else np.arange(len(df))\n        \n        # Subplot 1: pitch_0 (colectivo)\n        if \'pitch_0\' in df.columns:\n            ax = axes[0]\n            ax.plot(time, df[\'pitch_0\'], linewidth=1, color=\'steelblue\', alpha=0.8)\n            ax.set_ylabel(\'Œ∏‚ÇÄ (¬∞)\', fontsize=11, fontweight=\'bold\')\n            ax.set_title(\'Componente Colectivo\', fontsize=11, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            \n            # Estad√≠sticas\n            mean_val = df[\'pitch_0\'].mean()\n            std_val = df[\'pitch_0\'].std()\n            ax.text(0.02, 0.95, f\'Media: {mean_val:.2f}¬∞\\nStd: {std_val:.2f}¬∞\',\n                   transform=ax.transAxes, verticalalignment=\'top\',\n                   bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.8),\n                   fontsize=9)\n        \n        # Subplot 2: pitch_1c y pitch_1s (componentes 1P)\n        ax = axes[1]\n        if \'pitch_1c\' in df.columns:\n            ax.plot(time, df[\'pitch_1c\'], linewidth=1, color=\'red\', \n                   alpha=0.7, label=\'Œ∏‚ÇÅc (coseno)\')\n        if \'pitch_1s\' in df.columns:\n            ax.plot(time, df[\'pitch_1s\'], linewidth=1, color=\'blue\', \n                   alpha=0.7, label=\'Œ∏‚ÇÅs (seno)\')\n        ax.set_ylabel(\'Œ∏‚ÇÅc, Œ∏‚ÇÅs (¬∞)\', fontsize=11, fontweight=\'bold\')\n        ax.set_title(\'Componentes 1P (Ejes Fijos)\', fontsize=11, fontweight=\'bold\')\n        ax.grid(True, alpha=0.3, linestyle=\'--\')\n        ax.legend(loc=\'upper right\', fontsize=10)\n        \n        # Subplot 3: Rates\n        ax = axes[2]\n        if \'pitch_0_rate\' in df.columns:\n            ax.plot(time, df[\'pitch_0_rate\'], linewidth=0.8, color=\'green\', \n                   alpha=0.6, label=\'Œ∏Ãá‚ÇÄ\')\n        if \'pitch_1c_rate\' in df.columns:\n            ax.plot(time, df[\'pitch_1c_rate\'], linewidth=0.8, color=\'orange\', \n                   alpha=0.6, label=\'Œ∏Ãá‚ÇÅc\')\n        if \'pitch_1s_rate\' in df.columns:\n            ax.plot(time, df[\'pitch_1s_rate\'], linewidth=0.8, color=\'purple\', \n                   alpha=0.6, label=\'Œ∏Ãá‚ÇÅs\')\n        ax.set_xlabel(\'Time (s)\', fontsize=11, fontweight=\'bold\')\n        ax.set_ylabel(\'Rates (¬∞/s)\', fontsize=11, fontweight=\'bold\')\n        ax.set_title(\'Derivadas Temporales (Velocidades Angulares)\', \n                    fontsize=11, fontweight=\'bold\')\n        ax.grid(True, alpha=0.3, linestyle=\'--\')\n        ax.legend(loc=\'upper right\', fontsize=10, ncol=3)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print("‚úÖ Gr√°fico 1 generado")\n    \n    # ========================================================================\n    # GR√ÅFICO 2: PITCH - SCATTER PLOTS (relaciones)\n    # ========================================================================\n    \n    if \'pitch_1c\' in df.columns and \'pitch_1s\' in df.columns:\n        print(f"\\n{\'=\'*70}")\n        print("GR√ÅFICO 2: PITCH COLEMAN - SCATTER PLOTS")\n        print("="*70)\n        \n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n        fig.suptitle(f\'Pitch Coleman - Relaciones - {csv_path.name}\', \n                     fontsize=14, fontweight=\'bold\')\n        \n        # Subplot 1: Œ∏‚ÇÅc vs Œ∏‚ÇÅs (deber√≠a formar un c√≠rculo/elipse)\n        ax = axes[0]\n        scatter = ax.scatter(df[\'pitch_1c\'], df[\'pitch_1s\'], \n                           c=time, cmap=\'viridis\', s=2, alpha=0.6)\n        ax.set_xlabel(\'Œ∏‚ÇÅc (¬∞)\', fontsize=11, fontweight=\'bold\')\n        ax.set_ylabel(\'Œ∏‚ÇÅs (¬∞)\', fontsize=11, fontweight=\'bold\')\n        ax.set_title(\'Componentes 1P: Œ∏‚ÇÅc vs Œ∏‚ÇÅs\', fontsize=11, fontweight=\'bold\')\n        ax.grid(True, alpha=0.3, linestyle=\'--\')\n        ax.axis(\'equal\')\n        plt.colorbar(scatter, ax=ax, label=\'Time (s)\')\n        \n        # Subplot 2: Œ∏‚ÇÄ vs amplitud 1P\n        if \'pitch_0\' in df.columns:\n            ax = axes[1]\n            amplitude_1P = np.sqrt(df[\'pitch_1c\']**2 + df[\'pitch_1s\']**2)\n            ax.scatter(df[\'pitch_0\'], amplitude_1P, s=2, alpha=0.5, color=\'steelblue\')\n            ax.set_xlabel(\'Œ∏‚ÇÄ - Colectivo (¬∞)\', fontsize=11, fontweight=\'bold\')\n            ax.set_ylabel(\'Amplitud 1P (¬∞)\', fontsize=11, fontweight=\'bold\')\n            ax.set_title(\'Colectivo vs Amplitud 1P\', fontsize=11, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            \n            # Correlaci√≥n\n            corr = np.corrcoef(df[\'pitch_0\'].dropna(), amplitude_1P.dropna())[0, 1]\n            ax.text(0.02, 0.95, f\'Correlaci√≥n: {corr:.3f}\',\n                   transform=ax.transAxes, verticalalignment=\'top\',\n                   bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.8),\n                   fontsize=10)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print("‚úÖ Gr√°fico 2 generado")\n    \n    # ========================================================================\n    # GR√ÅFICO 3: ESTAD√çSTICAS DE VIENTO - TIME SERIES\n    # ========================================================================\n    \n    if len(wind_available) > 0:\n        print(f"\\n{\'=\'*70}")\n        print("GR√ÅFICO 3: ESTAD√çSTICAS DEL CAMPO DE VIENTO")\n        print("="*70)\n        \n        fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n        fig.suptitle(f\'Estad√≠sticas del Campo de Viento LIDAR - {csv_path.name}\', \n                     fontsize=14, fontweight=\'bold\', y=0.995)\n        \n        time = df[\'Time\'].values if \'Time\' in df.columns else np.arange(len(df))\n        \n        # Subplot 1: U_mean\n        if \'U_mean\' in df.columns:\n            ax = axes[0]\n            ax.plot(time, df[\'U_mean\'], linewidth=1, color=\'navy\', alpha=0.8)\n            ax.set_ylabel(\'U_mean (m/s)\', fontsize=11, fontweight=\'bold\')\n            ax.set_title(\'Velocidad Media del Campo de Viento\', \n                        fontsize=11, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            \n            mean_val = df[\'U_mean\'].mean()\n            std_val = df[\'U_mean\'].std()\n            ax.text(0.02, 0.95, f\'Media: {mean_val:.2f} m/s\\nStd: {std_val:.2f} m/s\',\n                   transform=ax.transAxes, verticalalignment=\'top\',\n                   bbox=dict(boxstyle=\'round\', facecolor=\'lightblue\', alpha=0.8),\n                   fontsize=9)\n        \n        # Subplot 2: U_std\n        if \'U_std\' in df.columns:\n            ax = axes[1]\n            ax.plot(time, df[\'U_std\'], linewidth=1, color=\'darkorange\', alpha=0.8)\n            ax.set_ylabel(\'U_std (m/s)\', fontsize=11, fontweight=\'bold\')\n            ax.set_title(\'Heterogeneidad del Campo (Desviaci√≥n Est√°ndar)\', \n                        fontsize=11, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            \n            mean_val = df[\'U_std\'].mean()\n            ax.text(0.02, 0.95, f\'Media: {mean_val:.2f} m/s\',\n                   transform=ax.transAxes, verticalalignment=\'top\',\n                   bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.8),\n                   fontsize=9)\n        \n        # Subplot 3: U_shear_vert\n        if \'U_shear_vert\' in df.columns:\n            ax = axes[2]\n            ax.plot(time, df[\'U_shear_vert\'], linewidth=1, color=\'green\', alpha=0.8)\n            ax.axhline(y=0, color=\'black\', linestyle=\'--\', linewidth=0.8, alpha=0.5)\n            ax.set_ylabel(\'U_shear_vert (m/s)\', fontsize=11, fontweight=\'bold\')\n            ax.set_title(\'Shear Vertical (Arriba - Abajo)\', \n                        fontsize=11, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            \n            mean_val = df[\'U_shear_vert\'].mean()\n            ax.text(0.02, 0.95, f\'Media: {mean_val:.2f} m/s\',\n                   transform=ax.transAxes, verticalalignment=\'top\',\n                   bbox=dict(boxstyle=\'round\', facecolor=\'lightgreen\', alpha=0.8),\n                   fontsize=9)\n        \n        # Subplot 4: U_shear_horiz\n        if \'U_shear_horiz\' in df.columns:\n            ax = axes[3]\n            ax.plot(time, df[\'U_shear_horiz\'], linewidth=1, color=\'purple\', alpha=0.8)\n            ax.axhline(y=0, color=\'black\', linestyle=\'--\', linewidth=0.8, alpha=0.5)\n            ax.set_xlabel(\'Time (s)\', fontsize=11, fontweight=\'bold\')\n            ax.set_ylabel(\'U_shear_horiz (m/s)\', fontsize=11, fontweight=\'bold\')\n            ax.set_title(\'Gradiente Horizontal (Izquierda - Derecha)\', \n                        fontsize=11, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            \n            mean_val = df[\'U_shear_horiz\'].mean()\n            ax.text(0.02, 0.95, f\'Media: {mean_val:.2f} m/s\',\n                   transform=ax.transAxes, verticalalignment=\'top\',\n                   bbox=dict(boxstyle=\'round\', facecolor=\'plum\', alpha=0.8),\n                   fontsize=9)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print("‚úÖ Gr√°fico 3 generado")\n    \n    # ========================================================================\n    # GR√ÅFICO 4: HISTOGRAMAS Y DISTRIBUCIONES\n    # ========================================================================\n    \n    all_vars = pitch_available + wind_available\n    \n    if len(all_vars) > 0:\n        print(f"\\n{\'=\'*70}")\n        print("GR√ÅFICO 4: DISTRIBUCIONES (HISTOGRAMAS)")\n        print("="*70)\n        \n        # Calcular grid size\n        n_vars = len(all_vars)\n        n_cols = 4\n        n_rows = int(np.ceil(n_vars / n_cols))\n        \n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 3*n_rows))\n        fig.suptitle(f\'Distribuciones de Features - {csv_path.name}\', \n                     fontsize=14, fontweight=\'bold\')\n        \n        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n        \n        for idx, var in enumerate(all_vars):\n            ax = axes[idx]\n            \n            data = df[var].dropna()\n            \n            # Histograma\n            ax.hist(data, bins=50, color=\'steelblue\', edgecolor=\'black\', \n                   alpha=0.7, density=True)\n            \n            # Curva de densidad (KDE)\n            from scipy import stats\n            if len(data) > 10:\n                try:\n                    kde = stats.gaussian_kde(data)\n                    x_range = np.linspace(data.min(), data.max(), 200)\n                    ax.plot(x_range, kde(x_range), \'r-\', linewidth=2, \n                           label=\'KDE\', alpha=0.8)\n                except:\n                    pass\n            \n            # Estad√≠sticas\n            mean_val = data.mean()\n            median_val = data.median()\n            std_val = data.std()\n            \n            ax.axvline(mean_val, color=\'red\', linestyle=\'--\', linewidth=1.5, \n                      alpha=0.7, label=f\'Media: {mean_val:.2f}\')\n            ax.axvline(median_val, color=\'green\', linestyle=\'--\', linewidth=1.5, \n                      alpha=0.7, label=f\'Mediana: {median_val:.2f}\')\n            \n            ax.set_title(var, fontsize=10, fontweight=\'bold\')\n            ax.set_xlabel(\'Valor\', fontsize=9)\n            ax.set_ylabel(\'Densidad\', fontsize=9)\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            ax.legend(fontsize=7, loc=\'upper right\')\n            \n            # Texto con estad√≠sticas\n            stats_text = f\'Œº={mean_val:.2f}\\nœÉ={std_val:.2f}\'\n            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n                   verticalalignment=\'top\', fontsize=8,\n                   bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.6))\n        \n        # Ocultar axes sobrantes\n        for idx in range(len(all_vars), len(axes)):\n            axes[idx].axis(\'off\')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print("‚úÖ Gr√°fico 4 generado")\n    \n    # ========================================================================\n    # GR√ÅFICO 5: CORRELACIONES ENTRE VARIABLES (INCLUYENDO TARGETS)\n    # ========================================================================\n    \n    # A√±adir targets de momento para la correlaci√≥n\n    target_vars = [\'M_0\', \'M_1c\', \'M_1s\']\n    target_available = [v for v in target_vars if v in df.columns]\n    \n    # Variables para correlaci√≥n: features + targets\n    corr_vars = all_vars + target_available\n    \n    if len(corr_vars) >= 2:\n        print(f"\\n{\'=\'*70}")\n        print("GR√ÅFICO 5: MATRIZ DE CORRELACI√ìN (FEATURES + TARGETS)")\n        print("="*70)\n        \n        print(f"Variables incluidas en correlaci√≥n:")\n        print(f"  - Features pitch: {len(pitch_available)}")\n        print(f"  - Features viento: {len(wind_available)}")\n        print(f"  - Targets: {len(target_available)} {target_available}")\n        \n        # Calcular matriz de correlaci√≥n\n        corr_matrix = df[corr_vars].corr()\n        \n        # Tama√±o de figura adaptativo\n        fig_size = max(12, len(corr_vars) * 0.8)\n        fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n        \n        import seaborn as sns\n        \n        # Crear anotaciones personalizadas (m√°s grandes para targets)\n        annot_size = 8 if len(corr_vars) <= 12 else 6\n        \n        sns.heatmap(corr_matrix, annot=True, fmt=\'.2f\', cmap=\'coolwarm\', \n                   center=0, vmin=-1, vmax=1, square=True, ax=ax,\n                   cbar_kws={\'label\': \'Correlaci√≥n de Pearson\'},\n                   linewidths=0.5, linecolor=\'gray\',\n                   annot_kws={\'size\': annot_size})\n        \n        ax.set_title(f\'Matriz de Correlaci√≥n - Features y Targets\\n{csv_path.name}\', \n                    fontsize=14, fontweight=\'bold\', pad=20)\n        \n        # Resaltar targets en los labels\n        ylabels = [label.get_text() for label in ax.get_yticklabels()]\n        xlabels = [label.get_text() for label in ax.get_xticklabels()]\n        \n        # Poner en negrita los targets\n        new_ylabels = []\n        for label in ylabels:\n            if label in target_available:\n                new_ylabels.append(f\'**{label}**\')\n            else:\n                new_ylabels.append(label)\n        \n        new_xlabels = []\n        for label in xlabels:\n            if label in target_available:\n                new_xlabels.append(f\'**{label}**\')\n            else:\n                new_xlabels.append(label)\n        \n        ax.set_yticklabels(new_ylabels, fontweight=\'bold\' if len(target_available) > 0 else \'normal\')\n        ax.set_xticklabels(new_xlabels, fontweight=\'bold\' if len(target_available) > 0 else \'normal\')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print("‚úÖ Gr√°fico 5 generado")\n        \n        # ====================================================================\n        # An√°lisis de correlaciones m√°s relevantes con targets\n        # ====================================================================\n        \n        if len(target_available) > 0:\n            print(f"\\n{\'=\'*70}")\n            print("AN√ÅLISIS DE CORRELACIONES CON TARGETS")\n            print("="*70)\n            \n            for target in target_available:\n                print(f"\\nüéØ Target: {target}")\n                print("-" * 50)\n                \n                # Obtener correlaciones con el target\n                target_corrs = corr_matrix[target].drop(target).abs().sort_values(ascending=False)\n                \n                # Top 5 correlaciones\n                top_n = min(5, len(target_corrs))\n                print(f"Top {top_n} correlaciones (valor absoluto):")\n                for i, (var, corr_val) in enumerate(target_corrs.head(top_n).items(), 1):\n                    # Obtener correlaci√≥n con signo\n                    corr_signed = corr_matrix.loc[var, target]\n                    print(f"  {i}. {var:20s}: {corr_signed:+.3f}")\n    \n    # ========================================================================\n    # RESUMEN ESTAD√çSTICO\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("RESUMEN ESTAD√çSTICO")\n    print("="*70)\n    \n    summary = df[all_vars].describe()\n    print(summary)\n    \n    print(f"\\n{\'=\'*70}")\n    print("AN√ÅLISIS COMPLETADO")\n    print("="*70)',
            '# ============================================================================\n# DIAGN√ìSTICO: THETA_DELTA Y PITCH ANGLES\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Archivo a analizar\ncsv_path = Path(r"C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub\\\\data_train_traditional_ML\\\\0040_DLC12a_150_000.csv")\n\nprint("="*70)\nprint("DIAGN√ìSTICO: THETA_DELTA Y PITCH ANGLES")\nprint("="*70)\nprint(f"Archivo: {csv_path.name}\\n")\n\n# Cargar datos\ndf = pd.read_csv(csv_path)\nprint(f"‚úÖ Datos cargados: {df.shape[0]:,} filas, {df.shape[1]} columnas\\n")\n\n# Identificar columnas de pitch\npitch_cols = [col for col in df.columns if \'pitch angle\' in col.lower()]\nprint(f"Columnas de pitch encontradas:")\nfor col in pitch_cols:\n    print(f"  - {col}")\n\n# Usar las dos primeras columnas de pitch (Blade 1 y Blade 2)\nif len(pitch_cols) >= 2:\n    theta_1 = df[pitch_cols[0]].values\n    theta_2 = df[pitch_cols[1]].values\n    \n    # Calcular componentes Coleman\n    theta_0 = (theta_1 + theta_2) / 2.0      # Componente colectivo\n    theta_delta = (theta_1 - theta_2) / 2.0  # Semi-diferencia (para 1P)\n    \n    # Estad√≠sticas\n    print(f"\\nüìä ESTAD√çSTICAS:")\n    print(f"\\n{pitch_cols[0]}:")\n    print(f"  M√≠n: {theta_1.min():.3f}¬∞, M√°x: {theta_1.max():.3f}¬∞")\n    print(f"  Media: {theta_1.mean():.3f}¬∞, Std: {theta_1.std():.3f}¬∞")\n    \n    print(f"\\n{pitch_cols[1]}:")\n    print(f"  M√≠n: {theta_2.min():.3f}¬∞, M√°x: {theta_2.max():.3f}¬∞")\n    print(f"  Media: {theta_2.mean():.3f}¬∞, Std: {theta_2.std():.3f}¬∞")\n    \n    print(f"\\ntheta_0 (colectivo = promedio):")\n    print(f"  M√≠n: {theta_0.min():.3f}¬∞, M√°x: {theta_0.max():.3f}¬∞")\n    print(f"  Media: {theta_0.mean():.3f}¬∞, Std: {theta_0.std():.3f}¬∞")\n    \n    print(f"\\ntheta_delta (semi-diferencia):")\n    print(f"  M√≠n: {theta_delta.min():.3f}¬∞, M√°x: {theta_delta.max():.3f}¬∞")\n    print(f"  Media: {theta_delta.mean():.3f}¬∞, Std: {theta_delta.std():.3f}¬∞")\n    \n    # Verificar si son iguales\n    diff_abs = np.abs(theta_1 - theta_2)\n    print(f"\\n|theta_1 - theta_2|:")\n    print(f"  M√≠n: {diff_abs.min():.3f}¬∞, M√°x: {diff_abs.max():.3f}¬∞")\n    print(f"  Media: {diff_abs.mean():.3f}¬∞, Std: {diff_abs.std():.3f}¬∞")\n    \n    if diff_abs.max() < 0.001:\n        print(f"\\n‚ö†Ô∏è  DIAGN√ìSTICO: Las palas tienen PITCH ID√âNTICO")\n        print(f"   ‚Üí No hay control individual por pala (IPC)")\n        print(f"   ‚Üí Solo control colectivo ‚Üí theta_delta ‚âà 0")\n        print(f"   ‚Üí Por tanto: theta_1c ‚âà 0 y theta_1s ‚âà 0")\n        print(f"   ‚Üí Esto es CORRECTO para turbinas sin IPC")\n    \n    # GR√ÅFICOS\n    time = df[\'Time\'].values if \'Time\' in df.columns else np.arange(len(df))\n    \n    fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n    fig.suptitle(f\'Diagn√≥stico Pitch Angles - {csv_path.name}\', \n                 fontsize=14, fontweight=\'bold\')\n    \n    # Subplot 1: Pitch de cada pala\n    axes[0].plot(time, theta_1, linewidth=1, color=\'blue\', alpha=0.7, label=pitch_cols[0])\n    axes[0].plot(time, theta_2, linewidth=1, color=\'red\', alpha=0.7, label=pitch_cols[1])\n    axes[0].set_ylabel(\'Pitch Angle (¬∞)\', fontsize=11, fontweight=\'bold\')\n    axes[0].set_title(\'Pitch Angles Originales (Blade 1 vs Blade 2)\', fontsize=11, fontweight=\'bold\')\n    axes[0].legend(loc=\'upper right\', fontsize=9)\n    axes[0].grid(True, alpha=0.3, linestyle=\'--\')\n    \n    # Subplot 2: theta_0 (colectivo)\n    axes[1].plot(time, theta_0, linewidth=1, color=\'green\', alpha=0.8, label=\'Œ∏‚ÇÄ (colectivo)\')\n    axes[1].set_ylabel(\'Œ∏‚ÇÄ (¬∞)\', fontsize=11, fontweight=\'bold\')\n    axes[1].set_title(\'Componente Colectivo (promedio)\', fontsize=11, fontweight=\'bold\')\n    axes[1].legend(loc=\'upper right\', fontsize=9)\n    axes[1].grid(True, alpha=0.3, linestyle=\'--\')\n    \n    # Subplot 3: theta_delta (semi-diferencia)\n    axes[2].plot(time, theta_delta, linewidth=1, color=\'purple\', alpha=0.8, label=\'Œ∏_delta\')\n    axes[2].axhline(y=0, color=\'black\', linestyle=\'--\', linewidth=0.8, alpha=0.5)\n    axes[2].set_ylabel(\'Œ∏_delta (¬∞)\', fontsize=11, fontweight=\'bold\')\n    axes[2].set_title(\'Semi-diferencia (base para Œ∏‚ÇÅc y Œ∏‚ÇÅs)\', fontsize=11, fontweight=\'bold\')\n    axes[2].legend(loc=\'upper right\', fontsize=9)\n    axes[2].grid(True, alpha=0.3, linestyle=\'--\')\n    \n    # Subplot 4: Diferencia absoluta\n    axes[3].plot(time, diff_abs, linewidth=1, color=\'orange\', alpha=0.8, label=\'|Œ∏‚ÇÅ - Œ∏‚ÇÇ|\')\n    axes[3].axhline(y=0, color=\'black\', linestyle=\'--\', linewidth=0.8, alpha=0.5)\n    axes[3].set_xlabel(\'Time (s)\', fontsize=11, fontweight=\'bold\')\n    axes[3].set_ylabel(\'|Diferencia| (¬∞)\', fontsize=11, fontweight=\'bold\')\n    axes[3].set_title(\'Diferencia Absoluta entre Palas\', fontsize=11, fontweight=\'bold\')\n    axes[3].legend(loc=\'upper right\', fontsize=9)\n    axes[3].grid(True, alpha=0.3, linestyle=\'--\')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print("\\n‚úÖ Gr√°ficos generados")\n    \n    # Si existe azimuth, calcular theta_1c y theta_1s\n    azimuth_col = [col for col in df.columns if \'azimuth\' in col.lower()]\n    if azimuth_col and len(azimuth_col) > 0:\n        print(f"\\nüìê Calculando Œ∏‚ÇÅc y Œ∏‚ÇÅs con azimuth...")\n        azimuth = df[azimuth_col[0]].values\n        \n        # Convertir a radianes si est√° en grados\n        if azimuth.max() > 7:  # Probablemente en grados\n            azimuth_rad = np.deg2rad(azimuth)\n            print(f"   Azimuth convertido de grados a radianes")\n        else:\n            azimuth_rad = azimuth\n        \n        theta_1c = theta_delta * np.cos(azimuth_rad)\n        theta_1s = theta_delta * np.sin(azimuth_rad)\n        \n        print(f"\\nŒ∏‚ÇÅc:")\n        print(f"  M√≠n: {theta_1c.min():.3f}¬∞, M√°x: {theta_1c.max():.3f}¬∞")\n        print(f"  Media: {theta_1c.mean():.3f}¬∞, Std: {theta_1c.std():.3f}¬∞")\n        \n        print(f"\\nŒ∏‚ÇÅs:")\n        print(f"  M√≠n: {theta_1s.min():.3f}¬∞, M√°x: {theta_1s.max():.3f}¬∞")\n        print(f"  Media: {theta_1s.mean():.3f}¬∞, Std: {theta_1s.std():.3f}¬∞")\n        \n        # Gr√°fico adicional\n        fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n        fig.suptitle(f\'Componentes 1P Coleman - {csv_path.name}\', \n                     fontsize=14, fontweight=\'bold\')\n        \n        axes[0].plot(time, theta_1c, linewidth=1, color=\'red\', alpha=0.7, label=\'Œ∏‚ÇÅc\')\n        axes[0].set_ylabel(\'Œ∏‚ÇÅc (¬∞)\', fontsize=11, fontweight=\'bold\')\n        axes[0].set_title(\'Componente 1P Coseno\', fontsize=11, fontweight=\'bold\')\n        axes[0].legend(loc=\'upper right\')\n        axes[0].grid(True, alpha=0.3, linestyle=\'--\')\n        \n        axes[1].plot(time, theta_1s, linewidth=1, color=\'blue\', alpha=0.7, label=\'Œ∏‚ÇÅs\')\n        axes[1].set_xlabel(\'Time (s)\', fontsize=11, fontweight=\'bold\')\n        axes[1].set_ylabel(\'Œ∏‚ÇÅs (¬∞)\', fontsize=11, fontweight=\'bold\')\n        axes[1].set_title(\'Componente 1P Seno\', fontsize=11, fontweight=\'bold\')\n        axes[1].legend(loc=\'upper right\')\n        axes[1].grid(True, alpha=0.3, linestyle=\'--\')\n        \n        plt.tight_layout()\n        plt.show()\n\nelse:\n    print("\\n‚ùå ERROR: No se encontraron suficientes columnas de pitch angle")\n\nprint("\\n" + "="*70)',
        ],
    },
    {
        "title": 'PASO 3.6: Unir todos los CSVs en un dataset completo',
        "cells": [
            '# ============================================================================\n# PASO 3.6: UNIR TODOS LOS CSVs EN UN DATASET COMPLETO (OPTIMIZADO PARA MEMORIA)\n# ============================================================================\n\nprint("="*70)\nprint("CREANDO DATASET COMPLETO - 0000_Complete_dataset.csv")\nprint("="*70)\n\n# Funcion auxiliar para optimizar tipos de datos y reducir memoria\ndef optimize_dataframe_memory(df):\n    """\n    Optimiza el uso de memoria de un DataFrame convirtiendo tipos de datos.\n    - float64 -> float32 (reduce 50% memoria)\n    - int64 -> int16 si es posible\n    """\n    memory_before = df.memory_usage(deep=True).sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # Convertir float64 a float32\n        if col_type == \'float64\':\n            df[col] = df[col].astype(\'float32\')\n        \n        # Convertir int64 a int16 si los valores lo permiten\n        elif col_type == \'int64\':\n            col_min = df[col].min()\n            col_max = df[col].max()\n            if col_min >= -32768 and col_max <= 32767:\n                df[col] = df[col].astype(\'int16\')\n            elif col_min >= -2147483648 and col_max <= 2147483647:\n                df[col] = df[col].astype(\'int32\')\n    \n    memory_after = df.memory_usage(deep=True).sum() / 1024**2\n    memory_saved = memory_before - memory_after\n    \n    return df, memory_before, memory_after, memory_saved\n\n# Variables de control\nloaded_files = 0\nfailed_files = 0\ntotal_memory_before = 0\ntotal_memory_after = 0\nbatch_size = 10  # Procesar en lotes de 10 archivos\n\n# Obtener lista de CSVs en data_train_traditional_ML\ncsv_files = list(data_folder_ml.glob("*.csv"))\n\nprint(f"\\nTotal de archivos CSV encontrados: {len(csv_files)}")\nprint(f"Procesamiento por lotes de {batch_size} archivos para optimizar memoria\\n")\n\n# Lista para almacenar DataFrames concatenados por lote\nbatched_dataframes = []\n\n# Procesar en lotes\nnum_batches = (len(csv_files) + batch_size - 1) // batch_size\n\nfor batch_idx in range(num_batches):\n    print(f"\\n{\'=\'*70}")\n    print(f"PROCESANDO LOTE {batch_idx + 1}/{num_batches}")\n    print(f"{\'=\'*70}")\n    \n    # Obtener archivos del lote actual\n    start_idx = batch_idx * batch_size\n    end_idx = min((batch_idx + 1) * batch_size, len(csv_files))\n    batch_files = csv_files[start_idx:end_idx]\n    \n    # Lista temporal para este lote\n    batch_dfs = []\n    \n    # Cargar archivos del lote\n    for csv_file in batch_files:\n        try:\n            print(f"  Cargando: {csv_file.name}", end="")\n            df_temp = pd.read_csv(csv_file)\n            \n            # Optimizar tipos de datos inmediatamente\n            df_temp, mem_before, mem_after, mem_saved = optimize_dataframe_memory(df_temp)\n            \n            batch_dfs.append(df_temp)\n            loaded_files += 1\n            total_memory_before += mem_before\n            total_memory_after += mem_after\n            \n            print(f" ... OK ({df_temp.shape[0]} filas, {df_temp.shape[1]} cols, {mem_saved:.2f} MB ahorrados)")\n            \n        except Exception as e:\n            print(f" ... ERROR: {str(e)}")\n            failed_files += 1\n    \n    # Concatenar lote actual si hay archivos cargados\n    if batch_dfs:\n        print(f"\\n  Concatenando lote {batch_idx + 1}...")\n        batch_concatenated = pd.concat(batch_dfs, ignore_index=True)\n        batched_dataframes.append(batch_concatenated)\n        print(f"  OK - Lote {batch_idx + 1} concatenado: {batch_concatenated.shape}")\n        \n        # Liberar memoria del lote\n        del batch_dfs\n        import gc\n        gc.collect()\n\nprint(f"\\n{\'=\'*70}")\nprint(f"Archivos cargados exitosamente: {loaded_files}")\nprint(f"Archivos con errores:           {failed_files}")\nprint(f"{\'=\'*70}")\nprint(f"\\nOptimizaci√≥n de memoria:")\nprint(f"  - Memoria ANTES:   {total_memory_before:.2f} MB")\nprint(f"  - Memoria DESPU√âS: {total_memory_after:.2f} MB")\nprint(f"  - Memoria AHORRADA: {total_memory_before - total_memory_after:.2f} MB ({(1 - total_memory_after/total_memory_before)*100:.1f}% reducci√≥n)")\nprint(f"{\'=\'*70}\\n")\n\nif loaded_files > 0:\n    # Concatenar todos los lotes\n    print("Concatenando todos los lotes en DataFrame final...")\n    df_complete = pd.concat(batched_dataframes, ignore_index=True)\n    \n    # Liberar memoria de lotes\n    del batched_dataframes\n    import gc\n    gc.collect()\n    \n    print(f"Dataset completo creado:")\n    print(f"  - Total de filas:    {df_complete.shape[0]:,}")\n    print(f"  - Total de columnas: {df_complete.shape[1]:,}")\n    print(f"  - Tama√±o en memoria: {df_complete.memory_usage(deep=True).sum() / 1024**2:.2f} MB")\n    \n    # Verificar que no hay NaN\n    nan_count = df_complete.isna().sum().sum()\n    print(f"  - Total de valores NaN: {nan_count}")\n    \n    if nan_count > 0:\n        print(f"\\nADVERTENCIA: Se encontraron {nan_count} valores NaN en el dataset completo")\n    \n    # Guardar el dataset completo\n    output_path = data_folder_ml / "0000_Complete_dataset.csv"\n    print(f"\\nGuardando dataset completo en:")\n    print(f"  {output_path}")\n    \n    df_complete.to_csv(output_path, index=False)\n    \n    print("\\n" + "="*70)\n    print("DATASET COMPLETO CREADO EXITOSAMENTE")\n    print("="*70)\n    print(f"\\nArchivo: 0000_Complete_dataset.csv")\n    print(f"Ubicaci√≥n: {data_folder_ml}")\n    print(f"\\nEste dataset contiene:")\n    print(f"  - Todas las simulaciones de viento combinadas")\n    print(f"  - Todas las features originales")\n    print(f"  - Lags de VLOS (5-25 segundos)")\n    print(f"  - Componentes sin/cos del azimuth")\n    print(f"  - Componentes 1P y 2P de momentos flectores")\n    print("="*70)\n    \n    # Mostrar primeras columnas del dataset\n    print("\\nPrimeras 5 filas del dataset completo:")\n    print(df_complete.head())\n    \nelse:\n    print("\\nERROR: No se pudo cargar ning√∫n archivo CSV")\n    print("Verifica que los archivos existan en data_train_traditional_ML")',
            'import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\n\n# ============================================================================\n# Cargar y visualizar datos del dataset completo\n# ============================================================================\n\n# Ruta al archivo\ndata_file = Path(\'C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub\\\\data_train_traditional_ML\\\\0000_Complete_dataset.csv\')\n\nprint("Cargando dataset completo...")\ndf = pd.read_csv(data_file)\n\nprint(f"Dataset cargado: {len(df)} filas, {len(df.columns)} columnas")\nprint(f"Columnas principales: {df.columns.tolist()[:10]}...")\n\n# Muestrear cada 1 segundo (cada 50 filas, ya que dt=0.02s)\nsampling_interval = 50\ndf_sampled = df.iloc[::sampling_interval].copy()\n\nprint(f"Dataset muestreado: {len(df_sampled)} filas (1 punto por segundo)")\n\n# Verificar columnas necesarias\nrequired_cols = [\'U_mean\', \'TI\', \'Name_DLC\', \'Extreme_condition\']\nfor col in required_cols:\n    if col not in df_sampled.columns:\n        print(f"ERROR: Columna \'{col}\' no encontrada en el dataset")\n    else:\n        print(f"‚úì Columna \'{col}\' encontrada")\n\n# ============================================================================\n# Clasificar los datos\n# ============================================================================\n\n# DLC12a - Normal (Extreme_condition = 0)\nmask_12a_normal = df_sampled[\'Name_DLC\'].str.contains(\'DLC12\', na=False) & (df_sampled[\'Extreme_condition\'] == 0)\ndf_12a_normal = df_sampled[mask_12a_normal]\n\n# DLC12a - Extremo/Err√≥neo (Extreme_condition = 1) - X ROJAS\nmask_12a_extreme = df_sampled[\'Name_DLC\'].str.contains(\'DLC12\', na=False) & (df_sampled[\'Extreme_condition\'] == 1)\ndf_12a_extreme = df_sampled[mask_12a_extreme]\n\n# DLC13 - Normal (Extreme_condition = 1 es lo esperado)\nmask_13_normal = df_sampled[\'Name_DLC\'].str.contains(\'DLC13\', na=False) & (df_sampled[\'Extreme_condition\'] == 1)\ndf_13_normal = df_sampled[mask_13_normal]\n\n# DLC13 - Err√≥neo (Extreme_condition = 0) - X ROJAS\nmask_13_error = df_sampled[\'Name_DLC\'].str.contains(\'DLC13\', na=False) & (df_sampled[\'Extreme_condition\'] == 0)\ndf_13_error = df_sampled[mask_13_error]\n\nprint(f"\\nClasificaci√≥n de datos:")\nprint(f"  DLC12a normales:  {len(df_12a_normal)} puntos")\nprint(f"  DLC12a extremos:  {len(df_12a_extreme)} puntos (X rojas)")\nprint(f"  DLC13 normales:   {len(df_13_normal)} puntos")\nprint(f"  DLC13 err√≥neos:   {len(df_13_error)} puntos (X rojas)")\n\n# ============================================================================\n# GR√ÅFICO 1: TI vs U_mean con clasificaci√≥n\n# ============================================================================\n\nplt.figure(figsize=(16, 10))\n\n# DLC12a normales (azul)\nif len(df_12a_normal) > 0:\n    plt.scatter(df_12a_normal[\'U_mean\'], df_12a_normal[\'TI\'], \n                alpha=0.4, s=15, c=\'blue\', edgecolors=\'none\', \n                label=f\'DLC12a Normal ({len(df_12a_normal)} pts)\')\n\n# DLC13 normales (verde)\nif len(df_13_normal) > 0:\n    plt.scatter(df_13_normal[\'U_mean\'], df_13_normal[\'TI\'], \n                alpha=0.4, s=15, c=\'green\', edgecolors=\'none\', \n                label=f\'DLC13 Normal ({len(df_13_normal)} pts)\')\n\n# DLC12a extremos (X rojas)\nif len(df_12a_extreme) > 0:\n    plt.scatter(df_12a_extreme[\'U_mean\'], df_12a_extreme[\'TI\'], \n                marker=\'x\', s=50, c=\'red\', linewidths=2, \n                label=f\'DLC12a Extremo ({len(df_12a_extreme)} pts)\', zorder=5)\n\n# DLC13 err√≥neos (X rojas)\nif len(df_13_error) > 0:\n    plt.scatter(df_13_error[\'U_mean\'], df_13_error[\'TI\'], \n                marker=\'x\', s=50, c=\'red\', linewidths=2, \n                label=f\'DLC13 Err√≥neo ({len(df_13_error)} pts)\', zorder=5)\n\nplt.xlabel(\'U_mean [m/s]\', fontsize=14)\nplt.ylabel(\'Intensidad de Turbulencia (TI) [-]\', fontsize=14)\nplt.title(\'Clasificaci√≥n de Condiciones: TI vs U_mean\', fontsize=16, fontweight=\'bold\')\nplt.ylim(0, 1)\nplt.grid(True, alpha=0.3)\nplt.legend(loc=\'upper right\', fontsize=11)\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# GR√ÅFICO 2: Conteo de err√≥neos por DLC\n# ============================================================================\n\n# Contar totales y err√≥neos\ntotal_12a = len(df_sampled[df_sampled[\'Name_DLC\'].str.contains(\'DLC12\', na=False)])\ntotal_13 = len(df_sampled[df_sampled[\'Name_DLC\'].str.contains(\'DLC13\', na=False)])\n\nerroneos_12a = len(df_12a_extreme)\nerroneos_13 = len(df_13_error)\n\npct_error_12a = (erroneos_12a / total_12a * 100) if total_12a > 0 else 0\npct_error_13 = (erroneos_13 / total_13 * 100) if total_13 > 0 else 0\n\nprint(f"\\n{\'=\'*60}")\nprint(f"RESUMEN DE ERRORES:")\nprint(f"{\'=\'*60}")\nprint(f"DLC12a:")\nprint(f"  - Total:    {total_12a} puntos")\nprint(f"  - Err√≥neos: {erroneos_12a} puntos ({pct_error_12a:.2f}%)")\nprint(f"\\nDLC13:")\nprint(f"  - Total:    {total_13} puntos")\nprint(f"  - Err√≥neos: {erroneos_13} puntos ({pct_error_13:.2f}%)")\n\n# Gr√°fico de barras\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n\n# Subplot 1: Conteo absoluto\ncategories = [\'DLC12a\', \'DLC13\']\nerroneos = [erroneos_12a, erroneos_13]\ntotales = [total_12a, total_13]\n\nx_pos = np.arange(len(categories))\nwidth = 0.35\n\nbars1 = ax1.bar(x_pos - width/2, totales, width, label=\'Total puntos\', color=\'steelblue\', alpha=0.7)\nbars2 = ax1.bar(x_pos + width/2, erroneos, width, label=\'Puntos err√≥neos\', color=\'red\', alpha=0.7)\n\nax1.set_xlabel(\'Tipo DLC\', fontsize=13)\nax1.set_ylabel(\'N√∫mero de puntos\', fontsize=13)\nax1.set_title(\'Conteo de Puntos Err√≥neos por DLC\', fontsize=14, fontweight=\'bold\')\nax1.set_xticks(x_pos)\nax1.set_xticklabels(categories)\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3, axis=\'y\')\n\n# A√±adir valores en las barras\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax1.text(bar.get_x() + bar.get_width()/2., height,\n                f\'{int(height)}\',\n                ha=\'center\', va=\'bottom\', fontsize=10, fontweight=\'bold\')\n\n# Subplot 2: Porcentaje de errores\nbars3 = ax2.bar(categories, [pct_error_12a, pct_error_13], color=[\'blue\', \'green\'], alpha=0.6)\n\nax2.set_xlabel(\'Tipo DLC\', fontsize=13)\nax2.set_ylabel(\'Porcentaje de err√≥neos (%)\', fontsize=13)\nax2.set_title(\'Porcentaje de Puntos Err√≥neos por DLC\', fontsize=14, fontweight=\'bold\')\nax2.grid(True, alpha=0.3, axis=\'y\')\nax2.set_ylim(0, max(pct_error_12a, pct_error_13) * 1.2 if max(pct_error_12a, pct_error_13) > 0 else 10)\n\n# A√±adir valores en las barras\nfor i, (bar, pct) in enumerate(zip(bars3, [pct_error_12a, pct_error_13])):\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n            f\'{pct:.2f}%\',\n            ha=\'center\', va=\'bottom\', fontsize=11, fontweight=\'bold\')\n\nplt.tight_layout()\nplt.show()\n\nprint(f"\\n{\'=\'*60}")\nprint("AN√ÅLISIS COMPLETADO")\nprint(f"{\'=\'*60}")',
            '# ============================================================================\n# AN√ÅLISIS DE YAW RATE Y YAW ACCELERATION CON COMPONENTE 1P\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal as sp_signal\nimport seaborn as sns\n\n# Cargar el archivo CSV\nfile_path = r\'C:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\data_train_traditional_ML\\0033_DLC12a_130_000.csv\'\n\nprint("="*70)\nprint("AN√ÅLISIS DE YAW RATE Y YAW ACCELERATION")\nprint("="*70)\nprint(f"\\nCargando archivo: {file_path.split(\'\\\\\')[-1]}")\n\n# Leer el CSV\ndf = pd.read_csv(file_path)\n\nprint(f"  ‚úì Datos cargados: {len(df)} muestras")\nprint(f"  ‚úì Columnas disponibles: {len(df.columns)}")\n\n# Verificar que existen las columnas necesarias\nrequired_cols = [\'Time\', \'Yaw_rate_rad_s\', \'Yaw_acc_rad_s2\', \'Rotor speed\', \n                 \'M_0\', \'M_1c\', \'M_1s\']\nmissing_cols = [col for col in required_cols if col not in df.columns]\n\nif missing_cols:\n    print(f"\\n  ‚ö†Ô∏è ADVERTENCIA: Columnas faltantes: {missing_cols}")\nelse:\n    print(f"  ‚úì Todas las columnas requeridas est√°n presentes")\n\n# ============================================================================\n# PASO 1: Obtener datos de las columnas existentes\n# ============================================================================\nprint("\\n" + "="*70)\nprint("PASO 1: Cargando se√±ales desde columnas existentes")\nprint("="*70)\n\n# Obtener datos\ntime = df[\'Time\'].values\nyaw_rate_rad_s = df[\'Yaw_rate_rad_s\'].values\nyaw_acc_rad_s2 = df[\'Yaw_acc_rad_s2\'].values\nrotor_speed_rad_s = df[\'Rotor speed\'].values  # Ya est√° en rad/s\n\n# Obtener targets Coleman\nM_0 = df[\'M_0\'].values\nM_1c = df[\'M_1c\'].values\nM_1s = df[\'M_1s\'].values\n\n# Calcular dt\ndt_array = np.diff(time)\ndt_mean = np.mean(dt_array)\nfs = 1.0 / dt_mean  # Frecuencia de muestreo\n\nprint(f"\\n  Par√°metros temporales:")\nprint(f"  - dt promedio: {dt_mean:.4f} s")\nprint(f"  - Frecuencia de muestreo: {fs:.2f} Hz")\nprint(f"  - Duraci√≥n total: {time[-1]:.2f} s")\n\nprint(f"\\n  Se√±ales cargadas:")\nprint(f"  - Yaw rate: [{yaw_rate_rad_s.min():.4f}, {yaw_rate_rad_s.max():.4f}] rad/s")\nprint(f"  - Yaw acc: [{yaw_acc_rad_s2.min():.4f}, {yaw_acc_rad_s2.max():.4f}] rad/s¬≤")\nprint(f"  - M_0: [{M_0.min():.2e}, {M_0.max():.2e}] Nm")\nprint(f"  - M_1c: [{M_1c.min():.2e}, {M_1c.max():.2e}] Nm")\nprint(f"  - M_1s: [{M_1s.min():.2e}, {M_1s.max():.2e}] Nm")\n\n# ============================================================================\n# PASO 2: Extraer componente 1P usando filtro pasa-banda\n# ============================================================================\nprint("\\n" + "="*70)\nprint("PASO 2: Extrayendo componente 1P")\nprint("="*70)\n\n# Calcular frecuencia 1P del rotor\nrotor_freq_hz = rotor_speed_rad_s / (2 * np.pi)  # Convertir rad/s a Hz\nrotor_freq_mean = np.mean(rotor_freq_hz)\nrotor_freq_std = np.std(rotor_freq_hz)\n\nprint(f"\\n  Frecuencia del rotor (1P):")\nprint(f"  - Media: {rotor_freq_mean:.4f} Hz ({rotor_freq_mean*60:.2f} rpm)")\nprint(f"  - Std: {rotor_freq_std:.4f} Hz")\nprint(f"  - Rango: [{rotor_freq_hz.min():.4f}, {rotor_freq_hz.max():.4f}] Hz")\n\n# Definir ancho de banda para filtro 1P (¬±20% de la frecuencia nominal)\nbandwidth_factor = 0.2\nlowcut_1P = rotor_freq_mean * (1 - bandwidth_factor)\nhighcut_1P = rotor_freq_mean * (1 + bandwidth_factor)\n\nprint(f"\\n  Par√°metros del filtro 1P:")\nprint(f"  - Frecuencia central: {rotor_freq_mean:.4f} Hz")\nprint(f"  - Ancho de banda: ¬±{bandwidth_factor*100:.0f}%")\nprint(f"  - Rango del filtro: [{lowcut_1P:.4f}, {highcut_1P:.4f}] Hz")\n\n# Funci√≥n para filtro pasa-banda\ndef bandpass_filter(signal_data, lowcut, highcut, fs, order=2):\n    """Aplica un filtro pasa-banda Butterworth."""\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    \n    # Asegurar valores v√°lidos\n    low = max(0.001, min(low, 0.999))\n    high = max(low + 0.001, min(high, 0.999))\n    \n    sos = sp_signal.butter(order, [low, high], btype=\'band\', output=\'sos\')\n    filtered_signal = sp_signal.sosfilt(sos, signal_data)\n    \n    return filtered_signal\n\n# Aplicar filtro 1P a yaw rate y yaw acceleration\nprint(f"\\n  Aplicando filtro 1P...")\nyaw_rate_1P = bandpass_filter(yaw_rate_rad_s, lowcut_1P, highcut_1P, fs, order=2)\nyaw_acc_1P = bandpass_filter(yaw_acc_rad_s2, lowcut_1P, highcut_1P, fs, order=2)\n\nprint(f"  ‚úì Yaw rate 1P: [{yaw_rate_1P.min():.4f}, {yaw_rate_1P.max():.4f}] rad/s")\nprint(f"  ‚úì Yaw acc 1P: [{yaw_acc_1P.min():.4f}, {yaw_acc_1P.max():.4f}] rad/s¬≤")\n\n# ============================================================================\n# PASO 3: Visualizaci√≥n - Series temporales y espectros\n# ============================================================================\nprint("\\n" + "="*70)\nprint("PASO 3: Creando gr√°ficos de series temporales y espectros")\nprint("="*70)\n\n# Crear figura con 4 subplots (2x2)\nfig1, axes = plt.subplots(2, 2, figsize=(16, 10))\nfig1.suptitle(f\'An√°lisis de Yaw Rate y Yaw Acceleration - {file_path.split(chr(92))[-1]}\', \n             fontsize=14, fontweight=\'bold\')\n\n# Limitar visualizaci√≥n a primeros 100 segundos para mejor legibilidad\ntime_limit = 100  # segundos\nmask = time <= time_limit\n\n# --- SUBPLOT 1: Yaw Rate (completo y 1P) ---\nax1 = axes[0, 0]\nax1.plot(time[mask], yaw_rate_rad_s[mask], \'b-\', alpha=0.6, linewidth=0.8, label=\'Yaw Rate (total)\')\nax1.plot(time[mask], yaw_rate_1P[mask], \'r-\', linewidth=1.5, label=\'Yaw Rate (1P)\')\nax1.set_xlabel(\'Time (s)\', fontsize=10)\nax1.set_ylabel(\'Yaw Rate (rad/s)\', fontsize=10)\nax1.set_title(\'Yaw Rate: Se√±al Total vs Componente 1P\', fontsize=11, fontweight=\'bold\')\nax1.legend(loc=\'upper right\')\nax1.grid(True, alpha=0.3)\n\n# --- SUBPLOT 2: Yaw Acceleration (completo y 1P) ---\nax2 = axes[0, 1]\nax2.plot(time[mask], yaw_acc_rad_s2[mask], \'b-\', alpha=0.6, linewidth=0.8, label=\'Yaw Acc (total)\')\nax2.plot(time[mask], yaw_acc_1P[mask], \'r-\', linewidth=1.5, label=\'Yaw Acc (1P)\')\nax2.set_xlabel(\'Time (s)\', fontsize=10)\nax2.set_ylabel(\'Yaw Acceleration (rad/s¬≤)\', fontsize=10)\nax2.set_title(\'Yaw Acceleration: Se√±al Total vs Componente 1P\', fontsize=11, fontweight=\'bold\')\nax2.legend(loc=\'upper right\')\nax2.grid(True, alpha=0.3)\n\n# --- SUBPLOT 3: Espectro de frecuencia - Yaw Rate ---\nax3 = axes[1, 0]\n# Calcular FFT\nn = len(yaw_rate_rad_s)\nfreqs = np.fft.rfftfreq(n, dt_mean)\nyaw_rate_fft = np.abs(np.fft.rfft(yaw_rate_rad_s))\n\nax3.semilogy(freqs, yaw_rate_fft, \'b-\', linewidth=1)\nax3.axvline(rotor_freq_mean, color=\'r\', linestyle=\'--\', linewidth=2, label=f\'1P = {rotor_freq_mean:.3f} Hz\')\nax3.axvline(lowcut_1P, color=\'orange\', linestyle=\':\', linewidth=1, alpha=0.7, label=\'Banda 1P\')\nax3.axvline(highcut_1P, color=\'orange\', linestyle=\':\', linewidth=1, alpha=0.7)\nax3.set_xlabel(\'Frequency (Hz)\', fontsize=10)\nax3.set_ylabel(\'Magnitude\', fontsize=10)\nax3.set_title(\'Espectro de Frecuencia - Yaw Rate\', fontsize=11, fontweight=\'bold\')\nax3.set_xlim(0, 1.0)  # Limitar a 1 Hz para ver detalle de 1P\nax3.legend(loc=\'upper right\')\nax3.grid(True, alpha=0.3)\n\n# --- SUBPLOT 4: Espectro de frecuencia - Yaw Acceleration ---\nax4 = axes[1, 1]\nyaw_acc_fft = np.abs(np.fft.rfft(yaw_acc_rad_s2))\n\nax4.semilogy(freqs, yaw_acc_fft, \'b-\', linewidth=1)\nax4.axvline(rotor_freq_mean, color=\'r\', linestyle=\'--\', linewidth=2, label=f\'1P = {rotor_freq_mean:.3f} Hz\')\nax4.axvline(lowcut_1P, color=\'orange\', linestyle=\':\', linewidth=1, alpha=0.7, label=\'Banda 1P\')\nax4.axvline(highcut_1P, color=\'orange\', linestyle=\':\', linewidth=1, alpha=0.7)\nax4.set_xlabel(\'Frequency (Hz)\', fontsize=10)\nax4.set_ylabel(\'Magnitude\', fontsize=10)\nax4.set_title(\'Espectro de Frecuencia - Yaw Acceleration\', fontsize=11, fontweight=\'bold\')\nax4.set_xlim(0, 1.0)  # Limitar a 1 Hz para ver detalle de 1P\nax4.legend(loc=\'upper right\')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# PASO 4: Matriz de Correlaci√≥n\n# ============================================================================\nprint("\\n" + "="*70)\nprint("PASO 4: Calculando matriz de correlaci√≥n")\nprint("="*70)\n\n# Crear DataFrame con las variables de inter√©s\ncorrelation_data = pd.DataFrame({\n    \'M_0\': M_0,\n    \'M_1c\': M_1c,\n    \'M_1s\': M_1s,\n    \'Yaw_rate_rad_s\': yaw_rate_rad_s,\n    \'Yaw_acc_rad_s2\': yaw_acc_rad_s2\n})\n\n# Calcular matriz de correlaci√≥n\ncorrelation_matrix = correlation_data.corr()\n\nprint("\\nMatriz de Correlaci√≥n:")\nprint(correlation_matrix)\n\n# Crear figura para la matriz de correlaci√≥n\nfig2, ax = plt.subplots(figsize=(10, 8))\n\n# Crear heatmap\nsns.heatmap(correlation_matrix, \n            annot=True,  # Mostrar valores\n            fmt=\'.3f\',   # Formato de 3 decimales\n            cmap=\'coolwarm\',  # Paleta de colores (azul=negativo, rojo=positivo)\n            center=0,    # Centrar escala de colores en 0\n            square=True, # Celdas cuadradas\n            linewidths=1,\n            cbar_kws={\'label\': \'Coeficiente de Correlaci√≥n\'},\n            vmin=-1, vmax=1,  # Rango de -1 a 1\n            ax=ax)\n\n# Configurar t√≠tulo y etiquetas\nax.set_title(\'Matriz de Correlaci√≥n: Momentos Coleman vs Yaw Rate/Acc\', \n             fontsize=14, fontweight=\'bold\', pad=20)\nax.set_xlabel(\'Variables\', fontsize=12, fontweight=\'bold\')\nax.set_ylabel(\'Variables\', fontsize=12, fontweight=\'bold\')\n\n# Rotar etiquetas para mejor legibilidad\nplt.xticks(rotation=45, ha=\'right\')\nplt.yticks(rotation=0)\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# PASO 5: Estad√≠sticas de las componentes 1P y correlaciones\n# ============================================================================\nprint("\\n" + "="*70)\nprint("PASO 5: Estad√≠sticas y an√°lisis de correlaciones")\nprint("="*70)\n\n# Calcular contenido de energ√≠a 1P\nyaw_rate_power_total = np.sum(yaw_rate_rad_s**2)\nyaw_rate_power_1P = np.sum(yaw_rate_1P**2)\nyaw_rate_1P_percentage = (yaw_rate_power_1P / yaw_rate_power_total) * 100\n\nyaw_acc_power_total = np.sum(yaw_acc_rad_s2**2)\nyaw_acc_power_1P = np.sum(yaw_acc_1P**2)\nyaw_acc_1P_percentage = (yaw_acc_power_1P / yaw_acc_power_total) * 100\n\nprint(f"\\n  Yaw Rate:")\nprint(f"  - RMS total: {np.sqrt(np.mean(yaw_rate_rad_s**2)):.6f} rad/s")\nprint(f"  - RMS componente 1P: {np.sqrt(np.mean(yaw_rate_1P**2)):.6f} rad/s")\nprint(f"  - Contenido energ√©tico 1P: {yaw_rate_1P_percentage:.2f}%")\n\nprint(f"\\n  Yaw Acceleration:")\nprint(f"  - RMS total: {np.sqrt(np.mean(yaw_acc_rad_s2**2)):.6f} rad/s¬≤")\nprint(f"  - RMS componente 1P: {np.sqrt(np.mean(yaw_acc_1P**2)):.6f} rad/s¬≤")\nprint(f"  - Contenido energ√©tico 1P: {yaw_acc_1P_percentage:.2f}%")\n\nprint(f"\\n  Correlaciones m√°s significativas con Yaw Rate:")\nyaw_rate_corrs = correlation_matrix[\'Yaw_rate_rad_s\'].abs().sort_values(ascending=False)\nfor var, corr in yaw_rate_corrs.items():\n    if var != \'Yaw_rate_rad_s\':\n        print(f"  - {var}: {correlation_matrix.loc[\'Yaw_rate_rad_s\', var]:+.3f}")\n\nprint(f"\\n  Correlaciones m√°s significativas con Yaw Acc:")\nyaw_acc_corrs = correlation_matrix[\'Yaw_acc_rad_s2\'].abs().sort_values(ascending=False)\nfor var, corr in yaw_acc_corrs.items():\n    if var != \'Yaw_acc_rad_s2\':\n        print(f"  - {var}: {correlation_matrix.loc[\'Yaw_acc_rad_s2\', var]:+.3f}")\n\nprint("\\n" + "="*70)\nprint("‚úì AN√ÅLISIS COMPLETADO")\nprint("="*70)',
        ],
    },
    {
        "title": 'üìä STEP 4: An√°lisis Exploratorio de Datos (EDA) - Modelos Tradicionales',
        "cells": [
            '# ============================================================================\n# PASO 4.1: Configurar carpeta para guardar gr√°ficos del EDA\n# ============================================================================\n\n# Crear carpeta para gr√°ficos del EDA dentro de notebook/\neda_folder = Path.cwd() / "00_EDA_traditional_ML"\neda_folder.mkdir(exist_ok=True)\n\nprint("="*70)\nprint("CONFIGURACI√ìN EDA - AN√ÅLISIS EXPLORATORIO DE DATOS")\nprint("="*70)\nprint(f"Carpeta para gr√°ficos: {eda_folder}")\nprint(f"Estado: {\'‚úÖ Carpeta creada/verificada\' if eda_folder.exists() else \'‚ùå Error al crear carpeta\'}")\nprint("="*70)\n\n# Configurar estilo de gr√°ficos para el EDA\nplt.style.use(\'seaborn-v0_8-whitegrid\')\nsns.set_context("notebook", font_scale=1.2)\nsns.set_palette("husl")\n\nprint("\\n‚úÖ Configuraci√≥n de visualizaci√≥n aplicada")\nprint("üìä Listo para generar gr√°ficos del EDA")\n',
        ],
    },
    {
        "title": 'üìä PASO 4.2: Histogramas de Variables F√≠sicas',
        "cells": [
            '# ============================================================================\n# PASO 4.2: GENERAR HISTOGRAMAS DE VARIABLES F√çSICAS (LECTURA POR CHUNKS)\n# ============================================================================\n\n# Crear subcarpeta para histogramas\nhistograms_folder = eda_folder / "00_Histogramas"\nhistograms_folder.mkdir(exist_ok=True)\n\nprint("="*70)\nprint("GENERANDO HISTOGRAMAS - VARIABLES F√çSICAS EST√ÅNDAR")\nprint("="*70)\nprint(f"Carpeta destino: {histograms_folder}")\nprint("="*70)\n\n# Cargar dataset completo\ncomplete_dataset_path = data_folder_ml / "0000_Complete_dataset.csv"\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\n    print("   Por favor, ejecuta primero el PASO 3.6 para crear el dataset completo")\nelse:\n    print(f"\\nArchivo: {complete_dataset_path.name}")\n    print("‚öôÔ∏è  M√©todo: Lectura por CHUNKS (optimizado para memoria)")\n    \n    # ========================================================================\n    # PASO 1: Leer primera l√≠nea para obtener nombres de columnas\n    # ========================================================================\n    \n    print("\\n[1/3] Leyendo columnas del CSV...")\n    df_sample = pd.read_csv(complete_dataset_path, nrows=0)  # Solo headers\n    all_columns = df_sample.columns.tolist()\n    print(f"‚úÖ Total de columnas: {len(all_columns)}")\n    \n    # ========================================================================\n    # PASO 2: Definir variables f√≠sicas est√°ndar a graficar\n    # ========================================================================\n    \n    print("\\n[2/3] Seleccionando variables a graficar...")\n    \n    variables_to_plot = []\n    \n    # 1. Rotor Speed\n    if \'Rotor speed\' in all_columns:\n        variables_to_plot.append(\'Rotor speed\')\n    \n    # 2. Pitch angles\n    pitch_vars = [\'Blade 1 pitch angle\', \'Blade 2 pitch angle\']\n    for var in pitch_vars:\n        if var in all_columns:\n            variables_to_plot.append(var)\n    \n    # 3. Azimuth components\n    azimuth_vars = [\'sin_rotor_azimuth\', \'cos_rotor_azimuth\']\n    for var in azimuth_vars:\n        if var in all_columns:\n            variables_to_plot.append(var)\n    \n    # 4. Yaw error components\n    yawerror_vars = [\'sin_yawerror\', \'cos_yawerror\']\n    for var in yawerror_vars:\n        if var in all_columns:\n            variables_to_plot.append(var)\n    \n    # 5. Wind statistics\n    wind_stats_vars = [\'U_mean\', \'U_std\', \'U_shear_vert\', \'U_shear_horiz\']\n    for var in wind_stats_vars:\n        if var in all_columns:\n            variables_to_plot.append(var)\n    \n    # 4. Blade moments (TARGETS)\n    moment_vars = [\'Blade root 1 My\', \'Blade root 2 My\']\n    for var in moment_vars:\n        if var in all_columns:\n            variables_to_plot.append(var)\n\n    # 5. Componentes de frecuencia (0P, 1P, 2P)\n    frequency_components = [\'M_0\', \'M_1c\', \'M_1s\', \'M_2c\', \'M_2s\']\n    for var in frequency_components:\n        if var in all_columns:\n            variables_to_plot.append(var)\n    \n    # 6. VLOS variables (sin lags)\n    vlos_vars = [col for col in all_columns \n                 if \'LAC_VLOS\' in col and \'lag\' not in col]\n    variables_to_plot.extend(vlos_vars)\n    \n    print(f"\\nüìä Variables a graficar: {len(variables_to_plot)}")\n    print("\\nCategor√≠as:")\n    print(f"  - Rotor speed: 1")\n    print(f"  - Pitch angles: {len([v for v in variables_to_plot if \'pitch\' in v.lower()])}")\n    print(f"  - Azimuth components: {len([v for v in variables_to_plot if \'azimuth\' in v.lower()])}")\n    print(f"  - Yaw error components: {len([v for v in variables_to_plot if \'yawerror\' in v.lower()])}")\n    print(f"  - Wind statistics: {len([v for v in variables_to_plot if any(s in v for s in [\'U_mean\', \'U_std\', \'U_shear\'])])}")\n    print(f"  - Blade moments: {len([v for v in variables_to_plot if \'My\' in v])}")\n    print(f"  - Frequency components: {len([v for v in variables_to_plot if v in frequency_components])}")\n    print(f"  - VLOS (sin lags): {len(vlos_vars)}")\n    \n    \n    # ========================================================================\n    # PASO 3: Leer CSV por chunks y acumular datos para cada variable\n    # ========================================================================\n    \n    print(f"\\n[3/3] Leyendo CSV por chunks...")\n    \n    chunk_size = 100000  # 100k filas por chunk\n    data_accumulated = {var: [] for var in variables_to_plot}\n    \n    # Contador de chunks procesados\n    chunk_count = 0\n    total_rows = 0\n    \n    # Leer CSV en chunks\n    for chunk in pd.read_csv(complete_dataset_path, chunksize=chunk_size, \n                             usecols=variables_to_plot):\n        chunk_count += 1\n        total_rows += len(chunk)\n        \n        # Acumular datos de cada variable\n        for var in variables_to_plot:\n            if var in chunk.columns:\n                # Extraer valores no-NaN y convertir a lista\n                valid_data = chunk[var].dropna().values.tolist()\n                data_accumulated[var].extend(valid_data)\n        \n        print(f"  Chunk {chunk_count} procesado ({len(chunk):,} filas) - Total acumulado: {total_rows:,}")\n        \n        # Liberar memoria del chunk\n        del chunk\n        import gc\n        gc.collect()\n    \n    print(f"\\n‚úÖ Lectura completada: {total_rows:,} filas procesadas en {chunk_count} chunks")\n    \n    # ========================================================================\n    # PASO 4: Generar histogramas con datos acumulados\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("GENERANDO HISTOGRAMAS...")\n    print("="*70)\n    \n    num_bins = 50  # N√∫mero de bins para detalle\n    generated_count = 0\n    failed_count = 0\n    \n    for var in variables_to_plot:\n        try:\n            # Obtener datos acumulados\n            data = np.array(data_accumulated[var])\n            \n            if len(data) == 0:\n                print(f"  ‚ö†Ô∏è  {var}: Sin datos v√°lidos")\n                failed_count += 1\n                continue\n            \n            # Crear figura\n            fig, ax = plt.subplots(figsize=(10, 6))\n            \n            # Crear histograma\n            n, bins, patches = ax.hist(data, bins=num_bins, \n                                       color=\'steelblue\', \n                                       edgecolor=\'black\', \n                                       alpha=0.7)\n            \n            # Calcular estad√≠sticas\n            mean_val = np.mean(data)\n            median_val = np.median(data)\n            std_val = np.std(data)\n            min_val = np.min(data)\n            max_val = np.max(data)\n            \n            # A√±adir l√≠neas de media y mediana\n            ax.axvline(mean_val, color=\'red\', linestyle=\'--\', \n                      linewidth=2, label=f\'Media: {mean_val:.2f}\')\n            ax.axvline(median_val, color=\'green\', linestyle=\'--\', \n                      linewidth=2, label=f\'Mediana: {median_val:.2f}\')\n            \n            # Configurar t√≠tulo y etiquetas\n            ax.set_title(f\'Distribuci√≥n de {var}\', \n                        fontsize=14, fontweight=\'bold\', pad=20)\n            ax.set_xlabel(var, fontsize=12)\n            ax.set_ylabel(\'Frecuencia\', fontsize=12)\n            \n            # A√±adir cuadro de estad√≠sticas\n            stats_text = f\'n = {len(data):,}\\nMedia = {mean_val:.2f}\\nMediana = {median_val:.2f}\\nStd = {std_val:.2f}\\nMin = {min_val:.2f}\\nMax = {max_val:.2f}\'\n            ax.text(0.02, 0.98, stats_text,\n                   transform=ax.transAxes,\n                   verticalalignment=\'top\',\n                   bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.5),\n                   fontsize=10)\n            \n            # Leyenda\n            ax.legend(loc=\'upper right\')\n            \n            # Grid\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            \n            # Ajustar layout\n            plt.tight_layout()\n            \n            # Guardar figura\n            safe_filename = var.replace(\' \', \'_\').replace(\'/\', \'_\')\n            output_path = histograms_folder / f"hist_{safe_filename}.png"\n            plt.savefig(output_path, dpi=150, bbox_inches=\'tight\')\n            plt.close()\n            \n            generated_count += 1\n            print(f"  ‚úÖ {var}")\n            \n            # Liberar memoria de datos\n            del data\n            \n        except Exception as e:\n            failed_count += 1\n            print(f"  ‚ùå {var}: {str(e)}")\n    \n    # Liberar memoria de datos acumulados\n    del data_accumulated\n    import gc\n    gc.collect()\n    \n    # Resumen final\n    print(f"\\n{\'=\'*70}")\n    print("RESUMEN - GENERACI√ìN DE HISTOGRAMAS")\n    print("="*70)\n    print(f"Histogramas generados:  {generated_count}")\n    print(f"Errores:                {failed_count}")\n    print(f"Total intentados:       {len(variables_to_plot)}")\n    print(f"\\nüìÅ Gr√°ficos guardados en: {histograms_folder}")\n    print("="*70)',
        ],
    },
    {
        "title": 'üåç PASO 4.3: An√°lisis del Efecto Gravitacional en las Palas',
        "cells": [
            '# ============================================================================\n# PASO 4.4: EFECTO GRAVITACIONAL - AZIMUTH VS CARGAS\n# ============================================================================\n\n# Crear subcarpeta para efecto gravitacional\ngravity_folder = eda_folder / "01_EfectoGravedad"\ngravity_folder.mkdir(exist_ok=True)\n\nprint("="*70)\nprint("AN√ÅLISIS EFECTO GRAVITACIONAL - AZIMUTH VS CARGAS")\nprint("="*70)\nprint(f"Carpeta destino: {gravity_folder}")\nprint("="*70)\n\n# Cargar dataset completo\ncomplete_dataset_path = data_folder_ml / "0000_Complete_dataset.csv"\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    print(f"\\nArchivo: {complete_dataset_path.name}")\n    print("‚öôÔ∏è  M√©todo: Lectura por CHUNKS + Muestreo para gr√°ficos")\n    \n    # ========================================================================\n    # PASO 1: Leer columnas necesarias por chunks con muestreo\n    # ========================================================================\n    \n    print("\\n[1/3] Cargando datos (muestreo 1 de cada 10 filas)...")\n    \n    # Columnas necesarias\n    columns_needed = [\'Rotor azimuth angle\', \'sin_rotor_azimuth\', \'cos_rotor_azimuth\',\n                  \'Blade root 1 My\', \'Blade root 2 My\',\n                  \'M_0\', \'M_1c\', \'M_1s\', \'M_2c\', \'M_2s\']\n    \n    # Leer con muestreo (skiprows para reducir datos)\n    # Tomar 1 de cada 10 filas para reducir memoria\n    sample_rate = 10\n    df_sample = pd.read_csv(complete_dataset_path, \n                           usecols=columns_needed,\n                           skiprows=lambda i: i > 0 and i % sample_rate != 0)\n    \n    print(f"‚úÖ Datos cargados: {df_sample.shape[0]:,} filas (muestreadas), {df_sample.shape[1]} columnas")\n    \n    # ========================================================================\n    # PASO 2: Reconstruir azimuth a grados 0-360\n    # ========================================================================\n    \n    print("\\n[2/3] Reconstruyendo azimuth a grados 0-360...")\n    \n    # Verificar si ya est√° en grados o radianes\n    if df_sample[\'Rotor azimuth angle\'].max() > 6.5:\n        print("   Azimuth ya est√° en grados")\n        azimuth_degrees = df_sample[\'Rotor azimuth angle\']\n    else:\n        print("   Convirtiendo azimuth de radianes a grados")\n        azimuth_degrees = np.rad2deg(df_sample[\'Rotor azimuth angle\'])\n    \n    # Normalizar a rango 0-360\n    azimuth_degrees = azimuth_degrees % 360\n    \n    print(f"   Rango azimuth: {azimuth_degrees.min():.1f}¬∞ - {azimuth_degrees.max():.1f}¬∞")\n    \n    # ========================================================================\n    # PASO 3: Generar gr√°ficos\n    # ========================================================================\n    \n    print(f"\\n[3/3] Generando gr√°ficos...")\n    \n    # Variables a graficar contra azimuth\n    y_variables = [\n        \'Blade root 1 My\',\n        \'Blade root 2 My\',\n        \'Blade root 1 My 1P\',\n        \'Blade root 1 My 2P\',\n        \'Blade root 2 My 1P\',\n        \'Blade root 2 My 2P\'\n    ]\n    \n    generated_count = 0\n    failed_count = 0\n    \n    for y_var in y_variables:\n        if y_var not in df_sample.columns:\n            print(f"  ‚ö†Ô∏è  {y_var}: No existe en el dataset")\n            failed_count += 1\n            continue\n        \n        try:\n            # Datos limpios (sin NaN)\n            mask = ~(df_sample[y_var].isna() | azimuth_degrees.isna())\n            x_data = azimuth_degrees[mask]\n            y_data = df_sample[y_var][mask]\n            \n            if len(x_data) == 0:\n                print(f"  ‚ö†Ô∏è  {y_var}: Sin datos v√°lidos")\n                failed_count += 1\n                continue\n            \n            # -------------------------------------------------------------------\n            # GR√ÅFICO 1: SCATTER PLOT\n            # -------------------------------------------------------------------\n            \n            fig, ax = plt.subplots(figsize=(12, 6))\n            \n            # Scatter plot con transparencia\n            scatter = ax.scatter(x_data, y_data, \n                               alpha=0.3, s=1, c=y_data, \n                               cmap=\'viridis\')\n            \n            # Colorbar\n            cbar = plt.colorbar(scatter, ax=ax)\n            cbar.set_label(f\'{y_var} (kNm)\', fontsize=10)\n            \n            # Etiquetas y t√≠tulo\n            ax.set_xlabel(\'√Ångulo de Azimuth (grados)\', fontsize=12)\n            ax.set_ylabel(f\'{y_var} (kNm)\', fontsize=12)\n            ax.set_title(f\'Efecto Gravitacional: Azimuth vs {y_var}\', \n                        fontsize=14, fontweight=\'bold\', pad=20)\n            \n            # Grid\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            \n            # Ajustar l√≠mites del eje X\n            ax.set_xlim(0, 360)\n            \n            # Ticks cada 45 grados\n            ax.set_xticks(np.arange(0, 361, 45))\n            \n            plt.tight_layout()\n            \n            # Guardar scatter\n            safe_filename = y_var.replace(\' \', \'_\').replace(\'/\', \'_\')\n            output_scatter = gravity_folder / f"scatter_azimuth_vs_{safe_filename}.png"\n            plt.savefig(output_scatter, dpi=150, bbox_inches=\'tight\')\n            plt.close()\n            \n            # -------------------------------------------------------------------\n            # GR√ÅFICO 2: HEXBIN PLOT (Mapa de densidad)\n            # -------------------------------------------------------------------\n            \n            fig, ax = plt.subplots(figsize=(12, 6))\n            \n            # Hexbin plot\n            hexbin = ax.hexbin(x_data, y_data, \n                              gridsize=50, cmap=\'YlOrRd\', \n                              mincnt=1, bins=\'log\')\n            \n            # Colorbar\n            cbar = plt.colorbar(hexbin, ax=ax)\n            cbar.set_label(\'Densidad (log scale)\', fontsize=10)\n            \n            # Etiquetas y t√≠tulo\n            ax.set_xlabel(\'√Ångulo de Azimuth (grados)\', fontsize=12)\n            ax.set_ylabel(f\'{y_var} (kNm)\', fontsize=12)\n            ax.set_title(f\'Mapa de Densidad: Azimuth vs {y_var}\', \n                        fontsize=14, fontweight=\'bold\', pad=20)\n            \n            # Grid\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            \n            # Ajustar l√≠mites del eje X\n            ax.set_xlim(0, 360)\n            \n            # Ticks cada 45 grados\n            ax.set_xticks(np.arange(0, 361, 45))\n            \n            plt.tight_layout()\n            \n            # Guardar hexbin\n            output_hexbin = gravity_folder / f"hexbin_azimuth_vs_{safe_filename}.png"\n            plt.savefig(output_hexbin, dpi=150, bbox_inches=\'tight\')\n            plt.close()\n            \n            generated_count += 1\n            print(f"  ‚úÖ {y_var} (scatter + hexbin)")\n            \n        except Exception as e:\n            failed_count += 1\n            print(f"  ‚ùå {y_var}: {str(e)}")\n    \n    # Liberar memoria\n    del df_sample\n    import gc\n    gc.collect()\n    \n    # ========================================================================\n    # RESUMEN FINAL\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("RESUMEN - AN√ÅLISIS EFECTO GRAVITACIONAL")\n    print("="*70)\n    print(f"Variables procesadas:    {generated_count}")\n    print(f"Errores:                 {failed_count}")\n    print(f"Total gr√°ficos:          {generated_count * 2} (scatter + hexbin)")\n    print(f"\\nüìÅ Gr√°ficos guardados en: {gravity_folder}")\n    print("="*70)\n    print("\\n‚úÖ Se espera observar una ONDA SENOIDAL clara")\n    print("   Esto confirma que el sensor mide correctamente el peso de la pala al girar")',
            'import pandas as pd\nfrom pathlib import Path\n\n# Ruta del CSV\ncsv_path = Path(r"C:\\Users\\Bladedgreen\\Desktop\\_GitHub\\data_train_traditional_ML\\0000_Complete_dataset.csv")\n\n# Leer solo cabecera (m√°s r√°pido y sin cargar todo el dataset)\ndf_head = pd.read_csv(csv_path, nrows=0)\n\n# Crear ruta del TXT en la misma carpeta\ntxt_path = csv_path.with_name(f"{csv_path.stem}_columnas.txt")\n\n# Guardar nombres de columnas\nwith open(txt_path, "w", encoding="utf-8") as f:\n    f.write(f"COLUMNAS DE: {csv_path.name}\\n")\n    f.write("=" * 80 + "\\n")\n    f.write(f"Total columnas: {len(df_head.columns)}\\n\\n")\n    for i, col in enumerate(df_head.columns, 1):\n        f.write(f"{i:4d}. {col}\\n")\n\nprint(f"‚úÖ TXT generado: {txt_path}")\nprint(f"‚úÖ Total columnas: {len(df_head.columns)}")\n',
        ],
    },
    {
        "title": 'üîó PASO 4.4: Matriz de Correlaci√≥n - Variables F√≠sicas',
        "cells": [
            '# ============================================================================\n# PASO 4.5: MATRIZ DE CORRELACI√ìN - VARIABLES F√çSICAS\n# ============================================================================\n\n# Crear subcarpeta para correlaciones\ncorrelation_folder = eda_folder / "02_Correlations"\ncorrelation_folder.mkdir(exist_ok=True)\n\nprint("="*70)\nprint("MATRIZ DE CORRELACI√ìN - VARIABLES F√çSICAS")\nprint("="*70)\nprint(f"Carpeta destino: {correlation_folder}")\nprint("="*70)\n\n# Cargar dataset completo\ncomplete_dataset_path = data_folder_ml / "0000_Complete_dataset.csv"\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    print(f"\\nArchivo: {complete_dataset_path.name}")\n    print("‚öôÔ∏è  M√©todo: Lectura por CHUNKS + Correlaci√≥n incremental")\n    \n    # ========================================================================\n    # PASO 1: Definir variables f√≠sicas (sin lags)\n    # ========================================================================\n    \n    print("\\n[1/4] Definiendo variables f√≠sicas a correlacionar...")\n    \n    # Variables f√≠sicas est√°ndar (sin lags de VLOS)\n    physical_vars = [\n        \'Rotor speed\',\n        \'Blade 1 pitch angle\',\n        \'Blade 2 pitch angle\',\n        \'Rotor azimuth angle\',\n        \'sin_rotor_azimuth\',\n        \'cos_rotor_azimuth\',\n        \'sin_yawerror\',\n        \'cos_yawerror\',\n        \'Blade root 1 My\',\n        \'Blade root 2 My\',\n        \'pitch_0\',\n        \'pitch_1c\',\n        \'pitch_1s\',\n        \'pitch_0_rate\',\n        \'pitch_1c_rate\',\n        \'pitch_1s_rate\', \n        \'U_mean\',\n        \'U_std\',\n        \'U_shear_vert\',\n        \'M_0\', \n        \'M_1c\', \n        \'M_1s\', \n        \'M_2c\', \n        \'M_2s\',\n        \'U0_RANGE5\', \n        \'dU_dy_RANGE5\', \n        \'dU_dz_RANGE5\',\n    ]\n    \n    # A√±adir VLOS sin lags\n    print("   Leyendo columnas disponibles...")\n    df_sample = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_sample.columns.tolist()\n    \n    vlos_vars = [col for col in all_columns \n                 if \'LAC_VLOS\' in col and \'lag\' not in col]\n    \n    physical_vars.extend(vlos_vars)\n    \n    # Filtrar solo las que existen en el dataset\n    available_vars = [var for var in physical_vars if var in all_columns]\n    \n    print(f"\\nüìä Variables a correlacionar: {len(available_vars)}")\n    print("\\nCategor√≠as:")\n    print(f"  - Rotor speed: 1")\n    print(f"  - Pitch angles: {len([v for v in available_vars if \'pitch\' in v.lower()])}")\n    print(f"  - Azimuth: {len([v for v in available_vars if \'azimuth\' in v.lower()])}")\n    print(f"  - Blade moments: {len([v for v in available_vars if \'My\' in v])}")\n    print(f"  - VLOS (sin lags): {len([v for v in available_vars if \'LAC_VLOS\' in v])}")\n    \n    # ========================================================================\n    # PASO 2: Cargar datos con muestreo\n    # ========================================================================\n    \n    print(f"\\n[2/4] Cargando datos (muestreo para eficiencia)...")\n    \n    # Cargar con muestreo (1 de cada 5 filas)\n    sample_rate = 5\n    df_corr = pd.read_csv(complete_dataset_path,\n                          usecols=available_vars,\n                          skiprows=lambda i: i > 0 and i % sample_rate != 0)\n    \n    print(f"‚úÖ Datos cargados: {df_corr.shape[0]:,} filas, {df_corr.shape[1]} columnas")\n    \n    # ========================================================================\n    # PASO 3: Calcular matriz de correlaci√≥n\n    # ========================================================================\n    \n    print("\\n[3/4] Calculando matriz de correlaci√≥n de Pearson...")\n    \n    # Calcular correlaci√≥n\n    corr_matrix = df_corr.corr(method=\'pearson\')\n    \n    print(f"‚úÖ Matriz de correlaci√≥n: {corr_matrix.shape}")\n    \n    # Liberar memoria\n    del df_corr\n    import gc\n    gc.collect()\n    \n    # ========================================================================\n    # PASO 4: Generar heatmap\n    # ========================================================================\n    \n    print("\\n[4/4] Generando heatmap...")\n    \n    # Crear figura grande para ver todos los detalles\n    fig_size = max(12, len(available_vars) * 0.5)\n    fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n    \n    # Crear heatmap con seaborn\n    sns.heatmap(corr_matrix,\n                annot=True,           # Mostrar valores num√©ricos\n                fmt=\'.2f\',            # Formato con 2 decimales\n                cmap=\'coolwarm\',      # Mapa de color divergente\n                center=0,             # Centrar en 0\n                vmin=-1, vmax=1,      # Rango de correlaci√≥n\n                square=True,          # Celdas cuadradas\n                linewidths=0.5,       # L√≠neas entre celdas\n                cbar_kws={\'label\': \'Correlaci√≥n de Pearson\'},\n                ax=ax)\n    \n    # T√≠tulo\n    ax.set_title(\'Matriz de Correlaci√≥n - Variables F√≠sicas (sin lags)\',\n                fontsize=16, fontweight=\'bold\', pad=20)\n    \n    # Rotar etiquetas para mejor legibilidad\n    plt.xticks(rotation=45, ha=\'right\', fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    \n    # Ajustar layout\n    plt.tight_layout()\n    \n    # Guardar figura\n    output_path = correlation_folder / "correlation_matrix_physical_vars.png"\n    plt.savefig(output_path, dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    \n    print(f"‚úÖ Heatmap generado")\n    \n    # ========================================================================\n    # PASO 5: An√°lisis de correlaciones fuertes\n    # ========================================================================\n    \n    print("\\n[5/4] Analizando correlaciones fuertes...")\n    \n    # Encontrar correlaciones fuertes (|r| > 0.7, excluyendo diagonal)\n    strong_correlations = []\n    \n    for i in range(len(corr_matrix.columns)):\n        for j in range(i+1, len(corr_matrix.columns)):\n            corr_value = corr_matrix.iloc[i, j]\n            if abs(corr_value) > 0.7:\n                var1 = corr_matrix.columns[i]\n                var2 = corr_matrix.columns[j]\n                strong_correlations.append((var1, var2, corr_value))\n    \n    # Ordenar por valor absoluto de correlaci√≥n\n    strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n    \n    print(f"\\nüîç Correlaciones fuertes encontradas (|r| > 0.7): {len(strong_correlations)}")\n    print("\\nTop 10 correlaciones m√°s fuertes:")\n    for i, (var1, var2, corr_val) in enumerate(strong_correlations[:10], 1):\n        print(f"  {i}. {var1} <-> {var2}: {corr_val:.3f}")\n    \n    # Guardar correlaciones fuertes en CSV\n    if strong_correlations:\n        df_strong = pd.DataFrame(strong_correlations, \n                                columns=[\'Variable 1\', \'Variable 2\', \'Correlaci√≥n\'])\n        csv_path = correlation_folder / "strong_correlations.csv"\n        df_strong.to_csv(csv_path, index=False)\n        print(f"\\nüíæ Correlaciones fuertes guardadas en: {csv_path.name}")\n    \n    # ========================================================================\n    # RESUMEN FINAL\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("RESUMEN - MATRIZ DE CORRELACI√ìN")\n    print("="*70)\n    print(f"Variables analizadas:      {len(available_vars)}")\n    print(f"Correlaciones calculadas:  {(len(available_vars) * (len(available_vars)-1)) // 2}")\n    print(f"Correlaciones fuertes:     {len(strong_correlations)} (|r| > 0.7)")\n    print(f"\\nüìÅ Archivos generados:")\n    print(f"  - Heatmap: correlation_matrix_physical_vars.png")\n    print(f"  - CSV: strong_correlations.csv")\n    print(f"\\nüìç Ubicaci√≥n: {correlation_folder}")\n    print("="*70)\n    print("\\n‚úÖ An√°lisis completado")\n    print("   Usa el heatmap para identificar relaciones lineales entre variables")',
        ],
    },
    {
        "title": 'üïí PASO 4.5: An√°lisis de Retardo (Lag) - Tiempo de Viaje del Viento',
        "cells": [
            '# ============================================================================\n# PASO 4.6: AN√ÅLISIS DE RETARDO (LAG) - TIEMPO DE VIAJE DEL VIENTO\n# ============================================================================\n\n# Crear subcarpeta para an√°lisis de lags\nlags_folder = eda_folder / "03_Lags"\nlags_folder.mkdir(exist_ok=True)\n\nprint("="*70)\nprint("AN√ÅLISIS DE RETARDO - TIEMPO DE VIAJE DEL VIENTO")\nprint("="*70)\nprint(f"Carpeta destino: {lags_folder}")\nprint("="*70)\n\n# Cargar dataset completo\ncomplete_dataset_path = data_folder_ml / "0000_Complete_dataset.csv"\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    print(f"\\nArchivo: {complete_dataset_path.name}")\n    print("‚öôÔ∏è  M√©todo: C√°lculo de correlaciones lag por lag")\n    \n    # ========================================================================\n    # PASO 1: Identificar columnas con lags y variables objetivo\n    # ========================================================================\n    \n    print("\\n[1/4] Identificando variables con lag y targets...")\n    \n    # Leer columnas del dataset\n    df_sample = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_sample.columns.tolist()\n    \n    # Identificar variables VLOS con lag\n    vlos_lag_columns = [col for col in all_columns \n                        if \'LAC_VLOS\' in col and \'lag\' in col]\n    \n    # Variables objetivo\n    target_vars = [\'Blade root 1 My\', \'Blade root 2 My\']\n    \n    print(f"\\nüìä Variables VLOS con lag encontradas: {len(vlos_lag_columns)}")\n    print(f"üìä Variables objetivo: {len(target_vars)}")\n    \n    # Extraer informaci√≥n de lags\n    # Formato esperado: LAC_VLOS_BEAMX_RANGE5_lag6s, lag10s, etc\n    # Extraer BEAM y segundos de lag\n    \n    lag_info = {}\n    for col in vlos_lag_columns:\n        # Extraer BEAM number\n        if \'BEAM\' in col:\n            beam_part = col.split(\'BEAM\')[1].split(\'_\')[0]\n            beam_num = int(beam_part)\n            \n            # Extraer segundos de lag (formato: lag6s, lag10s, etc)\n            if \'lag\' in col:\n                # Buscar \'lag\' seguido de d√≠gitos y \'s\'\n                import re\n                lag_match = re.search(r\'lag(\\d+)s\', col)\n                if lag_match:\n                    lag_seconds = int(lag_match.group(1))\n                    \n                    # Almacenar info\n                    if beam_num not in lag_info:\n                        lag_info[beam_num] = {}\n                    \n                    lag_info[beam_num][lag_seconds] = col\n    \n    print(f"\\nüîç BEAMs identificados: {sorted(lag_info.keys())}")\n    \n    # Obtener lista de tiempos de lag √∫nicos\n    all_lag_times = set()\n    for beam_data in lag_info.values():\n        all_lag_times.update(beam_data.keys())\n    lag_times_sorted = sorted(all_lag_times)\n    \n    print(f"üîç Tiempos de lag: {lag_times_sorted[0]}s - {lag_times_sorted[-1]}s ({len(lag_times_sorted)} valores)")\n    \n    # ========================================================================\n    # PASO 2: Cargar datos necesarios\n    # ========================================================================\n    \n    print(f"\\n[2/4] Cargando datos...")\n    \n    # Columnas a cargar: targets + todas las VLOS con lag\n    columns_to_load = target_vars + vlos_lag_columns\n    \n    # Cargar con muestreo para eficiencia\n    sample_rate = 5\n    df_lags = pd.read_csv(complete_dataset_path,\n                          usecols=columns_to_load,\n                          skiprows=lambda i: i > 0 and i % sample_rate != 0)\n    \n    print(f"‚úÖ Datos cargados: {df_lags.shape[0]:,} filas, {df_lags.shape[1]} columnas")\n    \n    # ========================================================================\n    # PASO 3: Calcular correlaciones para cada BEAM\n    # ========================================================================\n    \n    print(f"\\n[3/4] Calculando correlaciones lag por lag...")\n    \n    # Diccionario para almacenar resultados\n    # Estructura: {beam_num: {target_var: {lag_time: correlation}}}\n    correlation_results = {}\n    \n    for beam_num in sorted(lag_info.keys()):\n        print(f"\\n  üì° Procesando BEAM {beam_num}...")\n        \n        correlation_results[beam_num] = {}\n        \n        for target_var in target_vars:\n            correlation_results[beam_num][target_var] = {}\n            \n            for lag_seconds, vlos_col in lag_info[beam_num].items():\n                # Calcular correlaci√≥n de Pearson\n                corr_value = df_lags[vlos_col].corr(df_lags[target_var])\n                correlation_results[beam_num][target_var][lag_seconds] = corr_value\n    \n    print(f"\\n‚úÖ Correlaciones calculadas para {len(correlation_results)} BEAMs")\n    \n    # Liberar memoria\n    del df_lags\n    import gc\n    gc.collect()\n    \n    # ========================================================================\n    # PASO 4: Generar gr√°ficos de correlaci√≥n vs tiempo de lag\n    # ========================================================================\n    \n    print(f"\\n[4/4] Generando gr√°ficos...")\n    \n    # Crear un gr√°fico por cada BEAM (con ambos targets)\n    for beam_num in sorted(correlation_results.keys()):\n        \n        fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n        fig.suptitle(f\'Correlaci√≥n Lag vs Tiempo - BEAM {beam_num}\', \n                     fontsize=16, fontweight=\'bold\', y=0.995)\n        \n        for idx, target_var in enumerate(target_vars):\n            ax = axes[idx]\n            \n            # Extraer datos para este target\n            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n            \n            # Graficar curva de correlaci√≥n\n            ax.plot(lag_times, correlations, \'o-\', linewidth=2, markersize=6,\n                   color=\'steelblue\', label=\'Correlaci√≥n\')\n            \n            # Encontrar el m√°ximo\n            max_corr = max(correlations)\n            max_lag = lag_times[correlations.index(max_corr)]\n            \n            # Marcar el m√°ximo con l√≠nea vertical\n            ax.axvline(x=max_lag, color=\'red\', linestyle=\'--\', linewidth=2,\n                      label=f\'M√°ximo: {max_lag}s (r={max_corr:.3f})\')\n            \n            # A√±adir anotaci√≥n en el pico\n            ax.annotate(f\'{max_lag}s\\nr={max_corr:.3f}\',\n                       xy=(max_lag, max_corr),\n                       xytext=(max_lag + 2, max_corr - 0.05),\n                       fontsize=11, fontweight=\'bold\',\n                       bbox=dict(boxstyle=\'round,pad=0.5\', facecolor=\'yellow\', alpha=0.7),\n                       arrowprops=dict(arrowstyle=\'->\', color=\'red\', lw=1.5))\n            \n            # Configurar ejes y etiquetas\n            ax.set_xlabel(\'Tiempo de Lag (segundos)\', fontsize=12, fontweight=\'bold\')\n            ax.set_ylabel(\'Correlaci√≥n de Pearson\', fontsize=12, fontweight=\'bold\')\n            ax.set_title(f\'Target: {target_var}\', fontsize=13, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            ax.legend(loc=\'best\', fontsize=10)\n            \n            # Ajustar l√≠mites del eje Y para mejor visualizaci√≥n\n            y_min = min(correlations) - 0.05\n            y_max = max(correlations) + 0.05\n            ax.set_ylim([y_min, y_max])\n        \n        plt.tight_layout()\n        \n        # Guardar figura\n        output_path = lags_folder / f"lag_correlation_BEAM{beam_num}.png"\n        plt.savefig(output_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        # Imprimir m√°ximos para este BEAM\n        max_info = []\n        for target_var in target_vars:\n            lag_times_temp = sorted(correlation_results[beam_num][target_var].keys())\n            correlations_temp = [correlation_results[beam_num][target_var][t] for t in lag_times_temp]\n            max_corr = max(correlations_temp)\n            max_lag = lag_times_temp[correlations_temp.index(max_corr)]\n            max_info.append(f"{target_var}={max_lag}s")\n        \n        print(f"  ‚úÖ BEAM {beam_num}: M√°ximos en " + ", ".join(max_info))\n    \n    # ========================================================================\n    # PASO 5: Crear gr√°fico resumen con todos los BEAMs\n    # ========================================================================\n    \n    print(f"\\n[5/4] Generando gr√°fico resumen...")\n    \n    # Crear figura con subplots para cada target\n    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n    fig.suptitle(\'Resumen: Correlaci√≥n Lag vs Tiempo - Todos los BEAMs\', \n                 fontsize=16, fontweight=\'bold\', y=0.995)\n    \n    # Paleta de colores para distinguir BEAMs\n    colors = plt.cm.tab10(np.linspace(0, 1, len(correlation_results)))\n    \n    for idx, target_var in enumerate(target_vars):\n        ax = axes[idx]\n        \n        max_correlations_summary = []\n        \n        for beam_idx, beam_num in enumerate(sorted(correlation_results.keys())):\n            # Extraer datos\n            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n            \n            # Graficar curva\n            ax.plot(lag_times, correlations, \'o-\', linewidth=1.5, markersize=4,\n                   color=colors[beam_idx], label=f\'BEAM {beam_num}\', alpha=0.8)\n            \n            # Encontrar m√°ximo\n            max_corr = max(correlations)\n            max_lag = lag_times[correlations.index(max_corr)]\n            max_correlations_summary.append((beam_num, max_lag, max_corr))\n            \n            # Marcar m√°ximo con punto destacado\n            ax.plot(max_lag, max_corr, \'o\', markersize=10, color=colors[beam_idx],\n                   markeredgecolor=\'black\', markeredgewidth=1.5)\n        \n        # Configurar ejes y etiquetas\n        ax.set_xlabel(\'Tiempo de Lag (segundos)\', fontsize=12, fontweight=\'bold\')\n        ax.set_ylabel(\'Correlaci√≥n de Pearson\', fontsize=12, fontweight=\'bold\')\n        ax.set_title(f\'Target: {target_var}\', fontsize=13, fontweight=\'bold\')\n        ax.grid(True, alpha=0.3, linestyle=\'--\')\n        ax.legend(loc=\'best\', fontsize=9, ncol=2)\n    \n    plt.tight_layout()\n    \n    # Guardar figura resumen\n    output_path = lags_folder / "lag_correlation_summary_all_beams.png"\n    plt.savefig(output_path, dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    \n    print(f"‚úÖ Gr√°fico resumen generado")\n    \n    # ========================================================================\n    # PASO 6: Guardar resultados en CSV\n    # ========================================================================\n    \n    print(f"\\n[6/4] Guardando resultados en CSV...")\n    \n    # Crear DataFrame con todos los resultados\n    results_list = []\n    \n    for beam_num in sorted(correlation_results.keys()):\n        for target_var in target_vars:\n            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n            \n            # Encontrar m√°ximo\n            max_corr = max(correlations)\n            max_lag = lag_times[correlations.index(max_corr)]\n            \n            for lag_time, corr_value in zip(lag_times, correlations):\n                results_list.append({\n                    \'BEAM\': beam_num,\n                    \'Target\': target_var,\n                    \'Lag_Seconds\': lag_time,\n                    \'Correlation\': corr_value,\n                    \'Is_Maximum\': (lag_time == max_lag)\n                })\n    \n    df_results = pd.DataFrame(results_list)\n    csv_path = lags_folder / "lag_correlations_detailed.csv"\n    df_results.to_csv(csv_path, index=False)\n    \n    print(f"‚úÖ Resultados detallados guardados en: {csv_path.name}")\n    \n    # Crear CSV resumen con m√°ximos\n    max_results = []\n    for beam_num in sorted(correlation_results.keys()):\n        for target_var in target_vars:\n            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n            \n            max_corr = max(correlations)\n            max_lag = lag_times[correlations.index(max_corr)]\n            \n            max_results.append({\n                \'BEAM\': beam_num,\n                \'Target\': target_var,\n                \'Optimal_Lag_Seconds\': max_lag,\n                \'Max_Correlation\': max_corr\n            })\n    \n    df_max_results = pd.DataFrame(max_results)\n    csv_max_path = lags_folder / "optimal_lag_summary.csv"\n    df_max_results.to_csv(csv_max_path, index=False)\n    \n    print(f"‚úÖ Resumen de m√°ximos guardado en: {csv_max_path.name}")\n    \n    # ========================================================================\n    # RESUMEN FINAL\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("RESUMEN - AN√ÅLISIS DE RETARDO")\n    print("="*70)\n    print(f"BEAMs analizados:          {len(correlation_results)}")\n    print(f"Targets analizados:        {len(target_vars)}")\n    print(f"Rango de lags:             {lag_times_sorted[0]}s - {lag_times_sorted[-1]}s")\n    print(f"\\nüéØ TIEMPOS √ìPTIMOS DE VIAJE DEL VIENTO:")\n    print("-"*70)\n    \n    for target_var in target_vars:\n        print(f"\\n  Target: {target_var}")\n        for beam_num in sorted(correlation_results.keys()):\n            lag_times = sorted(correlation_results[beam_num][target_var].keys())\n            correlations = [correlation_results[beam_num][target_var][t] for t in lag_times]\n            max_corr = max(correlations)\n            max_lag = lag_times[correlations.index(max_corr)]\n            print(f"    BEAM {beam_num}: {max_lag}s (r={max_corr:.3f})")\n    \n    print(f"\\nüìÅ Archivos generados:")\n    print(f"  - Gr√°ficos individuales: lag_correlation_BEAMX.png ({len(correlation_results)} archivos)")\n    print(f"  - Gr√°fico resumen: lag_correlation_summary_all_beams.png")\n    print(f"  - CSV detallado: lag_correlations_detailed.csv")\n    print(f"  - CSV resumen: optimal_lag_summary.csv")\n    print(f"\\nüìç Ubicaci√≥n: {lags_folder}")\n    print("="*70)\n    print("\\n‚úÖ An√°lisis completado")\n    print("   El pico de correlaci√≥n indica el tiempo de viaje del viento del LIDAR al rotor")',
        ],
    },
    {
        "title": 'üå¨Ô∏è PASO 4.6: An√°lisis de Retardo por Categor√≠a de Viento',
        "cells": [
            '# ============================================================================\n# PASO 4.7: AN√ÅLISIS DE RETARDO POR CATEGOR√çA DE VIENTO\n# ============================================================================\n\n# Crear subcarpeta para an√°lisis por velocidad de viento\nwind_lags_folder = eda_folder / "04_Lag_per_Wind"\nwind_lags_folder.mkdir(exist_ok=True)\n\nprint("="*70)\nprint("AN√ÅLISIS DE RETARDO POR CATEGOR√çA DE VIENTO")\nprint("="*70)\nprint(f"Carpeta destino: {wind_lags_folder}")\nprint("="*70)\n\n# Cargar dataset completo\ncomplete_dataset_path = data_folder_ml / "0000_Complete_dataset.csv"\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    print(f"\\nArchivo: {complete_dataset_path.name}")\n    print("‚öôÔ∏è  M√©todo: Estratificaci√≥n por velocidad de viento")\n    \n    # ========================================================================\n    # PASO 1: Identificar columnas VLOS y lag\n    # ========================================================================\n    \n    print("\\n[1/6] Identificando variables VLOS...")\n    \n    # Leer columnas del dataset\n    df_sample = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_sample.columns.tolist()\n    \n    # VLOS sin lag (para calcular wind_mean)\n    vlos_base_columns = [col for col in all_columns \n                         if \'LAC_VLOS\' in col and \'lag\' not in col]\n    \n    # VLOS con lag\n    vlos_lag_columns = [col for col in all_columns \n                        if \'LAC_VLOS\' in col and \'lag\' in col]\n    \n    # Variables objetivo\n    target_vars = [\'Blade root 1 My\', \'Blade root 2 My\']\n    \n    print(f"\\nüìä Variables VLOS base (sin lag): {len(vlos_base_columns)}")\n    print(f"üìä Variables VLOS con lag: {len(vlos_lag_columns)}")\n    print(f"üìä Variables objetivo: {len(target_vars)}")\n    \n    # ========================================================================\n    # PASO 2: Extraer informaci√≥n de lags\n    # ========================================================================\n    \n    print("\\n[2/6] Extrayendo informaci√≥n de lags...")\n    \n    lag_info = {}\n    import re\n    \n    for col in vlos_lag_columns:\n        # Extraer BEAM number\n        if \'BEAM\' in col:\n            beam_part = col.split(\'BEAM\')[1].split(\'_\')[0]\n            beam_num = int(beam_part)\n            \n            # Extraer segundos de lag (formato: lag6s, lag10s, etc)\n            lag_match = re.search(r\'lag(\\d+)s\', col)\n            if lag_match:\n                lag_seconds = int(lag_match.group(1))\n                \n                # Almacenar info\n                if beam_num not in lag_info:\n                    lag_info[beam_num] = {}\n                \n                lag_info[beam_num][lag_seconds] = col\n    \n    print(f"üîç BEAMs identificados: {sorted(lag_info.keys())}")\n    \n    # Obtener lista de tiempos de lag √∫nicos\n    all_lag_times = set()\n    for beam_data in lag_info.values():\n        all_lag_times.update(beam_data.keys())\n    lag_times_sorted = sorted(all_lag_times)\n    \n    print(f"üîç Tiempos de lag: {lag_times_sorted[0]}s - {lag_times_sorted[-1]}s ({len(lag_times_sorted)} valores)")\n    \n    # ========================================================================\n    # PASO 3: Cargar datos y calcular wind_mean\n    # ========================================================================\n    \n    print(f"\\n[3/6] Cargando datos y calculando velocidad media del viento...")\n    \n    # Columnas necesarias: VLOS base + VLOS lag + targets\n    columns_to_load = vlos_base_columns + vlos_lag_columns + target_vars\n    \n    # Cargar con muestreo\n    sample_rate = 5\n    df_wind = pd.read_csv(complete_dataset_path,\n                          usecols=columns_to_load,\n                          skiprows=lambda i: i > 0 and i % sample_rate != 0)\n    \n    print(f"‚úÖ Datos cargados: {df_wind.shape[0]:,} filas, {df_wind.shape[1]} columnas")\n    \n    # Calcular wind_mean: promedio de variables VLOS sin lag\n    print("\\n‚öôÔ∏è  Calculando wind_mean (promedio de VLOS sin lag)...")\n    df_wind[\'wind_mean\'] = df_wind[vlos_base_columns].mean(axis=1)\n    \n    print(f"‚úÖ wind_mean calculado")\n    print(f"   Rango: {df_wind[\'wind_mean\'].min():.2f} - {df_wind[\'wind_mean\'].max():.2f} m/s")\n    print(f"   Media: {df_wind[\'wind_mean\'].mean():.2f} m/s")\n    print(f"   Mediana: {df_wind[\'wind_mean\'].median():.2f} m/s")\n    \n    # ========================================================================\n    # PASO 4: Categorizar por velocidad de viento\n    # ========================================================================\n    \n    print(f"\\n[4/6] Categorizando por velocidad de viento...")\n    \n    # Definir categor√≠as\n    def categorize_wind(wind_speed):\n        if wind_speed < 9:\n            return \'Bajo (0-9 m/s)\'\n        elif wind_speed < 18:\n            return \'Medio (9-18 m/s)\'\n        else:\n            return \'Alto (18-30 m/s)\'\n    \n    df_wind[\'wind_category\'] = df_wind[\'wind_mean\'].apply(categorize_wind)\n    \n    # Contar muestras por categor√≠a\n    category_counts = df_wind[\'wind_category\'].value_counts().sort_index()\n    \n    print("\\nüìä Distribuci√≥n de categor√≠as:")\n    for category, count in category_counts.items():\n        percentage = (count / len(df_wind)) * 100\n        print(f"   {category}: {count:,} muestras ({percentage:.1f}%)")\n    \n    # ========================================================================\n    # PASO 5: Calcular correlaciones por categor√≠a\n    # ========================================================================\n    \n    print(f"\\n[5/6] Calculando correlaciones por categor√≠a de viento...")\n    \n    # Estructura: {category: {beam_num: {target_var: {lag_time: correlation}}}}\n    correlation_by_wind = {}\n    \n    for category in [\'Bajo (0-9 m/s)\', \'Medio (9-18 m/s)\', \'Alto (18-30 m/s)\']:\n        print(f"\\n  üå¨Ô∏è  Procesando: {category}...")\n        \n        # Filtrar datos por categor√≠a\n        df_category = df_wind[df_wind[\'wind_category\'] == category]\n        \n        if len(df_category) == 0:\n            print(f"     ‚ö†Ô∏è  No hay datos para esta categor√≠a")\n            continue\n        \n        print(f"     Muestras: {len(df_category):,}")\n        \n        correlation_by_wind[category] = {}\n        \n        for beam_num in sorted(lag_info.keys()):\n            correlation_by_wind[category][beam_num] = {}\n            \n            for target_var in target_vars:\n                correlation_by_wind[category][beam_num][target_var] = {}\n                \n                for lag_seconds, vlos_col in lag_info[beam_num].items():\n                    # Calcular correlaci√≥n\n                    corr_value = df_category[vlos_col].corr(df_category[target_var])\n                    correlation_by_wind[category][beam_num][target_var][lag_seconds] = corr_value\n    \n    print(f"\\n‚úÖ Correlaciones calculadas para {len(correlation_by_wind)} categor√≠as")\n    \n    # Liberar memoria\n    del df_wind\n    import gc\n    gc.collect()\n    \n    # ========================================================================\n    # PASO 6: Generar gr√°ficos comparativos por categor√≠a de viento\n    # ========================================================================\n    \n    print(f"\\n[6/6] Generando gr√°ficos comparativos...")\n    \n    # Colores para cada categor√≠a\n    category_colors = {\n        \'Bajo (0-9 m/s)\': \'#3498db\',      # Azul\n        \'Medio (9-18 m/s)\': \'#f39c12\',    # Naranja\n        \'Alto (18-30 m/s)\': \'#e74c3c\'     # Rojo\n    }\n    \n    category_labels = {\n        \'Bajo (0-9 m/s)\': \'Viento Bajo (0-9 m/s)\',\n        \'Medio (9-18 m/s)\': \'Viento Medio (9-18 m/s)\',\n        \'Alto (18-30 m/s)\': \'Viento Alto (18-30 m/s)\'\n    }\n    \n    # Crear un gr√°fico por cada BEAM (con ambos targets)\n    for beam_num in sorted(lag_info.keys()):\n        \n        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n        fig.suptitle(f\'Perfil de Correlaci√≥n Temporal por Velocidad de Viento - BEAM {beam_num}\', \n                     fontsize=16, fontweight=\'bold\', y=0.995)\n        \n        for idx, target_var in enumerate(target_vars):\n            ax = axes[idx]\n            \n            # Graficar cada categor√≠a de viento\n            for category in [\'Bajo (0-9 m/s)\', \'Medio (9-18 m/s)\', \'Alto (18-30 m/s)\']:\n                \n                if category not in correlation_by_wind:\n                    continue\n                \n                # Extraer datos para esta categor√≠a y target\n                lag_times = sorted(correlation_by_wind[category][beam_num][target_var].keys())\n                correlations = [correlation_by_wind[category][beam_num][target_var][t] \n                              for t in lag_times]\n                \n                # Graficar curva\n                ax.plot(lag_times, correlations, \'o-\', \n                       linewidth=2.5, markersize=7,\n                       color=category_colors[category], \n                       label=category_labels[category],\n                       alpha=0.85)\n                \n                # Encontrar y marcar el m√°ximo\n                max_corr = max(correlations)\n                max_lag = lag_times[correlations.index(max_corr)]\n                \n                # L√≠nea vertical en el pico\n                ax.axvline(x=max_lag, color=category_colors[category], \n                          linestyle=\'--\', linewidth=1.5, alpha=0.5)\n                \n                # Punto destacado en el m√°ximo\n                ax.plot(max_lag, max_corr, \'o\', markersize=12, \n                       color=category_colors[category],\n                       markeredgecolor=\'black\', markeredgewidth=2)\n                \n                # Anotaci√≥n con valor del pico\n                y_offset = 0.02 if category == \'Bajo (0-9 m/s)\' else (-0.02 if category == \'Alto (18-30 m/s)\' else 0)\n                ax.annotate(f\'{max_lag}s\',\n                           xy=(max_lag, max_corr),\n                           xytext=(max_lag, max_corr + y_offset),\n                           fontsize=9, fontweight=\'bold\',\n                           color=category_colors[category],\n                           ha=\'center\',\n                           bbox=dict(boxstyle=\'round,pad=0.3\', \n                                   facecolor=\'white\', \n                                   edgecolor=category_colors[category],\n                                   alpha=0.8))\n            \n            # Configurar ejes y etiquetas\n            ax.set_xlabel(\'Tiempo de Lag (segundos)\', fontsize=12, fontweight=\'bold\')\n            ax.set_ylabel(\'Correlaci√≥n de Pearson\', fontsize=12, fontweight=\'bold\')\n            ax.set_title(f\'Target: {target_var}\', fontsize=13, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            ax.legend(loc=\'best\', fontsize=11, framealpha=0.95)\n        \n        plt.tight_layout()\n        \n        # Guardar figura\n        output_path = wind_lags_folder / f"lag_correlation_by_wind_BEAM{beam_num}.png"\n        plt.savefig(output_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"  ‚úÖ BEAM {beam_num}: Gr√°fico comparativo generado")\n    \n    # ========================================================================\n    # PASO 7: Crear gr√°fico resumen multi-BEAM por categor√≠a\n    # ========================================================================\n    \n    print(f"\\n[7/6] Generando gr√°fico resumen multi-BEAM...")\n    \n    # Crear figura con 3 subplots (uno por categor√≠a)\n    fig, axes = plt.subplots(3, 1, figsize=(14, 16))\n    fig.suptitle(\'Resumen: Perfil de Correlaci√≥n por Velocidad de Viento - Todos los BEAMs\', \n                 fontsize=16, fontweight=\'bold\', y=0.995)\n    \n    # Usar solo el primer target para simplificar\n    target_var = target_vars[0]\n    \n    # Colores para BEAMs\n    beam_colors = plt.cm.tab10(np.linspace(0, 1, len(lag_info)))\n    \n    for idx, category in enumerate([\'Bajo (0-9 m/s)\', \'Medio (9-18 m/s)\', \'Alto (18-30 m/s)\']):\n        ax = axes[idx]\n        \n        if category not in correlation_by_wind:\n            continue\n        \n        for beam_idx, beam_num in enumerate(sorted(lag_info.keys())):\n            # Extraer datos\n            lag_times = sorted(correlation_by_wind[category][beam_num][target_var].keys())\n            correlations = [correlation_by_wind[category][beam_num][target_var][t] \n                          for t in lag_times]\n            \n            # Graficar curva\n            ax.plot(lag_times, correlations, \'o-\', \n                   linewidth=1.5, markersize=4,\n                   color=beam_colors[beam_idx], \n                   label=f\'BEAM {beam_num}\',\n                   alpha=0.8)\n            \n            # Marcar m√°ximo\n            max_corr = max(correlations)\n            max_lag = lag_times[correlations.index(max_corr)]\n            ax.plot(max_lag, max_corr, \'o\', markersize=10, \n                   color=beam_colors[beam_idx],\n                   markeredgecolor=\'black\', markeredgewidth=1.5)\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo de Lag (segundos)\', fontsize=11, fontweight=\'bold\')\n        ax.set_ylabel(\'Correlaci√≥n de Pearson\', fontsize=11, fontweight=\'bold\')\n        ax.set_title(f\'{category_labels[category]} - Target: {target_var}\', \n                    fontsize=12, fontweight=\'bold\',\n                    color=category_colors[category])\n        ax.grid(True, alpha=0.3, linestyle=\'--\')\n        ax.legend(loc=\'best\', fontsize=9, ncol=2)\n    \n    plt.tight_layout()\n    \n    # Guardar\n    output_path = wind_lags_folder / "lag_correlation_by_wind_summary.png"\n    plt.savefig(output_path, dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    \n    print(f"‚úÖ Gr√°fico resumen generado")\n    \n    # ========================================================================\n    # PASO 8: Guardar resultados en CSV\n    # ========================================================================\n    \n    print(f"\\n[8/6] Guardando resultados en CSV...")\n    \n    # CSV con picos √≥ptimos por categor√≠a\n    optimal_results = []\n    \n    for category in correlation_by_wind.keys():\n        for beam_num in sorted(lag_info.keys()):\n            for target_var in target_vars:\n                lag_times = sorted(correlation_by_wind[category][beam_num][target_var].keys())\n                correlations = [correlation_by_wind[category][beam_num][target_var][t] \n                              for t in lag_times]\n                \n                max_corr = max(correlations)\n                max_lag = lag_times[correlations.index(max_corr)]\n                \n                optimal_results.append({\n                    \'Wind_Category\': category,\n                    \'BEAM\': beam_num,\n                    \'Target\': target_var,\n                    \'Optimal_Lag_Seconds\': max_lag,\n                    \'Max_Correlation\': max_corr\n                })\n    \n    df_optimal = pd.DataFrame(optimal_results)\n    csv_path = wind_lags_folder / "optimal_lag_by_wind_category.csv"\n    df_optimal.to_csv(csv_path, index=False)\n    \n    print(f"‚úÖ CSV guardado: {csv_path.name}")\n    \n    # ========================================================================\n    # RESUMEN FINAL\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("RESUMEN - AN√ÅLISIS POR CATEGOR√çA DE VIENTO")\n    print("="*70)\n    print(f"Categor√≠as analizadas:     {len(correlation_by_wind)}")\n    print(f"BEAMs analizados:          {len(lag_info)}")\n    print(f"Targets analizados:        {len(target_vars)}")\n    \n    print(f"\\nüéØ DESPLAZAMIENTO DE PICOS (DEMOSTRACI√ìN DE NO LINEALIDAD):")\n    print("-"*70)\n    \n    for target_var in target_vars:\n        print(f"\\n  Target: {target_var}")\n        \n        for beam_num in sorted(lag_info.keys()):\n            print(f"    BEAM {beam_num}:")\n            \n            for category in [\'Bajo (0-9 m/s)\', \'Medio (9-18 m/s)\', \'Alto (18-30 m/s)\']:\n                if category in correlation_by_wind:\n                    lag_times = sorted(correlation_by_wind[category][beam_num][target_var].keys())\n                    correlations = [correlation_by_wind[category][beam_num][target_var][t] \n                                  for t in lag_times]\n                    max_corr = max(correlations)\n                    max_lag = lag_times[correlations.index(max_corr)]\n                    \n                    icon = \'üîµ\' if \'Bajo\' in category else (\'üü°\' if \'Medio\' in category else \'üî¥\')\n                    print(f"      {icon} {category}: {max_lag}s (r={max_corr:.3f})")\n    \n    print(f"\\nüí° CONCLUSI√ìN:")\n    print("   El pico de correlaci√≥n se DESPLAZA seg√∫n la velocidad del viento:")\n    print("   - Viento Alto ‚Üí Pico temprano (delay corto)")\n    print("   - Viento Bajo ‚Üí Pico tard√≠o (delay largo)")\n    print("   Esto confirma la necesidad de MODELOS NO LINEALES que capturen")\n    print("   esta variabilidad en el tiempo de viaje del viento.")\n    \n    print(f"\\nüìÅ Archivos generados:")\n    print(f"  - Gr√°ficos comparativos: lag_correlation_by_wind_BEAMX.png")\n    print(f"  - Gr√°fico resumen: lag_correlation_by_wind_summary.png")\n    print(f"  - CSV: optimal_lag_by_wind_category.csv")\n    print(f"\\nüìç Ubicaci√≥n: {wind_lags_folder}")\n    print("="*70)\n    print("\\n‚úÖ An√°lisis completado")',
        ],
    },
    {
        "title": 'üå¨Ô∏è PASO 4.9: Lag √ìptimo por Viento (M√©todo Alineado con el Lag)',
        "cells": [
            '# ============================================================================\n# PASO 4.9: LAG √ìPTIMO POR VIENTO (M√âTODO ALINEADO CON EL LAG)\n# ============================================================================\n\nimport re\n\nprint("="*70)\nprint("LAG √ìPTIMO POR VIENTO (M√âTODO ALINEADO)")\nprint("="*70)\nprint(f"Carpeta destino: {wind_lags_folder}")\nprint("="*70)\n\n# Par√°metros (ajusta si quieres m√°s/menos estabilidad)\nsample_rate = 5                 # 1 de cada N filas\nmin_samples_per_bin = 200       # m√≠nimo filas por bin para calcular correlaci√≥n\nuse_abs_for_optimum = True      # True: argmax(|corr|) ; False: argmax(corr)\n\n# Archivo\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    # ------------------------------------------------------------------------\n    # 1) Identificar columnas lag y construir mapping lag -> columnas (por BEAM)\n    # ------------------------------------------------------------------------\n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n\n    target_vars = [\'Blade root 1 My\', \'Blade root 2 My\']\n    for t in target_vars:\n        if t not in all_columns:\n            raise ValueError(f"No encuentro target \'{t}\' en el dataset")\n\n    vlos_lag_cols = [c for c in all_columns if (\'LAC_VLOS\' in c and \'lag\' in c)]\n    if len(vlos_lag_cols) == 0:\n        raise ValueError("No se encontraron columnas VLOS con lag. Revisa nombres de columnas.")\n\n    lag_to_cols = {}\n    beam_set = set()\n\n    for col in vlos_lag_cols:\n        m_lag = re.search(r\'lag(\\d+)s\', col)\n        m_beam = re.search(r\'BEAM(\\d+)\', col)\n        if not (m_lag and m_beam):\n            continue\n        lag_s = int(m_lag.group(1))\n        beam = int(m_beam.group(1))\n        beam_set.add(beam)\n        lag_to_cols.setdefault(lag_s, []).append(col)\n\n    lags = sorted(lag_to_cols.keys())\n    beams = sorted(beam_set)\n\n    print(f"\\nüìå Lags detectados: {lags[0]}s - {lags[-1]}s ({len(lags)} lags)")\n    print(f"üìå BEAMs detectados en lags: {beams}")\n\n    # ------------------------------------------------------------------------\n    # 2) Cargar datos (targets + lags). No cargamos VLOS sin lag: aqu√≠ binning es por wind_mean_lagXs\n    # ------------------------------------------------------------------------\n    print("\\n[1/4] Cargando datos (targets + VLOS_lag*)...")\n    usecols = target_vars + vlos_lag_cols\n\n    df = pd.read_csv(\n        complete_dataset_path,\n        usecols=usecols,\n        skiprows=lambda i: i > 0 and i % sample_rate != 0\n    )\n\n    print(f"‚úÖ Cargado: {df.shape[0]:,} filas, {df.shape[1]} columnas (sample_rate={sample_rate})")\n\n    # ------------------------------------------------------------------------\n    # 3) Construir rejilla correlaci√≥n(viento_bin, lag) para cada target\n    #    Bin se define con wind_mean_lagXs = mean( columnas lagXs a trav√©s de BEAMs )\n    # ------------------------------------------------------------------------\n    print("\\n[2/4] Calculando correlaci√≥n por bin(1 m/s) y lag (alineado)...")\n\n    wind_edges = np.arange(0, 31, 1)  # 0..30\n    wind_mids = wind_edges[:-1] + 0.5\n    wind_bin_labels = [f"{i}-{i+1}" for i in range(30)]\n\n    results_grids = {}   # target -> dict(grid, counts)\n\n    for target in target_vars:\n        corr_grid = np.full((len(wind_mids), len(lags)), np.nan, dtype=float)\n        n_grid = np.zeros((len(wind_mids), len(lags)), dtype=int)\n\n        for j, lag_s in enumerate(lags):\n            cols_lag = lag_to_cols[lag_s]\n\n            # wind_mean_lagXs para cada fila\n            wind_mean_lag = df[cols_lag].mean(axis=1).to_numpy()\n\n            # asignaci√≥n r√°pida de bin\n            bin_idx = np.digitize(wind_mean_lag, wind_edges, right=False) - 1\n\n            # para cada bin con suficientes muestras\n            for i_bin in range(len(wind_mids)):\n                mask = (bin_idx == i_bin)\n                n = int(mask.sum())\n                if n < min_samples_per_bin:\n                    continue\n\n                # correlaci√≥n por BEAM en este lag; luego promedio\n                # (cada col ya es un BEAM concreto a ese lag)\n                corr_per_beam = df.loc[mask, cols_lag].corrwith(df.loc[mask, target])\n                mean_corr = float(corr_per_beam.mean(skipna=True))\n\n                corr_grid[i_bin, j] = mean_corr\n                n_grid[i_bin, j] = n\n\n        results_grids[target] = {"corr": corr_grid, "n": n_grid}\n        \n        # Quick diagnostic\n        valid_cells = np.isfinite(corr_grid).sum()\n        print(f"  - Target \'{target}\': celdas v√°lidas en rejilla = {valid_cells:,}")\n\n    # ------------------------------------------------------------------------\n    # 4) Extraer lag √≥ptimo por bin y generar plots\n    # ------------------------------------------------------------------------\n    print("\\n[3/4] Extrayendo lag √≥ptimo por bin y generando plots...")\n\n    optimal_rows = []\n\n    for target in target_vars:\n        corr_grid = results_grids[target]["corr"]\n        n_grid = results_grids[target]["n"]\n\n        # Lag √≥ptimo por bin\n        optimal_lag = np.full(len(wind_mids), np.nan, dtype=float)\n        optimal_corr = np.full(len(wind_mids), np.nan, dtype=float)\n        optimal_n = np.full(len(wind_mids), 0, dtype=int)\n\n        for i_bin in range(len(wind_mids)):\n            row = corr_grid[i_bin, :]\n            if not np.isfinite(row).any():\n                continue\n\n            if use_abs_for_optimum:\n                j_best = int(np.nanargmax(np.abs(row)))\n            else:\n                j_best = int(np.nanargmax(row))\n\n            optimal_lag[i_bin] = lags[j_best]\n            optimal_corr[i_bin] = row[j_best]\n            optimal_n[i_bin] = int(n_grid[i_bin, j_best])\n\n            optimal_rows.append({\n                "Target": target,\n                "Wind_Bin": wind_bin_labels[i_bin],\n                "Wind_Speed_Mid": float(wind_mids[i_bin]),\n                "Optimal_Lag_Seconds": float(optimal_lag[i_bin]),\n                "Optimal_Correlation": float(optimal_corr[i_bin]),\n                "N_at_Optimal": int(optimal_n[i_bin]),\n            })\n\n        # ---- Plot A: Heatmap viento x lag ----\n        # (y=wind, x=lag)\n        fig, ax = plt.subplots(figsize=(14, 10))\n        sns.heatmap(\n            corr_grid,\n            ax=ax,\n            cmap=\'coolwarm\',\n            center=0,\n            vmin=-1, vmax=1,\n            cbar_kws={\'label\': \'Correlaci√≥n Pearson (promedio BEAMs)\'},\n            xticklabels=lags,\n            yticklabels=[f"{w:.1f}" for w in wind_mids],\n        )\n        ax.set_title(f"Heatmap Corr vs (Viento, Lag) - M√©todo Alineado\\nTarget: {target}", fontsize=14, fontweight=\'bold\')\n        ax.set_xlabel("Lag (s)")\n        ax.set_ylabel("Velocidad viento (m/s) [bin mid]")\n        plt.tight_layout()\n        heatmap_path = wind_lags_folder / f"heatmap_corr_aligned_target_{target.replace(\' \', \'_\')}.png"\n        plt.savefig(heatmap_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n\n        # ---- Plot B: Lag √≥ptimo vs viento ----\n        df_opt = pd.DataFrame([r for r in optimal_rows if r["Target"] == target])\n        df_opt = df_opt.sort_values("Wind_Speed_Mid")\n\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.plot(df_opt["Wind_Speed_Mid"], df_opt["Optimal_Lag_Seconds"], \'o-\', linewidth=2.5, markersize=6, color=\'steelblue\')\n        ax.set_title(f"Lag √ìptimo vs Viento (Alineado) - Target: {target}", fontsize=14, fontweight=\'bold\')\n        ax.set_xlabel("Velocidad viento (m/s) [bin mid]")\n        ax.set_ylabel("Lag √≥ptimo (s)")\n        ax.grid(True, alpha=0.3, linestyle=\'--\')\n\n        # l√≠nea de tendencia simple (opcional)\n        if len(df_opt) >= 5:\n            x = df_opt["Wind_Speed_Mid"].values\n            y = df_opt["Optimal_Lag_Seconds"].values\n            coefs = np.polyfit(x, y, 1)\n            ax.plot(x, np.polyval(coefs, x), \'--\', color=\'red\', alpha=0.7, label=f"Tendencia lineal: {coefs[0]:.2f} s/(m/s)")\n            ax.legend(loc=\'best\')\n\n        plt.tight_layout()\n        line_path = wind_lags_folder / f"optimal_lag_vs_wind_aligned_target_{target.replace(\' \', \'_\')}.png"\n        plt.savefig(line_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n\n        print(f"  ‚úÖ Target \'{target}\': guardados {heatmap_path.name} y {line_path.name}")\n\n    # ------------------------------------------------------------------------\n    # 5) Guardar CSVs\n    # ------------------------------------------------------------------------\n    print("\\n[4/4] Guardando CSVs...")\n\n    # CSV √≥ptimos\n    df_optimal_aligned = pd.DataFrame(optimal_rows)\n    opt_csv = wind_lags_folder / "optimal_lag_per_wind_speed_1ms_bins_ALIGNED.csv"\n    df_optimal_aligned.to_csv(opt_csv, index=False)\n\n    # CSV rejilla (formato largo: target, wind_mid, lag, corr, n)\n    long_rows = []\n    for target in target_vars:\n        corr_grid = results_grids[target]["corr"]\n        n_grid = results_grids[target]["n"]\n        for i_bin, wmid in enumerate(wind_mids):\n            for j, lag_s in enumerate(lags):\n                cval = corr_grid[i_bin, j]\n                nval = int(n_grid[i_bin, j])\n                if not np.isfinite(cval):\n                    continue\n                long_rows.append({\n                    "Target": target,\n                    "Wind_Speed_Mid": float(wmid),\n                    "Wind_Bin": wind_bin_labels[i_bin],\n                    "Lag_Seconds": int(lag_s),\n                    "Correlation_MeanAcrossBeams": float(cval),\n                    "N": nval,\n                })\n\n    df_grid_long = pd.DataFrame(long_rows)\n    grid_csv = wind_lags_folder / "corr_grid_windbin_lag_ALIGNED_long.csv"\n    df_grid_long.to_csv(grid_csv, index=False)\n\n    print(f"‚úÖ CSV √≥ptimos: {opt_csv.name}")\n    print(f"‚úÖ CSV rejilla: {grid_csv.name}")\n\n    print("\\nüí° Nota:")\n    print("- Este m√©todo alinea el binning de viento con el mismo instante del lag.")\n    print("- Si a√∫n ves no-monoton√≠a, suele indicar mezcla de reg√≠menes (DLCs/transitorios) o que el pico real cae fuera del rango de lags.")\n',
        ],
    },
    {
        "title": 'üì° PASO 4.10: Desplazamiento √ìptimo de Se√±al VLOS (Cross-Correlation)',
        "cells": [
            '# ============================================================================\n# PASO 4.10: DESPLAZAMIENTO √ìPTIMO DE SE√ëAL VLOS (CROSS-CORRELATION)\n# ESTRATIFICADO POR BINS DE VELOCIDAD DE VIENTO\n# ============================================================================\n\nimport re\n\n# Crear carpeta para resultados\nsignal_shift_folder = eda_folder / "05_Lag_VLOS_signal"\nsignal_shift_folder.mkdir(exist_ok=True)\n\nprint("="*70)\nprint("DESPLAZAMIENTO √ìPTIMO DE SE√ëAL VLOS - ESTRATIFICADO POR VIENTO")\nprint("="*70)\nprint(f"Carpeta destino: {signal_shift_folder}")\nprint("="*70)\n\n# Par√°metros\nmax_shift_samples = 250   # rango: [-250, +250] samples\nsample_rate_load = 10     # cargar 1 de cada N filas para acelerar\nuse_abs_corr = True       # True: argmax(|corr|), False: argmax(corr)\nwind_bin_width = 2.0      # Ancho del bin de viento en m/s\nmin_samples_per_bin = 500 # M√≠nimo de muestras por bin para calcular\n\n# Archivo\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    # ------------------------------------------------------------------------\n    # 1) Identificar columnas VLOS sin lag y targets\n    # ------------------------------------------------------------------------\n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n\n    # VLOS sin lag\n    vlos_base_cols = [c for c in all_columns if \'LAC_VLOS\' in c and \'lag\' not in c]\n    \n    # Targets\n    target_vars = [\n        \'Blade root 1 My\',\n        \'Blade root 2 My\',\n        \'Blade root 1 My 1P\',\n        \'Blade root 2 My 1P\'\n    ]\n    \n    # Verificar que existen\n    missing = [t for t in target_vars if t not in all_columns]\n    if missing:\n        print(f"\\n‚ö†Ô∏è Targets no encontrados: {missing}")\n        target_vars = [t for t in target_vars if t in all_columns]\n    \n    print(f"\\nüìå Columnas VLOS sin lag detectadas: {len(vlos_base_cols)}")\n    print(f"üìå Targets a evaluar: {len(target_vars)}")\n    \n    # Extraer BEAMs\n    beams = []\n    vlos_by_beam = {}\n    for col in vlos_base_cols:\n        m = re.search(r\'BEAM(\\d+)\', col)\n        if m:\n            beam = int(m.group(1))\n            beams.append(beam)\n            vlos_by_beam[beam] = col\n    beams = sorted(set(beams))\n    \n    print(f"üìå BEAMs detectados: {beams}")\n    \n    # ------------------------------------------------------------------------\n    # 2) Cargar datos (VLOS sin lag + targets + calcular wind_mean)\n    # ------------------------------------------------------------------------\n    print(f"\\n[1/5] Cargando datos (sample_rate={sample_rate_load})...")\n    \n    usecols = list(vlos_by_beam.values()) + target_vars\n    \n    df = pd.read_csv(\n        complete_dataset_path,\n        usecols=usecols,\n        skiprows=lambda i: i > 0 and i % sample_rate_load != 0\n    )\n    \n    print(f"‚úÖ Cargado: {df.shape[0]:,} filas, {df.shape[1]} columnas")\n    \n    # Calcular wind_mean (promedio de VLOS sin lag)\n    print("‚öôÔ∏è  Calculando wind_mean...")\n    df[\'wind_mean\'] = df[list(vlos_by_beam.values())].mean(axis=1)\n    print(f"‚úÖ wind_mean: {df[\'wind_mean\'].min():.2f} - {df[\'wind_mean\'].max():.2f} m/s")\n    \n    # Detectar sampling time (aproximado)\n    # Asumimos que Time existe o usamos √≠ndice\n    if \'Time\' in all_columns:\n        df_time_sample = pd.read_csv(complete_dataset_path, usecols=[\'Time\'], nrows=1000, \n                                     skiprows=lambda i: i > 0 and i % sample_rate_load != 0)\n        if len(df_time_sample) >= 2:\n            dt = float(df_time_sample[\'Time\'].diff().median())\n        else:\n            dt = 0.1  # default\n    else:\n        dt = 0.1  # default\n    \n    print(f"üìå Sampling time estimado: {dt:.3f} s")\n    print(f"üìå Rango de shifts: ¬±{max_shift_samples} samples ‚Üí ¬±{max_shift_samples*dt:.1f} s")\n    \n    # ------------------------------------------------------------------------\n    # 3) Crear bins de velocidad de viento\n    # ------------------------------------------------------------------------\n    print(f"\\n[2/5] Creando bins de velocidad de viento ({wind_bin_width} m/s)...")\n    \n    wind_edges = np.arange(0, 30 + wind_bin_width, wind_bin_width)\n    df[\'wind_bin\'] = pd.cut(df[\'wind_mean\'], bins=wind_edges, include_lowest=True)\n    \n    bin_counts = df[\'wind_bin\'].value_counts().sort_index()\n    valid_bins = [b for b, count in bin_counts.items() if count >= min_samples_per_bin]\n    \n    print(f"‚úÖ Bins creados: {len(bin_counts)}")\n    print(f"‚úÖ Bins v√°lidos (‚â•{min_samples_per_bin} muestras): {len(valid_bins)}")\n    \n    # ------------------------------------------------------------------------\n    # 4) Calcular correlaci√≥n vs shift POR BIN de viento\n    # ------------------------------------------------------------------------\n    print(f"\\n[3/5] Calculando correlaci√≥n vs shift por bin de viento...")\n    \n    shift_range = np.arange(-max_shift_samples, max_shift_samples + 1)\n    \n    results = []\n    \n    for wind_bin in valid_bins:\n        df_bin = df[df[\'wind_bin\'] == wind_bin]\n        wind_mid = (wind_bin.left + wind_bin.right) / 2\n        \n        print(f"\\n  üå¨Ô∏è  Bin: {wind_bin} (mid={wind_mid:.1f} m/s, N={len(df_bin):,})...")\n        \n        for beam in beams:\n            vlos_col = vlos_by_beam[beam]\n            vlos_signal = df_bin[vlos_col].to_numpy()\n            \n            for target in target_vars:\n                target_signal = df_bin[target].to_numpy()\n                \n                corr_vs_shift = np.full(len(shift_range), np.nan, dtype=float)\n                \n                for i, shift in enumerate(shift_range):\n                    if shift >= 0:\n                        if shift == 0:\n                            v = vlos_signal\n                            tg = target_signal\n                        else:\n                            v = vlos_signal[:-shift]\n                            tg = target_signal[shift:]\n                    else:\n                        v = vlos_signal[-shift:]\n                        tg = target_signal[:shift]\n                    \n                    if len(v) >= 100 and len(tg) >= 100 and len(v) == len(tg):\n                        valid_mask = np.isfinite(v) & np.isfinite(tg)\n                        if valid_mask.sum() >= 100:\n                            corr_vs_shift[i] = np.corrcoef(v[valid_mask], tg[valid_mask])[0, 1]\n                \n                # Encontrar shift √≥ptimo\n                if use_abs_corr:\n                    idx_opt = int(np.nanargmax(np.abs(corr_vs_shift)))\n                else:\n                    idx_opt = int(np.nanargmax(corr_vs_shift))\n                \n                shift_opt = int(shift_range[idx_opt])\n                corr_opt = float(corr_vs_shift[idx_opt])\n                delay_opt = shift_opt * dt\n                \n                results.append({\n                    \'Wind_Bin\': str(wind_bin),\n                    \'Wind_Speed_Mid\': wind_mid,\n                    \'BEAM\': beam,\n                    \'Target\': target,\n                    \'Optimal_Shift_Samples\': shift_opt,\n                    \'Optimal_Delay_Seconds\': delay_opt,\n                    \'Max_Correlation\': corr_opt,\n                    \'N_Samples\': len(df_bin),\n                })\n    \n    print(f"\\n‚úÖ An√°lisis completado para {len(valid_bins)} bins, {len(beams)} BEAMs, {len(target_vars)} targets")\n    \n    # ------------------------------------------------------------------------\n    # 5) Guardar resultados en CSV\n    # ------------------------------------------------------------------------\n    print(f"\\n[4/5] Guardando resultados...")\n    \n    df_results = pd.DataFrame(results)\n    csv_path = signal_shift_folder / "optimal_shift_per_wind_bin_beam_target.csv"\n    df_results.to_csv(csv_path, index=False)\n    \n    print(f"‚úÖ CSV guardado: {csv_path.name}")\n    \n    # ------------------------------------------------------------------------\n    # 6) Generar plots: Delay √ìptimo vs Velocidad de Viento\n    # ------------------------------------------------------------------------\n    print(f"\\n[5/5] Generando plots de delay vs velocidad de viento...")\n    \n    # Plot por cada BEAM y target\n    for beam in beams:\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        fig.suptitle(f\'Delay √ìptimo vs Velocidad de Viento - BEAM {beam}\', fontsize=16, fontweight=\'bold\')\n        \n        for idx, target in enumerate(target_vars):\n            ax = axes.flat[idx]\n            \n            df_plot = df_results[\n                (df_results[\'BEAM\'] == beam) & \n                (df_results[\'Target\'] == target)\n            ].sort_values(\'Wind_Speed_Mid\')\n            \n            if len(df_plot) == 0:\n                continue\n            \n            # Graficar delay √≥ptimo vs viento\n            ax.plot(df_plot[\'Wind_Speed_Mid\'], df_plot[\'Optimal_Delay_Seconds\'], \n                   \'o-\', linewidth=2.5, markersize=7, color=\'steelblue\', label=\'Delay √≥ptimo\')\n            \n            # Superponer curva te√≥rica 100/U\n            wind_theory = np.linspace(df_plot[\'Wind_Speed_Mid\'].min(), \n                                     df_plot[\'Wind_Speed_Mid\'].max(), 100)\n            delay_theory = 100 / wind_theory  # Asumiendo distancia = 100m\n            ax.plot(wind_theory, delay_theory, \'--\', linewidth=2, color=\'red\', \n                   alpha=0.7, label=\'Te√≥rico: 100m/U\')\n            \n            ax.set_xlabel(\'Velocidad del Viento (m/s)\', fontsize=11, fontweight=\'bold\')\n            ax.set_ylabel(\'Delay √ìptimo (s)\', fontsize=11, fontweight=\'bold\')\n            ax.set_title(f\'Target: {target}\', fontsize=12, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n            ax.legend(loc=\'best\', fontsize=10)\n            ax.invert_yaxis()  # Invertir para mostrar que delay disminuye con viento\n        \n        plt.tight_layout()\n        \n        plot_path = signal_shift_folder / f"delay_vs_wind_BEAM{beam}.png"\n        plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"  ‚úÖ BEAM {beam}: delay_vs_wind_BEAM{beam}.png")\n    \n    # Plot resumen: todos los BEAMs superpuestos (solo primer target)\n    target_main = target_vars[0]\n    \n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    beam_colors_map = {b: plt.cm.tab10(i) for i, b in enumerate(beams)}\n    \n    for beam in beams:\n        df_plot = df_results[\n            (df_results[\'BEAM\'] == beam) & \n            (df_results[\'Target\'] == target_main)\n        ].sort_values(\'Wind_Speed_Mid\')\n        \n        if len(df_plot) == 0:\n            continue\n        \n        ax.plot(df_plot[\'Wind_Speed_Mid\'], df_plot[\'Optimal_Delay_Seconds\'], \n               \'o-\', linewidth=2, markersize=5, color=beam_colors_map[beam], \n               label=f\'BEAM {beam}\', alpha=0.8)\n    \n    # Curva te√≥rica\n    wind_theory = np.linspace(5, 25, 100)\n    delay_theory = 100 / wind_theory\n    ax.plot(wind_theory, delay_theory, \'--\', linewidth=2.5, color=\'black\', \n           alpha=0.7, label=\'Te√≥rico: 100m/U\')\n    \n    ax.set_xlabel(\'Velocidad del Viento (m/s)\', fontsize=12, fontweight=\'bold\')\n    ax.set_ylabel(\'Delay √ìptimo (s)\', fontsize=12, fontweight=\'bold\')\n    ax.set_title(f\'Delay √ìptimo vs Velocidad - Todos los BEAMs\\nTarget: {target_main}\', \n                fontsize=14, fontweight=\'bold\')\n    ax.grid(True, alpha=0.3, linestyle=\'--\')\n    ax.legend(loc=\'best\', fontsize=10, ncol=2)\n    ax.invert_yaxis()\n    \n    plt.tight_layout()\n    \n    summary_path = signal_shift_folder / "delay_vs_wind_all_beams_summary.png"\n    plt.savefig(summary_path, dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    \n    print(f"  ‚úÖ Resumen: delay_vs_wind_all_beams_summary.png")\n    \n    # ------------------------------------------------------------------------\n    # RESUMEN FINAL\n    # ------------------------------------------------------------------------\n    print(f"\\n{\'=\'*70}")\n    print("RESUMEN - DESPLAZAMIENTO √ìPTIMO ESTRATIFICADO POR VIENTO")\n    print("="*70)\n    print(f"Bins de viento analizados: {len(valid_bins)}")\n    print(f"BEAMs analizados:          {len(beams)}")\n    print(f"Targets analizados:        {len(target_vars)}")\n    print(f"Rango de b√∫squeda:         ¬±{max_shift_samples} samples (¬±{max_shift_samples*dt:.1f} s)")\n    \n    print(f"\\nüéØ AN√ÅLISIS DE CONSISTENCIA F√çSICA:")\n    print("   Se compara delay √≥ptimo vs curva te√≥rica 100m/U")\n    print("   - Si los puntos siguen la curva ‚Üí advecci√≥n simple validada")\n    print("   - Desviaciones indican efectos de din√°mica/filtrado/control")\n    \n    # Calcular error medio respecto a teor√≠a para cada BEAM\n    print(f"\\nüìä Error vs teor√≠a (100m/U) por BEAM:")\n    for beam in beams:\n        df_beam = df_results[df_results[\'BEAM\'] == beam]\n        if len(df_beam) == 0:\n            continue\n        \n        delay_theory_beam = 100 / df_beam[\'Wind_Speed_Mid\']\n        error_mean = (df_beam[\'Optimal_Delay_Seconds\'] - delay_theory_beam).mean()\n        error_std = (df_beam[\'Optimal_Delay_Seconds\'] - delay_theory_beam).std()\n        \n        print(f"  BEAM {beam}: error medio = {error_mean:+.2f} ¬± {error_std:.2f} s")\n    \n    print(f"\\nüí° INTERPRETACI√ìN:")\n    print(f"   - Delay √≥ptimo debe DECRECER con velocidad (curva ~100/U)")\n    print(f"   - Estratificaci√≥n elimina mezcla de reg√≠menes ‚Üí picos m√°s claros")\n    print(f"   - Variaci√≥n entre BEAMs: geometr√≠a del cono LIDAR")\n    \n    print(f"\\nüìÅ Archivos generados:")\n    print(f"  - Plots por BEAM: delay_vs_wind_BEAMX.png ({len(beams)} archivos)")\n    print(f"  - Plot resumen: delay_vs_wind_all_beams_summary.png")\n    print(f"  - CSV detallado: optimal_shift_per_wind_bin_beam_target.csv")\n    print(f"\\nüìç Ubicaci√≥n: {signal_shift_folder}")\n    print("="*70)\n    print("\\n‚úÖ An√°lisis completado")',
            '# =============================================================================\n# PASO 4.10.b: DELAY RESIDUAL TRAS ADELANTAR VLOS A 100/U\n# =============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\n\nprint("="*70)\nprint("DELAY RESIDUAL TRAS SHIFT TE√ìRICO 100/U")\nprint("="*70)\n\nsignal_shift_folder = eda_folder / "05_Lag_VLOS_signal"\nsignal_shift_folder.mkdir(exist_ok=True)\n\n# Par√°metros espec√≠ficos de este an√°lisis\ndistance_upwind_m = 100.0           # distancia advecci√≥n (m)\nsample_rate_residual = 10            # 1 de cada N filas\nwind_bin_width = 2.0                 # bins iguales a los del paso anterior\nmin_samples_per_bin = 500\nmin_overlap_samples = 150\nresidual_shift_span = 80             # rango adicional alrededor del delay te√≥rico (samples)\ntarget_vars = [\'Blade root 1 My\', \'Blade root 2 My\']\n\nif not complete_dataset_path.exists():\n    print(f"‚ùå No se encuentra el archivo {complete_dataset_path}")\nelse:\n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n\n    vlos_base_cols = [c for c in all_columns if \'LAC_VLOS\' in c and \'lag\' not in c]\n    if not vlos_base_cols:\n        raise ValueError("No se detectaron columnas LAC_VLOS sin lag para este an√°lisis")\n\n    missing_targets = [t for t in target_vars if t not in all_columns]\n    if missing_targets:\n        print(f"‚ö†Ô∏è Targets ausentes: {missing_targets}")\n        target_vars = [t for t in target_vars if t in all_columns]\n    if not target_vars:\n        raise ValueError("No quedan targets v√°lidos tras la verificaci√≥n de columnas")\n\n    # Mapeo BEAM -> columna\n    vlos_by_beam = {}\n    for col in vlos_base_cols:\n        match = re.search(r\'BEAM(\\d+)\', col)\n        if match:\n            vlos_by_beam[int(match.group(1))] = col\n    beams = sorted(vlos_by_beam.keys())\n    if not beams:\n        raise ValueError("No se pudieron identificar BEAMs en las columnas LAC_VLOS")\n\n    usecols = list(vlos_by_beam.values()) + target_vars\n    df = pd.read_csv(\n        complete_dataset_path,\n        usecols=usecols,\n        skiprows=lambda i: i > 0 and i % sample_rate_residual != 0\n    )\n\n    df[\'wind_mean\'] = df[list(vlos_by_beam.values())].mean(axis=1)\n\n    # Estimar dt a partir de la columna Time si existe\n    if \'Time\' in all_columns:\n        df_time = pd.read_csv(\n            complete_dataset_path,\n            usecols=[\'Time\'],\n            nrows=2000,\n            skiprows=lambda i: i > 0 and i % sample_rate_residual != 0\n        )\n        time_diff = df_time[\'Time\'].diff().dropna().abs()\n        dt = float(time_diff.median()) if not time_diff.empty else 0.1\n    else:\n        dt = 0.1\n\n    print(f"üìå dt estimado: {dt:.3f} s | Rango residual ¬±{residual_shift_span} samples (~¬±{residual_shift_span*dt:.1f} s)")\n\n    wind_edges = np.arange(0, 30 + wind_bin_width, wind_bin_width)\n    df[\'wind_bin\'] = pd.cut(df[\'wind_mean\'], bins=wind_edges, include_lowest=True)\n    bin_counts = df[\'wind_bin\'].value_counts().sort_index()\n    valid_bins = [b for b, n in bin_counts.items() if n >= min_samples_per_bin]\n\n    print(f"üìä Bins v√°lidos: {len(valid_bins)}/{len(bin_counts)}")\n\n    shift_range = np.arange(-residual_shift_span, residual_shift_span + 1)\n    results = []\n\n    def _align_signals(sig_vlos: np.ndarray, sig_target: np.ndarray, shift_samples: int):\n        """Devuelve las partes solapadas tras aplicar shift (None si no queda solape √∫til)."""\n        if shift_samples == 0:\n            v = sig_vlos\n            t = sig_target\n        elif shift_samples > 0:\n            if shift_samples >= len(sig_vlos) or shift_samples >= len(sig_target):\n                return None, None\n            v = sig_vlos[:-shift_samples]\n            t = sig_target[shift_samples:]\n        else:\n            shift_abs = abs(shift_samples)\n            if shift_abs >= len(sig_vlos) or shift_abs >= len(sig_target):\n                return None, None\n            v = sig_vlos[shift_abs:]\n            t = sig_target[:-shift_abs]\n        if len(v) < min_overlap_samples:\n            return None, None\n        valid = np.isfinite(v) & np.isfinite(t)\n        if valid.sum() < min_overlap_samples:\n            return None, None\n        return v[valid], t[valid]\n\n    for wind_bin in valid_bins:\n        df_bin = df[df[\'wind_bin\'] == wind_bin]\n        if df_bin.empty:\n            continue\n        wind_mid = float((wind_bin.left + wind_bin.right) / 2)\n        if not np.isfinite(wind_mid) or wind_mid <= 0.1:\n            continue\n        theoretical_delay = distance_upwind_m / wind_mid\n        theoretical_shift_samples = int(round(theoretical_delay / dt))\n\n        for beam in beams:\n            vlos_signal = df_bin[vlos_by_beam[beam]].to_numpy(dtype=float)\n            for target in target_vars:\n                target_signal = df_bin[target].to_numpy(dtype=float)\n                corr_vs_residual = np.full(len(shift_range), np.nan, dtype=float)\n\n                for idx, residual_shift in enumerate(shift_range):\n                    total_shift = theoretical_shift_samples + residual_shift\n                    aligned_v, aligned_t = _align_signals(vlos_signal, target_signal, total_shift)\n                    if aligned_v is None:\n                        continue\n                    corr_vs_residual[idx] = np.corrcoef(aligned_v, aligned_t)[0, 1]\n\n                if not np.isfinite(corr_vs_residual).any():\n                    continue\n                idx_opt = int(np.nanargmax(np.abs(corr_vs_residual)))\n                residual_opt = int(shift_range[idx_opt])\n                total_shift_opt = theoretical_shift_samples + residual_opt\n\n                results.append({\n                    \'Wind_Bin\': str(wind_bin),\n                    \'Wind_Speed_Mid\': wind_mid,\n                    \'BEAM\': beam,\n                    \'Target\': target,\n                    \'N_Samples_Bin\': len(df_bin),\n                    \'dt_seconds\': dt,\n                    \'Theoretical_Delay_s\': theoretical_delay,\n                    \'Theoretical_Shift_samples\': theoretical_shift_samples,\n                    \'Residual_Shift_samples\': residual_opt,\n                    \'Residual_Delay_s\': residual_opt * dt,\n                    \'Total_Shift_samples\': total_shift_opt,\n                    \'Total_Delay_s\': total_shift_opt * dt,\n                    \'Max_Correlation\': float(corr_vs_residual[idx_opt])\n                })\n\n    if not results:\n        print("‚ö†Ô∏è No se obtuvieron resultados v√°lidos (revisa bins o par√°metros)")\n    else:\n        df_results = pd.DataFrame(results)\n        csv_path = signal_shift_folder / "residual_delay_after_100m_over_U.csv"\n        df_results.to_csv(csv_path, index=False)\n        print(f"‚úÖ Resultados guardados en {csv_path.name} ({len(df_results)} filas)")\n\n        beam_colors = {beam: plt.cm.tab10(i % 10) for i, beam in enumerate(beams)}\n\n        for target in target_vars:\n            df_target = df_results[df_results[\'Target\'] == target]\n            if df_target.empty:\n                continue\n\n            fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharex=True)\n            fig.suptitle(f"Delay tras adelantar 100/U - Target: {target}", fontsize=15, fontweight=\'bold\')\n\n            for beam in beams:\n                df_plot = df_target[df_target[\'BEAM\'] == beam].sort_values(\'Wind_Speed_Mid\')\n                if df_plot.empty:\n                    continue\n                axes[0].plot(\n                    df_plot[\'Wind_Speed_Mid\'],\n                    df_plot[\'Residual_Delay_s\'],\n                    \'o-\',\n                    color=beam_colors[beam],\n                    label=f"BEAM {beam}",\n                    linewidth=2,\n                    markersize=5\n                )\n                axes[1].plot(\n                    df_plot[\'Wind_Speed_Mid\'],\n                    df_plot[\'Total_Delay_s\'],\n                    \'o-\',\n                    color=beam_colors[beam],\n                    label=f"BEAM {beam}",\n                    linewidth=2,\n                    markersize=5\n                )\n\n            axes[0].axhline(0, color=\'black\', linewidth=1, linestyle=\'--\', alpha=0.7)\n            axes[0].set_ylabel(\'Delay residual (s)\')\n            axes[0].set_xlabel(\'Velocidad viento (m/s)\')\n            axes[0].grid(True, linestyle=\'--\', alpha=0.3)\n            axes[0].set_title(\'Delay adicional necesario\')\n\n            axes[1].set_ylabel(\'Delay total (s)\')\n            axes[1].set_xlabel(\'Velocidad viento (m/s)\')\n            axes[1].grid(True, linestyle=\'--\', alpha=0.3)\n\n            wind_min = max(0.5, df_target[\'Wind_Speed_Mid\'].min())\n            wind_max = df_target[\'Wind_Speed_Mid\'].max()\n            if wind_max > wind_min:\n                theory_wind = np.linspace(wind_min, wind_max, 200)\n                theory_delay = distance_upwind_m / theory_wind\n                axes[1].plot(theory_wind, theory_delay, \'--\', color=\'black\', linewidth=2, alpha=0.8, label=\'Te√≥rico 100/U\')\n            axes[1].set_title(\'Delay total vs te√≥rico 100/U\')\n\n            axes[0].legend(loc=\'best\', fontsize=9)\n            axes[1].legend(loc=\'best\', fontsize=9)\n            plt.tight_layout()\n\n            plot_path = signal_shift_folder / f"residual_delay_after_100overU_target_{target.replace(\' \', \'_\')}.png"\n            plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n            plt.close()\n            print(f"  üìà Guardado: {plot_path.name}")\n\n        print("\\n‚úÖ Gr√°ficos residuales generados en 05_Lag_VLOS_signal")',
        ],
    },
    {
        "title": 'üå™Ô∏è PASO 4.11: Evoluci√≥n Temporal de R√°fagas en los Lags',
        "cells": [
            '# =============================================================================\n# PASO 4.11: EVOLUCI√ìN TEMPORAL DE R√ÅFAGAS EN LOS LAGS\n# =============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom pathlib import Path\n\n# Crear carpeta para resultados\nwind_evolution_folder = eda_folder / "06_Wind_Evolution"\nwind_evolution_folder.mkdir(exist_ok=True)\n\nprint("="*70)\nprint("‚ö° EVOLUCI√ìN TEMPORAL DE R√ÅFAGAS EN LOS LAGS")\nprint("="*70)\nprint(f"Carpeta destino: {wind_evolution_folder}")\nprint("="*70)\n\n# Par√°metros\nn_samples = 2000  # N√∫mero de filas consecutivas a visualizar\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    # ------------------------------------------------------------------------\n    # 1) Identificar columnas VLOS con lag\n    # ------------------------------------------------------------------------\n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n    \n    # Extraer columnas con lag\n    vlos_lag_cols = [c for c in all_columns if \'LAC_VLOS\' in c and \'lag\' in c]\n    \n    if not vlos_lag_cols:\n        print("\\n‚ùå ERROR: No se encontraron columnas VLOS con lag")\n    else:\n        print(f"\\nüìå Columnas VLOS con lag detectadas: {len(vlos_lag_cols)}")\n        \n        # Extraer lag values y organizar\n        lag_info = []\n        for col in vlos_lag_cols:\n            m_lag = re.search(r\'lag(\\d+)s\', col)\n            m_beam = re.search(r\'BEAM(\\d+)\', col)\n            if m_lag and m_beam:\n                lag_s = int(m_lag.group(1))\n                beam = int(m_beam.group(1))\n                lag_info.append({\n                    \'column\': col,\n                    \'lag_s\': lag_s,\n                    \'beam\': beam\n                })\n        \n        df_lag_info = pd.DataFrame(lag_info)\n        lags_sorted = sorted(df_lag_info[\'lag_s\'].unique())\n        beams = sorted(df_lag_info[\'beam\'].unique())\n        \n        print(f"üìå Lags detectados: {lags_sorted[0]}s - {lags_sorted[-1]}s ({len(lags_sorted)} lags)")\n        print(f"üìå BEAMs detectados: {beams}")\n        \n        # ------------------------------------------------------------------------\n        # 2) Cargar √∫ltimas n_samples filas\n        # ------------------------------------------------------------------------\n        print(f"\\n[1/3] Cargando √∫ltimas {n_samples} filas...")\n        \n        # Determinar n√∫mero total de filas\n        total_rows = sum(1 for _ in open(complete_dataset_path)) - 1  # -1 para header\n        start_row = max(1, total_rows - n_samples)\n        \n        df_sample = pd.read_csv(\n            complete_dataset_path,\n            usecols=vlos_lag_cols,\n            skiprows=range(1, start_row)\n        )\n        \n        print(f"‚úÖ Cargado: {df_sample.shape[0]:,} filas, {df_sample.shape[1]} columnas")\n        \n        # ------------------------------------------------------------------------\n        # 3) Crear heatmaps por BEAM\n        # ------------------------------------------------------------------------\n        print(f"\\n[2/3] Generando heatmaps de evoluci√≥n temporal...")\n        \n        for beam in beams:\n            # Filtrar columnas de este BEAM\n            beam_cols = df_lag_info[df_lag_info[\'beam\'] == beam].sort_values(\'lag_s\')\n            \n            if beam_cols.empty:\n                continue\n            \n            # Crear matriz: filas = lags (ordenados), columnas = tiempo\n            data_matrix = []\n            lag_labels = []\n            \n            for _, row in beam_cols.iterrows():\n                col_name = row[\'column\']\n                lag_s = row[\'lag_s\']\n                \n                if col_name in df_sample.columns:\n                    data_matrix.append(df_sample[col_name].values)\n                    lag_labels.append(f"{lag_s}s")\n            \n            if not data_matrix:\n                continue\n            \n            data_matrix = np.array(data_matrix)  # shape: (n_lags, n_samples)\n            \n            # Crear heatmap\n            fig, ax = plt.subplots(figsize=(18, 10))\n            \n            im = ax.imshow(\n                data_matrix,\n                aspect=\'auto\',\n                cmap=\'RdYlBu_r\',\n                interpolation=\'nearest\',\n                origin=\'lower\'\n            )\n            \n            # Configurar ejes\n            ax.set_xlabel(\'Tiempo (muestras) ‚Üí\', fontsize=13, fontweight=\'bold\')\n            ax.set_ylabel(\'Lag (segundos) ‚Üë [memoria antigua ‚Üê reciente]\', fontsize=13, fontweight=\'bold\')\n            ax.set_title(f\'Evoluci√≥n Temporal de R√°fagas - BEAM {beam}\\n(R√°fagas "envejecen" diagonal ‚ÜóÔ∏è: abajo-izq ‚Üí arriba-der)\', \n                        fontsize=15, fontweight=\'bold\')\n            \n            # Etiquetas eje Y (lags)\n            ax.set_yticks(range(len(lag_labels)))\n            ax.set_yticklabels(lag_labels)\n            \n            # A√±adir l√≠nea diagonal de referencia para visualizar mejor\n            n_points_diag = min(len(df_sample), len(lag_labels))\n            if n_points_diag > 10:\n                # Diagonal que sube de izquierda a derecha\n                x_diag = np.linspace(0, len(df_sample)-1, n_points_diag)\n                y_diag = np.linspace(0, len(lag_labels)-1, n_points_diag)\n                ax.plot(x_diag, y_diag, \'w--\', linewidth=2, alpha=0.6, label=\'Trayectoria te√≥rica ‚ÜóÔ∏è\')\n            \n            # Etiquetas eje X (tiempo) - mostrar cada N muestras\n            x_ticks_step = max(1, len(df_sample) // 10)\n            x_ticks = range(0, len(df_sample), x_ticks_step)\n            ax.set_xticks(x_ticks)\n            ax.set_xticklabels([str(i) for i in x_ticks])\n            \n            # Colorbar\n            cbar = plt.colorbar(im, ax=ax, label=\'Velocidad Viento (m/s)\')\n            cbar.ax.tick_params(labelsize=11)\n            \n            # Grid sutil para visualizar mejor\n            ax.grid(True, alpha=0.2, linestyle=\'--\', linewidth=0.5)\n            ax.legend(loc=\'upper left\', fontsize=10, framealpha=0.8)\n            \n            plt.tight_layout()\n            \n            # Guardar\n            plot_path = wind_evolution_folder / f"wind_evolution_heatmap_BEAM{beam}.png"\n            plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n            plt.close()\n            \n            print(f"  ‚úÖ BEAM {beam}: {plot_path.name}")\n            \n            # Guardar CSV de datos utilizados\n            df_beam_data = df_sample[[col for col in beam_cols[\'column\'] if col in df_sample.columns]]\n            csv_path = wind_evolution_folder / f"wind_evolution_data_BEAM{beam}.csv"\n            df_beam_data.to_csv(csv_path, index=False)\n        \n        # ------------------------------------------------------------------------\n        # 4) Crear heatmap promedio (todos los BEAMs)\n        # ------------------------------------------------------------------------\n        print(f"\\n[3/3] Generando heatmap promedio (todos los BEAMs)...")\n        \n        # Para cada lag, promediar todos los BEAMs\n        avg_data_matrix = []\n        avg_lag_labels = []\n        \n        for lag_s in lags_sorted:\n            lag_cols = df_lag_info[df_lag_info[\'lag_s\'] == lag_s][\'column\'].tolist()\n            valid_cols = [c for c in lag_cols if c in df_sample.columns]\n            \n            if valid_cols:\n                avg_signal = df_sample[valid_cols].mean(axis=1).values\n                avg_data_matrix.append(avg_signal)\n                avg_lag_labels.append(f"{lag_s}s")\n        \n        if avg_data_matrix:\n            avg_data_matrix = np.array(avg_data_matrix)\n            \n            fig, ax = plt.subplots(figsize=(18, 10))\n            \n            im = ax.imshow(\n                avg_data_matrix,\n                aspect=\'auto\',\n                cmap=\'RdYlBu_r\',\n                interpolation=\'nearest\',\n                origin=\'lower\'\n            )\n            \n            ax.set_xlabel(\'Tiempo (muestras) ‚Üí\', fontsize=13, fontweight=\'bold\')\n            ax.set_ylabel(\'Lag (segundos) ‚Üë [memoria antigua ‚Üê reciente]\', fontsize=13, fontweight=\'bold\')\n            ax.set_title(\'Evoluci√≥n Temporal de R√°fagas - Promedio Todos los BEAMs\\n(R√°fagas "envejecen" diagonal ‚ÜóÔ∏è: abajo-izq ‚Üí arriba-der)\', \n                        fontsize=15, fontweight=\'bold\')\n            \n            ax.set_yticks(range(len(avg_lag_labels)))\n            ax.set_yticklabels(avg_lag_labels)\n            \n            # A√±adir l√≠nea diagonal de referencia\n            n_points_diag = min(len(df_sample), len(avg_lag_labels))\n            if n_points_diag > 10:\n                x_diag = np.linspace(0, len(df_sample)-1, n_points_diag)\n                y_diag = np.linspace(0, len(avg_lag_labels)-1, n_points_diag)\n                ax.plot(x_diag, y_diag, \'w--\', linewidth=2, alpha=0.6, label=\'Trayectoria te√≥rica ‚ÜóÔ∏è\')\n            \n            x_ticks_step = max(1, len(df_sample) // 10)\n            x_ticks = range(0, len(df_sample), x_ticks_step)\n            ax.set_xticks(x_ticks)\n            ax.set_xticklabels([str(i) for i in x_ticks])\n            \n            cbar = plt.colorbar(im, ax=ax, label=\'Velocidad Viento (m/s)\')\n            cbar.ax.tick_params(labelsize=11)\n            \n            ax.grid(True, alpha=0.2, linestyle=\'--\', linewidth=0.5)\n            ax.legend(loc=\'upper left\', fontsize=10, framealpha=0.8)\n            \n            plt.tight_layout()\n            \n            plot_path = wind_evolution_folder / "wind_evolution_heatmap_ALL_BEAMS_avg.png"\n            plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n            plt.close()\n            \n            print(f"  ‚úÖ Promedio: {plot_path.name}")\n            \n            # CSV promedio\n            df_avg = pd.DataFrame(\n                avg_data_matrix.T,\n                columns=avg_lag_labels\n            )\n            csv_path = wind_evolution_folder / "wind_evolution_data_ALL_BEAMS_avg.csv"\n            df_avg.to_csv(csv_path, index=False)\n        \n        # ------------------------------------------------------------------------\n        # RESUMEN\n        # ------------------------------------------------------------------------\n        print(f"\\n{\'=\'*70}")\n        print("RESUMEN - EVOLUCI√ìN TEMPORAL DE R√ÅFAGAS")\n        print("="*70)\n        print(f"Muestras analizadas:  {len(df_sample):,}")\n        print(f"Lags visualizados:    {len(lags_sorted)} ({lags_sorted[0]}s - {lags_sorted[-1]}s)")\n        print(f"BEAMs analizados:     {len(beams)}")\n        \n        print(f"\\nüéØ INTERPRETACI√ìN:")\n        print("   - Patrones DIAGONALES ‚ÜóÔ∏è (abajo-izq ‚Üí arriba-der) indican advecci√≥n temporal")\n        print("   - Una r√°faga aparece en lag=5s (abajo) y \'envejece\' subiendo a lag=25s (arriba)")\n        print("   - F√≠sica: lag5s=\'hace 5s\', lag10s=\'hace 10s\' ‚Üí memoria reciente a antigua")\n        print("   - Diagonales claras = coherencia temporal del LIDAR")\n        print("   - Patrones verticales = transitorios/cambios abruptos")\n        \n        print(f"\\nüìä VISUALIZACI√ìN:")\n        print("   - Rojo = viento alto (r√°faga)")\n        print("   - Azul = viento bajo")\n        print("   - Inclinaci√≥n diagonal ~ dt/dlag")\n        \n        print(f"\\nüìÅ Archivos generados:")\n        print(f"  - Heatmaps por BEAM: wind_evolution_heatmap_BEAMX.png ({len(beams)} archivos)")\n        print(f"  - Heatmap promedio: wind_evolution_heatmap_ALL_BEAMS_avg.png")\n        print(f"  - CSVs con datos: wind_evolution_data_*.csv")\n        print(f"\\nüìç Ubicaci√≥n: {wind_evolution_folder}")\n        print("="*70)\n        print("\\n‚úÖ An√°lisis completado")',
        ],
    },
    {
        "title": 'üå¨Ô∏è PASO 4.11.b: Evoluci√≥n Espacial de R√°fagas (Distancia al Rotor)',
        "cells": [
            '# =============================================================================\n# PASO 4.11.b: EVOLUCI√ìN ESPACIAL DE R√ÅFAGAS (DISTANCIA AL ROTOR)\n# =============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom pathlib import Path\n\nprint("\\n" + "="*70)\nprint("üå¨Ô∏è EVOLUCI√ìN ESPACIAL DE R√ÅFAGAS (100m ‚Üí ROTOR)")\nprint("="*70)\nprint(f"Carpeta destino: {wind_evolution_folder}")\nprint("="*70)\n\n# Par√°metros\nn_samples_spatial = 2000  # √öltimas N filas\ndistance_lidar_m = 100.0  # Distancia del LIDAR al rotor (m)\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    # ------------------------------------------------------------------------\n    # 1) Cargar columnas VLOS con y sin lag + calcular velocidad promedio\n    # ------------------------------------------------------------------------\n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n    \n    # Columnas VLOS con lag\n    vlos_lag_cols = [c for c in all_columns if \'LAC_VLOS\' in c and \'lag\' in c]\n    # Columnas VLOS sin lag (medici√≥n en 100m)\n    vlos_base_cols = [c for c in all_columns if \'LAC_VLOS\' in c and \'lag\' not in c]\n    \n    if not vlos_lag_cols or not vlos_base_cols:\n        print("\\n‚ùå ERROR: No se encontraron columnas VLOS necesarias")\n    else:\n        print(f"\\nüìå Columnas VLOS con lag: {len(vlos_lag_cols)}")\n        print(f"üìå Columnas VLOS sin lag: {len(vlos_base_cols)}")\n        \n        # Extraer informaci√≥n de lags\n        lag_info = []\n        for col in vlos_lag_cols:\n            m_lag = re.search(r\'lag(\\d+)s\', col)\n            m_beam = re.search(r\'BEAM(\\d+)\', col)\n            if m_lag and m_beam:\n                lag_s = int(m_lag.group(1))\n                beam = int(m_beam.group(1))\n                lag_info.append({\n                    \'column\': col,\n                    \'lag_s\': lag_s,\n                    \'beam\': beam\n                })\n        \n        # Informaci√≥n de VLOS sin lag\n        base_info = []\n        for col in vlos_base_cols:\n            m_beam = re.search(r\'BEAM(\\d+)\', col)\n            if m_beam:\n                beam = int(m_beam.group(1))\n                base_info.append({\n                    \'column\': col,\n                    \'lag_s\': 0,  # lag = 0 para sin lag\n                    \'beam\': beam\n                })\n        \n        # Combinar\n        all_info = base_info + lag_info\n        df_all_info = pd.DataFrame(all_info)\n        \n        lags_sorted = sorted(df_all_info[\'lag_s\'].unique())\n        beams = sorted(df_all_info[\'beam\'].unique())\n        \n        print(f"üìå Lags (con 0s=sin lag): {lags_sorted[0]}s - {lags_sorted[-1]}s ({len(lags_sorted)} lags)")\n        print(f"üìå BEAMs detectados: {beams}")\n        \n        # ------------------------------------------------------------------------\n        # 2) Cargar √∫ltimas n_samples_spatial filas\n        # ------------------------------------------------------------------------\n        print(f"\\n[1/3] Cargando √∫ltimas {n_samples_spatial} filas...")\n        \n        all_vlos_cols = vlos_base_cols + vlos_lag_cols\n        \n        # Determinar n√∫mero total de filas\n        total_rows = sum(1 for _ in open(complete_dataset_path)) - 1\n        start_row = max(1, total_rows - n_samples_spatial)\n        \n        df_sample = pd.read_csv(\n            complete_dataset_path,\n            usecols=all_vlos_cols,\n            skiprows=range(1, start_row)\n        )\n        \n        print(f"‚úÖ Cargado: {df_sample.shape[0]:,} filas, {df_sample.shape[1]} columnas")\n        \n        # Calcular velocidad promedio de viento (sin lag) por fila\n        wind_mean = df_sample[vlos_base_cols].mean(axis=1).values\n        print(f"‚úÖ Velocidad promedio: {wind_mean.min():.2f} - {wind_mean.max():.2f} m/s")\n        \n        # ------------------------------------------------------------------------\n        # 3) Crear heatmaps espaciales por BEAM\n        # ------------------------------------------------------------------------\n        print(f"\\n[2/3] Generando heatmaps espaciales por BEAM...")\n        \n        for beam in beams:\n            # Filtrar columnas de este BEAM, ordenar por lag\n            beam_data = df_all_info[df_all_info[\'beam\'] == beam].sort_values(\'lag_s\')\n            \n            if beam_data.empty:\n                continue\n            \n            # Crear matriz de datos y calcular distancias\n            data_matrix = []\n            distance_labels = []\n            \n            for _, row in beam_data.iterrows():\n                col_name = row[\'column\']\n                lag_s = row[\'lag_s\']\n                \n                if col_name in df_sample.columns:\n                    data_matrix.append(df_sample[col_name].values)\n                    \n                    # Calcular distancia promedio (d = 100 - U*lag)\n                    # Usar velocidad promedio del periodo\n                    avg_wind = wind_mean.mean()\n                    distance_m = distance_lidar_m - avg_wind * lag_s\n                    distance_m = max(0, distance_m)  # No negativo\n                    distance_labels.append(distance_m)\n            \n            if not data_matrix:\n                continue\n            \n            data_matrix = np.array(data_matrix)  # shape: (n_distances, n_samples)\n            distance_labels = np.array(distance_labels)\n            \n            # Invertir para que 100m est√© arriba y 0m abajo\n            data_matrix = data_matrix[::-1]\n            distance_labels = distance_labels[::-1]\n            \n            # Crear heatmap\n            fig, ax = plt.subplots(figsize=(18, 10))\n            \n            im = ax.imshow(\n                data_matrix,\n                aspect=\'auto\',\n                cmap=\'RdYlBu_r\',\n                interpolation=\'nearest\',\n                origin=\'lower\',\n                extent=[0, len(df_sample), distance_labels.min(), distance_labels.max()]\n            )\n            \n            # Configurar ejes\n            ax.set_xlabel(\'Tiempo (muestras) ‚Üí\', fontsize=13, fontweight=\'bold\')\n            ax.set_ylabel(\'Distancia al rotor (m) ‚Üì [100m LIDAR ‚Üí 0m rotor]\', fontsize=13, fontweight=\'bold\')\n            ax.set_title(f\'Propagaci√≥n Espacial de R√°fagas - BEAM {beam}\\n(R√°fagas viajan hacia el rotor ‚ÜòÔ∏è: arriba-izq ‚Üí abajo-der)\', \n                        fontsize=15, fontweight=\'bold\')\n            \n            # Etiquetas eje Y (distancias)\n            n_yticks = min(10, len(distance_labels))\n            y_indices = np.linspace(0, len(distance_labels)-1, n_yticks, dtype=int)\n            ax.set_yticks(distance_labels[y_indices])\n            ax.set_yticklabels([f"{d:.0f}m" for d in distance_labels[y_indices]])\n            \n            # A√±adir l√≠nea diagonal de referencia (descendente)\n            n_points_diag = min(len(df_sample), len(distance_labels))\n            if n_points_diag > 10:\n                x_diag = np.linspace(0, len(df_sample)-1, n_points_diag)\n                y_diag = np.linspace(distance_labels.max(), distance_labels.min(), n_points_diag)\n                ax.plot(x_diag, y_diag, \'w--\', linewidth=2, alpha=0.6, label=\'Trayectoria te√≥rica ‚ÜòÔ∏è\')\n            \n            # Etiquetas eje X\n            x_ticks_step = max(1, len(df_sample) // 10)\n            x_ticks = range(0, len(df_sample), x_ticks_step)\n            ax.set_xticks(x_ticks)\n            ax.set_xticklabels([str(i) for i in x_ticks])\n            \n            # Colorbar\n            cbar = plt.colorbar(im, ax=ax, label=\'Velocidad Viento (m/s)\')\n            cbar.ax.tick_params(labelsize=11)\n            \n            # Grid y leyenda\n            ax.grid(True, alpha=0.2, linestyle=\'--\', linewidth=0.5)\n            ax.legend(loc=\'upper right\', fontsize=10, framealpha=0.8)\n            \n            plt.tight_layout()\n            \n            # Guardar\n            plot_path = wind_evolution_folder / f"wind_spatial_evolution_BEAM{beam}.png"\n            plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n            plt.close()\n            \n            print(f"  ‚úÖ BEAM {beam}: {plot_path.name}")\n        \n        # ------------------------------------------------------------------------\n        # 4) Crear heatmap espacial promedio (todos los BEAMs)\n        # ------------------------------------------------------------------------\n        print(f"\\n[3/3] Generando heatmap espacial promedio...")\n        \n        # Promediar por lag a trav√©s de todos los BEAMs\n        avg_data_matrix = []\n        avg_distance_labels = []\n        \n        for lag_s in lags_sorted:\n            lag_cols = df_all_info[df_all_info[\'lag_s\'] == lag_s][\'column\'].tolist()\n            valid_cols = [c for c in lag_cols if c in df_sample.columns]\n            \n            if valid_cols:\n                avg_signal = df_sample[valid_cols].mean(axis=1).values\n                avg_data_matrix.append(avg_signal)\n                \n                # Distancia promedio para este lag\n                avg_wind = wind_mean.mean()\n                distance_m = distance_lidar_m - avg_wind * lag_s\n                distance_m = max(0, distance_m)\n                avg_distance_labels.append(distance_m)\n        \n        if avg_data_matrix:\n            avg_data_matrix = np.array(avg_data_matrix)\n            avg_distance_labels = np.array(avg_distance_labels)\n            \n            # Invertir para que 100m est√© arriba\n            avg_data_matrix = avg_data_matrix[::-1]\n            avg_distance_labels = avg_distance_labels[::-1]\n            \n            fig, ax = plt.subplots(figsize=(18, 10))\n            \n            im = ax.imshow(\n                avg_data_matrix,\n                aspect=\'auto\',\n                cmap=\'RdYlBu_r\',\n                interpolation=\'nearest\',\n                origin=\'lower\',\n                extent=[0, len(df_sample), avg_distance_labels.min(), avg_distance_labels.max()]\n            )\n            \n            ax.set_xlabel(\'Tiempo (muestras) ‚Üí\', fontsize=13, fontweight=\'bold\')\n            ax.set_ylabel(\'Distancia al rotor (m) ‚Üì [100m LIDAR ‚Üí 0m rotor]\', fontsize=13, fontweight=\'bold\')\n            ax.set_title(\'Propagaci√≥n Espacial de R√°fagas - Promedio Todos los BEAMs\\n(R√°fagas viajan hacia el rotor ‚ÜòÔ∏è: arriba-izq ‚Üí abajo-der)\', \n                        fontsize=15, fontweight=\'bold\')\n            \n            # Etiquetas eje Y\n            n_yticks = min(10, len(avg_distance_labels))\n            y_indices = np.linspace(0, len(avg_distance_labels)-1, n_yticks, dtype=int)\n            ax.set_yticks(avg_distance_labels[y_indices])\n            ax.set_yticklabels([f"{d:.0f}m" for d in avg_distance_labels[y_indices]])\n            \n            # Diagonal descendente\n            n_points_diag = min(len(df_sample), len(avg_distance_labels))\n            if n_points_diag > 10:\n                x_diag = np.linspace(0, len(df_sample)-1, n_points_diag)\n                y_diag = np.linspace(avg_distance_labels.max(), avg_distance_labels.min(), n_points_diag)\n                ax.plot(x_diag, y_diag, \'w--\', linewidth=2, alpha=0.6, label=\'Trayectoria te√≥rica ‚ÜòÔ∏è\')\n            \n            # Etiquetas eje X\n            x_ticks_step = max(1, len(df_sample) // 10)\n            x_ticks = range(0, len(df_sample), x_ticks_step)\n            ax.set_xticks(x_ticks)\n            ax.set_xticklabels([str(i) for i in x_ticks])\n            \n            cbar = plt.colorbar(im, ax=ax, label=\'Velocidad Viento (m/s)\')\n            cbar.ax.tick_params(labelsize=11)\n            \n            ax.grid(True, alpha=0.2, linestyle=\'--\', linewidth=0.5)\n            ax.legend(loc=\'upper right\', fontsize=10, framealpha=0.8)\n            \n            plt.tight_layout()\n            \n            plot_path = wind_evolution_folder / "wind_spatial_evolution_ALL_BEAMS_avg.png"\n            plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n            plt.close()\n            \n            print(f"  ‚úÖ Promedio: {plot_path.name}")\n            \n            # CSV con datos espaciales\n            df_spatial = pd.DataFrame(\n                avg_data_matrix.T,\n                columns=[f"{d:.1f}m" for d in avg_distance_labels]\n            )\n            csv_path = wind_evolution_folder / "wind_spatial_evolution_data.csv"\n            df_spatial.to_csv(csv_path, index=False)\n        \n        # ------------------------------------------------------------------------\n        # RESUMEN\n        # ------------------------------------------------------------------------\n        print(f"\\n{\'=\'*70}")\n        print("RESUMEN - EVOLUCI√ìN ESPACIAL DE R√ÅFAGAS")\n        print("="*70)\n        print(f"Muestras analizadas:  {len(df_sample):,}")\n        print(f"Distancia LIDAR:      {distance_lidar_m:.0f} m")\n        print(f"Velocidad promedio:   {wind_mean.mean():.2f} m/s")\n        print(f"BEAMs analizados:     {len(beams)}")\n        \n        print(f"\\nüéØ INTERPRETACI√ìN:")\n        print("   - Patrones DIAGONALES ‚ÜòÔ∏è (arriba-izq ‚Üí abajo-der) muestran propagaci√≥n espacial")\n        print("   - Una r√°faga medida a 100m (LIDAR) viaja hacia el rotor (0m)")\n        print("   - F√≠sica: distancia = 100 - U*lag, donde U = velocidad del viento")\n        print("   - Diagonales claras = advecci√≥n consistente desde LIDAR a turbina")\n        print("   - Velocidad de ca√≠da diagonal ~ velocidad del viento")\n        \n        print(f"\\nüìä VISUALIZACI√ìN:")\n        print("   - Rojo = viento alto (r√°faga)")\n        print("   - Azul = viento bajo")\n        print("   - Diagonal descendente muestra propagaci√≥n f√≠sica del viento")\n        print("   - Inclinaci√≥n diagonal ~ 1/velocidad del viento")\n        \n        print(f"\\nüìÅ Archivos generados:")\n        print(f"  - Heatmaps espaciales por BEAM: wind_spatial_evolution_BEAMX.png")\n        print(f"  - Heatmap espacial promedio: wind_spatial_evolution_ALL_BEAMS_avg.png")\n        print(f"  - CSV: wind_spatial_evolution_data.csv")\n        print(f"\\nüìç Ubicaci√≥n: {wind_evolution_folder}")\n        print("="*70)\n        print("\\n‚úÖ An√°lisis espacial completado")',
        ],
    },
    {
        "title": 'üåÄ PASO 4.12: Intensidad de Turbulencia vs Variabilidad de Cargas',
        "cells": [
            '# =============================================================================\n# PASO 4.12: INTENSIDAD DE TURBULENCIA VS VARIABILIDAD DE CARGAS\n# =============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\n# Crear carpeta para resultados\nturbulence_folder = eda_folder / "07_Turbulence"\nturbulence_folder.mkdir(exist_ok=True)\n\nprint("\\n" + "="*70)\nprint("üåÄ INTENSIDAD DE TURBULENCIA VS VARIABILIDAD DE CARGAS")\nprint("="*70)\nprint(f"Carpeta destino: {turbulence_folder}")\nprint("="*70)\n\n# Par√°metros\nwindow_start = 25      # Inicio de la primera ventana (s)\nwindow_duration = 575  # Duraci√≥n de cada ventana: 600 - 25 = 575s (~10 min)\nchunk_size = 600       # Tama√±o total del chunk (s)\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    # ------------------------------------------------------------------------\n    # 1) Verificar columnas necesarias\n    # ------------------------------------------------------------------------\n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n    \n    # Verificar columnas necesarias\n    required_cols = [\'Time\', \'Blade root 1 My\', \'Blade root 2 My\']\n    missing_cols = [c for c in required_cols if c not in all_columns]\n    \n    if missing_cols:\n        print(f"\\n‚ùå ERROR: Faltan columnas necesarias: {missing_cols}")\n    else:\n        # Columnas VLOS sin lag (para calcular TI del viento)\n        vlos_base_cols = [c for c in all_columns if \'LAC_VLOS\' in c and \'lag\' not in c]\n        \n        if not vlos_base_cols:\n            print("\\n‚ùå ERROR: No se encontraron columnas VLOS sin lag para calcular TI")\n        else:\n            print(f"\\nüìå Columnas VLOS sin lag detectadas: {len(vlos_base_cols)}")\n            print(f"üìå Ventana de an√°lisis: {window_duration}s (~{window_duration/60:.1f} min)")\n            print(f"üìå Periodo total por chunk: {chunk_size}s")\n            \n            # ------------------------------------------------------------------------\n            # 2) Leer datos y detectar reinicios de series temporales\n            # ------------------------------------------------------------------------\n            print(f"\\n[1/3] Leyendo datos y detectando series temporales concatenadas...")\n            \n            usecols = [\'Time\'] + vlos_base_cols + [\'Blade root 1 My\', \'Blade root 2 My\']\n            \n            # Leer todo el dataset\n            df = pd.read_csv(complete_dataset_path, usecols=usecols)\n            \n            print(f"‚úÖ Cargado: {len(df):,} filas")\n            \n            # Detectar reinicios de tiempo (cuando Time disminuye)\n            time_diff = df[\'Time\'].diff()\n            restarts = np.where(time_diff < 0)[0]  # √çndices donde el tiempo reinicia\n            \n            # Crear segmentos (cada segmento es una serie temporal continua)\n            segment_starts = [0] + restarts.tolist()\n            segment_ends = restarts.tolist() + [len(df)]\n            \n            print(f"üìå Series temporales detectadas: {len(segment_starts)}")\n            print(f"   Cada serie va de Time={window_start}s a ~{chunk_size}s")\n            \n            # ------------------------------------------------------------------------\n            # 3) Procesar cada segmento independientemente\n            # ------------------------------------------------------------------------\n            print(f"\\n[2/3] Procesando cada serie temporal (ventanas {window_start}s-{window_start+window_duration}s)...")\n            \n            results = []\n            \n            for seg_idx, (start_idx, end_idx) in enumerate(zip(segment_starts, segment_ends)):\n                df_segment = df.iloc[start_idx:end_idx].copy()\n                \n                # Verificar que el segmento tiene datos en el rango de inter√©s\n                if df_segment[\'Time\'].min() > window_start + window_duration:\n                    continue  # Este segmento no tiene datos √∫tiles\n                if df_segment[\'Time\'].max() < window_start:\n                    continue\n                \n                # Filtrar ventana de inter√©s: del segundo 25 al 600\n                mask = (df_segment[\'Time\'] >= window_start) & (df_segment[\'Time\'] <= window_start + window_duration)\n                df_window = df_segment[mask]\n                \n                if len(df_window) < 50:  # M√≠nimo de muestras para c√°lculo confiable\n                    continue\n                \n                # Calcular viento promedio (media de todos los BEAMs)\n                wind_values = df_window[vlos_base_cols].values\n                wind_mean_per_sample = np.nanmean(wind_values, axis=1)\n                \n                # TI: std(viento) / mean(viento)\n                wind_mean = np.nanmean(wind_mean_per_sample)\n                wind_std = np.nanstd(wind_mean_per_sample)\n                \n                if wind_mean > 0.1:  # Evitar divisi√≥n por cero\n                    TI = wind_std / wind_mean\n                else:\n                    continue\n                \n                # Desviaci√≥n est√°ndar de las cargas\n                load1_std = df_window[\'Blade root 1 My\'].std()\n                load2_std = df_window[\'Blade root 2 My\'].std()\n                \n                # Desviaci√≥n est√°ndar de las se√±ales VLOS (variabilidad espacial del viento)\n                # Para cada muestra temporal, calcular std entre los BEAMs\n                vlos_std_per_sample = np.nanstd(wind_values, axis=1)\n                vlos_std_mean = np.nanmean(vlos_std_per_sample)\n                \n                # Guardar resultados\n                results.append({\n                    \'Segment_ID\': seg_idx,\n                    \'Time_Start_s\': df_window[\'Time\'].min(),\n                    \'Time_End_s\': df_window[\'Time\'].max(),\n                    \'Wind_Mean_ms\': wind_mean,\n                    \'Wind_Std_ms\': wind_std,\n                    \'VLOS_Std_Spatial_ms\': vlos_std_mean,\n                    \'Turbulence_Intensity_TI\': TI,\n                    \'Blade1_My_Std\': load1_std,\n                    \'Blade2_My_Std\': load2_std,\n                    \'N_Samples\': len(df_window)\n                })\n                \n                if (seg_idx + 1) % 100 == 0:\n                    print(f"  ... procesados {seg_idx + 1} segmentos, {len(results)} ventanas v√°lidas")\n            \n            if not results:\n                print("\\n‚ö†Ô∏è No se obtuvieron ventanas v√°lidas")\n            else:\n                df_results = pd.DataFrame(results)\n                print(f"\\n‚úÖ Total series temporales analizadas: {len(df_results):,}")\n                print(f"   TI rango: {df_results[\'Turbulence_Intensity_TI\'].min():.3f} - {df_results[\'Turbulence_Intensity_TI\'].max():.3f}")\n                print(f"   Viento medio rango: {df_results[\'Wind_Mean_ms\'].min():.2f} - {df_results[\'Wind_Mean_ms\'].max():.2f} m/s")\n                \n                # ------------------------------------------------------------------------\n                # 4) Guardar CSV\n                # ------------------------------------------------------------------------\n                csv_path = turbulence_folder / "turbulence_intensity_vs_load_variability.csv"\n                df_results.to_csv(csv_path, index=False)\n                print(f"\\n[3/3] CSV guardado: {csv_path.name}")\n                \n                # ------------------------------------------------------------------------\n                # 5) Crear gr√°ficos de dispersi√≥n\n                # ------------------------------------------------------------------------\n                print(f"\\nGenerando gr√°ficos de dispersi√≥n...")\n                \n                for blade_num, blade_col in [(1, \'Blade1_My_Std\'), (2, \'Blade2_My_Std\')]:\n                    fig, ax = plt.subplots(figsize=(12, 8))\n                    \n                    # Scatter plot con color por velocidad de viento\n                    scatter = ax.scatter(\n                        df_results[\'Turbulence_Intensity_TI\'],\n                        df_results[blade_col],\n                        c=df_results[\'Wind_Mean_ms\'],\n                        cmap=\'viridis\',\n                        s=30,\n                        alpha=0.6,\n                        edgecolors=\'k\',\n                        linewidth=0.3\n                    )\n                    \n                    # Colorbar\n                    cbar = plt.colorbar(scatter, ax=ax, label=\'Velocidad Viento Media (m/s)\')\n                    cbar.ax.tick_params(labelsize=10)\n                    \n                    # Ajustar l√≠nea de tendencia\n                    if len(df_results) >= 10:\n                        # Regresi√≥n lineal\n                        from scipy.stats import linregress\n                        valid_mask = np.isfinite(df_results[\'Turbulence_Intensity_TI\']) & np.isfinite(df_results[blade_col])\n                        x_fit = df_results.loc[valid_mask, \'Turbulence_Intensity_TI\'].values\n                        y_fit = df_results.loc[valid_mask, blade_col].values\n                        \n                        if len(x_fit) >= 10:\n                            slope, intercept, r_value, p_value, std_err = linregress(x_fit, y_fit)\n                            x_line = np.linspace(x_fit.min(), x_fit.max(), 100)\n                            y_line = slope * x_line + intercept\n                            \n                            ax.plot(x_line, y_line, \'r--\', linewidth=2, alpha=0.8,\n                                   label=f\'Tendencia: R¬≤={r_value**2:.3f}, p={p_value:.2e}\')\n                            ax.legend(loc=\'upper left\', fontsize=10, framealpha=0.9)\n                    \n                    # Etiquetas y t√≠tulo\n                    ax.set_xlabel(\'Intensidad de Turbulencia (TI = œÉ/Œº)\', fontsize=12, fontweight=\'bold\')\n                    ax.set_ylabel(f\'Desviaci√≥n Est√°ndar Blade {blade_num} My (kNm)\', fontsize=12, fontweight=\'bold\')\n                    ax.set_title(f\'TI vs Variabilidad de Carga - Blade {blade_num}\\n(Mayor turbulencia ‚Üí Mayor variabilidad de carga)\', \n                                fontsize=14, fontweight=\'bold\')\n                    \n                    ax.grid(True, alpha=0.3, linestyle=\'--\')\n                    \n                    plt.tight_layout()\n                    \n                    # Guardar\n                    plot_path = turbulence_folder / f"TI_vs_Load_Std_Blade{blade_num}.png"\n                    plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n                    plt.close()\n                    \n                    print(f"  ‚úÖ Blade {blade_num}: {plot_path.name}")\n                \n                # ------------------------------------------------------------------------\n                # Gr√°fico combinado (ambas palas)\n                # ------------------------------------------------------------------------\n                fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n                fig.suptitle(\'TI vs Variabilidad de Carga - Comparaci√≥n Ambas Palas\', \n                           fontsize=16, fontweight=\'bold\')\n                \n                for idx, (blade_num, blade_col) in enumerate([(1, \'Blade1_My_Std\'), (2, \'Blade2_My_Std\')]):\n                    ax = axes[idx]\n                    \n                    scatter = ax.scatter(\n                        df_results[\'Turbulence_Intensity_TI\'],\n                        df_results[blade_col],\n                        c=df_results[\'Wind_Mean_ms\'],\n                        cmap=\'viridis\',\n                        s=30,\n                        alpha=0.6,\n                        edgecolors=\'k\',\n                        linewidth=0.3\n                    )\n                    \n                    cbar = plt.colorbar(scatter, ax=ax, label=\'Viento (m/s)\')\n                    cbar.ax.tick_params(labelsize=9)\n                    \n                    # L√≠nea de tendencia\n                    if len(df_results) >= 10:\n                        valid_mask = np.isfinite(df_results[\'Turbulence_Intensity_TI\']) & np.isfinite(df_results[blade_col])\n                        x_fit = df_results.loc[valid_mask, \'Turbulence_Intensity_TI\'].values\n                        y_fit = df_results.loc[valid_mask, blade_col].values\n                        \n                        if len(x_fit) >= 10:\n                            slope, intercept, r_value, _, _ = linregress(x_fit, y_fit)\n                            x_line = np.linspace(x_fit.min(), x_fit.max(), 100)\n                            y_line = slope * x_line + intercept\n                            ax.plot(x_line, y_line, \'r--\', linewidth=2, alpha=0.8, label=f\'R¬≤={r_value**2:.3f}\')\n                            ax.legend(loc=\'upper left\', fontsize=9)\n                    \n                    ax.set_xlabel(\'TI (œÉ/Œº)\', fontsize=11, fontweight=\'bold\')\n                    ax.set_ylabel(f\'std(Blade {blade_num} My) [kNm]\', fontsize=11, fontweight=\'bold\')\n                    ax.set_title(f\'Blade {blade_num}\', fontsize=12, fontweight=\'bold\')\n                    ax.grid(True, alpha=0.3, linestyle=\'--\')\n                \n                plt.tight_layout()\n                \n                plot_path = turbulence_folder / "TI_vs_Load_Std_Combined.png"\n                plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n                plt.close()\n                \n                print(f"  ‚úÖ Combinado: {plot_path.name}")\n                \n                # ------------------------------------------------------------------------\n                # RESUMEN\n                # ------------------------------------------------------------------------\n                print(f"\\n{\'=\'*70}")\n                print("RESUMEN - INTENSIDAD DE TURBULENCIA VS VARIABILIDAD")\n                print("="*70)\n                print(f"Series temporales analizadas: {len(df_results):,}")\n                print(f"Duraci√≥n ventana por serie:   {window_duration}s (~{window_duration/60:.1f} min)")\n                print(f"Rango temporal por serie:     {window_start}s - {window_start+window_duration}s")\n                print(f"TI m√≠nimo:                    {df_results[\'Turbulence_Intensity_TI\'].min():.4f}")\n                print(f"TI m√°ximo:                    {df_results[\'Turbulence_Intensity_TI\'].max():.4f}")\n                print(f"TI medio:                     {df_results[\'Turbulence_Intensity_TI\'].mean():.4f}")\n                \n                # Correlaciones\n                from scipy.stats import pearsonr\n                corr1, p1 = pearsonr(df_results[\'Turbulence_Intensity_TI\'], df_results[\'Blade1_My_Std\'])\n                corr2, p2 = pearsonr(df_results[\'Turbulence_Intensity_TI\'], df_results[\'Blade2_My_Std\'])\n                \n                print(f"\\nüéØ CORRELACIONES (Pearson):")\n                print(f"  Blade 1: r = {corr1:.3f}, p-value = {p1:.2e}")\n                print(f"  Blade 2: r = {corr2:.3f}, p-value = {p2:.2e}")\n                \n                if corr1 > 0.5 or corr2 > 0.5:\n                    print("\\n‚úÖ VALIDADO: Correlaci√≥n positiva fuerte detectada!")\n                    print("   ‚Üí Mayor turbulencia ‚Üí Mayor variabilidad de carga")\n                    print("   ‚Üí El modelo DEBE capturar varianza, no solo media")\n                elif corr1 > 0.3 or corr2 > 0.3:\n                    print("\\n‚ö†Ô∏è Correlaci√≥n moderada detectada")\n                    print("   ‚Üí Se observa tendencia pero con dispersi√≥n")\n                else:\n                    print("\\nüîµ Correlaci√≥n d√©bil")\n                    print("   ‚Üí Revisar c√°lculo de TI o ventanas temporales")\n                \n                print(f"\\nüí° IMPLICACIONES PARA EL MODELO:")\n                print("   1. Loss Functions deben penalizar outliers/picos (no solo MSE)")\n                print("   2. Considerar redes neuronales probabil√≠sticas (Bayesianas, Dropout)")\n                print("   3. Evaluar modelos con m√©tricas de varianza (std, quantiles)")\n                print("   4. En fatiga, predecir distribuciones > predecir medias")\n                \n                print(f"\\nüìÅ Archivos generados:")\n                print(f"  - TI_vs_Load_Std_Blade1.png")\n                print(f"  - TI_vs_Load_Std_Blade2.png")\n                print(f"  - TI_vs_Load_Std_Combined.png")\n                print(f"  - turbulence_intensity_vs_load_variability.csv")\n                print(f"\\nüìç Ubicaci√≥n: {turbulence_folder}")\n                print("="*70)\n                print("\\n‚úÖ An√°lisis de turbulencia completado")',
        ],
    },
    {
        "title": 'üìä PASO 5.2: Validaci√≥n Visual de VLOS_Rolling_Std',
        "cells": [
            '# =============================================================================\n# PASO 5.2: VALIDACI√ìN VISUAL DE VLOS_Rolling_Std\n# =============================================================================\n\nprint("\\n" + "="*70)\nprint("üìä VALIDACI√ìN VISUAL: VLOS_Rolling_Std")\nprint("="*70)\n\n# Crear carpeta para resultados de validaci√≥n\nvalidation_folder = eda_folder / "08_VLOS_Rolling_Std_Validation"\nvalidation_folder.mkdir(exist_ok=True)\n\nprint(f"Carpeta destino: {validation_folder}")\nprint("="*70)\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    # ------------------------------------------------------------------------\n    # 1) Verificar que existe la columna VLOS_Rolling_Std\n    # ------------------------------------------------------------------------\n    print(f"\\n[1/5] Verificando columna VLOS_Rolling_Std...")\n    \n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n    \n    if \'VLOS_Rolling_Std\' not in all_columns:\n        print("\\n‚ùå ERROR: La columna VLOS_Rolling_Std no existe en el dataset")\n        print("   Por favor, ejecuta primero el PASO 5.1 para crear esta feature")\n    else:\n        print("‚úÖ Columna VLOS_Rolling_Std encontrada")\n        \n        # ------------------------------------------------------------------------\n        # 2) Cargar muestra representativa de datos\n        # ------------------------------------------------------------------------\n        print(f"\\n[2/5] Cargando muestra representativa de datos...")\n        \n        # Cargar √∫ltimas 5000 filas para an√°lisis visual\n        n_samples = 2000\n        \n        # Columnas necesarias\n        cols_needed = [\'Time\', \'VLOS_Rolling_Std\', \'Blade root 1 My\', \'Blade root 2 My\']\n        \n        # A√±adir columnas VLOS sin lag para comparaci√≥n\n        vlos_base_cols = [c for c in all_columns if \'LAC_VLOS\' in c and \'lag\' not in c]\n        cols_needed.extend(vlos_base_cols)\n        \n        # Determinar total de filas\n        total_rows = sum(1 for _ in open(complete_dataset_path)) - 1\n        start_row = max(1, total_rows - n_samples)\n        \n        # Cargar muestra\n        df_sample = pd.read_csv(\n            complete_dataset_path,\n            usecols=cols_needed,\n            skiprows=range(1, start_row)\n        )\n        \n        print(f"‚úÖ Cargado: {len(df_sample):,} filas")\n        print(f"   Tiempo inicial: {df_sample[\'Time\'].min():.1f}s")\n        print(f"   Tiempo final: {df_sample[\'Time\'].max():.1f}s")\n        \n        # Calcular velocidad media del viento\n        df_sample[\'Wind_Mean\'] = df_sample[vlos_base_cols].mean(axis=1)\n        \n        # Estad√≠sticas b√°sicas\n        print(f"\\nüìä Estad√≠sticas VLOS_Rolling_Std:")\n        print(f"   M√≠n:    {df_sample[\'VLOS_Rolling_Std\'].min():.4f} m/s")\n        print(f"   M√°x:    {df_sample[\'VLOS_Rolling_Std\'].max():.4f} m/s")\n        print(f"   Media:  {df_sample[\'VLOS_Rolling_Std\'].mean():.4f} m/s")\n        print(f"   Mediana:{df_sample[\'VLOS_Rolling_Std\'].median():.4f} m/s")\n        print(f"   NaNs:   {df_sample[\'VLOS_Rolling_Std\'].isna().sum()}")\n        \n        # ------------------------------------------------------------------------\n        # 3) PLOT 1: Serie Temporal\n        # ------------------------------------------------------------------------\n        print(f"\\n[3/5] Generando serie temporal...")\n        \n        fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)\n        fig.suptitle(\'Validaci√≥n VLOS_Rolling_Std - Serie Temporal\', \n                     fontsize=16, fontweight=\'bold\')\n        \n        # Subplot 1: Velocidad media del viento\n        axes[0].plot(df_sample[\'Time\'], df_sample[\'Wind_Mean\'], \n                    linewidth=1, color=\'steelblue\', label=\'Viento medio\')\n        axes[0].set_ylabel(\'Velocidad Viento (m/s)\', fontsize=11, fontweight=\'bold\')\n        axes[0].set_title(\'Velocidad Media del Viento\', fontsize=12, fontweight=\'bold\')\n        axes[0].grid(True, alpha=0.3, linestyle=\'--\')\n        axes[0].legend(loc=\'upper right\')\n        \n        # Subplot 2: VLOS_Rolling_Std\n        axes[1].plot(df_sample[\'Time\'], df_sample[\'VLOS_Rolling_Std\'], \n                    linewidth=1, color=\'orange\', label=\'VLOS Rolling Std\')\n        axes[1].set_ylabel(\'Desv. Est√°ndar M√≥vil (m/s)\', fontsize=11, fontweight=\'bold\')\n        axes[1].set_title(\'VLOS_Rolling_Std - Captura Turbulencia Local\', \n                         fontsize=12, fontweight=\'bold\')\n        axes[1].grid(True, alpha=0.3, linestyle=\'--\')\n        axes[1].legend(loc=\'upper right\')\n        \n        # Subplot 3: Cargas en las palas\n        axes[2].plot(df_sample[\'Time\'], df_sample[\'Blade root 1 My\'], \n                    linewidth=0.8, alpha=0.7, label=\'Blade 1 My\')\n        axes[2].plot(df_sample[\'Time\'], df_sample[\'Blade root 2 My\'], \n                    linewidth=0.8, alpha=0.7, label=\'Blade 2 My\')\n        axes[2].set_ylabel(\'Momento Flector (kNm)\', fontsize=11, fontweight=\'bold\')\n        axes[2].set_xlabel(\'Tiempo (s)\', fontsize=11, fontweight=\'bold\')\n        axes[2].set_title(\'Cargas en las Palas\', fontsize=12, fontweight=\'bold\')\n        axes[2].grid(True, alpha=0.3, linestyle=\'--\')\n        axes[2].legend(loc=\'upper right\')\n        \n        plt.tight_layout()\n        \n        plot_path = validation_folder / "time_series_validation.png"\n        plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"‚úÖ Serie temporal: {plot_path.name}")\n        \n        # ------------------------------------------------------------------------\n        # 4) PLOT 2: Histograma de distribuci√≥n\n        # ------------------------------------------------------------------------\n        print(f"\\n[4/5] Generando histograma de distribuci√≥n...")\n        \n        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n        \n        # Subplot 1: Histograma b√°sico\n        axes[0].hist(df_sample[\'VLOS_Rolling_Std\'].dropna(), bins=50, \n                    color=\'teal\', edgecolor=\'black\', alpha=0.7)\n        \n        mean_val = df_sample[\'VLOS_Rolling_Std\'].mean()\n        median_val = df_sample[\'VLOS_Rolling_Std\'].median()\n        \n        axes[0].axvline(mean_val, color=\'red\', linestyle=\'--\', linewidth=2, \n                       label=f\'Media: {mean_val:.3f}\')\n        axes[0].axvline(median_val, color=\'green\', linestyle=\'--\', linewidth=2, \n                       label=f\'Mediana: {median_val:.3f}\')\n        \n        axes[0].set_xlabel(\'VLOS_Rolling_Std (m/s)\', fontsize=11, fontweight=\'bold\')\n        axes[0].set_ylabel(\'Frecuencia\', fontsize=11, fontweight=\'bold\')\n        axes[0].set_title(\'Distribuci√≥n de VLOS_Rolling_Std\', \n                         fontsize=12, fontweight=\'bold\')\n        axes[0].legend(loc=\'upper right\')\n        axes[0].grid(True, alpha=0.3, linestyle=\'--\')\n        \n        # Subplot 2: Box plot\n        bp = axes[1].boxplot(df_sample[\'VLOS_Rolling_Std\'].dropna(), \n                            vert=True, patch_artist=True,\n                            boxprops=dict(facecolor=\'lightblue\', alpha=0.7),\n                            medianprops=dict(color=\'red\', linewidth=2),\n                            whiskerprops=dict(linewidth=1.5),\n                            capprops=dict(linewidth=1.5))\n        \n        axes[1].set_ylabel(\'VLOS_Rolling_Std (m/s)\', fontsize=11, fontweight=\'bold\')\n        axes[1].set_title(\'Box Plot - Detecci√≥n de Outliers\', \n                         fontsize=12, fontweight=\'bold\')\n        axes[1].grid(True, alpha=0.3, linestyle=\'--\', axis=\'y\')\n        \n        plt.tight_layout()\n        \n        plot_path = validation_folder / "distribution_validation.png"\n        plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"‚úÖ Distribuci√≥n: {plot_path.name}")\n        \n        # ------------------------------------------------------------------------\n        # 5) PLOT 3: Correlaciones con cargas\n        # ------------------------------------------------------------------------\n        print(f"\\n[5/5] Generando correlaciones con cargas...")\n        \n        # Calcular rolling std de las cargas (misma ventana)\n        rolling_window_samples = 300  # ~30s con dt=0.1s\n        df_sample[\'Blade1_My_Rolling_Std\'] = df_sample[\'Blade root 1 My\'].rolling(\n            window=rolling_window_samples, min_periods=50, center=True\n        ).std()\n        df_sample[\'Blade2_My_Rolling_Std\'] = df_sample[\'Blade root 2 My\'].rolling(\n            window=rolling_window_samples, min_periods=50, center=True\n        ).std()\n        \n        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n        \n        for idx, (blade_num, blade_col) in enumerate([\n            (1, \'Blade1_My_Rolling_Std\'), \n            (2, \'Blade2_My_Rolling_Std\')\n        ]):\n            ax = axes[idx]\n            \n            # Scatter plot\n            valid_mask = (\n                df_sample[\'VLOS_Rolling_Std\'].notna() & \n                df_sample[blade_col].notna()\n            )\n            \n            scatter = ax.scatter(\n                df_sample.loc[valid_mask, \'VLOS_Rolling_Std\'],\n                df_sample.loc[valid_mask, blade_col],\n                c=df_sample.loc[valid_mask, \'Wind_Mean\'],\n                cmap=\'viridis\',\n                s=20,\n                alpha=0.5,\n                edgecolors=\'none\'\n            )\n            \n            # Colorbar\n            cbar = plt.colorbar(scatter, ax=ax, label=\'Viento medio (m/s)\')\n            cbar.ax.tick_params(labelsize=9)\n            \n            # Calcular correlaci√≥n\n            from scipy.stats import pearsonr\n            x_corr = df_sample.loc[valid_mask, \'VLOS_Rolling_Std\'].values\n            y_corr = df_sample.loc[valid_mask, blade_col].values\n            \n            if len(x_corr) >= 10:\n                corr, p_val = pearsonr(x_corr, y_corr)\n                \n                # L√≠nea de tendencia\n                from scipy.stats import linregress\n                slope, intercept, _, _, _ = linregress(x_corr, y_corr)\n                x_line = np.linspace(x_corr.min(), x_corr.max(), 100)\n                y_line = slope * x_line + intercept\n                ax.plot(x_line, y_line, \'r--\', linewidth=2, alpha=0.8,\n                       label=f\'R={corr:.3f}, p={p_val:.2e}\')\n                ax.legend(loc=\'upper left\', fontsize=10)\n            \n            ax.set_xlabel(\'VLOS_Rolling_Std (m/s)\', fontsize=11, fontweight=\'bold\')\n            ax.set_ylabel(f\'std(Blade {blade_num} My) [kNm]\', fontsize=11, fontweight=\'bold\')\n            ax.set_title(f\'Correlaci√≥n: Turbulencia ‚Üî Variabilidad Carga (Blade {blade_num})\', \n                        fontsize=12, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n        \n        plt.tight_layout()\n        \n        plot_path = validation_folder / "correlation_with_loads.png"\n        plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"‚úÖ Correlaciones: {plot_path.name}")\n        \n        # ------------------------------------------------------------------------\n        # 6) PLOT 4: VLOS_Rolling_Std vs Velocidad Media\n        # ------------------------------------------------------------------------\n        print(f"\\n[Bonus] Generando relaci√≥n con velocidad media...")\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        \n        scatter = ax.scatter(\n            df_sample[\'Wind_Mean\'],\n            df_sample[\'VLOS_Rolling_Std\'],\n            c=df_sample[\'Time\'],\n            cmap=\'plasma\',\n            s=15,\n            alpha=0.4,\n            edgecolors=\'none\'\n        )\n        \n        cbar = plt.colorbar(scatter, ax=ax, label=\'Tiempo (s)\')\n        cbar.ax.tick_params(labelsize=10)\n        \n        ax.set_xlabel(\'Velocidad Media del Viento (m/s)\', fontsize=12, fontweight=\'bold\')\n        ax.set_ylabel(\'VLOS_Rolling_Std (m/s)\', fontsize=12, fontweight=\'bold\')\n        ax.set_title(\'VLOS_Rolling_Std vs Velocidad Media\\n(Turbulencia vs Intensidad del Viento)\', \n                    fontsize=14, fontweight=\'bold\')\n        ax.grid(True, alpha=0.3, linestyle=\'--\')\n        \n        plt.tight_layout()\n        \n        plot_path = validation_folder / "rolling_std_vs_wind_mean.png"\n        plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"‚úÖ Relaci√≥n con viento: {plot_path.name}")\n        \n        # ------------------------------------------------------------------------\n        # RESUMEN Y DIAGN√ìSTICO\n        # ------------------------------------------------------------------------\n        print(f"\\n{\'=\'*70}")\n        print("RESUMEN - VALIDACI√ìN VLOS_Rolling_Std")\n        print("="*70)\n        \n        print(f"\\nüìä ESTAD√çSTICAS:")\n        print(f"   M√≠nimo:       {df_sample[\'VLOS_Rolling_Std\'].min():.4f} m/s")\n        print(f"   M√°ximo:       {df_sample[\'VLOS_Rolling_Std\'].max():.4f} m/s")\n        print(f"   Media:        {df_sample[\'VLOS_Rolling_Std\'].mean():.4f} m/s")\n        print(f"   Mediana:      {df_sample[\'VLOS_Rolling_Std\'].median():.4f} m/s")\n        print(f"   Desv. Std:    {df_sample[\'VLOS_Rolling_Std\'].std():.4f} m/s")\n        print(f"   NaNs:         {df_sample[\'VLOS_Rolling_Std\'].isna().sum()} ({df_sample[\'VLOS_Rolling_Std\'].isna().sum()/len(df_sample)*100:.2f}%)")\n        \n        print(f"\\nüîç VALIDACI√ìN VISUAL:")\n        print("   ‚úÖ Serie temporal: ¬øCaptura picos de turbulencia?")\n        print("   ‚úÖ Distribuci√≥n: ¬øForma coherente sin outliers an√≥malos?")\n        print("   ‚úÖ Correlaci√≥n: ¬øSe correlaciona con variabilidad de cargas?")\n        print("   ‚úÖ Relaci√≥n viento: ¬øAumenta con velocidad o es independiente?")\n        \n        # Calcular correlaciones con cargas\n        valid_mask = (\n            df_sample[\'VLOS_Rolling_Std\'].notna() & \n            df_sample[\'Blade1_My_Rolling_Std\'].notna() &\n            df_sample[\'Blade2_My_Rolling_Std\'].notna()\n        )\n        \n        if valid_mask.sum() >= 10:\n            corr1, p1 = pearsonr(\n                df_sample.loc[valid_mask, \'VLOS_Rolling_Std\'],\n                df_sample.loc[valid_mask, \'Blade1_My_Rolling_Std\']\n            )\n            corr2, p2 = pearsonr(\n                df_sample.loc[valid_mask, \'VLOS_Rolling_Std\'],\n                df_sample.loc[valid_mask, \'Blade2_My_Rolling_Std\']\n            )\n            \n            print(f"\\nüìà CORRELACIONES CON CARGAS:")\n            print(f"   Blade 1: r = {corr1:.3f}, p = {p1:.2e}")\n            print(f"   Blade 2: r = {corr2:.3f}, p = {p2:.2e}")\n            \n            if corr1 > 0.3 or corr2 > 0.3:\n                print("\\n   ‚úÖ VALIDADO: Correlaci√≥n positiva detectada")\n                print("      ‚Üí Mayor turbulencia ‚Üí Mayor variabilidad de carga")\n                print("      ‚Üí La feature captura informaci√≥n relevante")\n            else:\n                print("\\n   ‚ö†Ô∏è Correlaci√≥n d√©bil")\n                print("      ‚Üí Puede indicar que la ventana m√≥vil es incorrecta")\n                print("      ‚Üí O que la turbulencia no es el factor dominante en esta muestra")\n        \n        print(f"\\nüí° USO EN MODELO ML:")\n        print("   - Feature para predecir picos/variabilidad de cargas")\n        print("   - Complementa velocidad media con informaci√≥n de turbulencia local")\n        print("   - √ötil en modelos que predicen distribuciones (no solo media)")\n        \n        print(f"\\nüìÅ Archivos generados:")\n        print(f"   - time_series_validation.png")\n        print(f"   - distribution_validation.png")\n        print(f"   - correlation_with_loads.png")\n        print(f"   - rolling_std_vs_wind_mean.png")\n        \n        print(f"\\nüìç Ubicaci√≥n: {validation_folder}")\n        print("="*70)\n        print("\\n‚úÖ Validaci√≥n visual completada")\n',
            '# =============================================================================\n# PASO 5.2: VALIDACI√ìN VISUAL DE VLOS_Rolling_Std\n# =============================================================================\n\nprint("\\n" + "="*70)\nprint("üìä VALIDACI√ìN VISUAL: VLOS_Rolling_Std")\nprint("="*70)\n\n# Crear carpeta para resultados de validaci√≥n\nvalidation_folder = eda_folder / "08_VLOS_Rolling_Std_Validation"\nvalidation_folder.mkdir(exist_ok=True)\n\nprint(f"Carpeta destino: {validation_folder}")\nprint("="*70)\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    # ------------------------------------------------------------------------\n    # 1) Verificar que existe la columna VLOS_Rolling_Std\n    # ------------------------------------------------------------------------\n    print(f"\\n[1/5] Verificando columna VLOS_Rolling_Std...")\n    \n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n    \n    if \'VLOS_Rolling_Std\' not in all_columns:\n        print("\\n‚ùå ERROR: La columna VLOS_Rolling_Std no existe en el dataset")\n        print("   Por favor, ejecuta primero el PASO 5.1 para crear esta feature")\n    else:\n        print("‚úÖ Columna VLOS_Rolling_Std encontrada")\n        \n        # ------------------------------------------------------------------------\n        # 2) Cargar muestra representativa de datos\n        # ------------------------------------------------------------------------\n        print(f"\\n[2/5] Cargando muestra representativa de datos...")\n        \n        # Cargar √∫ltimas 5000 filas para an√°lisis visual\n        n_samples = 5000\n        \n        # Columnas necesarias\n        cols_needed = [\'Time\', \'VLOS_Rolling_Std\', \'Blade root 1 My\', \'Blade root 2 My\']\n        \n        # A√±adir columnas VLOS sin lag para comparaci√≥n\n        vlos_base_cols = [c for c in all_columns if \'LAC_VLOS\' in c and \'lag\' not in c]\n        cols_needed.extend(vlos_base_cols)\n        \n        # Determinar total de filas\n        total_rows = sum(1 for _ in open(complete_dataset_path)) - 1\n        start_row = max(1, total_rows - n_samples)\n        \n        # Cargar muestra\n        df_sample = pd.read_csv(\n            complete_dataset_path,\n            usecols=cols_needed,\n            skiprows=range(1, start_row)\n        )\n        \n        print(f"‚úÖ Cargado: {len(df_sample):,} filas")\n        print(f"   Tiempo inicial: {df_sample[\'Time\'].min():.1f}s")\n        print(f"   Tiempo final: {df_sample[\'Time\'].max():.1f}s")\n        \n        # Calcular velocidad media del viento\n        df_sample[\'Wind_Mean\'] = df_sample[vlos_base_cols].mean(axis=1)\n        \n        # Estad√≠sticas b√°sicas\n        print(f"\\nüìä Estad√≠sticas VLOS_Rolling_Std:")\n        print(f"   M√≠n:    {df_sample[\'VLOS_Rolling_Std\'].min():.4f} m/s")\n        print(f"   M√°x:    {df_sample[\'VLOS_Rolling_Std\'].max():.4f} m/s")\n        print(f"   Media:  {df_sample[\'VLOS_Rolling_Std\'].mean():.4f} m/s")\n        print(f"   Mediana:{df_sample[\'VLOS_Rolling_Std\'].median():.4f} m/s")\n        print(f"   NaNs:   {df_sample[\'VLOS_Rolling_Std\'].isna().sum()}")\n        \n        # ------------------------------------------------------------------------\n        # 3) PLOT 1: Serie Temporal\n        # ------------------------------------------------------------------------\n        print(f"\\n[3/5] Generando serie temporal...")\n        \n        fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)\n        fig.suptitle(\'Validaci√≥n VLOS_Rolling_Std - Serie Temporal\', \n                     fontsize=16, fontweight=\'bold\')\n        \n        # Subplot 1: Velocidad media del viento\n        axes[0].plot(df_sample[\'Time\'], df_sample[\'Wind_Mean\'], \n                    linewidth=1, color=\'steelblue\', label=\'Viento medio\')\n        axes[0].set_ylabel(\'Velocidad Viento (m/s)\', fontsize=11, fontweight=\'bold\')\n        axes[0].set_title(\'Velocidad Media del Viento\', fontsize=12, fontweight=\'bold\')\n        axes[0].grid(True, alpha=0.3, linestyle=\'--\')\n        axes[0].legend(loc=\'upper right\')\n        \n        # Subplot 2: VLOS_Rolling_Std\n        axes[1].plot(df_sample[\'Time\'], df_sample[\'VLOS_Rolling_Std\'], \n                    linewidth=1, color=\'orange\', label=\'VLOS Rolling Std\')\n        axes[1].set_ylabel(\'Desv. Est√°ndar M√≥vil (m/s)\', fontsize=11, fontweight=\'bold\')\n        axes[1].set_title(\'VLOS_Rolling_Std - Captura Turbulencia Local\', \n                         fontsize=12, fontweight=\'bold\')\n        axes[1].grid(True, alpha=0.3, linestyle=\'--\')\n        axes[1].legend(loc=\'upper right\')\n        \n        # Subplot 3: Cargas en las palas\n        axes[2].plot(df_sample[\'Time\'], df_sample[\'Blade root 1 My\'], \n                    linewidth=0.8, alpha=0.7, label=\'Blade 1 My\')\n        axes[2].plot(df_sample[\'Time\'], df_sample[\'Blade root 2 My\'], \n                    linewidth=0.8, alpha=0.7, label=\'Blade 2 My\')\n        axes[2].set_ylabel(\'Momento Flector (kNm)\', fontsize=11, fontweight=\'bold\')\n        axes[2].set_xlabel(\'Tiempo (s)\', fontsize=11, fontweight=\'bold\')\n        axes[2].set_title(\'Cargas en las Palas\', fontsize=12, fontweight=\'bold\')\n        axes[2].grid(True, alpha=0.3, linestyle=\'--\')\n        axes[2].legend(loc=\'upper right\')\n        \n        plt.tight_layout()\n        \n        plot_path = validation_folder / "time_series_validation.png"\n        plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"‚úÖ Serie temporal: {plot_path.name}")\n        \n        # ------------------------------------------------------------------------\n        # 4) PLOT 2: Histograma de distribuci√≥n\n        # ------------------------------------------------------------------------\n        print(f"\\n[4/5] Generando histograma de distribuci√≥n...")\n        \n        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n        \n        # Subplot 1: Histograma b√°sico\n        axes[0].hist(df_sample[\'VLOS_Rolling_Std\'].dropna(), bins=50, \n                    color=\'teal\', edgecolor=\'black\', alpha=0.7)\n        \n        mean_val = df_sample[\'VLOS_Rolling_Std\'].mean()\n        median_val = df_sample[\'VLOS_Rolling_Std\'].median()\n        \n        axes[0].axvline(mean_val, color=\'red\', linestyle=\'--\', linewidth=2, \n                       label=f\'Media: {mean_val:.3f}\')\n        axes[0].axvline(median_val, color=\'green\', linestyle=\'--\', linewidth=2, \n                       label=f\'Mediana: {median_val:.3f}\')\n        \n        axes[0].set_xlabel(\'VLOS_Rolling_Std (m/s)\', fontsize=11, fontweight=\'bold\')\n        axes[0].set_ylabel(\'Frecuencia\', fontsize=11, fontweight=\'bold\')\n        axes[0].set_title(\'Distribuci√≥n de VLOS_Rolling_Std\', \n                         fontsize=12, fontweight=\'bold\')\n        axes[0].legend(loc=\'upper right\')\n        axes[0].grid(True, alpha=0.3, linestyle=\'--\')\n        \n        # Subplot 2: Box plot\n        bp = axes[1].boxplot(df_sample[\'VLOS_Rolling_Std\'].dropna(), \n                            vert=True, patch_artist=True,\n                            boxprops=dict(facecolor=\'lightblue\', alpha=0.7),\n                            medianprops=dict(color=\'red\', linewidth=2),\n                            whiskerprops=dict(linewidth=1.5),\n                            capprops=dict(linewidth=1.5))\n        \n        axes[1].set_ylabel(\'VLOS_Rolling_Std (m/s)\', fontsize=11, fontweight=\'bold\')\n        axes[1].set_title(\'Box Plot - Detecci√≥n de Outliers\', \n                         fontsize=12, fontweight=\'bold\')\n        axes[1].grid(True, alpha=0.3, linestyle=\'--\', axis=\'y\')\n        \n        plt.tight_layout()\n        \n        plot_path = validation_folder / "distribution_validation.png"\n        plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"‚úÖ Distribuci√≥n: {plot_path.name}")\n        \n        # ------------------------------------------------------------------------\n        # 5) PLOT 3: Correlaciones con cargas\n        # ------------------------------------------------------------------------\n        print(f"\\n[5/5] Generando correlaciones con cargas...")\n        \n        # Calcular rolling std de las cargas (misma ventana)\n        rolling_window_samples = 300  # ~30s con dt=0.1s\n        df_sample[\'Blade1_My_Rolling_Std\'] = df_sample[\'Blade root 1 My\'].rolling(\n            window=rolling_window_samples, min_periods=50, center=True\n        ).std()\n        df_sample[\'Blade2_My_Rolling_Std\'] = df_sample[\'Blade root 2 My\'].rolling(\n            window=rolling_window_samples, min_periods=50, center=True\n        ).std()\n        \n        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n        \n        for idx, (blade_num, blade_col) in enumerate([\n            (1, \'Blade1_My_Rolling_Std\'), \n            (2, \'Blade2_My_Rolling_Std\')\n        ]):\n            ax = axes[idx]\n            \n            # Scatter plot\n            valid_mask = (\n                df_sample[\'VLOS_Rolling_Std\'].notna() & \n                df_sample[blade_col].notna()\n            )\n            \n            scatter = ax.scatter(\n                df_sample.loc[valid_mask, \'VLOS_Rolling_Std\'],\n                df_sample.loc[valid_mask, blade_col],\n                c=df_sample.loc[valid_mask, \'Wind_Mean\'],\n                cmap=\'viridis\',\n                s=20,\n                alpha=0.5,\n                edgecolors=\'none\'\n            )\n            \n            # Colorbar\n            cbar = plt.colorbar(scatter, ax=ax, label=\'Viento medio (m/s)\')\n            cbar.ax.tick_params(labelsize=9)\n            \n            # Calcular correlaci√≥n\n            from scipy.stats import pearsonr\n            x_corr = df_sample.loc[valid_mask, \'VLOS_Rolling_Std\'].values\n            y_corr = df_sample.loc[valid_mask, blade_col].values\n            \n            if len(x_corr) >= 10:\n                corr, p_val = pearsonr(x_corr, y_corr)\n                \n                # L√≠nea de tendencia\n                from scipy.stats import linregress\n                slope, intercept, _, _, _ = linregress(x_corr, y_corr)\n                x_line = np.linspace(x_corr.min(), x_corr.max(), 100)\n                y_line = slope * x_line + intercept\n                ax.plot(x_line, y_line, \'r--\', linewidth=2, alpha=0.8,\n                       label=f\'R={corr:.3f}, p={p_val:.2e}\')\n                ax.legend(loc=\'upper left\', fontsize=10)\n            \n            ax.set_xlabel(\'VLOS_Rolling_Std (m/s)\', fontsize=11, fontweight=\'bold\')\n            ax.set_ylabel(f\'std(Blade {blade_num} My) [kNm]\', fontsize=11, fontweight=\'bold\')\n            ax.set_title(f\'Correlaci√≥n: Turbulencia ‚Üî Variabilidad Carga (Blade {blade_num})\', \n                        fontsize=12, fontweight=\'bold\')\n            ax.grid(True, alpha=0.3, linestyle=\'--\')\n        \n        plt.tight_layout()\n        \n        plot_path = validation_folder / "correlation_with_loads.png"\n        plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"‚úÖ Correlaciones: {plot_path.name}")\n        \n        # ------------------------------------------------------------------------\n        # 6) PLOT 4: VLOS_Rolling_Std vs Velocidad Media\n        # ------------------------------------------------------------------------\n        print(f"\\n[Bonus] Generando relaci√≥n con velocidad media...")\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        \n        scatter = ax.scatter(\n            df_sample[\'Wind_Mean\'],\n            df_sample[\'VLOS_Rolling_Std\'],\n            c=df_sample[\'Time\'],\n            cmap=\'plasma\',\n            s=15,\n            alpha=0.4,\n            edgecolors=\'none\'\n        )\n        \n        cbar = plt.colorbar(scatter, ax=ax, label=\'Tiempo (s)\')\n        cbar.ax.tick_params(labelsize=10)\n        \n        ax.set_xlabel(\'Velocidad Media del Viento (m/s)\', fontsize=12, fontweight=\'bold\')\n        ax.set_ylabel(\'VLOS_Rolling_Std (m/s)\', fontsize=12, fontweight=\'bold\')\n        ax.set_title(\'VLOS_Rolling_Std vs Velocidad Media\\n(Turbulencia vs Intensidad del Viento)\', \n                    fontsize=14, fontweight=\'bold\')\n        ax.grid(True, alpha=0.3, linestyle=\'--\')\n        \n        plt.tight_layout()\n        \n        plot_path = validation_folder / "rolling_std_vs_wind_mean.png"\n        plt.savefig(plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"‚úÖ Relaci√≥n con viento: {plot_path.name}")\n        \n        # ------------------------------------------------------------------------\n        # RESUMEN Y DIAGN√ìSTICO\n        # ------------------------------------------------------------------------\n        print(f"\\n{\'=\'*70}")\n        print("RESUMEN - VALIDACI√ìN VLOS_Rolling_Std")\n        print("="*70)\n        \n        print(f"\\nüìä ESTAD√çSTICAS:")\n        print(f"   M√≠nimo:       {df_sample[\'VLOS_Rolling_Std\'].min():.4f} m/s")\n        print(f"   M√°ximo:       {df_sample[\'VLOS_Rolling_Std\'].max():.4f} m/s")\n        print(f"   Media:        {df_sample[\'VLOS_Rolling_Std\'].mean():.4f} m/s")\n        print(f"   Mediana:      {df_sample[\'VLOS_Rolling_Std\'].median():.4f} m/s")\n        print(f"   Desv. Std:    {df_sample[\'VLOS_Rolling_Std\'].std():.4f} m/s")\n        print(f"   NaNs:         {df_sample[\'VLOS_Rolling_Std\'].isna().sum()} ({df_sample[\'VLOS_Rolling_Std\'].isna().sum()/len(df_sample)*100:.2f}%)")\n        \n        print(f"\\nüîç VALIDACI√ìN VISUAL:")\n        print("   ‚úÖ Serie temporal: ¬øCaptura picos de turbulencia?")\n        print("   ‚úÖ Distribuci√≥n: ¬øForma coherente sin outliers an√≥malos?")\n        print("   ‚úÖ Correlaci√≥n: ¬øSe correlaciona con variabilidad de cargas?")\n        print("   ‚úÖ Relaci√≥n viento: ¬øAumenta con velocidad o es independiente?")\n        \n        # Calcular correlaciones con cargas\n        valid_mask = (\n            df_sample[\'VLOS_Rolling_Std\'].notna() & \n            df_sample[\'Blade1_My_Rolling_Std\'].notna() &\n            df_sample[\'Blade2_My_Rolling_Std\'].notna()\n        )\n        \n        if valid_mask.sum() >= 10:\n            corr1, p1 = pearsonr(\n                df_sample.loc[valid_mask, \'VLOS_Rolling_Std\'],\n                df_sample.loc[valid_mask, \'Blade1_My_Rolling_Std\']\n            )\n            corr2, p2 = pearsonr(\n                df_sample.loc[valid_mask, \'VLOS_Rolling_Std\'],\n                df_sample.loc[valid_mask, \'Blade2_My_Rolling_Std\']\n            )\n            \n            print(f"\\nüìà CORRELACIONES CON CARGAS:")\n            print(f"   Blade 1: r = {corr1:.3f}, p = {p1:.2e}")\n            print(f"   Blade 2: r = {corr2:.3f}, p = {p2:.2e}")\n            \n            if corr1 > 0.3 or corr2 > 0.3:\n                print("\\n   ‚úÖ VALIDADO: Correlaci√≥n positiva detectada")\n                print("      ‚Üí Mayor turbulencia ‚Üí Mayor variabilidad de carga")\n                print("      ‚Üí La feature captura informaci√≥n relevante")\n            else:\n                print("\\n   ‚ö†Ô∏è Correlaci√≥n d√©bil")\n                print("      ‚Üí Puede indicar que la ventana m√≥vil es incorrecta")\n                print("      ‚Üí O que la turbulencia no es el factor dominante en esta muestra")\n        \n        print(f"\\nüí° USO EN MODELO ML:")\n        print("   - Feature para predecir picos/variabilidad de cargas")\n        print("   - Complementa velocidad media con informaci√≥n de turbulencia local")\n        print("   - √ötil en modelos que predicen distribuciones (no solo media)")\n        \n        print(f"\\nüìÅ Archivos generados:")\n        print(f"   - time_series_validation.png")\n        print(f"   - distribution_validation.png")\n        print(f"   - correlation_with_loads.png")\n        print(f"   - rolling_std_vs_wind_mean.png")\n        \n        print(f"\\nüìç Ubicaci√≥n: {validation_folder}")\n        print("="*70)\n        print("\\n‚úÖ Validaci√≥n visual completada")\n',
            '# =============================================================================\n# PASO 5.1: A√ëADIR DESVIACI√ìN EST√ÅNDAR M√ìVIL VLOS AL DATASET COMPLETO\n# =============================================================================\n\nprint("\\n" + "="*70)\nprint("üìä FEATURE ENGINEERING: VLOS_Rolling_Std")\nprint("="*70)\n\n# Par√°metros\nrolling_window_seconds = 30  # Ventana m√≥vil en segundos\ndt_estimated = 0.02          # Tiempo de muestreo estimado (s)\nrolling_window_samples = int(rolling_window_seconds / dt_estimated)\n\nprint(f"Ventana m√≥vil: {rolling_window_seconds}s (~{rolling_window_samples} muestras)")\nprint(f"Dataset: {complete_dataset_path.name}")\nprint("="*70)\n\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\nelse:\n    # ------------------------------------------------------------------------\n    # 1) Verificar columnas VLOS sin lag\n    # ------------------------------------------------------------------------\n    print(f"\\n[1/5] Verificando columnas VLOS sin lag...")\n    \n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n    \n    # Columnas VLOS sin lag\n    vlos_base_cols = [c for c in all_columns if \'LAC_VLOS\' in c and \'lag\' not in c]\n    \n    if not vlos_base_cols:\n        print("\\n‚ùå ERROR: No se encontraron columnas VLOS sin lag")\n    else:\n        print(f"‚úÖ Columnas VLOS sin lag detectadas: {len(vlos_base_cols)}")\n        for col in vlos_base_cols:\n            print(f"   - {col}")\n        \n        # Verificar si ya existe VLOS_Rolling_Std\n        if \'VLOS_Rolling_Std\' in all_columns:\n            print("\\n‚ö†Ô∏è  La columna VLOS_Rolling_Std ya existe en el dataset")\n            overwrite = input("   ¬øDeseas recalcularla? (s/n): ")\n            if overwrite.lower() != \'s\':\n                print("   Operaci√≥n cancelada")\n                raise SystemExit\n        \n        # ------------------------------------------------------------------------\n        # 2) Cargar datos y detectar segmentos (series temporales concatenadas)\n        # ------------------------------------------------------------------------\n        print(f"\\n[2/6] Cargando dataset y detectando series temporales concatenadas...")\n        \n        # Cargar solo las columnas necesarias\n        usecols = vlos_base_cols + [\'Time\']\n        df = pd.read_csv(complete_dataset_path, usecols=usecols)\n        \n        print(f"‚úÖ Cargado: {len(df):,} filas, {len(usecols)} columnas")\n        \n        # Detectar reinicios de tiempo (series concatenadas)\n        time_diff = df[\'Time\'].diff()\n        restarts = np.where(time_diff < 0)[0]  # √çndices donde Time reinicia\n        \n        # Crear segmentos\n        segment_starts = [0] + restarts.tolist()\n        segment_ends = restarts.tolist() + [len(df)]\n        \n        print(f"üìå Series temporales detectadas: {len(segment_starts)}")\n        \n        # Verificar dt real en el primer segmento\n        first_segment = df.iloc[segment_starts[0]:segment_ends[0]]\n        dt_real = first_segment[\'Time\'].diff().abs().median()\n        rolling_window_samples = int(rolling_window_seconds / dt_real)\n        \n        print(f"üìå dt real: {dt_real:.3f}s")\n        print(f"üìå Ventana ajustada: {rolling_window_samples} muestras")\n        \n        # ------------------------------------------------------------------------\n        # 3) Calcular rolling std POR SEGMENTO para cada BEAM\n        # ------------------------------------------------------------------------\n        print(f"\\n[3/6] Calculando rolling std por segmento para cada BEAM...")\n        \n        # Inicializar array para almacenar resultados\n        vlos_rolling_std_full = np.full(len(df), np.nan)\n        \n        for col in vlos_base_cols:\n            print(f"   Procesando {col}...")\n            col_rolling_std = np.full(len(df), np.nan)\n            \n            # Procesar cada segmento independientemente\n            for seg_idx, (start_idx, end_idx) in enumerate(zip(segment_starts, segment_ends)):\n                segment_data = df[col].iloc[start_idx:end_idx]\n                \n                # Calcular rolling std solo dentro de este segmento\n                segment_rolling = segment_data.rolling(\n                    window=rolling_window_samples,\n                    min_periods=max(1, rolling_window_samples // 2),\n                    center=True\n                ).std()\n                \n                # Asignar resultados al array completo\n                col_rolling_std[start_idx:end_idx] = segment_rolling.values\n            \n            # Acumular para promediar despu√©s\n            rolling_stds.append(col_rolling_std)\n        \n        # ------------------------------------------------------------------------\n        # 4) Promediar rolling stds de todos los BEAMs\n        # ------------------------------------------------------------------------\n        print(f"\\n[4/6] Promediando rolling stds de todos los BEAMs...")\n        \n        rolling_stds_array = np.array(rolling_stds)\n        vlos_rolling_std = np.nanmean(rolling_stds_array, axis=0)\n        \n        print(f"‚úÖ VLOS_Rolling_Std calculado")\n        print(f"   M√≠nimo: {np.nanmin(vlos_rolling_std):.3f} m/s")\n        print(f"   M√°ximo: {np.nanmax(vlos_rolling_std):.3f} m/s")\n        print(f"   Media: {np.nanmean(vlos_rolling_std):.3f} m/s")\n        print(f"   NaNs: {np.isnan(vlos_rolling_std).sum()} ({np.isnan(vlos_rolling_std).sum()/len(vlos_rolling_std)*100:.2f}%)")\n        \n        # ------------------------------------------------------------------------\n        # 5) A√±adir columna al dataset completo con procesamiento por chunks\n        # ------------------------------------------------------------------------\n        print(f"\\n[5/6] A√±adiendo columna VLOS_Rolling_Std al dataset (procesamiento por chunks)...")\n        \n        # Crear backup si no existe\n        backup_path = complete_dataset_path.parent / f"{complete_dataset_path.stem}_backup{complete_dataset_path.suffix}"\n        \n        if not backup_path.exists():\n            print(f"   Creando backup: {backup_path.name}")\n            import shutil\n            shutil.copy2(complete_dataset_path, backup_path)\n        \n        # Crear archivo temporal con la nueva columna\n        temp_output_path = complete_dataset_path.parent / "temp_with_rolling_std.csv"\n        \n        print(f"   Procesando dataset por chunks...")\n        chunk_size = 50000  # Chunks de 50k filas\n        first_chunk = True\n        row_idx = 0\n        \n        for chunk in pd.read_csv(complete_dataset_path, chunksize=chunk_size):\n            # A√±adir la columna calculada a este chunk\n            chunk_end = min(row_idx + len(chunk), len(vlos_rolling_std))\n            chunk[\'VLOS_Rolling_Std\'] = vlos_rolling_std[row_idx:chunk_end]\n            \n            # Escribir chunk (con header solo en el primero)\n            mode = \'w\' if first_chunk else \'a\'\n            header = first_chunk\n            chunk.to_csv(temp_output_path, index=False, mode=mode, header=header)\n            \n            row_idx += len(chunk)\n            first_chunk = False\n            \n            if row_idx % 200000 == 0:\n                print(f"   Procesadas {row_idx:,} filas...")\n        \n        print(f"   Total procesadas: {row_idx:,} filas")\n        \n        # Reemplazar archivo original con el temporal\n        print(f"   Reemplazando dataset original...")\n        import os\n        os.replace(temp_output_path, complete_dataset_path)\n        \n        print(f"\\n‚úÖ Dataset actualizado exitosamente")\n        print(f"   Archivo: {complete_dataset_path.name}")\n        print(f"   Total filas: {row_idx:,}")\n        print(f"   Total columnas: {len(df_complete.columns)}")\n        print(f"   Nueva feature: VLOS_Rolling_Std")\n        \n        # ------------------------------------------------------------------------\n        # 6) RESUMEN Y VALIDACI√ìN\n        # ------------------------------------------------------------------------\n        # ------------------------------------------------------------------------\n        # RESUMEN\n        # ------------------------------------------------------------------------\n        print(f"\\n{\'=\'*70}")\n        print("RESUMEN - VLOS_Rolling_Std")\n        print("="*70)\n        print(f"Ventana m√≥vil:          {rolling_window_seconds}s ({rolling_window_samples} muestras)")\n        print(f"BEAMs promediados:       {len(vlos_base_cols)}")\n        print(f"Filas procesadas:        {len(df_complete):,}")\n        print(f"")\n        print(f"üìä Estad√≠sticas VLOS_Rolling_Std:")\n        print(f"   M√≠nimo:  {vlos_rolling_std.min():.4f} m/s")\n        print(f"   M√°ximo:  {vlos_rolling_std.max():.4f} m/s")\n        print(f"   Media:   {vlos_rolling_std.mean():.4f} m/s")\n        print(f"   Mediana: {vlos_rolling_std.median():.4f} m/s")\n        print(f"   Std:     {vlos_rolling_std.std():.4f} m/s")\n        print(f"")\n        print(f"üí° INTERPRETACI√ìN:")\n        print(f"   - Valores altos ‚Üí Turbulencia local / R√°fagas")\n        print(f"   - Valores bajos ‚Üí Flujo estable")\n        print(f"   - Captura variabilidad temporal no visible en la media")\n        print(f"")\n        print(f"üéØ USO EN MODELO ML:")\n        print(f"   - Feature adicional para predecir cargas")\n        print(f"   - Especialmente √∫til para predecir variabilidad/picos")\n        print(f"   - Complementa TI con informaci√≥n temporal local")\n        print("="*70)\n        print("\\n‚úÖ Feature Engineering completado")',
        ],
    },
    {
        "title": 'ANALISIS DE LAS FEATURES',
        "cells": [
            '# ============================================================================\n# INSPECCI√ìN DE COLUMNAS DEL DATASET COMPLETO\n# ============================================================================\n\nimport pandas as pd\nfrom pathlib import Path\n\nprint("="*70)\nprint("INSPECCI√ìN DE COLUMNAS DEL DATASET COMPLETO")\nprint("="*70)\n\n# Ruta del dataset COMPLETO\ncomplete_data_path = Path("C:\\\\Users\\\\aitorredondoruiz\\\\Desktop\\\\2B_energy\\\\__Git\\\\Lidar_My_validation_VLOS\\\\data_train_traditional_ML\\\\0000_Complete_dataset.csv")\n\nif not complete_data_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_data_path}")\nelse:\n    print(f"\\nArchivo: {complete_data_path}")\n    \n    # Leer solo el header (primera fila)\n    df_header = pd.read_csv(complete_data_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n    \n    print(f"\\nüìä Total de columnas: {len(all_columns)}")\n    \n    # ========================================================================\n    # CATEGORIZAR COLUMNAS POR TIPO\n    # ========================================================================\n    \n    # Targets (momentos Coleman)\n    target_cols = [col for col in all_columns if col in [\'M_0\', \'M_1c\', \'M_1s\', \'M_2c\', \'M_2s\']]\n    \n    # Targets originales (por si a√∫n existen)\n    target_original = [col for col in all_columns if \'Blade root\' in col and \'My\' in col]\n    \n    # Time\n    time_cols = [col for col in all_columns if col == \'Time\']\n    \n    # Yaw Error (original y componentes sin/cos)\n    yaw_error_cols = [col for col in all_columns if col in [\'sin_yawerror\', \'cos_yawerror\']]\n    \n    # VLOS (sin lag)\n    vlos_base = [col for col in all_columns if \'LAC_VLOS\' in col and \'lag\' not in col.lower()]\n    \n    # VLOS lags\n    vlos_lags = [col for col in all_columns if \'LAC_VLOS\' in col and \'lag\' in col.lower()]\n    \n    # Estad√≠sticas de viento (sin lag)\n    wind_stats_base = [col for col in all_columns if col in [\'U_mean\', \'U_std\', \'U_shear_vert\', \'U_shear_horiz\']]\n    \n    # Estad√≠sticas de viento (con lag)\n    wind_stats_lags = [col for col in all_columns if any(col.startswith(f\'{base}_lag\') for base in [\'U_mean\', \'U_std\', \'U_shear_vert\', \'U_shear_horiz\'])]\n    \n    # Pitch Coleman (sin rate)\n    pitch_coleman = [col for col in all_columns if col in [\'pitch_0\', \'pitch_1c\', \'pitch_1s\']]\n    \n    # Pitch Coleman rates\n    pitch_rates = [col for col in all_columns if col in [\'pitch_0_rate\', \'pitch_1c_rate\', \'pitch_1s_rate\']]\n    \n    # Rotor speed y derivatives\n    rotor_cols = [col for col in all_columns if \'Rotor speed\' in col or \'rotor_speed_rate\' in col]\n    \n    # Azimuth (sin/cos)\n    azimuth_cols = [col for col in all_columns if \'azimuth\' in col.lower()]\n    \n    # Pitch original (blades)\n    pitch_original = [col for col in all_columns if \'Blade\' in col and \'pitch angle\' in col]\n    \n    # Otras columnas\n    other_cols = [col for col in all_columns if col not in \n                  target_cols + target_original + time_cols + yaw_error_cols + vlos_base + vlos_lags + \n                  wind_stats_base + wind_stats_lags + pitch_coleman + pitch_rates + \n                  rotor_cols + azimuth_cols + pitch_original]\n    \n    # ========================================================================\n    # MOSTRAR RESUMEN POR CATEGOR√çAS\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("RESUMEN POR CATEGOR√çAS")\n    print("="*70)\n    \n    categories = [\n        ("üéØ TARGETS (Coleman)", target_cols),\n        ("üéØ TARGETS (Originales)", target_original),\n        ("‚è∞ TIME", time_cols),\n        ("üß≠ YAW ERROR", yaw_error_cols),\n        ("üå¨Ô∏è  VLOS Base (sin lag)", vlos_base),\n        ("üå¨Ô∏è  VLOS Lags", vlos_lags),\n        ("üìä Estad√≠sticas Viento Base", wind_stats_base),\n        ("üìä Estad√≠sticas Viento Lags", wind_stats_lags),\n        ("üîß Pitch Coleman", pitch_coleman),\n        ("üîß Pitch Rates", pitch_rates),\n        ("‚öôÔ∏è  Rotor Speed", rotor_cols),\n        ("üîÑ Azimuth (sin/cos)", azimuth_cols),\n        ("üîß Pitch Original (Blades)", pitch_original),\n        ("‚ùì Otras", other_cols),\n    ]\n    \n    for category_name, cols in categories:\n        if len(cols) > 0:\n            print(f"\\n{category_name}: {len(cols)} columnas")\n            if len(cols) <= 10:\n                for col in cols:\n                    print(f"  - {col}")\n            else:\n                print(f"  Primeras 5:")\n                for col in cols[:5]:\n                    print(f"  - {col}")\n                print(f"  ...")\n                print(f"  √öltimas 3:")\n                for col in cols[-3:]:\n                    print(f"  - {col}")\n    \n    # ========================================================================\n    # CONTEO DE LAGS\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("AN√ÅLISIS DE LAGS")\n    print("="*70)\n    \n    # Detectar lags √∫nicos\n    import re\n    lag_times = set()\n    \n    for col in vlos_lags + wind_stats_lags:\n        match = re.search(r\'lag(\\d+)s\', col)\n        if match:\n            lag_times.add(int(match.group(1)))\n    \n    lag_times = sorted(lag_times)\n    \n    if len(lag_times) > 0:\n        print(f"\\nLags detectados: {lag_times[0]}s - {lag_times[-1]}s ({len(lag_times)} lags)")\n        print(f"Lags: {lag_times}")\n    \n    # Contar VLOS lags por BEAM\n    vlos_beams = set()\n    for col in vlos_lags:\n        match = re.search(r\'BEAM(\\d+)\', col)\n        if match:\n            vlos_beams.add(int(match.group(1)))\n    \n    vlos_beams = sorted(vlos_beams)\n    print(f"\\nVLOS - BEAMs detectados: {vlos_beams}")\n    print(f"VLOS - Lags por BEAM: {len(vlos_lags) // len(vlos_beams) if len(vlos_beams) > 0 else 0}")\n    \n    # ========================================================================\n    # FEATURES vs TARGETS\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("FEATURES vs TARGETS")\n    print("="*70)\n    \n    feature_cols = [col for col in all_columns if col not in target_cols + target_original + time_cols]\n    \n    print(f"\\n‚úÖ FEATURES (X): {len(feature_cols)} columnas")\n    print(f"‚úÖ TARGETS (y): {len(target_cols)} columnas: {target_cols}")\n    print(f"‚è∞ TIME: {len(time_cols)} columna: {time_cols}")\n    print(f"üß≠ YAW ERROR: {len(yaw_error_cols)} columna: {yaw_error_cols}")\n    \n    # ========================================================================\n    # VERIFICAR EXISTENCIA DE NUEVAS FEATURES\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("VERIFICACI√ìN DE NUEVAS FEATURES")\n    print("="*70)\n    \n    checks = [\n        ("Yaw Error sin/cos (sin_yawerror, cos_yawerror)",\n         all([col in all_columns for col in [\'sin_yawerror\', \'cos_yawerror\']])),\n        ("Pitch Coleman (pitch_0, pitch_1c, pitch_1s)", \n         all([col in all_columns for col in [\'pitch_0\', \'pitch_1c\', \'pitch_1s\']])),\n        ("Pitch Rates (pitch_0_rate, pitch_1c_rate, pitch_1s_rate)",\n         all([col in all_columns for col in [\'pitch_0_rate\', \'pitch_1c_rate\', \'pitch_1s_rate\']])),\n        ("Rotor Speed Rate (rotor_speed_rate)",\n         \'rotor_speed_rate\' in all_columns),\n        ("Estad√≠sticas Viento Base (U_mean, U_std, U_shear_vert, U_shear_horiz)",\n         all([col in all_columns for col in [\'U_mean\', \'U_std\', \'U_shear_vert\', \'U_shear_horiz\']])),\n        ("Estad√≠sticas Viento Lags",\n         len(wind_stats_lags) > 0),\n        ("Targets Coleman (M_0, M_1c, M_1s)",\n         all([col in all_columns for col in [\'M_0\', \'M_1c\', \'M_1s\']])),\n        ("VLOS Lags",\n         len(vlos_lags) > 0),\n        ("Azimuth sin/cos (sin_rotor_azimuth, cos_rotor_azimuth)",\n         all([col in all_columns for col in [\'sin_rotor_azimuth\', \'cos_rotor_azimuth\']])),\n    ]\n    \n    for feature_name, exists in checks:\n        status = "‚úÖ" if exists else "‚ùå"\n        print(f"{status} {feature_name}")\n    \n    # ========================================================================\n    # GUARDAR LISTA DE COLUMNAS EN TXT\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("GUARDANDO LISTA DE COLUMNAS")\n    print("="*70)\n    \n    output_path = Path("C:\\\\Users\\\\aitorredondoruiz\\\\Desktop\\\\2B_energy\\\\__Git\\\\Lidar_My_validation_VLOS\\\\data_train_traditional_ML\\\\complete_dataset_columns_summary.txt")\n    \n    with open(output_path, \'w\', encoding=\'utf-8\') as f:\n        f.write("="*70 + "\\n")\n        f.write(f"COLUMNAS DEL DATASET: {complete_data_path.name}\\n")\n        f.write(f"Total: {len(all_columns)} columnas\\n")\n        f.write("="*70 + "\\n\\n")\n        \n        for category_name, cols in categories:\n            if len(cols) > 0:\n                f.write(f"\\n{category_name}: {len(cols)} columnas\\n")\n                f.write("-" * 50 + "\\n")\n                for col in cols:\n                    f.write(f"  - {col}\\n")\n        \n        f.write("\\n" + "="*70 + "\\n")\n        f.write(f"FEATURES (X): {len(feature_cols)}\\n")\n        f.write(f"TARGETS (y): {len(target_cols)}\\n")\n        f.write("="*70 + "\\n")\n        \n        f.write("\\n" + "="*70 + "\\n")\n        f.write("LISTA COMPLETA DE COLUMNAS (en orden)\\n")\n        f.write("="*70 + "\\n")\n        for i, col in enumerate(all_columns, 1):\n            f.write(f"{i:3d}. {col}\\n")\n    \n    print(f"‚úÖ Lista guardada en: {output_path}")\n    \n    # ========================================================================\n    # MOSTRAR TODAS LAS COLUMNAS (OPCIONAL)\n    # ========================================================================\n    \n    print(f"\\n{\'=\'*70}")\n    print("LISTA COMPLETA DE COLUMNAS")\n    print("="*70)\n    print("(Mostrando primeras 50 y √∫ltimas 20)\\n")\n    \n    print("Primeras 50:")\n    for i, col in enumerate(all_columns[:50], 1):\n        print(f\'{i:3d}. {col}\')\n    \n    if len(all_columns) > 70:\n        print("\\n... [columnas intermedias omitidas] ...\\n")\n        \n        print(f"√öltimas 20:")\n        start_idx = len(all_columns) - 20\n        for i, col in enumerate(all_columns[-20:], start_idx + 1):\n            print(f\'{i:3d}. {col}\')\n    \n    print(f"\\nüí° Para ver TODAS las columnas, consulta el archivo:")\n    print(f"   {output_path}")\n\nprint(f"\\n{\'=\'*70}")\nprint("INSPECCI√ìN COMPLETADA")\nprint("="*70)',
        ],
    },
    {
        "title": 'ü§ñ PARTE 3: MACHINE LEARNING',
        "cells": [
            '# =============================================================================\n# PASO 6.1: CARGA DEL DATASET Y DEFINICI√ìN DE X e Y\n# =============================================================================\n\nprint("\\n" + "="*70)\nprint("ü§ñ MACHINE LEARNING: Preparaci√≥n de Datos")\nprint("="*70)\n\n# Verificar que existe el dataset completo\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\n    print("   Aseg√∫rate de haber ejecutado los pasos anteriores")\nelse:\n    print(f"Dataset: {complete_dataset_path.name}")\n    print("="*70)\n    \n    # ------------------------------------------------------------------------\n    # 1) Leer cabecera para identificar columnas disponibles\n    # ------------------------------------------------------------------------\n    print(f"\\n[1/4] Leyendo cabecera del dataset...")\n    \n    df_header = pd.read_csv(complete_dataset_path, nrows=0)\n    all_columns = df_header.columns.tolist()\n    \n    print(f"‚úÖ Total columnas disponibles: {len(all_columns)}")\n    \n    # ------------------------------------------------------------------------\n    # 2) Definir TARGETS (y)\n    # ------------------------------------------------------------------------\n    print(f"\\n[2/4] Definiendo targets (y)...")\n    \n    target_cols = [\'M_0\', \'M_1c\', \'M_1s\',]\n    \n    # Verificar que existen\n    missing_targets = [col for col in target_cols if col not in all_columns]\n    if missing_targets:\n        print(f"\\n‚ùå ERROR: Targets faltantes: {missing_targets}")\n        raise ValueError("No se encontraron todas las columnas target")\n    \n    print(f"‚úÖ Targets definidos:")\n    for i, target in enumerate(target_cols, 1):\n        print(f"   {i}. {target}")\n    \n    # ------------------------------------------------------------------------\n    # 3) Definir FEATURES (X)\n    # ------------------------------------------------------------------------\n    print(f"\\n[3/4] Definiendo features (X)...")\n    \n    # Features base (operacionales)\n    base_features = [\n        \n        \'Rotor speed\',\n        \'Blade 1 pitch angle\',\n        \'Blade 2 pitch angle\',\n        \'sin_rotor_azimuth\',\n        \'cos_rotor_azimuth\',\n        \'OPER_MEAS_YAWERROR\',\n        \'Extreme_condition\',\n        \'TI\',\n        \'Yaw_rate_rad_s\',\n        \'U_mean\',\n        \'U_std\',\n        \'U_shear_vert\',\n        \'U_shear_horiz\',\n        \'U_mean_lag2s\',\n        \'U_mean_lag5s\',\n        \'U_mean_lag8s\',\n        \'U_mean_lag11s\',\n        \'U_mean_lag14s\',\n        \'U_mean_lag17s\',\n        \'U_mean_lag20s\',\n        \'U_mean_lag23s\',\n        \'U_mean_lag26s\',\n        \'U_std_lag2s\',\n        \'U_std_lag5s\',\n        \'U_std_lag8s\',\n        \'U_std_lag11s\',\n        \'U_std_lag14s\',\n        \'U_std_lag17s\',\n        \'U_std_lag20s\',\n        \'U_std_lag23s\',\n        \'U_std_lag26s\',\n        \'U_shear_vert_lag2s\',\n        \'U_shear_vert_lag5s\',\n        \'U_shear_vert_lag8s\',\n        \'U_shear_vert_lag11s\',\n        \'U_shear_vert_lag14s\',\n        \'U_shear_vert_lag17s\',\n        \'U_shear_vert_lag20s\',\n        \'U_shear_vert_lag23s\',\n        \'U_shear_vert_lag26s\',\n        \'U_shear_horiz_lag2s\',\n        \'U_shear_horiz_lag5s\',\n        \'U_shear_horiz_lag8s\',\n        \'U_shear_horiz_lag11s\',\n        \'U_shear_horiz_lag14s\',\n        \'U_shear_horiz_lag17s\',\n        \'U_shear_horiz_lag20s\',\n        \'U_shear_horiz_lag23s\',\n        \'U_shear_horiz_lag26s\',\n        \'pitch_0\',\n        \'pitch_1c\',\n        \'pitch_1s\',\n        \'pitch_0_rate\',\n        \'pitch_1c_rate\',\n        \'pitch_1s_rate\',\n    ]\n    \n    # Verificar que existen\n    missing_base = [col for col in base_features if col not in all_columns]\n    if missing_base:\n        print(f"\\n‚ö†Ô∏è  Features base faltantes: {missing_base}")\n        base_features = [col for col in base_features if col in all_columns]\n    \n    print(f"\\nüìã Features base (operacionales):")\n    for feat in base_features:\n        print(f"   - {feat}")\n    \n    # Features LIDAR (autom√°ticas: todas con \'LAC_VLOS\' o \'lag\')\n    lidar_features = [\n       col for col in all_columns \n       if \'LAC_VLOS\' in col  # Incluye LAC_VLOS_BEAM0_RANGE5 y LAC_VLOS_BEAM0_RANGE5_lag5s\n    ]\n    \n    print(f"\\nüåÄ Features LIDAR (LAC_VLOS + lag):")\n    print(f"   Total detectadas: {len(lidar_features)}")\n    \n    # Mostrar algunos ejemplos\n    if len(lidar_features) > 0:\n        print(f"   Ejemplos:")\n        for feat in lidar_features[:5]:\n            print(f"   - {feat}")\n        if len(lidar_features) > 5:\n            print(f"   - ... y {len(lidar_features) - 5} m√°s")\n    \n    # Features engineered (si existen)\n    engineered_features = []\n    if \'No_inlcuir_esto\' in all_columns:\n        engineered_features.append(\'VLOS_Rolling_Std\')\n    \n    if engineered_features:\n        print(f"\\nüîß Features engineered:")\n        for feat in engineered_features:\n            print(f"   - {feat}")\n    \n    # Combinar todas las features\n    feature_cols = base_features + lidar_features \n    # feature_cols = base_features + lidar_features + engineered_features\n    \n    print(f"\\n{\'=\'*70}")\n    print(f"üìä RESUMEN DE FEATURES:")\n    print(f"   - Features base:       {len(base_features)}")\n    print(f"   - Features LIDAR:      {len(lidar_features)}")\n    # print(f"   - Features engineered: {len(engineered_features)}")\n    print(f"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")\n    print(f"   - TOTAL FEATURES (X):  {len(feature_cols)}")\n    print(f"   - TOTAL TARGETS (y):   {len(target_cols)}")\n    print("="*70)\n    \n    # ------------------------------------------------------------------------\n    # 4) Cargar dataset completo por chunks (OPTIMIZADO)\n    # ------------------------------------------------------------------------\n    print(f"\\n[4/4] Cargando dataset completo (por chunks optimizados)...")\n    \n    # Columnas a cargar: features + targets + Time (para identificar series)\n    usecols = feature_cols + target_cols\n    \n    # Agregar Time si existe (necesario para identificar series temporales)\n    if \'Time\' in all_columns:\n        usecols.append(\'Time\')\n        print("\\n   ‚úÖ Columna \'Time\' incluida para identificaci√≥n de series temporales")\n    else:\n        print("\\n   ‚ö†Ô∏è  Columna \'Time\' no encontrada - No se podr√° hacer split por series")\n    \n    # Verificar que todas existen\n    missing_cols = [col for col in usecols if col not in all_columns]\n    if missing_cols:\n        print(f"\\n‚ö†Ô∏è  Columnas faltantes (se omitir√°n): {missing_cols}")\n        usecols = [col for col in usecols if col in all_columns]\n    \n    print(f"   Total columnas a cargar: {len(usecols)}")\n    \n    # ‚ö° OPTIMIZACI√ìN: Chunks m√°s peque√±os + dtypes optimizados\n    chunk_size = 5000  # Reducido de 100,000 a 5,000 para evitar MemoryError\n    \n    # Definir dtypes optimizados (float32 en vez de float64 ahorra 50% de memoria)\n    dtype_dict = {col: \'float32\' for col in usecols}\n    \n    chunks = []\n    \n    print(f"\\n   üì¶ Configuraci√≥n de carga:")\n    print(f"      - Chunk size: {chunk_size:,} filas")\n    print(f"      - Dtype: float32 (ahorro 50% memoria vs float64)")\n    print(f"      - Memoria estimada: ~{len(usecols) * 4 / 1024:.1f} MB por chunk")\n    print(f"\\n   ‚è≥ Leyendo chunks...")\n    \n    chunk_count = 0\n    try:\n        for chunk in pd.read_csv(\n            complete_dataset_path, \n            usecols=usecols, \n            chunksize=chunk_size,\n            dtype=dtype_dict,\n            low_memory=True\n        ):\n            chunks.append(chunk)\n            chunk_count += 1\n            \n            # Mostrar progreso cada 50 chunks (250k filas)\n            if chunk_count % 50 == 0:\n                total_rows_loaded = chunk_count * chunk_size\n                print(f"      ‚úì Cargadas ~{total_rows_loaded:,} filas ({chunk_count} chunks)...")\n        \n        print(f"\\n   ‚úÖ Lectura completada: {chunk_count} chunks le√≠dos")\n        \n    except Exception as e:\n        print(f"\\n   ‚ùå ERROR al leer chunks: {str(e)}")\n        if chunks:\n            print(f"      Se cargaron {len(chunks)} chunks antes del error")\n            print(f"      Continuando con datos parciales...")\n        else:\n            raise\n    \n    # Concatenar todos los chunks\n    print(f"\\n   üîó Concatenando {len(chunks)} chunks...")\n    df_ml = pd.concat(chunks, ignore_index=True)\n    del chunks  # Liberar memoria\n    # 5) Separar X e y (manteniendo Time como metadata)\n    print(f"\\n‚úÖ Dataset cargado exitosamente")\n    print(f"   Total filas: {len(df_ml):,}")\n    print(f"   Total columnas: {len(df_ml.columns)}")\n    print(f"   Memoria utilizada: {df_ml.memory_usage(deep=True).sum() / 1024**2:.1f} MB")\n    \n    # ------------------------------------------------------------------------\n    # 5) Separar X e y\n    # ------------------------------------------------------------------------\n    print(f"\\n{\'=\'*70}")\n    print("üì¶ CREACI√ìN DE MATRICES X e Y")\n    print("="*70)\n    \n    # Verificar que los targets est√°n en el dataframe\n    available_targets = [col for col in target_cols if col in df_ml.columns]\n    available_features = [col for col in feature_cols if col in df_ml.columns]\n    \n    if len(available_targets) == 0:\n    # Crear matrices (sin Time, que se mantendr√° en df_ml para el split)\n        raise ValueError("Targets no disponibles")\n    \n    if len(available_features) == 0:\n        print("\\n‚ùå ERROR: No se encontr√≥ ninguna feature")\n        raise ValueError("Features no disponibles")\n    \n    # Crear matrices\n    X = df_ml[available_features].copy()\n    y = df_ml[available_targets].copy()\n    # Verificar si Time est√° disponible\n    if \'Time\' in df_ml.columns:\n        print(f"   ‚ÑπÔ∏è  \'Time\' disponible en df_ml para identificar series temporales")\n    else:\n        print(f"   ‚ö†Ô∏è  \'Time\' no disponible - Split ser√° secuencial simple")\n    \n    # Verificar NaNs\n    nan_features = X.isna().sum().sum()\n    nan_targets = y.isna().sum().sum()\n    \n    print(f"\\nüîç Verificaci√≥n de NaNs:")\n    print(f"   NaNs en X: {nan_features:,} ({nan_features / X.size * 100:.2f}%)")\n    print(f"   NaNs en y: {nan_targets:,} ({nan_targets / y.size * 100:.2f}%)")\n    print(f"   NaNs en y: {nan_targets:,} ({nan_targets / y.size * 100:.2f}%)")\n    if nan_features > 0 or nan_targets > 0:\n        print(f"\\n‚ö†Ô∏è  Se detectaron NaNs. Opciones:")\n        print(f"      1. Eliminar filas con NaNs: df_ml.dropna()")\n        print(f"      2. Imputar valores: SimpleImputer")\n        print(f"      3. Dejar como est√° si el modelo lo soporta (XGBoost, LightGBM)")\n    \n    # Estad√≠sticas b√°sicas de X\n    print(f"\\nüìä ESTAD√çSTICAS DE FEATURES (X):")\n    print(f"\\n   Top 5 features con mayor varianza:")\n    feature_variances = X.var().sort_values(ascending=False).head(5)\n    for feat, var in feature_variances.items():\n        print(f"      - {feat}: {var:.2e}")\n    \n    # Estad√≠sticas de targets\n    print(f"\\nüìä ESTAD√çSTICAS DE TARGETS (y):")\n    for target in available_targets:\n        print(f"\\n   {target}:")\n        print(f"      Min:    {y[target].min():.2f}")\n        print(f"      Max:    {y[target].max():.2f}")\n        print(f"      Mean:   {y[target].mean():.2f}")\n        print(f"      Median: {y[target].median():.2f}")\n        print(f"      Std:    {y[target].std():.2f}")\n    \n    print(f"\\n{\'=\'*70}")\n    print("‚úÖ PREPARACI√ìN DE DATOS COMPLETADA")\n    print("="*70)\n    print(f"\\nüìå PR√ìXIMOS PASOS:")\n\n    print(f"   1. Tratamiento de NaNs (si es necesario)")\n    print(f"\\nüìå PR√ìXIMOS PASOS:")    \n    print("="*70)\n\n    print(f"   2. Split train/test")\n    print(f"   1. Tratamiento de NaNs (si es necesario)")    \n    print(f"   5. Evaluaci√≥n y m√©tricas")\n\n    print(f"   3. Normalizaci√≥n/Escalado (opcional)")\n    print(f"   2. Split train/test")    \n    print("="*70)\n\n    print(f"   4. Entrenamiento de modelos")\n    print(f"   3. Normalizaci√≥n/Escalado (opcional)")    \n    print(f"   4. Entrenamiento de modelos")\n    print(f"   5. Evaluaci√≥n y m√©tricas")',
        ],
    },
    {
        "title": 'üîÑ PASO 6.2: Divisi√≥n Train/Test (Por Bloques de Series Temporales)',
        "cells": [
            '# Mini c√≥digo para ver cu√°ntas series temporales hay\n\ntime_col = df_ml[\'Time\'].copy()\n\n# Detectar inicio de cada serie (cuando Time reinicia)\nseries_id = np.zeros(len(time_col), dtype=int)\ncurrent_series = 0\n\nfor i in range(1, len(time_col)):\n    if time_col.iloc[i] < time_col.iloc[i-1]:\n        current_series += 1\n    series_id[i] = current_series\n\nn_series = series_id.max() + 1\n\nprint(f"{\'=\'*70}")\nprint(f"N√öMERO DE SERIES TEMPORALES: {n_series}")\nprint(f"{\'=\'*70}")\n\n# Ver detalle de cada serie\nprint(f"\\nDetalle de las series:")\nfor sid in range(n_series):\n    mask = series_id == sid\n    n_rows = mask.sum()\n    time_min = time_col[mask].min()\n    time_max = time_col[mask].max()\n    print(f"  Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s")',
            '# =============================================================================\n# MAPEO DE SERIES TEMPORALES A Name_DLC (desde CSV completo)\n# =============================================================================\n\nprint("\\n" + "="*70)\nprint("üìã CREANDO MAPEO: series_id ‚Üí Name_DLC")\nprint("="*70)\n\n# Verificar que existe el dataset completo\nif not complete_dataset_path.exists():\n    print(f"\\n‚ùå ERROR: No se encuentra el archivo {complete_dataset_path.name}")\n    raise ValueError("Dataset completo no encontrado")\n\nprint(f"‚úÖ Dataset completo encontrado: {complete_dataset_path.name}")\n\n# ------------------------------------------------------------------------\n# 1) Cargar solo Time y Name_DLC del CSV completo (eficiente)\n# ------------------------------------------------------------------------\nprint(f"\\n[1/3] Cargando Time y Name_DLC del CSV completo...")\n\n# Leer solo las columnas necesarias\ndf_mapping_temp = pd.read_csv(\n    complete_dataset_path, \n    usecols=[\'Time\', \'Name_DLC\'],\n    dtype={\'Time\': \'float32\', \'Name_DLC\': \'str\'}\n)\n\nprint(f"   ‚úÖ Datos cargados: {len(df_mapping_temp):,} filas")\n\n# ------------------------------------------------------------------------\n# 2) Identificar series temporales\n# ------------------------------------------------------------------------\nprint(f"\\n[2/3] Identificando series temporales...")\n\ntime_col = df_mapping_temp[\'Time\'].values\nname_dlc_col = df_mapping_temp[\'Name_DLC\'].values\n\n# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\nseries_id = np.zeros(len(time_col), dtype=int)\ncurrent_series = 0\n\nfor i in range(1, len(time_col)):\n    if time_col[i] < time_col[i-1]:\n        current_series += 1\n    series_id[i] = current_series\n\n# Agregar series_id al dataframe temporal\ndf_mapping_temp[\'series_id\'] = series_id\n\nn_series = series_id.max() + 1\n\nprint(f"   ‚úÖ Series temporales identificadas: {n_series}")\n\n# ------------------------------------------------------------------------\n# 3) Crear mapeo series_id ‚Üí Name_DLC\n# ------------------------------------------------------------------------\nprint(f"\\n[3/3] Creando mapeo series_id ‚Üí Name_DLC...")\n\n# Para cada series_id, obtener el Name_DLC (deber√≠a ser √∫nico por serie)\nseries_mapping = []\n\nfor sid in range(n_series):\n    mask = df_mapping_temp[\'series_id\'] == sid\n    \n    # Obtener Name_DLC √∫nico de esta serie\n    name_dlc_values = df_mapping_temp.loc[mask, \'Name_DLC\'].unique()\n    \n    if len(name_dlc_values) > 1:\n        print(f"   ‚ö†Ô∏è  ADVERTENCIA: series_id {sid} tiene m√∫ltiples Name_DLC: {name_dlc_values}")\n        name_dlc = name_dlc_values[0]  # Tomar el primero\n    else:\n        name_dlc = name_dlc_values[0]\n    \n    # Informaci√≥n adicional de la serie\n    n_rows = mask.sum()\n    time_min = df_mapping_temp.loc[mask, \'Time\'].min()\n    time_max = df_mapping_temp.loc[mask, \'Time\'].max()\n    duration = time_max - time_min\n    \n    series_mapping.append({\n        \'series_id\': sid,\n        \'Name_DLC\': name_dlc,\n        \'n_rows\': n_rows,\n        \'time_start\': time_min,\n        \'time_end\': time_max,\n        \'duration_s\': duration\n    })\n\n# Crear DataFrame con el mapeo\ndf_series_mapping = pd.DataFrame(series_mapping)\n\nprint(f"   ‚úÖ Mapeo creado para {len(df_series_mapping)} series")\n\n# Liberar memoria del dataframe temporal\ndel df_mapping_temp\nimport gc\ngc.collect()\n\n# Mostrar primeras 10 series\nprint(f"\\n   üìä Primeras 10 series:")\nfor idx, row in df_series_mapping.head(10).iterrows():\n    print(f"      Serie {row[\'series_id\']:3d}: {row[\'Name_DLC\']:<30} | {row[\'n_rows\']:6,} filas | {row[\'duration_s\']:6.1f}s")\n\nif n_series > 10:\n    print(f"      ... y {n_series - 10} series m√°s")\n\n# ------------------------------------------------------------------------\n# 4) Guardar mapeo en archivo TXT\n# ------------------------------------------------------------------------\nprint(f"\\n[4/5] Guardando mapeo en archivos...")\n\n# Crear carpeta para metadatos si no existe\nsplit_metadata_folder = Path(root_dir) / "notebook" / "02_Data_ML_traditional" / "split_metadata"\nsplit_metadata_folder.mkdir(parents=True, exist_ok=True)\n\n# Guardar mapeo completo en TXT\nmapping_path = split_metadata_folder / "series_id_to_Name_DLC_mapping.txt"\n\nwith open(mapping_path, \'w\', encoding=\'utf-8\') as f:\n    f.write("="*90 + "\\n")\n    f.write("MAPEO: series_id ‚Üí Name_DLC\\n")\n    f.write("="*90 + "\\n\\n")\n    f.write(f"Total de series temporales: {n_series}\\n")\n    f.write(f"Generado autom√°ticamente desde: {complete_dataset_path.name}\\n\\n")\n    f.write("="*90 + "\\n\\n")\n    \n    # Escribir tabla\n    f.write(f"{\'series_id\':<12} {\'Name_DLC\':<35} {\'n_rows\':<10} {\'duration_s\':<12} {\'time_range\':<25}\\n")\n    f.write("-"*90 + "\\n")\n    \n    for idx, row in df_series_mapping.iterrows():\n        time_range = f"{row[\'time_start\']:.1f} ‚Üí {row[\'time_end\']:.1f}"\n        f.write(f"{row[\'series_id\']:<12} {row[\'Name_DLC\']:<35} {row[\'n_rows\']:<10} {row[\'duration_s\']:<12.1f} {time_range:<25}\\n")\n    \n    f.write("\\n" + "="*90 + "\\n")\n    f.write("NOTAS:\\n")\n    f.write("  - series_id:   ID num√©rico secuencial asignado autom√°ticamente (0 a N-1)\\n")\n    f.write("  - Name_DLC:    Nombre original del archivo CSV de simulaci√≥n\\n")\n    f.write("  - n_rows:      N√∫mero de filas (muestras temporales) en esta serie\\n")\n    f.write("  - duration_s:  Duraci√≥n de la simulaci√≥n en segundos\\n")\n    f.write("  - time_range:  Rango de tiempo [inicio ‚Üí fin] de la serie\\n")\n    f.write("\\n")\n    f.write("IMPORTANTE:\\n")\n    f.write("  Este mapeo se usa en el split train/test para identificar qu√© simulaciones\\n")\n    f.write("  est√°n en train vs test. El series_id se asigna autom√°ticamente en base a\\n")\n    f.write("  reinicios en la columna \'Time\'.\\n")\n    f.write("="*90 + "\\n")\n\nprint(f"\\n   ‚úÖ Mapeo TXT guardado en:")\nprint(f"      üìÑ {mapping_path}")\n\n# ------------------------------------------------------------------------\n# 5) Guardar tambi√©n en formato CSV para facilitar el an√°lisis\n# ------------------------------------------------------------------------\ncsv_path = split_metadata_folder / "series_id_to_Name_DLC_mapping.csv"\ndf_series_mapping.to_csv(csv_path, index=False)\n\nprint(f"      üìÑ {csv_path}")\n\n# ------------------------------------------------------------------------\n# 6) Estad√≠sticas del mapeo\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*70}")\nprint("üìä ESTAD√çSTICAS DEL MAPEO")\nprint("="*70)\n\nprint(f"\\n   Total series:     {n_series}")\nprint(f"   Total filas:      {len(series_id):,}")\nprint(f"   Filas por serie:")\nprint(f"      Media:         {df_series_mapping[\'n_rows\'].mean():.0f}")\nprint(f"      Mediana:       {df_series_mapping[\'n_rows\'].median():.0f}")\nprint(f"      M√≠nimo:        {df_series_mapping[\'n_rows\'].min()}")\nprint(f"      M√°ximo:        {df_series_mapping[\'n_rows\'].max()}")\n\nprint(f"\\n   Duraci√≥n por serie (segundos):")\nprint(f"      Media:         {df_series_mapping[\'duration_s\'].mean():.1f}s")\nprint(f"      Mediana:       {df_series_mapping[\'duration_s\'].median():.1f}s")\nprint(f"      M√≠nimo:        {df_series_mapping[\'duration_s\'].min():.1f}s")\nprint(f"      M√°ximo:        {df_series_mapping[\'duration_s\'].max():.1f}s")\n\n# Identificar patrones en Name_DLC (DLC12a vs DLC13)\ndlc12a_count = df_series_mapping[\'Name_DLC\'].str.contains(\'DLC12a\').sum()\ndlc13_count = df_series_mapping[\'Name_DLC\'].str.contains(\'DLC13\').sum()\n\nprint(f"\\n   Distribuci√≥n por tipo de DLC:")\nprint(f"      DLC12a:        {dlc12a_count} series ({dlc12a_count/n_series*100:.1f}%)")\nprint(f"      DLC13:         {dlc13_count} series ({dlc13_count/n_series*100:.1f}%)")\n\n# Extraer velocidades de viento de los nombres (asumiendo formato: XXXX_DLCXX_VVV_000)\ndf_series_mapping[\'wind_speed\'] = df_series_mapping[\'Name_DLC\'].str.extract(r\'_(\\d{3})_\\d{3}\')[0].astype(float) / 10\n\nif df_series_mapping[\'wind_speed\'].notna().any():\n    print(f"\\n   Distribuci√≥n por velocidad de viento:")\n    wind_speeds = df_series_mapping[\'wind_speed\'].value_counts().sort_index()\n    for ws, count in wind_speeds.items():\n        print(f"      {ws:4.1f} m/s:      {count} series ({count/n_series*100:.1f}%)")\n\nprint(f"\\n{\'=\'*70}")\nprint("‚úÖ MAPEO COMPLETADO")\nprint("="*70)\n\nprint(f"\\nüí° USO DEL MAPEO:")\nprint(f"   ‚Ä¢ Para ver qu√© simulaci√≥n (Name_DLC) corresponde a cada series_id")\nprint(f"   ‚Ä¢ El c√≥digo de split train/test usa el mismo m√©todo de detecci√≥n")\nprint(f"   ‚Ä¢ Los series_id ser√°n id√©nticos en ambos c√≥digos")\nprint(f"   ‚Ä¢ √ötil para analizar distribuci√≥n de condiciones en train/test")\n\nprint(f"\\nüìÅ Archivos generados:")\nprint(f"   üìÑ {mapping_path.name} (formato legible)")\nprint(f"   üìÑ {csv_path.name} (formato CSV para an√°lisis)")\n\nprint(f"\\nüëâ Ahora puedes ejecutar el c√≥digo de split train/test")\nprint("="*70)',
            '# =============================================================================\n# PASO 6.2: DIVISI√ìN TRAIN/TEST (POR BLOQUES DE SERIES TEMPORALES)\n# =============================================================================\n\nprint("\\n" + "="*70)\nprint("üîÑ DIVISI√ìN TRAIN/TEST (Por Bloques de Series Temporales)")\nprint("="*70)\n\n# Verificar que tenemos la columna Time\nif \'Time\' not in df_ml.columns:\n    print("\\n‚ùå ERROR: Se requiere la columna \'Time\' para identificar series temporales")\n    raise ValueError("Columna \'Time\' no encontrada")\n\nprint("‚úÖ Columna \'Time\' detectada")\n\n# ------------------------------------------------------------------------\n# 1) Identificar cada serie temporal completa\n# ------------------------------------------------------------------------\nprint(f"\\n[1/5] Identificando series temporales (bloques completos)...")\n\ntime_col = df_ml[\'Time\'].copy()\n\n# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\nseries_id = np.zeros(len(time_col), dtype=int)\ncurrent_series = 0\n\nfor i in range(1, len(time_col)):\n    if time_col.iloc[i] < time_col.iloc[i-1]:\n        current_series += 1\n    series_id[i] = current_series\n\n# Agregar series_id al dataframe\ndf_ml[\'series_id\'] = series_id\n\nn_series = series_id.max() + 1\n\nprint(f"\\n   ‚úÖ Series temporales identificadas: {n_series}")\n\n# Analizar cada serie\nprint(f"\\n   üìä Primeras 10 series:")\nfor sid in range(min(10, n_series)):\n    mask = df_ml[\'series_id\'] == sid\n    n_rows = mask.sum()\n    time_min = df_ml.loc[mask, \'Time\'].min()\n    time_max = df_ml.loc[mask, \'Time\'].max()\n    print(f"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s")\n\nif n_series > 10:\n    print(f"      ... y {n_series - 10} series m√°s")\n\n# ------------------------------------------------------------------------\n# 2) DEFINIR SERIES PARA TEST (TRAIN se calcula autom√°ticamente)\n# ------------------------------------------------------------------------\nprint(f"\\n[2/5] Definiendo series para Test (Train = resto autom√°tico)...")\n\n# ============================================================================\n# üéØ CONFIGURACI√ìN: Selecciona UNA de las siguientes opciones\n# ============================================================================\n\n# Opci√≥n 1: Definir expl√≠citamente solo las series de TEST\n#test_series = np.array([3, 6, 7, 18, 19, 30, 31, 46,47, 54, 55, 66, 67, 80, 81, 90, 91, 108, 109, 114, 115, 126, 127,])\n\n# Opci√≥n 2: Cargar desde archivo\n# test_series = np.loadtxt(\'test_series.txt\', dtype=int)\n\n# Opci√≥n 3: Autom√°tico - Cada 5ta serie va a TEST\ntest_series = np.arange(n_series)[4::5]  # Series: 4, 9, 14, 19, ...\n\n# Opci√≥n 4: Primeras N series a TEST\n# test_series = np.arange(14)  # Primeras 14 series\n\n# Opci√≥n 5: √öltimas N series a TEST\n# test_series = np.arange(n_series - 14, n_series)  # √öltimas 14 series\n\n# ============================================================================\n\n# Calcular TRAIN autom√°ticamente (todas las series que NO est√°n en TEST)\nseries_indices = np.arange(n_series)\ntrain_series = np.array([s for s in series_indices if s not in test_series])\n\nprint(f"\\n   ‚úÖ Configuraci√≥n:")\nprint(f"      TEST:  {len(test_series)} series definidas manualmente")\nprint(f"      TRAIN: {len(train_series)} series calculadas autom√°ticamente (resto)")\n\n# Validar que las series de test existen\ninvalid_test = [s for s in test_series if s >= n_series or s < 0]\nif invalid_test:\n    print(f"\\n‚ùå ERROR: Series TEST inv√°lidas: {invalid_test}")\n    print(f"   Rango v√°lido: 0 a {n_series-1}")\n    raise ValueError("Series fuera de rango")\n\n# Validar que hay suficientes series en ambos conjuntos\nif len(train_series) == 0:\n    print(f"\\n‚ùå ERROR: No hay series para TRAIN")\n    raise ValueError("Todas las series est√°n asignadas a TEST")\n\nif len(test_series) == 0:\n    print(f"\\n‚ùå ERROR: No hay series para TEST")\n    raise ValueError("Debes definir al menos una serie para TEST")\n\nprint(f"\\n   Total series:    {n_series}")\nprint(f"   Series train:    {len(train_series)} ({len(train_series)/n_series*100:.1f}%)")\nprint(f"   Series test:     {len(test_series)} ({len(test_series)/n_series*100:.1f}%)")\n\n# ------------------------------------------------------------------------\n# 2.5) GUARDAR TRAZABILIDAD EN ARCHIVOS TXT\n# ------------------------------------------------------------------------\nprint(f"\\n[2.5/5] Guardando trazabilidad de divisi√≥n train/test...")\n\n# Crear carpeta para metadatos si no existe\nsplit_metadata_folder = Path(root_dir) / "notebook" / "02_Data_ML_traditional" / "split_metadata"\nsplit_metadata_folder.mkdir(parents=True, exist_ok=True)\n\n# Guardar series de train\ntrain_series_path = split_metadata_folder / "train_series.txt"\nnp.savetxt(train_series_path, train_series, fmt=\'%d\', \n           header=f\'Series para TRAIN ({len(train_series)} series, {len(train_series)/n_series*100:.1f}%)\\nGenerado autom√°ticamente como complemento de test_series\',\n           comments=\'# \')\n\n# Guardar series de test\ntest_series_path = split_metadata_folder / "test_series.txt"\nnp.savetxt(test_series_path, test_series, fmt=\'%d\',\n           header=f\'Series para TEST ({len(test_series)} series, {len(test_series)/n_series*100:.1f}%)\\nDefinido manualmente\',\n           comments=\'# \')\n\n# Guardar resumen con estad√≠sticas\nsummary_path = split_metadata_folder / "split_summary.txt"\nwith open(summary_path, \'w\') as f:\n    f.write("="*70 + "\\n")\n    f.write("RESUMEN DE DIVISI√ìN TRAIN/TEST\\n")\n    f.write("="*70 + "\\n\\n")\n    f.write(f"Total series temporales detectadas: {n_series}\\n\\n")\n    f.write(f"TRAIN:\\n")\n    f.write(f"  - N√∫mero de series: {len(train_series)} ({len(train_series)/n_series*100:.1f}%)\\n")\n    f.write(f"  - Series: {train_series.tolist()}\\n\\n")\n    f.write(f"TEST:\\n")\n    f.write(f"  - N√∫mero de series: {len(test_series)} ({len(test_series)/n_series*100:.1f}%)\\n")\n    f.write(f"  - Series: {test_series.tolist()}\\n\\n")\n    f.write(f"Rango de IDs: 0 a {n_series-1}\\n")\n    f.write("="*70 + "\\n")\n\nprint(f"\\n   ‚úÖ Archivos de trazabilidad guardados:")\nprint(f"      üìÑ {train_series_path}")\nprint(f"      üìÑ {test_series_path}")\nprint(f"      üìÑ {summary_path}")\n\n# ------------------------------------------------------------------------\n# 3) Mostrar distribuci√≥n de series\n# ------------------------------------------------------------------------\nprint(f"\\n[3/5] Distribuci√≥n de series...")\n\nprint(f"\\n   üî¢ Series en TRAIN: {train_series[:10].tolist()}{\'...\' if len(train_series) > 10 else \'\'}")\nprint(f"   üî¢ Series en TEST:  {test_series[:10].tolist()}{\'...\' if len(test_series) > 10 else \'\'}")\n\n# ------------------------------------------------------------------------\n# 4) Crear m√°scaras y dividir datos\n# ------------------------------------------------------------------------\nprint(f"\\n[4/5] Creando conjuntos train y test...")\n\n# Crear m√°scaras booleanas\ntrain_mask = df_ml[\'series_id\'].isin(train_series)\ntest_mask = df_ml[\'series_id\'].isin(test_series)\n\n# Dividir X e y\nX_train = X[train_mask].copy()\nX_test = X[test_mask].copy()\n\ny_train = y[train_mask].copy()\ny_test = y[test_mask].copy()\n\n# Guardar metadata temporal (Time y series_id) para an√°lisis posterior\nTime_train = df_ml.loc[train_mask, \'Time\'].copy()\nTime_test = df_ml.loc[test_mask, \'Time\'].copy()\n\nseries_id_train = df_ml.loc[train_mask, \'series_id\'].copy()\nseries_id_test = df_ml.loc[test_mask, \'series_id\'].copy()\n\nprint(f"\\n‚úÖ Divisi√≥n completada:")\nprint(f"   X_train: {X_train.shape} ({len(X_train):,} filas)")\nprint(f"   X_test:  {X_test.shape} ({len(X_test):,} filas)")\nprint(f"   y_train: {y_train.shape}")\nprint(f"   y_test:  {y_test.shape}")\n\nprint(f"\\nüìã Metadata temporal guardada:")\nprint(f"   Time_train: {len(Time_train):,} valores")\nprint(f"   Time_test:  {len(Time_test):,} valores")\nprint(f"   series_id_train: {len(series_id_train):,} valores (series √∫nicas: {series_id_train.nunique()})")\nprint(f"   series_id_test:  {len(series_id_test):,} valores (series √∫nicas: {series_id_test.nunique()})")\n\nprint(f"\\nüí° Nota: Time y series_id NO est√°n en X (solo son metadata)")\nprint(f"   X contiene √∫nicamente features predictivas para el modelo")\n\n# Porcentajes reales de filas\nactual_train_pct = len(X_train) / len(X) * 100\nactual_test_pct = len(X_test) / len(X) * 100\n\nprint(f"\\nüìä Porcentajes reales (filas):")\nprint(f"   Train: {actual_train_pct:.2f}%")\nprint(f"   Test:  {actual_test_pct:.2f}%")\n\n# ------------------------------------------------------------------------\n# 5) An√°lisis de distribuci√≥n de velocidades de viento (proxy: targets)\n# ------------------------------------------------------------------------\nprint(f"\\n[5/5] Verificando distribuci√≥n de cargas (proxy para velocidad)...")\n\nfor target in y.columns:\n    print(f"\\n   üìä {target}:")\n    print(f"      {\'\':12} {\'TRAIN\':>12} {\'TEST\':>12} {\'DIFERENCIA\':>12}")\n    print(f"      {\'‚îÄ\'*12} {\'‚îÄ\'*12} {\'‚îÄ\'*12} {\'‚îÄ\'*12}")\n    \n    train_min = y_train[target].min()\n    test_min = y_test[target].min()\n    train_max = y_train[target].max()\n    test_max = y_test[target].max()\n    train_mean = y_train[target].mean()\n    test_mean = y_test[target].mean()\n    train_std = y_train[target].std()\n    test_std = y_test[target].std()\n    \n    print(f"      {\'Min:\':12} {train_min:>12.2f} {test_min:>12.2f} {abs(train_min-test_min):>12.2f}")\n    print(f"      {\'Max:\':12} {train_max:>12.2f} {test_max:>12.2f} {abs(train_max-test_max):>12.2f}")\n    print(f"      {\'Mean:\':12} {train_mean:>12.2f} {test_mean:>12.2f} {abs(train_mean-test_mean):>12.2f}")\n    print(f"      {\'Std:\':12} {train_std:>12.2f} {test_std:>12.2f} {abs(train_std-test_std):>12.2f}")\n    \n    # Evaluar si las distribuciones son similares\n    mean_diff_pct = abs(train_mean - test_mean) / train_mean * 100\n    if mean_diff_pct < 10:\n        print(f"      ‚úÖ Distribuciones similares (diff media: {mean_diff_pct:.1f}%)")\n    else:\n        print(f"      ‚ö†Ô∏è  Distribuciones diferentes (diff media: {mean_diff_pct:.1f}%)")\n\n# Verificar NaNs\nprint(f"\\n{\'=\'*70}")\nprint("üîç VERIFICACI√ìN DE CALIDAD")\nprint("="*70)\n\nnan_X_train = X_train.isna().sum().sum()\nnan_X_test = X_test.isna().sum().sum()\nnan_y_train = y_train.isna().sum().sum()\nnan_y_test = y_test.isna().sum().sum()\n\nprint(f"\\nüìä NaNs por conjunto:")\nprint(f"   X_train: {nan_X_train:,} ({nan_X_train / X_train.size * 100:.2f}%)")\nprint(f"   X_test:  {nan_X_test:,} ({nan_X_test / X_test.size * 100:.2f}%)")\nprint(f"   y_train: {nan_y_train:,} ({nan_y_train / y_train.size * 100:.2f}%)")\nprint(f"   y_test:  {nan_y_test:,} ({nan_y_test / y_test.size * 100:.2f}%)")\n\nprint(f"\\n{\'=\'*70}")\nprint("‚úÖ DIVISI√ìN TRAIN/TEST COMPLETADA")\nprint("="*70)\n\nprint(f"\\nüìå VENTAJAS DE ESTE ENFOQUE:")\nprint(f"   ‚úì Solo defines TEST, TRAIN se calcula autom√°ticamente")\nprint(f"   ‚úì Cada serie temporal completa en train O test (no mezcladas)")\nprint(f"   ‚úì Sin data leakage temporal")\nprint(f"   ‚úì Trazabilidad completa en archivos TXT")\n\nprint(f"\\nüìå DATOS LISTOS PARA MODELADO:")\nprint(f"   ‚úì X_train: {X_train.shape}")\nprint(f"   ‚úì X_test:  {X_test.shape}")\nprint(f"   ‚úì y_train: {y_train.shape}")\nprint(f"   ‚úì y_test:  {y_test.shape}")\n\n# ============================================================================\n# RESUMEN DE SERIES TEMPORALES IDENTIFICADAS\n# ============================================================================\nprint(f"\\n{\'=\'*70}")\nprint("üìã RESUMEN FINAL: SERIES TEMPORALES IDENTIFICADAS")\nprint("="*70)\n\nprint(f"\\nüî¢ TOTAL DE SERIES TEMPORALES DETECTADAS: {n_series}")\nprint(f"\\n   Distribuci√≥n:")\nprint(f"   ‚Ä¢ {len(train_series)} series en TRAIN ({len(train_series)/n_series*100:.1f}%)")\nprint(f"   ‚Ä¢ {len(test_series)} series en TEST  ({len(test_series)/n_series*100:.1f}%)")\n\nprint(f"\\n   Rango de IDs:")\nprint(f"   ‚Ä¢ Serie m√≠nima: 0")\nprint(f"   ‚Ä¢ Serie m√°xima: {n_series - 1}")\n\nprint(f"\\n   Filas totales por conjunto:")\nprint(f"   ‚Ä¢ TRAIN: {len(X_train):,} filas")\nprint(f"   ‚Ä¢ TEST:  {len(X_test):,} filas")\nprint(f"   ‚Ä¢ TOTAL: {len(X):,} filas")\n\nprint(f"\\nüí° TRAZABILIDAD:")\nprint(f"   üìÅ Carpeta: {split_metadata_folder}")\nprint(f"   üìÑ train_series.txt  - Series usadas en TRAIN")\nprint(f"   üìÑ test_series.txt   - Series usadas en TEST")\nprint(f"   üìÑ split_summary.txt - Resumen completo")\nprint(f"\\n   üëâ Para cambiar divisi√≥n, edita solo \'test_series\' en secci√≥n [2/5]")\nprint("="*70)',
            '# ============================================================================\n# PASO 6.3: NORMALIZACI√ìN - Part 2/4: FIT SCALERS (INDEPENDIENTES)\n# ============================================================================\n\nprint("\\n" + "="*70)\nprint("PASO 6.3 - Part 2/4: AJUSTE DE ESCALADORES (INDEPENDIENTES)")\nprint("="*70)\n\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\nfrom pathlib import Path\nimport pandas as pd\n\n# =========================================================================\n# VERIFICACI√ìN Y LIMPIEZA DE COLUMNAS DUPLICADAS\n# =========================================================================\n\nprint(f"\\nüîç Verificaci√≥n de datos:")\nprint(f"   X_train: {X_train.shape}")\nprint(f"   y_train: {y_train.shape}")\n\n# Detectar columnas duplicadas\nn_cols = len(X_train.columns)\nn_unique = len(X_train.columns.unique())\nn_duplicated = n_cols - n_unique\n\nprint(f"   Total columnas: {n_cols}")\nprint(f"   Columnas √∫nicas: {n_unique}")\n\nif n_duplicated > 0:\n    print(f"\\n‚ö†Ô∏è  ADVERTENCIA: {n_duplicated} nombres de columnas duplicados")\n    print(f"   Renombrando columnas duplicadas...")\n    \n    # Renombrar columnas duplicadas agregando sufijo\n    new_columns = []\n    seen = {}\n    \n    for col in X_train.columns:\n        if col in seen:\n            seen[col] += 1\n            new_col = f"{col}_dup{seen[col]}"\n            new_columns.append(new_col)\n            print(f"      \'{col}\' ‚Üí \'{new_col}\'")\n        else:\n            seen[col] = 0\n            new_columns.append(col)\n    \n    # Aplicar nuevos nombres\n    X_train.columns = new_columns\n    \n    print(f"\\n   ‚úÖ Columnas renombradas. Ahora todas son √∫nicas.")\n    print(f"   Total columnas despu√©s: {len(X_train.columns)}")\n\n# =========================================================================\n# CREAR CARPETA PARA ESCALADORES\n# =========================================================================\n\nmodels_folder = Path("01_Models_scaler")\nmodels_folder.mkdir(parents=True, exist_ok=True)\nprint(f"\\n‚úÖ Carpeta para escaladores: {models_folder}")\n\n# =========================================================================\n# CREAR ESCALADORES INDEPENDIENTES PARA CADA FEATURE (X)\n# =========================================================================\n\nprint(f"\\n[1/2] Creando escaladores independientes para FEATURES (X)...")\nprint(f"      Total features: {len(X_train.columns)}")\n\n# Diccionario para almacenar un escalador por cada feature\nscalers_X = {}\n\n# Ajustar un escalador por cada columna de X_train\nfor i, col in enumerate(X_train.columns, 1):\n    scaler = StandardScaler()\n    scaler.fit(X_train[[col]])\n    scalers_X[col] = scaler\n    \n    # Mostrar progreso cada 100 features\n    if i % 100 == 0 or i == len(X_train.columns):\n        print(f"      ‚úÖ {i}/{len(X_train.columns)} escaladores ajustados...")\n\nprint(f"\\n‚úÖ Total escaladores X creados: {len(scalers_X)}")\n\n# Verificaci√≥n\nif len(scalers_X) != len(X_train.columns):\n    print(f"‚ùå ERROR: N√∫mero de escaladores ({len(scalers_X)}) != n√∫mero de columnas ({len(X_train.columns)})")\n    print(f"   Esto NO deber√≠a ocurrir despu√©s de renombrar duplicados.")\nelse:\n    print(f"‚úÖ Verificaci√≥n: Todos los features tienen escalador")\n\n# =========================================================================\n# CREAR ESCALADORES INDEPENDIENTES PARA CADA TARGET (Y)\n# =========================================================================\n\nprint(f"\\n[2/2] Creando escaladores independientes para TARGETS (Y)...")\n\n# Diccionario para almacenar un escalador por cada target\nscalers_y = {}\n\n# Ajustar un escalador por cada columna de y_train\nfor col in y_train.columns:\n    scaler = StandardScaler()\n    scaler.fit(y_train[[col]])\n    scalers_y[col] = scaler\n    print(f"      ‚úÖ Escalador creado para: {col}")\n\nprint(f"\\n‚úÖ Total escaladores Y creados: {len(scalers_y)}")\n\n# =========================================================================\n# GUARDAR ESCALADORES\n# =========================================================================\n\nprint(f"\\n{\'=\'*70}")\nprint("GUARDANDO ESCALADORES")\nprint("="*70)\n\n# Guardar escaladores de features\nscalers_X_path = models_folder / \'scalers_X.pkl\'\njoblib.dump(scalers_X, scalers_X_path)\nprint(f"‚úÖ Escaladores X guardados: {scalers_X_path}")\nprint(f"   Tipo: Diccionario con {len(scalers_X)} escaladores")\nprint(f"   Tama√±o: {scalers_X_path.stat().st_size / (1024**2):.2f} MB")\n\n# Guardar escaladores de targets\nscalers_y_path = models_folder / \'scalers_y.pkl\'\njoblib.dump(scalers_y, scalers_y_path)\nprint(f"‚úÖ Escaladores Y guardados: {scalers_y_path}")\nprint(f"   Tipo: Diccionario con {len(scalers_y)} escaladores")\n\n# =========================================================================\n# VERIFICACI√ìN FINAL\n# =========================================================================\n\nprint(f"\\n{\'=\'*70}")\nprint("VERIFICACI√ìN FINAL")\nprint("="*70)\n\n# Verificar que todas las columnas tienen escalador\nmissing_X = set(X_train.columns) - set(scalers_X.keys())\nmissing_y = set(y_train.columns) - set(scalers_y.keys())\n\nif missing_X:\n    print(f"‚ùå Faltan escaladores para features: {missing_X}")\nelse:\n    print(f"‚úÖ Todos los features ({len(X_train.columns)}) tienen escalador")\n\nif missing_y:\n    print(f"‚ùå Faltan escaladores para targets: {missing_y}")\nelse:\n    print(f"‚úÖ Todos los targets ({len(y_train.columns)}) tienen escalador")\n\n# =========================================================================\n# RESUMEN\n# =========================================================================\n\nprint(f"\\n{\'=\'*70}")\nprint("RESUMEN - PASO 2/4 COMPLETADO")\nprint("="*70)\nprint(f"üìä Escaladores X (features): {len(scalers_X)}")\nprint(f"üìä Escaladores Y (targets):  {len(scalers_y)}")\nprint(f"\\nüí° M√©todo: StandardScaler independiente por variable")\nprint(f"üí° Ventaja: Cada variable se escala seg√∫n su propia distribuci√≥n")\nprint(f"üí° Archivos guardados en: {models_folder}")\nprint(f"   - scalers_X.pkl ({len(scalers_X)} escaladores)")\nprint(f"   - scalers_y.pkl ({len(scalers_y)} escaladores)")\nprint("="*70)',
            '"""\ndata_norm.py - Normalizaci√≥n de datos con escaladores independientes\n\nEste script normaliza los datos de entrenamiento usando escaladores independientes\npor columna (un StandardScaler por cada feature y target).\n\nPrerequisitos:\n    - Los escaladores deben estar creados y guardados en:\n      * notebook/01_Models_scaler/scalers_X.pkl\n      * notebook/01_Models_scaler/scalers_y.pkl\n    - Los datos train/test deben estar disponibles en memoria (X_train, y_train)\n\nOutputs:\n    - notebook/02_Data_ML_traditional/X_train_norm.csv\n    - notebook/02_Data_ML_traditional/y_train_norm.csv\n    - notebook/02_Data_ML_traditional/normalization_metadata.json\n"""\n\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport json\nimport gc\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\n\nclass DataNormalizer:\n    """\n    Clase para normalizar datos usando escaladores independientes por columna.\n    """\n    \n    def __init__(self, scalers_folder: str = "notebook/01_Models_scaler"):\n        """\n        Inicializa el normalizador cargando los escaladores.\n        \n        Args:\n            scalers_folder: Carpeta donde est√°n guardados los escaladores\n        """\n        self.scalers_folder = Path(scalers_folder)\n        self.scalers_X = None\n        self.scalers_y = None\n        \n    def load_scalers(self) -> None:\n        """\n        Carga los escaladores desde archivos pickle.\n        """\n        print("="*70)\n        print("CARGANDO ESCALADORES")\n        print("="*70)\n        \n        # Cargar escaladores de features\n        scalers_X_path = self.scalers_folder / \'scalers_X.pkl\'\n        if not scalers_X_path.exists():\n            raise FileNotFoundError(f"No se encuentra {scalers_X_path}")\n        \n        self.scalers_X = joblib.load(scalers_X_path)\n        print(f"‚úÖ Escaladores X cargados: {len(self.scalers_X)} escaladores")\n        \n        # Cargar escaladores de targets\n        scalers_y_path = self.scalers_folder / \'scalers_y.pkl\'\n        if not scalers_y_path.exists():\n            raise FileNotFoundError(f"No se encuentra {scalers_y_path}")\n        \n        self.scalers_y = joblib.load(scalers_y_path)\n        print(f"‚úÖ Escaladores Y cargados: {len(self.scalers_y)} escaladores")\n        print()\n        \n    def normalize_X_chunked(self, X: pd.DataFrame, chunk_size: int = 100) -> pd.DataFrame:\n        """\n        Normaliza features X por chunks para eficiencia de memoria.\n        \n        Args:\n            X: DataFrame con features\n            chunk_size: N√∫mero de columnas a procesar por chunk\n            \n        Returns:\n            DataFrame normalizado\n        """\n        print("="*70)\n        print("NORMALIZANDO FEATURES (X)")\n        print("="*70)\n        \n        print(f"\\nüîç Verificaci√≥n inicial:")\n        print(f"   X shape: {X.shape}")\n        print(f"   Escaladores disponibles: {len(self.scalers_X)}")\n        \n        total_cols = len(X.columns)\n        n_chunks = int(np.ceil(total_cols / chunk_size))\n        \n        print(f"\\nüìä Procesamiento por chunks:")\n        print(f"   Total columnas: {total_cols}")\n        print(f"   Chunk size: {chunk_size}")\n        print(f"   Total chunks: {n_chunks}")\n        \n        chunks_list = []\n        \n        for chunk_idx in range(n_chunks):\n            start_idx = chunk_idx * chunk_size\n            end_idx = min((chunk_idx + 1) * chunk_size, total_cols)\n            cols_chunk = X.columns[start_idx:end_idx]\n            \n            print(f"\\n[Chunk {chunk_idx+1}/{n_chunks}] Columnas {start_idx}-{end_idx-1} ({len(cols_chunk)} cols)")\n            \n            # Crear lista de arrays normalizados\n            normalized_arrays = []\n            normalized_cols = []\n            \n            # Normalizar cada columna con su escalador\n            for i, col in enumerate(cols_chunk, 1):\n                if col not in self.scalers_X:\n                    raise KeyError(f"No se encuentra escalador para columna: {col}")\n                \n                # Transformar y agregar a la lista\n                arr_norm = self.scalers_X[col].transform(X[[col]]).ravel()\n                normalized_arrays.append(arr_norm)\n                normalized_cols.append(col)\n                \n                if i % 20 == 0 or i == len(cols_chunk):\n                    print(f"      ‚úÖ {i}/{len(cols_chunk)} columnas normalizadas...")\n            \n            # Crear DataFrame del chunk con np.column_stack\n            chunk_df = pd.DataFrame(\n                np.column_stack(normalized_arrays),\n                index=X.index,\n                columns=normalized_cols\n            )\n            chunks_list.append(chunk_df)\n            \n            print(f"      ‚úÖ Chunk {chunk_idx+1} completado: {chunk_df.shape}")\n        \n        # Concatenar todos los chunks\n        print(f"\\nConcatenando {len(chunks_list)} chunks...")\n        X_norm = pd.concat(chunks_list, axis=1)\n        \n        # Liberar memoria\n        del chunks_list\n        gc.collect()\n        \n        print(f"‚úÖ Normalizaci√≥n completada: {X_norm.shape}")\n        \n        # Verificaci√≥n estad√≠stica\n        print(f"\\nüìä Verificaci√≥n (primeras 5 columnas):")\n        stats = X_norm.iloc[:, :5].describe().loc[[\'mean\', \'std\']]\n        print(stats)\n        print()\n        \n        return X_norm\n    \n    def normalize_y(self, y: pd.DataFrame) -> pd.DataFrame:\n        """\n        Normaliza targets Y columna por columna.\n        \n        Args:\n            y: DataFrame con targets\n            \n        Returns:\n            DataFrame normalizado\n        """\n        print("="*70)\n        print("NORMALIZANDO TARGETS (Y)")\n        print("="*70)\n        \n        print(f"\\nüîç Verificaci√≥n inicial:")\n        print(f"   y shape: {y.shape}")\n        print(f"   Escaladores disponibles: {len(self.scalers_y)}")\n        \n        # Crear listas para datos normalizados\n        normalized_arrays = []\n        normalized_cols = []\n        \n        print(f"\\nNormalizando {len(y.columns)} targets:")\n        \n        # Normalizar cada target con su escalador\n        for col in y.columns:\n            if col not in self.scalers_y:\n                raise KeyError(f"No se encuentra escalador para target: {col}")\n            \n            arr_norm = self.scalers_y[col].transform(y[[col]]).ravel()\n            normalized_arrays.append(arr_norm)\n            normalized_cols.append(col)\n            print(f"   ‚úÖ Target normalizado: {col}")\n        \n        # Crear DataFrame con np.column_stack\n        y_norm = pd.DataFrame(\n            np.column_stack(normalized_arrays),\n            index=y.index,\n            columns=normalized_cols\n        )\n        \n        print(f"\\n‚úÖ Normalizaci√≥n completada: {y_norm.shape}")\n        \n        # Verificaci√≥n estad√≠stica\n        print(f"\\nüìä Verificaci√≥n estad√≠stica:")\n        stats = y_norm.describe().loc[[\'mean\', \'std\']]\n        print(stats)\n        print()\n        \n        return y_norm\n    \n    def save_normalized_data(self, \n                            X_norm: pd.DataFrame, \n                            y_norm: pd.DataFrame,\n                            output_folder: str = "02_Data_ML_traditional") -> None:\n        """\n        Guarda los datos normalizados y metadata.\n        \n        Args:\n            X_norm: Features normalizados\n            y_norm: Targets normalizados\n            output_folder: Carpeta de destino\n        """\n        print("="*70)\n        print("GUARDANDO DATOS NORMALIZADOS")\n        print("="*70)\n        \n        # Crear carpeta\n        output_path = Path(output_folder)\n        output_path.mkdir(parents=True, exist_ok=True)\n        \n        print(f"\\nüìÅ Carpeta destino: {output_path}")\n        \n        # Guardar X_train_norm\n        print(f"\\n[1/3] Guardando X_train_norm...")\n        X_norm_path = output_path / \'X_train_norm.csv\'\n        X_norm.to_csv(X_norm_path, index=False)\n        \n        file_size_mb = X_norm_path.stat().st_size / (1024**2)\n        print(f"   ‚úÖ Guardado: {X_norm_path.name}")\n        print(f"   Shape: {X_norm.shape}")\n        print(f"   Tama√±o: {file_size_mb:.2f} MB")\n        \n        # Guardar y_train_norm\n        print(f"\\n[2/3] Guardando y_train_norm...")\n        y_norm_path = output_path / \'y_train_norm.csv\'\n        y_norm.to_csv(y_norm_path, index=False)\n        \n        file_size_mb = y_norm_path.stat().st_size / (1024**2)\n        print(f"   ‚úÖ Guardado: {y_norm_path.name}")\n        print(f"   Shape: {y_norm.shape}")\n        print(f"   Tama√±o: {file_size_mb:.2f} MB")\n        \n        # Guardar metadata\n        print(f"\\n[3/3] Guardando metadata...")\n        \n        metadata = {\n            "normalization_method": "StandardScaler independiente por columna",\n            "n_features": len(X_norm.columns),\n            "n_targets": len(y_norm.columns),\n            "n_samples_train": len(X_norm),\n            "feature_columns": X_norm.columns.tolist(),\n            "target_columns": y_norm.columns.tolist(),\n            "scalers_X_path": str(self.scalers_folder / "scalers_X.pkl"),\n            "scalers_y_path": str(self.scalers_folder / "scalers_y.pkl"),\n            "scalers_X_count": len(self.scalers_X),\n            "scalers_y_count": len(self.scalers_y)\n        }\n        \n        metadata_path = output_path / \'normalization_metadata.json\'\n        with open(metadata_path, \'w\', encoding=\'utf-8\') as f:\n            json.dump(metadata, f, indent=2)\n        \n        print(f"   ‚úÖ Guardado: {metadata_path.name}")\n        \n        # Resumen final\n        print(f"\\n{\'=\'*70}")\n        print("RESUMEN")\n        print("="*70)\n        print(f"‚úÖ X_train_norm: {X_norm.shape}")\n        print(f"‚úÖ y_train_norm: {y_norm.shape}")\n        print(f"\\nüìÅ Archivos guardados:")\n        print(f"   - {X_norm_path}")\n        print(f"   - {y_norm_path}")\n        print(f"   - {metadata_path}")\n        print(f"\\nüìÅ Escaladores:")\n        print(f"   - {self.scalers_folder / \'scalers_X.pkl\'} ({len(self.scalers_X)} escaladores)")\n        print(f"   - {self.scalers_folder / \'scalers_y.pkl\'} ({len(self.scalers_y)} escaladores)")\n        print(f"\\nüí° M√©todo: Escalador independiente por cada variable")\n        print("="*70)\n\n\ndef normalize_train_data(X_train: pd.DataFrame, \n                         y_train: pd.DataFrame,\n                         scalers_folder: str = "notebook/01_Models_scaler",\n                         output_folder: str = "notebook/02_Data_ML_traditional",\n                         chunk_size: int = 100) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    """\n    Funci√≥n principal para normalizar datos de entrenamiento.\n    \n    Args:\n        X_train: DataFrame con features de entrenamiento\n        y_train: DataFrame con targets de entrenamiento\n        scalers_folder: Carpeta donde est√°n los escaladores\n        output_folder: Carpeta donde guardar los datos normalizados\n        chunk_size: Tama√±o de chunk para procesamiento de X\n        \n    Returns:\n        Tuple con (X_train_norm, y_train_norm)\n        \n    Example:\n        >>> X_train_norm, y_train_norm = normalize_train_data(X_train, y_train)\n    """\n    \n    print("\\n" + "="*70)\n    print("NORMALIZACI√ìN DE DATOS DE ENTRENAMIENTO")\n    print("="*70)\n    print(f"Entrada:")\n    print(f"  X_train: {X_train.shape}")\n    print(f"  y_train: {y_train.shape}")\n    print()\n    \n    # Crear normalizador\n    normalizer = DataNormalizer(scalers_folder=scalers_folder)\n    \n    # Cargar escaladores\n    normalizer.load_scalers()\n    \n    # Normalizar X_train\n    X_train_norm = normalizer.normalize_X_chunked(X_train, chunk_size=chunk_size)\n    \n    # Normalizar y_train\n    y_train_norm = normalizer.normalize_y(y_train)\n    \n    # Guardar datos\n    normalizer.save_normalized_data(X_train_norm, y_train_norm, output_folder=output_folder)\n    \n    print("\\n‚úÖ PROCESO COMPLETADO")\n    print("="*70)\n    \n    return X_train_norm, y_train_norm',
            '\n# Normalizar datos (asumiendo que X_train y y_train ya existen)\nX_train_norm, y_train_norm = normalize_train_data(\n    X_train, \n    y_train,\n    scalers_folder="01_Models_scaler",\n    output_folder="02_Data_ML_traditional",\n    chunk_size=100  # Ajusta seg√∫n tu memoria disponible\n)',
            '# Verificar escaladores\nimport joblib\n\nscalers_X = joblib.load("01_Models_scaler/scalers_X.pkl")\nscalers_y = joblib.load("01_Models_scaler/scalers_y.pkl")\n\nprint(f"Escaladores X: {len(scalers_X)}")\nprint(f"Escaladores Y: {len(scalers_y)}")\nprint(f"X_train shape: {X_train.shape}")\nprint(f"y_train shape: {y_train.shape}")\n\n# Verificar que los nombres coinciden\nprint(f"\\nColumnas en X_train: {len(X_train.columns)}")\nprint(f"Columnas en escaladores: {len(scalers_X)}")\nprint(f"¬øCoinciden? {set(X_train.columns) == set(scalers_X.keys())}")',
            '"""\nsave_data.py - Guardado de datasets de Machine Learning\n\nEste script guarda todos los datasets de machine learning en formato pickle:\n- Datos originales (X_train, y_train, X_test, y_test)\n- Datos normalizados (X_train_norm, y_train_norm)\n- Informaci√≥n temporal (Time_train, Time_test)\n- Identificadores de series (series_id_train, series_id_test)\n- Archivo de metadatos CSV\n\nPrerequisitos:\n    - Los datos deben estar disponibles en memoria\n    - Carpeta de destino: notebook/02_Data_ML_traditional/\n\nOutputs:\n    - 10 archivos .pkl con los datasets\n    - 1 archivo datasets_metadata.csv con informaci√≥n\n"""\n\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Optional\n\n\nclass DataSaver:\n    """\n    Clase para guardar datasets de Machine Learning de forma organizada.\n    """\n    \n    def __init__(self, output_folder: str = "notebook/02_Data_ML_traditional"):\n        """\n        Inicializa el guardador de datos.\n        \n        Args:\n            output_folder: Carpeta donde guardar los datasets\n        """\n        self.output_folder = Path(output_folder)\n        self.file_paths = {}\n        \n    def save_all_datasets(\n        self,\n        X_train: pd.DataFrame,\n        y_train: pd.DataFrame,\n        X_test: pd.DataFrame,\n        y_test: pd.DataFrame,\n        X_train_norm: pd.DataFrame,\n        y_train_norm: pd.DataFrame,\n        Time_train: pd.Series,\n        Time_test: pd.Series,\n        series_id_train: pd.Series,\n        series_id_test: pd.Series\n    ) -> Dict[str, Path]:\n        """\n        Guarda todos los datasets de ML.\n        \n        Args:\n            X_train: Features de entrenamiento (original)\n            y_train: Targets de entrenamiento (original)\n            X_test: Features de test (original)\n            y_test: Targets de test (original)\n            X_train_norm: Features de entrenamiento (normalizado)\n            y_train_norm: Targets de entrenamiento (normalizado)\n            Time_train: Timestamps de entrenamiento\n            Time_test: Timestamps de test\n            series_id_train: IDs de series de entrenamiento\n            series_id_test: IDs de series de test\n            \n        Returns:\n            Diccionario con rutas de archivos guardados\n        """\n        print("\\n" + "="*80)\n        print("üíæ GUARDANDO DATASETS DE MACHINE LEARNING")\n        print("="*80)\n        \n        # Crear carpeta de destino\n        self._create_output_folder()\n        \n        # Guardar datos originales\n        self._save_original_data(X_train, y_train, X_test, y_test)\n        \n        # Guardar datos normalizados\n        self._save_normalized_data(X_train_norm, y_train_norm)\n        \n        # Guardar informaci√≥n temporal y series\n        self._save_temporal_info(Time_train, Time_test, series_id_train, series_id_test)\n        \n        # Crear metadatos\n        self._create_metadata(\n            X_train, y_train, X_test, y_test,\n            X_train_norm, y_train_norm,\n            Time_train, Time_test,\n            series_id_train, series_id_test\n        )\n        \n        # Resumen final\n        self._print_summary(X_train, X_test, y_train)\n        \n        return self.file_paths\n    \n    def _create_output_folder(self) -> None:\n        """Crea la carpeta de salida si no existe."""\n        os.makedirs(self.output_folder, exist_ok=True)\n        print(f"\\nüìÅ Carpeta de destino: {self.output_folder}")\n    \n    def _save_original_data(\n        self,\n        X_train: pd.DataFrame,\n        y_train: pd.DataFrame,\n        X_test: pd.DataFrame,\n        y_test: pd.DataFrame\n    ) -> None:\n        """Guarda datos originales (sin normalizar)."""\n        print(f"\\n{\'=\'*80}")\n        print("PASO 1: GUARDAR DATOS ORIGINALES (SIN NORMALIZAR)")\n        print("="*80)\n        \n        # X_train\n        print(f"\\n[1/4] Guardando X_train...")\n        X_train_path = self.output_folder / \'X_train.pkl\'\n        joblib.dump(X_train, X_train_path)\n        self.file_paths[\'X_train\'] = X_train_path\n        print(f"   ‚úÖ X_train guardado: {X_train.shape}")\n        print(f"      Tama√±o: {X_train_path.stat().st_size / (1024**2):.2f} MB")\n        \n        # y_train\n        print(f"\\n[2/4] Guardando y_train...")\n        y_train_path = self.output_folder / \'y_train.pkl\'\n        joblib.dump(y_train, y_train_path)\n        self.file_paths[\'y_train\'] = y_train_path\n        print(f"   ‚úÖ y_train guardado: {y_train.shape}")\n        print(f"      Tama√±o: {y_train_path.stat().st_size / (1024**2):.2f} MB")\n        \n        # X_test\n        print(f"\\n[3/4] Guardando X_test...")\n        X_test_path = self.output_folder / \'X_test.pkl\'\n        joblib.dump(X_test, X_test_path)\n        self.file_paths[\'X_test\'] = X_test_path\n        print(f"   ‚úÖ X_test guardado: {X_test.shape}")\n        print(f"      Tama√±o: {X_test_path.stat().st_size / (1024**2):.2f} MB")\n        \n        # y_test\n        print(f"\\n[4/4] Guardando y_test...")\n        y_test_path = self.output_folder / \'y_test.pkl\'\n        joblib.dump(y_test, y_test_path)\n        self.file_paths[\'y_test\'] = y_test_path\n        print(f"   ‚úÖ y_test guardado: {y_test.shape}")\n        print(f"      Tama√±o: {y_test_path.stat().st_size / (1024**2):.2f} MB")\n    \n    def _save_normalized_data(\n        self,\n        X_train_norm: pd.DataFrame,\n        y_train_norm: pd.DataFrame\n    ) -> None:\n        """Guarda datos normalizados."""\n        print(f"\\n{\'=\'*80}")\n        print("PASO 2: GUARDAR DATOS NORMALIZADOS")\n        print("="*80)\n        \n        # X_train_norm\n        print(f"\\n[1/2] Guardando X_train_norm...")\n        X_train_norm_path = self.output_folder / \'X_train_norm.pkl\'\n        joblib.dump(X_train_norm, X_train_norm_path)\n        self.file_paths[\'X_train_norm\'] = X_train_norm_path\n        print(f"   ‚úÖ X_train_norm guardado: {X_train_norm.shape}")\n        print(f"      Tama√±o: {X_train_norm_path.stat().st_size / (1024**2):.2f} MB")\n        \n        # y_train_norm\n        print(f"\\n[2/2] Guardando y_train_norm...")\n        y_train_norm_path = self.output_folder / \'y_train_norm.pkl\'\n        joblib.dump(y_train_norm, y_train_norm_path)\n        self.file_paths[\'y_train_norm\'] = y_train_norm_path\n        print(f"   ‚úÖ y_train_norm guardado: {y_train_norm.shape}")\n        print(f"      Tama√±o: {y_train_norm_path.stat().st_size / (1024**2):.2f} MB")\n        print(f"      üí° Normalizado con scalers independientes por target")\n    \n    def _save_temporal_info(\n        self,\n        Time_train: pd.Series,\n        Time_test: pd.Series,\n        series_id_train: pd.Series,\n        series_id_test: pd.Series\n    ) -> None:\n        """Guarda informaci√≥n temporal y de series."""\n        print(f"\\n{\'=\'*80}")\n        print("PASO 3: GUARDAR INFORMACI√ìN TEMPORAL Y SERIES")\n        print("="*80)\n        \n        # Time_train\n        print(f"\\n[1/4] Guardando Time_train...")\n        Time_train_path = self.output_folder / \'Time_train.pkl\'\n        joblib.dump(Time_train, Time_train_path)\n        self.file_paths[\'Time_train\'] = Time_train_path\n        print(f"   ‚úÖ Time_train guardado: {Time_train.shape}")\n        print(f"      Tama√±o: {Time_train_path.stat().st_size / (1024**2):.2f} MB")\n        \n        # Time_test\n        print(f"\\n[2/4] Guardando Time_test...")\n        Time_test_path = self.output_folder / \'Time_test.pkl\'\n        joblib.dump(Time_test, Time_test_path)\n        self.file_paths[\'Time_test\'] = Time_test_path\n        print(f"   ‚úÖ Time_test guardado: {Time_test.shape}")\n        print(f"      Tama√±o: {Time_test_path.stat().st_size / (1024**2):.2f} MB")\n        \n        # series_id_train\n        print(f"\\n[3/4] Guardando series_id_train...")\n        series_id_train_path = self.output_folder / \'series_id_train.pkl\'\n        joblib.dump(series_id_train, series_id_train_path)\n        self.file_paths[\'series_id_train\'] = series_id_train_path\n        print(f"   ‚úÖ series_id_train guardado: {series_id_train.shape}")\n        print(f"      Tama√±o: {series_id_train_path.stat().st_size / (1024**2):.2f} MB")\n        \n        # series_id_test\n        print(f"\\n[4/4] Guardando series_id_test...")\n        series_id_test_path = self.output_folder / \'series_id_test.pkl\'\n        joblib.dump(series_id_test, series_id_test_path)\n        self.file_paths[\'series_id_test\'] = series_id_test_path\n        print(f"   ‚úÖ series_id_test guardado: {series_id_test.shape}")\n        print(f"      Tama√±o: {series_id_test_path.stat().st_size / (1024**2):.2f} MB")\n    \n    def _create_metadata(\n        self,\n        X_train: pd.DataFrame,\n        y_train: pd.DataFrame,\n        X_test: pd.DataFrame,\n        y_test: pd.DataFrame,\n        X_train_norm: pd.DataFrame,\n        y_train_norm: pd.DataFrame,\n        Time_train: pd.Series,\n        Time_test: pd.Series,\n        series_id_train: pd.Series,\n        series_id_test: pd.Series\n    ) -> None:\n        """Crea archivo de metadatos."""\n        print(f"\\n{\'=\'*80}")\n        print("PASO 4: CREAR ARCHIVO DE METADATOS")\n        print("="*80)\n        \n        # Crear DataFrame con metadatos\n        metadata = {\n            \'Dataset\': [\n                \'X_train\', \'y_train\', \'X_test\', \'y_test\',\n                \'X_train_norm\', \'y_train_norm\', \'Time_train\', \'Time_test\',\n                \'series_id_train\', \'series_id_test\'\n            ],\n            \'Shape\': [\n                str(X_train.shape), str(y_train.shape), str(X_test.shape), str(y_test.shape),\n                str(X_train_norm.shape), str(y_train_norm.shape), str(Time_train.shape), str(Time_test.shape),\n                str(series_id_train.shape), str(series_id_test.shape)\n            ],\n            \'Tipo\': [\n                \'Features (original)\', \'Targets (original)\', \'Features (original)\', \'Targets (original)\',\n                \'Features (normalizado)\', \'Targets (norm. independiente)\', \'Time info\', \'Time info\',\n                \'Series ID\', \'Series ID\'\n            ],\n            \'Archivo\': [\n                \'X_train.pkl\', \'y_train.pkl\', \'X_test.pkl\', \'y_test.pkl\',\n                \'X_train_norm.pkl\', \'y_train_norm.pkl\', \'Time_train.pkl\', \'Time_test.pkl\',\n                \'series_id_train.pkl\', \'series_id_test.pkl\'\n            ],\n            \'Tama√±o_MB\': [\n                f"{self.file_paths[\'X_train\'].stat().st_size / (1024**2):.2f}",\n                f"{self.file_paths[\'y_train\'].stat().st_size / (1024**2):.2f}",\n                f"{self.file_paths[\'X_test\'].stat().st_size / (1024**2):.2f}",\n                f"{self.file_paths[\'y_test\'].stat().st_size / (1024**2):.2f}",\n                f"{self.file_paths[\'X_train_norm\'].stat().st_size / (1024**2):.2f}",\n                f"{self.file_paths[\'y_train_norm\'].stat().st_size / (1024**2):.2f}",\n                f"{self.file_paths[\'Time_train\'].stat().st_size / (1024**2):.2f}",\n                f"{self.file_paths[\'Time_test\'].stat().st_size / (1024**2):.2f}",\n                f"{self.file_paths[\'series_id_train\'].stat().st_size / (1024**2):.2f}",\n                f"{self.file_paths[\'series_id_test\'].stat().st_size / (1024**2):.2f}"\n            ]\n        }\n        \n        metadata_df = pd.DataFrame(metadata)\n        metadata_path = self.output_folder / \'datasets_metadata.csv\'\n        metadata_df.to_csv(metadata_path, index=False)\n        self.file_paths[\'metadata\'] = metadata_path\n        \n        print(f"\\n   ‚úÖ Metadatos guardados: {metadata_path.name}")\n        print(f"\\n   üìä RESUMEN:")\n        print(metadata_df.to_string(index=False))\n    \n    def _print_summary(\n        self,\n        X_train: pd.DataFrame,\n        X_test: pd.DataFrame,\n        y_train: pd.DataFrame\n    ) -> None:\n        """Imprime resumen final."""\n        # Calcular tama√±o total\n        total_size = sum([\n            path.stat().st_size for key, path in self.file_paths.items() \n            if key != \'metadata\'\n        ]) / (1024**2)\n        \n        print(f"\\n{\'=\'*80}")\n        print("‚úÖ DATASETS GUARDADOS CORRECTAMENTE")\n        print("="*80)\n        \n        print(f"\\nüìä ESTAD√çSTICAS:")\n        print(f"   ‚Ä¢ Total de archivos: 10 + 1 metadata")\n        print(f"   ‚Ä¢ Tama√±o total: {total_size:.2f} MB")\n        print(f"   ‚Ä¢ Muestras train: {len(X_train):,}")\n        print(f"   ‚Ä¢ Muestras test: {len(X_test):,}")\n        print(f"   ‚Ä¢ Features: {X_train.shape[1]}")\n        print(f"   ‚Ä¢ Targets: {y_train.shape[1]} (M_0, M_1c, M_1s)")\n        \n        print(f"\\nüìÅ UBICACI√ìN:")\n        print(f"   {self.output_folder.absolute()}")\n        \n        print(f"\\nüí° C√ìMO CARGAR LOS DATOS:")\n        print(f"   ```python")\n        print(f"   import joblib")\n        print(f"   ")\n        print(f"   # Cargar datos originales")\n        print(f"   X_train = joblib.load(\'{self.output_folder / \'X_train.pkl\'}\')")\n        print(f"   y_train = joblib.load(\'{self.output_folder / \'y_train.pkl\'}\')")\n        print(f"   X_test = joblib.load(\'{self.output_folder / \'X_test.pkl\'}\')")\n        print(f"   y_test = joblib.load(\'{self.output_folder / \'y_test.pkl\'}\')")\n        print(f"   ")\n        print(f"   # Cargar datos normalizados")\n        print(f"   X_train_norm = joblib.load(\'{self.output_folder / \'X_train_norm.pkl\'}\')")\n        print(f"   y_train_norm = joblib.load(\'{self.output_folder / \'y_train_norm.pkl\'}\')")\n        print(f"   ")\n        print(f"   # IMPORTANTE: y_train_norm usa scalers independientes por target")\n        print(f"   # Para normalizar/desnormalizar datos nuevos:")\n        print(f"   scalers_y = joblib.load(\'notebook/01_Models_scaler/scalers_y.pkl\')")\n        print(f"   # Normalizar: y_norm[col] = scalers_y[col].transform(y[[col]])")\n        print(f"   # Desnormalizar: y[col] = scalers_y[col].inverse_transform(y_norm[[col]])")\n        print(f"   ```")\n        print("="*80)\n\n\ndef save_ml_datasets(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_test: pd.DataFrame,\n    X_train_norm: pd.DataFrame,\n    y_train_norm: pd.DataFrame,\n    Time_train: pd.Series,\n    Time_test: pd.Series,\n    series_id_train: pd.Series,\n    series_id_test: pd.Series,\n    output_folder: str = "02_Data_ML_traditional"\n) -> Dict[str, Path]:\n    """\n    Funci√≥n principal para guardar todos los datasets de ML.\n    \n    Args:\n        X_train: Features de entrenamiento (original)\n        y_train: Targets de entrenamiento (original)\n        X_test: Features de test (original)\n        y_test: Targets de test (original)\n        X_train_norm: Features de entrenamiento (normalizado)\n        y_train_norm: Targets de entrenamiento (normalizado)\n        Time_train: Timestamps de entrenamiento\n        Time_test: Timestamps de test\n        series_id_train: IDs de series de entrenamiento\n        series_id_test: IDs de series de test\n        output_folder: Carpeta de salida (default: notebook/02_Data_ML_traditional)\n        \n    Returns:\n        Diccionario con rutas de todos los archivos guardados\n        \n    Example:\n        >>> file_paths = save_ml_datasets(\n        ...     X_train, y_train, X_test, y_test,\n        ...     X_train_norm, y_train_norm,\n        ...     Time_train, Time_test,\n        ...     series_id_train, series_id_test\n        ... )\n    """\n    saver = DataSaver(output_folder)\n    return saver.save_all_datasets(\n        X_train, y_train, X_test, y_test,\n        X_train_norm, y_train_norm,\n        Time_train, Time_test,\n        series_id_train, series_id_test\n    )\n',
            '\n# Guardar todos los datasets\nfile_paths = save_ml_datasets(\n    X_train, y_train, X_test, y_test,\n    X_train_norm, y_train_norm,\n    Time_train, Time_test,\n    series_id_train, series_id_test\n)',
            '# Qu√© series estan en test ??????\n\nimport joblib\n\n# Cargar\nseries_id_test = joblib.load(Path(root_dir) / "notebook" / "02_Data_ML_traditional" / "series_id_test.pkl")\n\n# Ver series √∫nicas\nprint(f"Series en TEST: {sorted(series_id_test.unique())}")\nprint(f"Total series: {series_id_test.nunique()}")\nprint(f"Total filas: {len(series_id_test):,}")',
        ],
    },
    {
        "title": 'üéØ PASO 7: MODELADO - BASELINE RIDGE REGRESSION',
        "cells": [
            "Path('03_ML_traditional_models')",
            '# =============================================================================\n# PASO 7.1: BASELINE - RIDGE REGRESSION\n# =============================================================================\nfrom sklearn.linear_model import ElasticNet\n\nprint("\\n" + "="*80)\nprint("üéØ MODELO BASELINE: RIDGE REGRESSION")\nprint("="*80)\n\nimport time\n\n# Crear carpeta para guardar resultados\ntraining_folder = Path(\'03_ML_traditional_models\') / \'Linear_Ridge\'\nos.makedirs(training_folder, exist_ok=True)\n\nprint(f"\\nüìÅ Carpeta de entrenamiento: {training_folder}")\n\n# ------------------------------------------------------------------------\n# 1) Normalizar datos de TEST usando los scalers guardados\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: NORMALIZAR DATOS DE TEST")\nprint("="*80)\n\nprint(f"\\n[1/2] Normalizando X_test usando scaler guardado...")\n\n# Cargar el scaler de X desde la carpeta 01_Models_scaler\n# ‚úÖ (scalers independientes)\nscalers_X_path = root_dir / \'notebook\' / \'01_Models_scaler\' / \'scalers_X.pkl\'\nscalers_X = joblib.load(scalers_X_path)\n\n# Transformar X_test (cada columna con su scaler)\nX_test_norm = pd.DataFrame(index=X_test.index)\nfor col in X_test.columns:\n    if col not in scalers_X:\n        raise KeyError(f"No se encuentra scaler para columna: {col}")\n    X_test_norm[col] = scalers_X[col].transform(X_test[[col]]).ravel()\n\nprint(f"   ‚úÖ X_test normalizado: {X_test_norm.shape}")\nprint(f"   üìÇ Scalers cargados de: {scalers_X_path}")\nprint(f"   üí° {len(scalers_X)} scalers independientes usados")\n\nprint(f"   ‚úÖ X_test normalizado: {X_test_norm.shape}")\nprint(f"   üìÇ Scaler cargado de: {scalers_X_path}")\n\nprint(f"\\n[2/2] Normalizando y_test usando scalers independientes...")\n\n# Cargar los scalers de y desde la carpeta 01_Models_scaler\nscalers_y_path = root_dir / \'notebook\' / \'01_Models_scaler\' / \'scalers_y.pkl\'\nscalers_y = joblib.load(scalers_y_path)\n\n# Transformar y_test (cada columna con su scaler)\ny_test_norm = pd.DataFrame(index=y_test.index)\nfor col in y_test.columns:\n    y_test_norm[col] = scalers_y[col].transform(y_test[[col]]).ravel()\n\nprint(f"   ‚úÖ y_test normalizado: {y_test_norm.shape}")\nprint(f"   üìÇ Scalers cargados de: {scalers_y_path}")\nprint(f"   üí° {len(scalers_y)} scalers independientes: {list(scalers_y.keys())}")\n\n# ------------------------------------------------------------------------\n# 2) Verificar datos de entrenamiento normalizados\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("VERIFICACI√ìN: DATOS DE ENTRENAMIENTO NORMALIZADOS")\nprint("="*80)\n\nprint(f"\\n   ‚úÖ X_train_norm disponible: {X_train_norm.shape}")\nprint(f"   ‚úÖ y_train_norm disponible: {y_train_norm.shape}")\nprint(f"\\n   üí° Usando datos ya normalizados en pasos anteriores")\nprint(f"   üí° y_train_norm usa {len(scalers_y)} scalers independientes por target")\n\n# ------------------------------------------------------------------------\n# 3) Configurar y entrenar modelo Ridge\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: ENTRENAMIENTO DEL MODELO")\nprint("="*80)\n\nprint(f"\\n[1/3] Configurando Ridge Regression...")\n\n# Configurar Ridge con MultiOutputRegressor para predecir los 3 targets\nridge_model = MultiOutputRegressor(\n    Ridge(\n        alpha=1.0,          # Par√°metro de regularizaci√≥n\n        fit_intercept=True,\n        solver=\'auto\', #saga\n        max_iter=1000,      # Iteraciones m√°ximas\n        tol=1e-3,           # Tolerancia de convergencia\n        random_state=42\n    )\n)\n\n\n\nprint(f"   ‚úÖ Modelo configurado:")\nprint(f"      ‚Ä¢ Tipo: Ridge Regression")\nprint(f"      ‚Ä¢ Alpha (regularizaci√≥n): 1.0")\nprint(f"      ‚Ä¢ Outputs: {len(y_train_norm.columns)} targets (M_0, M_1c, M_1s)")\n\nprint(f"\\n[2/3] Entrenando modelo con datos normalizados...")\nprint(f"   ‚Ä¢ Datos de entrenamiento: {X_train_norm.shape}")\nprint(f"   ‚Ä¢ Targets de entrenamiento: {y_train_norm.shape}")\n\nstart_time = time.time()\n\n# Entrenar el modelo con datos normalizados\nridge_model.fit(X_train_norm, y_train_norm)\n\ntraining_time = time.time() - start_time\n\nprint(f"   ‚úÖ Modelo entrenado en {training_time:.2f} segundos")\n\nprint(f"\\n[3/3] Guardando modelo...")\n\n# Guardar el modelo entrenado\nmodel_path = training_folder / \'ridge_model.pkl\'\njoblib.dump(ridge_model, model_path)\n\nprint(f"   ‚úÖ Modelo guardado: {model_path.name}")\n\n# ------------------------------------------------------------------------\n# 4) Realizar predicciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: PREDICCIONES")\nprint("="*80)\n\nprint(f"\\n[1/2] Prediciendo sobre conjunto de entrenamiento...")\n\ny_train_pred_norm = ridge_model.predict(X_train_norm)\n# Justo DESPU√âS de y_train_pred = model.predict(X_train_norm), a√±ade:\n\nprint(f"\\nüîç VERIFICACI√ìN DE PREDICCIONES:")\nprint(f"   y_train_pred NaNs: {pd.DataFrame(y_train_pred_norm).isna().sum().sum():,}")\n\nif pd.DataFrame(y_train_pred_norm).isna().sum().sum() > 0:\n    print(f"\\n‚ùå ERROR: Predicciones contienen NaNs")\n    print(f"   Esto indica que:")\n    print(f"      1. X_train_norm ten√≠a NaNs (verifica arriba)")\n    print(f"      2. El modelo no convergi√≥ correctamente")\n    print(f"      3. Hay features con varianza 0 o infinita")\n    raise ValueError("Predicciones inv√°lidas")\n\ny_train_pred_norm = pd.DataFrame(\n    y_train_pred_norm,\n    index=y_train_norm.index,\n    columns=y_train_norm.columns\n)\n\nprint(f"   ‚úÖ Predicciones train: {y_train_pred_norm.shape}")\n\nprint(f"\\n[2/2] Prediciendo sobre conjunto de test...")\n\ny_test_pred_norm = ridge_model.predict(X_test_norm)\ny_test_pred_norm = pd.DataFrame(\n    y_test_pred_norm,\n    index=y_test_norm.index,\n    columns=y_test_norm.columns\n)\n\nprint(f"   ‚úÖ Predicciones test: {y_test_pred_norm.shape}")\n\n# ------------------------------------------------------------------------\n# 5) Desnormalizar predicciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: DESNORMALIZACI√ìN DE PREDICCIONES")\nprint("="*80)\n\nprint(f"\\n[1/2] Desnormalizando predicciones de train con scalers independientes...")\n\n# Usar los scalers independientes para desnormalizar\ny_train_pred = pd.DataFrame(index=y_train_pred_norm.index)\nfor col in y_train_pred_norm.columns:\n    y_train_pred[col] = scalers_y[col].inverse_transform(y_train_pred_norm[[col]]).ravel()\n\nprint(f"   ‚úÖ Predicciones train desnormalizadas")\n\nprint(f"\\n[2/2] Desnormalizando predicciones de test con scalers independientes...")\n\n# Usar los scalers independientes para desnormalizar\ny_test_pred = pd.DataFrame(index=y_test_pred_norm.index)\nfor col in y_test_pred_norm.columns:\n    y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm[[col]]).ravel()\n\nprint(f"   ‚úÖ Predicciones test desnormalizadas")\n\n# ------------------------------------------------------------------------\n# 6) Calcular m√©tricas\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 5: C√ÅLCULO DE M√âTRICAS")\nprint("="*80)\n\n# Diccionarios para almacenar m√©tricas\nmetrics_train = {}\nmetrics_test = {}\n\nprint(f"\\n{\'M√âTRICA\':20} {\'CONJUNTO\':10} {\'M_0\':>15} {\'M_1c\':>15} {\'M_1s\':>15}")\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\n\n# Calcular m√©tricas para cada target\nfor i, col in enumerate(y_train.columns):\n    # TRAIN\n    rmse_train = np.sqrt(mean_squared_error(y_train[col], y_train_pred[col]))\n    r2_train = r2_score(y_train[col], y_train_pred[col])\n    \n    # TEST\n    rmse_test = np.sqrt(mean_squared_error(y_test[col], y_test_pred[col]))\n    r2_test = r2_score(y_test[col], y_test_pred[col])\n    \n    # Guardar m√©tricas\n    metrics_train[col] = {\'RMSE\': rmse_train, \'R2\': r2_train}\n    metrics_test[col] = {\'RMSE\': rmse_test, \'R2\': r2_test}\n    \n    # Mostrar\n    if i == 0:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {rmse_train:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {rmse_test:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {r2_train:>15.4f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {r2_test:>15.4f} {\'-\':>15} {\'-\':>15}")\n    elif i == 1:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {rmse_train:>15.2f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {rmse_test:>15.2f} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {r2_train:>15.4f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {r2_test:>15.4f} {\'-\':>15}")\n    else:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {rmse_train:>15.2f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {rmse_test:>15.2f}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {r2_train:>15.4f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {r2_test:>15.4f}")\n\n# Calcular promedios\navg_rmse_train = np.mean([m[\'RMSE\'] for m in metrics_train.values()])\navg_rmse_test = np.mean([m[\'RMSE\'] for m in metrics_test.values()])\navg_r2_train = np.mean([m[\'R2\'] for m in metrics_train.values()])\navg_r2_test = np.mean([m[\'R2\'] for m in metrics_test.values()])\n\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\nprint(f"{\'PROMEDIO RMSE\':20} {\'TRAIN\':10} {avg_rmse_train:>15.2f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_rmse_test:>15.2f}")\nprint(f"{\'PROMEDIO R¬≤\':20} {\'TRAIN\':10} {avg_r2_train:>15.4f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_r2_test:>15.4f}")\n\n# ------------------------------------------------------------------------\n# 7) Visualizaciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 6: GENERACI√ìN DE GR√ÅFICAS")\nprint("="*80)\n\n# 7.1) Gr√°fica de m√©tricas por target\nprint(f"\\n[1/5] Creando gr√°fica de m√©tricas...")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# RMSE\ntargets = list(metrics_train.keys())\nrmse_train_vals = [metrics_train[t][\'RMSE\'] for t in targets]\nrmse_test_vals = [metrics_test[t][\'RMSE\'] for t in targets]\n\nx = np.arange(len(targets))\nwidth = 0.35\n\naxes[0].bar(x - width/2, rmse_train_vals, width, label=\'Train\', alpha=0.8)\naxes[0].bar(x + width/2, rmse_test_vals, width, label=\'Test\', alpha=0.8)\naxes[0].set_xlabel(\'Target\')\naxes[0].set_ylabel(\'RMSE\')\naxes[0].set_title(\'RMSE por Target - Ridge Regression\')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(targets, rotation=45, ha=\'right\')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# R¬≤\nr2_train_vals = [metrics_train[t][\'R2\'] for t in targets]\nr2_test_vals = [metrics_test[t][\'R2\'] for t in targets]\n\naxes[1].bar(x - width/2, r2_train_vals, width, label=\'Train\', alpha=0.8)\naxes[1].bar(x + width/2, r2_test_vals, width, label=\'Test\', alpha=0.8)\naxes[1].set_xlabel(\'Target\')\naxes[1].set_ylabel(\'R¬≤\')\naxes[1].set_title(\'R¬≤ por Target - Ridge Regression\')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(targets, rotation=45, ha=\'right\')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim([0, 1])\n\nplt.tight_layout()\nmetrics_plot_path = training_folder / \'metrics_comparison.png\'\nplt.savefig(metrics_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {metrics_plot_path.name}")\n\n\n# 7.2) Gr√°ficas de predicciones vs reales\nprint(f"\\n[2/3] Creando gr√°ficas de predicciones vs reales...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(y_train.columns):\n    row = idx\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train[col], y_train_pred[col], alpha=0.3, s=1)\n    axes[row, 0].plot([y_train[col].min(), y_train[col].max()], \n                       [y_train[col].min(), y_train[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 0].set_xlabel(\'Real\')\n    axes[row, 0].set_ylabel(\'Predicho\')\n    axes[row, 0].set_title(f\'{col} - TRAIN (R¬≤={metrics_train[col]["R2"]:.4f})\')\n    axes[row, 0].legend()\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test[col], y_test_pred[col], alpha=0.3, s=1, color=\'orange\')\n    axes[row, 1].plot([y_test[col].min(), y_test[col].max()], \n                       [y_test[col].min(), y_test[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 1].set_xlabel(\'Real\')\n    axes[row, 1].set_ylabel(\'Predicho\')\n    axes[row, 1].set_title(f\'{col} - TEST (R¬≤={metrics_test[col]["R2"]:.4f})\')\n    axes[row, 1].legend()\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\npredictions_plot_path = training_folder / \'predictions_vs_real.png\'\nplt.savefig(predictions_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {predictions_plot_path.name}")\n\n# 7.3) Gr√°fica de residuos\nprint(f"\\n[3/3] Creando gr√°fica de residuos...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(y_train.columns):\n    row = idx\n    \n    # Calcular residuos\n    residuals_train = y_train[col] - y_train_pred[col]\n    residuals_test = y_test[col] - y_test_pred[col]\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train_pred[col], residuals_train, alpha=0.3, s=1)\n    axes[row, 0].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 0].set_xlabel(\'Predicho\')\n    axes[row, 0].set_ylabel(\'Residuo (Real - Predicho)\')\n    axes[row, 0].set_title(f\'{col} - Residuos TRAIN\')\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test_pred[col], residuals_test, alpha=0.3, s=1, color=\'orange\')\n    axes[row, 1].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 1].set_xlabel(\'Predicho\')\n    axes[row, 1].set_ylabel(\'Residuo (Real - Predicho)\')\n    axes[row, 1].set_title(f\'{col} - Residuos TEST\')\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nresiduals_plot_path = training_folder / \'residuals_analysis.png\'\nplt.savefig(residuals_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {residuals_plot_path.name}")\n\n# 7.4) Gr√°fica de time series: Real vs Predicho (3 series aleatorias de test)\nprint(f"\\n[4/5] Creando gr√°ficas de time series (Real vs Predicho)...")\n\n# Obtener series √∫nicas del conjunto de test\nunique_test_series = series_id_test.unique()\n\n# Seleccionar 3 series aleatorias\nnp.random.seed(42)\nselected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n\nprint(f"   ‚Ä¢ Series seleccionadas: {selected_series}")\n\n# Crear figura con 3 filas x 3 columnas (3 series, 3 targets)\nfig, axes = plt.subplots(3, 3, figsize=(20, 14))\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales y predichos\n        y_real = y_test.loc[series_indices, col]\n        y_pred = y_test_pred.loc[series_indices, col]\n        \n        # Calcular m√©tricas para esta serie y target\n        r2_series = r2_score(y_real, y_pred)\n        rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n        \n        # Graficar\n        ax.plot(time_series, y_real, label=\'Real\', \n                linewidth=2, alpha=0.8, color=\'blue\')\n        ax.plot(time_series, y_pred, label=\'Predicho\', \n                linestyle=\'--\', linewidth=2, alpha=0.8, color=\'red\')\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Valor\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}\', \n                     fontsize=10, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=8)\n        ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\ntimeseries_plot_path = training_folder / \'timeseries_comparison.png\'\nplt.savefig(timeseries_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}")\n\n# 7.5) Gr√°fica de time series con ZOOM (50 segundos)\nprint(f"\\n[5/5] Creando gr√°ficas de time series con zoom (50s)...")\n\n# Crear figura con 3 filas x 3 columnas (3 series, 3 targets)\nfig, axes = plt.subplots(3, 3, figsize=(20, 14))\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Definir ventana de 50 segundos desde el inicio\n    time_min = time_series.min()\n    time_max_zoom = time_min + 50\n    \n    # Filtrar por ventana de tiempo\n    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n    zoom_indices = time_series[zoom_mask].index\n    time_zoom = time_series[zoom_mask]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales y predichos (con zoom)\n        y_real_zoom = y_test.loc[zoom_indices, col]\n        y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n        \n        # Calcular m√©tricas para esta ventana\n        r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n        rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n        \n        # Graficar\n        ax.plot(time_zoom, y_real_zoom, label=\'Real\', \n                linewidth=2.5, alpha=0.8, color=\'blue\', marker=\'o\', markersize=4)\n        ax.plot(time_zoom, y_pred_zoom, label=\'Predicho\', \n                linestyle=\'--\', linewidth=2.5, alpha=0.8, color=\'red\', marker=\'x\', markersize=5)\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Valor\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}\', \n                     fontsize=10, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=8)\n        ax.grid(True, alpha=0.3)\n        \n        # A√±adir texto con informaci√≥n de puntos\n        n_points = len(zoom_indices)\n        ax.text(0.02, 0.02, f\'Puntos: {n_points}\', transform=ax.transAxes, fontsize=8,\n                verticalalignment=\'bottom\', bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.5))\n\nplt.tight_layout()\ntimeseries_zoom_plot_path = training_folder / \'timeseries_comparison_zoom50s.png\'\nplt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 8) Guardar m√©tricas en archivo\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 7: GUARDAR RESULTADOS")\nprint("="*80)\n\n# Crear DataFrame con m√©tricas\nmetrics_df = pd.DataFrame({\n    \'Blade\': list(metrics_train.keys()) + [\'PROMEDIO\'],\n    \'RMSE_Train\': list([m[\'RMSE\'] for m in metrics_train.values()]) + [avg_rmse_train],\n    \'RMSE_Test\': list([m[\'RMSE\'] for m in metrics_test.values()]) + [avg_rmse_test],\n    \'R2_Train\': list([m[\'R2\'] for m in metrics_train.values()]) + [avg_r2_train],\n    \'R2_Test\': list([m[\'R2\'] for m in metrics_test.values()]) + [avg_r2_test]\n})\n\nmetrics_csv_path = training_folder / \'metrics_results.csv\'\nmetrics_df.to_csv(metrics_csv_path, index=False)\n\nprint(f"\\n   ‚úÖ M√©tricas guardadas: {metrics_csv_path.name}")\n\n# Crear resumen de entrenamiento\nsummary = {\n    \'Model\': \'Ridge Regression\',\n    \'Training_Time_seconds\': training_time,\n    \'Train_Samples\': len(X_train),\n    \'Test_Samples\': len(X_test),\n    \'Features\': X_train.shape[1],\n    \'Targets\': y_train.shape[1],\n    \'Avg_RMSE_Train\': avg_rmse_train,\n    \'Avg_RMSE_Test\': avg_rmse_test,\n    \'Avg_R2_Train\': avg_r2_train,\n    \'Avg_R2_Test\': avg_r2_test,\n    \'Alpha\': 1.0\n}\n\nsummary_df = pd.DataFrame([summary])\nsummary_path = training_folder / \'training_summary.csv\'\nsummary_df.to_csv(summary_path, index=False)\n\nprint(f"   ‚úÖ Resumen guardado: {summary_path.name}")\n\n# ------------------------------------------------------------------------\n# RESUMEN FINAL\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ ENTRENAMIENTO COMPLETADO - RIDGE REGRESSION")\nprint("="*80)\n\nprint(f"\\nüìä RESULTADOS FINALES:")\nprint(f"   ‚Ä¢ Tiempo de entrenamiento: {training_time:.2f} segundos")\nprint(f"   ‚Ä¢ RMSE promedio (Train): {avg_rmse_train:.2f}")\nprint(f"   ‚Ä¢ RMSE promedio (Test):  {avg_rmse_test:.2f}")\nprint(f"   ‚Ä¢ R¬≤ promedio (Train):   {avg_r2_train:.4f}")\nprint(f"   ‚Ä¢ R¬≤ promedio (Test):    {avg_r2_test:.4f}")\n\nprint(f"\\nüìÅ ARCHIVOS GENERADOS:")\nprint(f"   ‚Ä¢ {model_path.name}")\nprint(f"   ‚Ä¢ {metrics_csv_path.name}")\nprint(f"   ‚Ä¢ {summary_path.name}")\nprint(f"   ‚Ä¢ {metrics_plot_path.name}")\nprint(f"   ‚Ä¢ {predictions_plot_path.name}")\nprint(f"   ‚Ä¢ {residuals_plot_path.name}")\nprint(f"   ‚Ä¢ {timeseries_plot_path.name}")\nprint(f"   ‚Ä¢ {timeseries_zoom_plot_path.name}")\n\nprint(f"\\nüí° El modelo Ridge sirve como baseline para comparar con modelos m√°s complejos.")\nprint("="*80)',
            '# A√±ade esto despu√©s de cargar los datos normalizados\nprint("\\nüîç AN√ÅLISIS DE CORRELACIONES")\nprint("="*70)\n\n# Correlaci√≥n entre features y targets\ncorrelations = pd.concat([X_train, y_train], axis=1).corr()\n\nprint("\\nTop 10 features correlacionadas con cada target:")\nfor target in y_train.columns:\n    print(f"\\n{target}:")\n    top_corr = correlations[target].drop(y_train.columns).abs().sort_values(ascending=False).head(10)\n    print(top_corr)',
            '# Despu√©s de la normalizaci√≥n\nprint("\\nüìä VARIABILIDAD DE LOS TARGETS")\nprint("="*70)\n\nfor col in y_train.columns:\n    std_original = y_train[col].std()\n    mean_original = y_train[col].mean()\n    cv = std_original / abs(mean_original) if mean_original != 0 else 0\n    \n    print(f"\\n{col}:")\n    print(f"   Mean: {mean_original:.2e}")\n    print(f"   Std:  {std_original:.2e}")\n    print(f"   CV:   {cv:.4f} (coef. variaci√≥n)")',
        ],
    },
    {
        "title": '7.2 Random Forest Regressor',
        "cells": [
            '# =============================================================================\n# MODELO 2: RANDOM FOREST - ENTRENAMIENTO (OPTIMIZADO MEMORIA)\n# =============================================================================\n\nprint("\\n" + "="*80)\nprint("üéØ MODELO: RANDOM FOREST - ENTRENAMIENTO")\nprint("="*80)\n\nimport time\n\n# Crear carpeta para guardar resultados\ntraining_folder = Path(\'03_ML_traditional_models\') / \'Random_Forest\'\nos.makedirs(training_folder, exist_ok=True)\n\nprint(f"\\nüìÅ Carpeta de entrenamiento: {training_folder}")\n\n# ------------------------------------------------------------------------\n# 1) Cargar datos ya procesados desde 02_Data_ML_traditional\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: CARGAR DATOS YA PROCESADOS")\nprint("="*80)\n\ndata_ml_folder = root_dir / "notebook" / \'02_Data_ML_traditional\'\n\nprint(f"\\nüìÇ Carpeta de datos: {data_ml_folder}")\n\nprint(f"\\n[1/2] Cargando datos de entrenamiento normalizados...")\nX_train_norm = joblib.load(data_ml_folder / \'X_train_norm.pkl\')\ny_train_norm = joblib.load(data_ml_folder / \'y_train_norm.pkl\')\n\n# ‚úÖ OPTIMIZACI√ìN 1: Convertir a float32 (ahorra 50% de memoria)\nX_train_norm = X_train_norm.astype(\'float32\')\ny_train_norm = y_train_norm.astype(\'float32\')\n\nprint(f"   ‚úÖ X_train_norm: {X_train_norm.shape}")\nprint(f"   ‚úÖ y_train_norm: {y_train_norm.shape}")\nprint(f"   üíæ Memoria X_train_norm: {X_train_norm.memory_usage(deep=True).sum() / (1024**2):.1f} MB")\nprint(f"   üí° Targets: {list(y_train_norm.columns)}")\nprint(f"   üí° Normalizado con scalers independientes por target")\n\nprint(f"\\n[2/2] Cargando datos originales de train (para m√©tricas)...")\nX_train = joblib.load(data_ml_folder / \'X_train.pkl\')\ny_train = joblib.load(data_ml_folder / \'y_train.pkl\')\nprint(f"   ‚úÖ X_train: {X_train.shape}")\nprint(f"   ‚úÖ y_train: {y_train.shape}")\n\n# ------------------------------------------------------------------------\n# 2) Configurar y entrenar modelo Random Forest (OPTIMIZADO)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: CONFIGURAR Y ENTRENAR MODELO")\nprint("="*80)\n\nprint(f"\\n[1/3] Configurando Random Forest (optimizado para memoria)...")\n\n# ‚úÖ OPTIMIZACI√ìN 2: Configuraci√≥n optimizada de Random Forest\n# rf_model = MultiOutputRegressor(\n#     RandomForestRegressor(\n#         n_estimators=50,        # ‚úÖ Reducido de 100 a 50 (menos memoria)\n#         max_depth=15,           # ‚úÖ Reducido de 20 a 15 (√°rboles m√°s peque√±os)\n#         min_samples_split=10,   # ‚úÖ Aumentado de 5 a 10 (menos divisiones)\n#         min_samples_leaf=5,     # ‚úÖ Aumentado de 2 a 5 (hojas m√°s grandes)\n#         max_features=\'sqrt\',    # ‚úÖ Solo sqrt(n_features) por split (menos memoria)\n#         max_samples=0.8,        # ‚úÖ Bootstrap con 80% de datos (menos memoria)\n#         random_state=42,\n#         n_jobs=-1,              # Usar todos los cores\n#         verbose=1               # ‚úÖ Mostrar progreso\n#     )\n# )\n\nrf_model = MultiOutputRegressor(\n    RandomForestRegressor(\n        n_estimators=800,       # ‚¨ÜÔ∏è Sube a 100 si puedes (reduce la varianza al promediar m√°s)\n        max_depth=18,           # ‚¨áÔ∏è B√°jalo un poco m√°s. 10-12 suele bastar para f√≠sica.\n        min_samples_split=40,  # ‚¨ÜÔ∏è ¬°Dr√°stico! No intentes dividir si hay pocos datos.\n        min_samples_leaf=20,   # ‚¨ÜÔ∏è ¬°CR√çTICO! Una hoja debe tener ~100 muestras (10 segundos de datos)\n        max_features=\'sqrt\',    # ‚úÖ Mantenlo, es excelente.\n        max_samples=0.7,        # ‚¨áÔ∏è Bajar a 0.5 hace cada √°rbol m√°s independiente y reduce overfitting.\n        random_state=42,\n        n_jobs=-1,\n        verbose=1\n    )\n)\n\n\n\nprint(f"   ‚úÖ Modelo configurado (OPTIMIZADO PARA MEMORIA):")\nprint(f"      ‚Ä¢ Tipo: Random Forest")\nprint(f"      ‚Ä¢ N estimators: 50 (reducido para memoria)")\nprint(f"      ‚Ä¢ Max depth: 15 (√°rboles menos profundos)")\nprint(f"      ‚Ä¢ Min samples split: 10")\nprint(f"      ‚Ä¢ Min samples leaf: 5")\nprint(f"      ‚Ä¢ Max features: sqrt({X_train_norm.shape[1]}) ‚âà {int(np.sqrt(X_train_norm.shape[1]))}")\nprint(f"      ‚Ä¢ Max samples: 80% (bootstrap)")\nprint(f"      ‚Ä¢ Outputs: {len(y_train_norm.columns)} targets (M_0, M_1c, M_1s)")\n\n# ‚úÖ OPTIMIZACI√ìN 3 (OPCIONAL): Submuestreo si a√∫n hay problemas de memoria\nUSE_SUBSAMPLING = False  # Cambiar a True si sigue habiendo problemas\n\nif USE_SUBSAMPLING:\n    print(f"\\n‚ö†Ô∏è  SUBMUESTREO ACTIVADO (para reducir memoria)")\n    \n    # Submuestreo estratificado por series\n    unique_series = series_id_train.unique()\n    n_series_to_use = int(len(unique_series) * 0.5)  # 50% de series\n    \n    np.random.seed(42)\n    selected_series = np.random.choice(unique_series, size=n_series_to_use, replace=False)\n    \n    mask = series_id_train.isin(selected_series)\n    X_train_norm_sampled = X_train_norm[mask]\n    y_train_norm_sampled = y_train_norm[mask]\n    \n    print(f"   Series: {len(unique_series)} ‚Üí {n_series_to_use}")\n    print(f"   Muestras: {len(X_train_norm):,} ‚Üí {len(X_train_norm_sampled):,}")\n    \n    # Usar datos submuestreados\n    X_train_norm_fit = X_train_norm_sampled\n    y_train_norm_fit = y_train_norm_sampled\nelse:\n    X_train_norm_fit = X_train_norm\n    y_train_norm_fit = y_train_norm\n\nprint(f"\\n[2/3] Entrenando modelo con datos normalizados...")\nprint(f"   ‚Ä¢ Datos de entrenamiento: {X_train_norm_fit.shape}")\nprint(f"   ‚Ä¢ Targets de entrenamiento: {y_train_norm_fit.shape}")\nprint(f"   ‚Ä¢ Memoria estimada: {X_train_norm_fit.memory_usage(deep=True).sum() / (1024**2):.1f} MB")\nprint(f"   ‚è±Ô∏è  Esto puede tomar varios minutos...")\n\nstart_time = time.time()\n\n# Entrenar el modelo con datos normalizados\nrf_model.fit(X_train_norm_fit, y_train_norm_fit)\n\ntraining_time = time.time() - start_time\n\nprint(f"   ‚úÖ Modelo entrenado en {training_time:.2f} segundos ({training_time/60:.2f} minutos)")\n\nprint(f"\\n[3/3] Guardando modelo...")\n\n# Guardar el modelo entrenado\nmodel_path = training_folder / \'random_forest_model.pkl\'\njoblib.dump(rf_model, model_path)\n\nprint(f"   ‚úÖ Modelo guardado: {model_path.name}")\nprint(f"      Ubicaci√≥n: {model_path}")\nprint(f"      Tama√±o: {model_path.stat().st_size / (1024**2):.2f} MB")\n\n# ------------------------------------------------------------------------\n# RESUMEN DE ENTRENAMIENTO\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ ENTRENAMIENTO COMPLETADO")\nprint("="*80)\n\nprint(f"\\nüìä RESUMEN:")\nprint(f"   ‚Ä¢ Tiempo de entrenamiento: {training_time:.2f} segundos ({training_time/60:.2f} min)")\nprint(f"   ‚Ä¢ Muestras entrenamiento: {len(X_train_norm_fit):,}")\nprint(f"   ‚Ä¢ Features: {X_train_norm_fit.shape[1]}")\nprint(f"   ‚Ä¢ Targets: {y_train_norm_fit.shape[1]} (M_0, M_1c, M_1s)")\nprint(f"   ‚Ä¢ Normalizaci√≥n: Scalers independientes por target")\nprint(f"   ‚Ä¢ Tipo de datos: float32 (optimizado para memoria)")\n\nprint(f"\\nüìÅ ARCHIVOS GENERADOS:")\nprint(f"   ‚Ä¢ {model_path.name} ({model_path.stat().st_size / (1024**2):.2f} MB)")\n\nprint(f"\\nüí° Ejecuta la siguiente celda para validar el modelo y generar gr√°ficas.")\nprint("="*80)',
            '# =============================================================================\n# MODELO 2: RANDOM FOREST - VALIDACI√ìN Y VISUALIZACIONES\n# =============================================================================\n\nprint("\\n" + "="*80)\nprint("üéØ MODELO: RANDOM FOREST - VALIDACI√ìN")\nprint("="*80)\n\n# ------------------------------------------------------------------------\n# 1) Cargar modelo entrenado y datos de test\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: CARGAR MODELO Y DATOS DE TEST")\nprint("="*80)\n\ntraining_folder = Path(\'03_ML_traditional_models\') / \'Random_Forest\'\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\nprint(f"\\n[1/5] Cargando modelo entrenado...")\nmodel_path = training_folder / \'random_forest_model.pkl\'\nrf_model = joblib.load(model_path)\nprint(f"   ‚úÖ Modelo cargado desde: {model_path.name}")\n\nprint(f"\\n[2/5] Cargando datos de test originales...")\nX_test = joblib.load(data_ml_folder / \'X_test.pkl\')\ny_test = joblib.load(data_ml_folder / \'y_test.pkl\')\nprint(f"   ‚úÖ X_test: {X_test.shape}")\nprint(f"   ‚úÖ y_test: {y_test.shape}")\nprint(f"   üí° Targets: {list(y_test.columns)}")\n\nprint(f"\\n[3/5] Cargando datos de train originales (para m√©tricas)...")\nX_train = joblib.load(data_ml_folder / \'X_train.pkl\')\ny_train = joblib.load(data_ml_folder / \'y_train.pkl\')\nX_train_norm = joblib.load(data_ml_folder / \'X_train_norm.pkl\')\nprint(f"   ‚úÖ Datos de train cargados")\n\nprint(f"\\n[4/5] Cargando series_id y Time para gr√°ficas...")\n# Intentar cargar series_id y Time (si no existen, las gr√°ficas de time series se omitir√°n)\nseries_id_test_path = data_ml_folder / \'series_id_test.pkl\'\nTime_test_path = data_ml_folder / \'Time_test.pkl\'\n\nif series_id_test_path.exists() and Time_test_path.exists():\n    series_id_test = joblib.load(series_id_test_path)\n    Time_test = joblib.load(Time_test_path)\n    print(f"   ‚úÖ series_id_test: {len(series_id_test):,} valores")\n    print(f"   ‚úÖ Time_test: {len(Time_test):,} valores")\nelse:\n    series_id_test = None\n    Time_test = None\n    print(f"   ‚ö†Ô∏è  Archivos series_id_test.pkl o Time_test.pkl no encontrados")\n    print(f"   üí° Las gr√°ficas de time series se omitir√°n.")\n\n\n# ‚úÖ DESPU√âS (scalers independientes)\nscalers_X = joblib.load(root_dir / \'notebook\' / \'01_Models_scaler\' / \'scalers_X.pkl\')\nscalers_y = joblib.load(root_dir / \'notebook\' / \'01_Models_scaler\' / \'scalers_y.pkl\')  # ‚¨ÖÔ∏è A√ëADIR ESTA L√çNEA\n\nX_test_norm = pd.DataFrame(index=X_test.index)\nfor col in X_test.columns:\n    if col not in scalers_X:\n        raise KeyError(f"No se encuentra scaler para columna: {col}")\n    X_test_norm[col] = scalers_X[col].transform(X_test[[col]]).ravel()\nprint(f"   ‚úÖ X_test normalizado: {X_test_norm.shape}")\nprint(f"   üí° Scalers independientes: {list(scalers_y.keys())}")  # ‚¨ÖÔ∏è Ahora funcionar√°\n\n# ------------------------------------------------------------------------\n# 2) Realizar predicciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: PREDICCIONES")\nprint("="*80)\n\nprint(f"\\n[1/2] Prediciendo sobre conjunto de entrenamiento...")\n\ny_train_pred_norm = rf_model.predict(X_train_norm)\ny_train_pred_norm = pd.DataFrame(\n    y_train_pred_norm,\n    index=X_train_norm.index,\n    columns=y_train.columns\n)\n\nprint(f"   ‚úÖ Predicciones train: {y_train_pred_norm.shape}")\n\nprint(f"\\n[2/2] Prediciendo sobre conjunto de test...")\n\ny_test_pred_norm = rf_model.predict(X_test_norm)\ny_test_pred_norm = pd.DataFrame(\n    y_test_pred_norm,\n    index=y_test.index,\n    columns=y_test.columns\n)\n\nprint(f"   ‚úÖ Predicciones test: {y_test_pred_norm.shape}")\n\n# ------------------------------------------------------------------------\n# 3) Desnormalizar predicciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: DESNORMALIZACI√ìN DE PREDICCIONES")\nprint("="*80)\n\nprint(f"\\n[1/2] Desnormalizando predicciones de train con scalers independientes...")\n\n# Usar los scalers independientes para desnormalizar\ny_train_pred = pd.DataFrame(index=y_train_pred_norm.index)\nfor col in y_train_pred_norm.columns:\n    y_train_pred[col] = scalers_y[col].inverse_transform(y_train_pred_norm[[col]]).ravel()\n\nprint(f"   ‚úÖ Predicciones train desnormalizadas")\n\nprint(f"\\n[2/2] Desnormalizando predicciones de test con scalers independientes...")\n\n# Usar los scalers independientes para desnormalizar\ny_test_pred = pd.DataFrame(index=y_test_pred_norm.index)\nfor col in y_test_pred_norm.columns:\n    y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm[[col]]).ravel()\n\nprint(f"   ‚úÖ Predicciones test desnormalizadas")\n\n# ------------------------------------------------------------------------\n# 4) Calcular m√©tricas\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: C√ÅLCULO DE M√âTRICAS")\nprint("="*80)\n\n# Diccionarios para almacenar m√©tricas\nmetrics_train = {}\nmetrics_test = {}\n\nprint(f"\\n{\'M√âTRICA\':20} {\'CONJUNTO\':10} {\'M_0\':>15} {\'M_1c\':>15} {\'M_1s\':>15}")\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\n\n# Calcular m√©tricas para cada target\nfor i, col in enumerate(y_train.columns):\n    # TRAIN\n    rmse_train = np.sqrt(mean_squared_error(y_train[col], y_train_pred[col]))\n    r2_train = r2_score(y_train[col], y_train_pred[col])\n    \n    # TEST\n    rmse_test = np.sqrt(mean_squared_error(y_test[col], y_test_pred[col]))\n    r2_test = r2_score(y_test[col], y_test_pred[col])\n    \n    # Guardar m√©tricas\n    metrics_train[col] = {\'RMSE\': rmse_train, \'R2\': r2_train}\n    metrics_test[col] = {\'RMSE\': rmse_test, \'R2\': r2_test}\n    \n    # Mostrar\n    if i == 0:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {rmse_train:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {rmse_test:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {r2_train:>15.4f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {r2_test:>15.4f} {\'-\':>15} {\'-\':>15}")\n    elif i == 1:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {rmse_train:>15.2f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {rmse_test:>15.2f} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {r2_train:>15.4f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {r2_test:>15.4f} {\'-\':>15}")\n    else:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {rmse_train:>15.2f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {rmse_test:>15.2f}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {r2_train:>15.4f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {r2_test:>15.4f}")\n\n# Calcular promedios\navg_rmse_train = np.mean([m[\'RMSE\'] for m in metrics_train.values()])\navg_rmse_test = np.mean([m[\'RMSE\'] for m in metrics_test.values()])\navg_r2_train = np.mean([m[\'R2\'] for m in metrics_train.values()])\navg_r2_test = np.mean([m[\'R2\'] for m in metrics_test.values()])\n\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\nprint(f"{\'PROMEDIO RMSE\':20} {\'TRAIN\':10} {avg_rmse_train:>15.2f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_rmse_test:>15.2f}")\nprint(f"{\'PROMEDIO R¬≤\':20} {\'TRAIN\':10} {avg_r2_train:>15.4f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_r2_test:>15.4f}")\n\n# ------------------------------------------------------------------------\n# 5) Visualizaciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 5: GENERACI√ìN DE GR√ÅFICAS")\nprint("="*80)\n\n# 5.1) Gr√°fica de m√©tricas por target\nprint(f"\\n[1/5] Creando gr√°fica de m√©tricas...")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# RMSE\ntargets = list(metrics_train.keys())\nrmse_train_vals = [metrics_train[t][\'RMSE\'] for t in targets]\nrmse_test_vals = [metrics_test[t][\'RMSE\'] for t in targets]\n\nx = np.arange(len(targets))\nwidth = 0.35\n\naxes[0].bar(x - width/2, rmse_train_vals, width, label=\'Train\', alpha=0.8)\naxes[0].bar(x + width/2, rmse_test_vals, width, label=\'Test\', alpha=0.8)\naxes[0].set_xlabel(\'Target\')\naxes[0].set_ylabel(\'RMSE\')\naxes[0].set_title(\'RMSE por Target - Random Forest\')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(targets, rotation=45, ha=\'right\')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# R¬≤\nr2_train_vals = [metrics_train[t][\'R2\'] for t in targets]\nr2_test_vals = [metrics_test[t][\'R2\'] for t in targets]\n\naxes[1].bar(x - width/2, r2_train_vals, width, label=\'Train\', alpha=0.8)\naxes[1].bar(x + width/2, r2_test_vals, width, label=\'Test\', alpha=0.8)\naxes[1].set_xlabel(\'Target\')\naxes[1].set_ylabel(\'R¬≤\')\naxes[1].set_title(\'R¬≤ por Target - Random Forest\')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(targets, rotation=45, ha=\'right\')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim([0, 1])\n\nplt.tight_layout()\nmetrics_plot_path = training_folder / \'metrics_comparison.png\'\nplt.savefig(metrics_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {metrics_plot_path.name}")\n\n# 5.2) Gr√°ficas de predicciones vs reales (cambiar a 3x2)\nprint(f"\\n[2/5] Creando gr√°ficas de predicciones vs reales...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(y_train.columns):\n    row = idx\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train[col], y_train_pred[col], alpha=0.3, s=1)\n    axes[row, 0].plot([y_train[col].min(), y_train[col].max()], \n                       [y_train[col].min(), y_train[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 0].set_xlabel(\'Real\')\n    axes[row, 0].set_ylabel(\'Predicho\')\n    axes[row, 0].set_title(f\'{col} - TRAIN (R¬≤={metrics_train[col]["R2"]:.4f})\')\n    axes[row, 0].legend()\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test[col], y_test_pred[col], alpha=0.3, s=1, color=\'orange\')\n    axes[row, 1].plot([y_test[col].min(), y_test[col].max()], \n                       [y_test[col].min(), y_test[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 1].set_xlabel(\'Real\')\n    axes[row, 1].set_ylabel(\'Predicho\')\n    axes[row, 1].set_title(f\'{col} - TEST (R¬≤={metrics_test[col]["R2"]:.4f})\')\n    axes[row, 1].legend()\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\npredictions_plot_path = training_folder / \'predictions_vs_real.png\'\nplt.savefig(predictions_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {predictions_plot_path.name}")\n\n# 5.3) Gr√°fica de residuos (cambiar a 3x2)\nprint(f"\\n[3/5] Creando gr√°fica de residuos...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(y_train.columns):\n    row = idx\n    \n    # Calcular residuos\n    residuals_train = y_train[col] - y_train_pred[col]\n    residuals_test = y_test[col] - y_test_pred[col]\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train_pred[col], residuals_train, alpha=0.3, s=1)\n    axes[row, 0].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 0].set_xlabel(\'Predicho\')\n    axes[row, 0].set_ylabel(\'Residuo (Real - Predicho)\')\n    axes[row, 0].set_title(f\'{col} - Residuos TRAIN\')\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test_pred[col], residuals_test, alpha=0.3, s=1, color=\'orange\')\n    axes[row, 1].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 1].set_xlabel(\'Predicho\')\n    axes[row, 1].set_ylabel(\'Residuo (Real - Predicho)\')\n    axes[row, 1].set_title(f\'{col} - Residuos TEST\')\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nresiduals_plot_path = training_folder / \'residuals_analysis.png\'\nplt.savefig(residuals_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {residuals_plot_path.name}")\n\n# 5.4) y 5.5) Gr√°ficas de time series (cambiar a 3x3)\nif series_id_test is not None and Time_test is not None:\n    print(f"\\n[4/5] Creando gr√°ficas de time series (Real vs Predicho)...")\n\n    unique_test_series = series_id_test.unique()\n    np.random.seed(42)\n    selected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n\n    print(f"   ‚Ä¢ Series seleccionadas: {selected_series}")\n\n    # Crear figura con 3 filas x 3 columnas (3 series, 3 targets)\n    fig, axes = plt.subplots(3, 3, figsize=(20, 14))\n\n    for plot_idx, series_num in enumerate(selected_series):\n        series_mask = series_id_test == series_num\n        series_indices = series_mask[series_mask].index\n        time_series = Time_test.loc[series_indices]\n        \n        for target_idx, col in enumerate(y_test.columns):\n            ax = axes[plot_idx, target_idx]\n            \n            y_real = y_test.loc[series_indices, col]\n            y_pred = y_test_pred.loc[series_indices, col]\n            \n            r2_series = r2_score(y_real, y_pred)\n            rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n            \n            ax.plot(time_series, y_real, label=\'Real\', linewidth=2, alpha=0.8, color=\'blue\')\n            ax.plot(time_series, y_pred, label=\'Predicho\', linestyle=\'--\', linewidth=2, alpha=0.8, color=\'red\')\n            \n            ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n            ax.set_ylabel(\'Valor\', fontsize=10)\n            ax.set_title(f\'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}\', \n                         fontsize=10, fontweight=\'bold\')\n            ax.legend(loc=\'best\', fontsize=8)\n            ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    timeseries_plot_path = training_folder / \'timeseries_comparison.png\'\n    plt.savefig(timeseries_plot_path, dpi=300, bbox_inches=\'tight\')\n    plt.show()\n\n    print(f"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}")\n\n    # 5.5) Zoom (3x3)\n    print(f"\\n[5/5] Creando gr√°ficas de time series con zoom (50s)...")\n\n    fig, axes = plt.subplots(3, 3, figsize=(20, 14))\n\n    for plot_idx, series_num in enumerate(selected_series):\n        series_mask = series_id_test == series_num\n        series_indices = series_mask[series_mask].index\n        time_series = Time_test.loc[series_indices]\n        \n        time_min = time_series.min()\n        time_max_zoom = time_min + 50\n        \n        zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n        zoom_indices = time_series[zoom_mask].index\n        time_zoom = time_series[zoom_mask]\n        \n        for target_idx, col in enumerate(y_test.columns):\n            ax = axes[plot_idx, target_idx]\n            \n            y_real_zoom = y_test.loc[zoom_indices, col]\n            y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n            \n            r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n            rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n            \n            ax.plot(time_zoom, y_real_zoom, label=\'Real\', \n                    linewidth=2.5, alpha=0.8, color=\'blue\', marker=\'o\', markersize=4)\n            ax.plot(time_zoom, y_pred_zoom, label=\'Predicho\', \n                    linestyle=\'--\', linewidth=2.5, alpha=0.8, color=\'red\', marker=\'x\', markersize=5)\n            \n            ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n            ax.set_ylabel(\'Valor\', fontsize=10)\n            ax.set_title(f\'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}\', \n                         fontsize=10, fontweight=\'bold\')\n            ax.legend(loc=\'best\', fontsize=8)\n            ax.grid(True, alpha=0.3)\n            \n            n_points = len(zoom_indices)\n            ax.text(0.02, 0.02, f\'Puntos: {n_points}\', transform=ax.transAxes, fontsize=8,\n                    verticalalignment=\'bottom\', bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.5))\n\n    plt.tight_layout()\n    timeseries_zoom_plot_path = training_folder / \'timeseries_comparison_zoom50s.png\'\n    plt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches=\'tight\')\n    plt.show()\n\n    print(f"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}")\nelse:\n    print(f"\\n   ‚ö†Ô∏è Saltando gr√°ficas de time series [4/5 y 5/5]")\n    timeseries_plot_path = None\n    timeseries_zoom_plot_path = None\n\n# ------------------------------------------------------------------------\n# 6) Guardar m√©tricas en archivo\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 6: GUARDAR RESULTADOS")\nprint("="*80)\n\n# Crear DataFrame con m√©tricas\nmetrics_df = pd.DataFrame({\n    \'Target\': list(metrics_train.keys()) + [\'PROMEDIO\'],\n    \'RMSE_Train\': list([m[\'RMSE\'] for m in metrics_train.values()]) + [avg_rmse_train],\n    \'RMSE_Test\': list([m[\'RMSE\'] for m in metrics_test.values()]) + [avg_rmse_test],\n    \'R2_Train\': list([m[\'R2\'] for m in metrics_train.values()]) + [avg_r2_train],\n    \'R2_Test\': list([m[\'R2\'] for m in metrics_test.values()]) + [avg_r2_test]\n})\n\nmetrics_csv_path = training_folder / \'metrics_results.csv\'\nmetrics_df.to_csv(metrics_csv_path, index=False)\n\nprint(f"\\n   ‚úÖ M√©tricas guardadas: {metrics_csv_path.name}")\n\n# Crear resumen de validaci√≥n\nsummary = {\n    \'Model\': \'Random Forest\',\n    \'Train_Samples\': len(X_train),\n    \'Test_Samples\': len(X_test),\n    \'Features\': X_train.shape[1],\n    \'Targets\': y_train.shape[1],\n    \'Targets_Names\': \', \'.join(y_train.columns),\n    \'Avg_RMSE_Train\': avg_rmse_train,\n    \'Avg_RMSE_Test\': avg_rmse_test,\n    \'Avg_R2_Train\': avg_r2_train,\n    \'Avg_R2_Test\': avg_r2_test,\n    \'N_Estimators\': 100,\n    \'Max_Depth\': 20,\n    \'Normalization\': \'Independent scalers per target\'\n}\n\nsummary_df = pd.DataFrame([summary])\nsummary_path = training_folder / \'validation_summary.csv\'\nsummary_df.to_csv(summary_path, index=False)\n\nprint(f"   ‚úÖ Resumen guardado: {summary_path.name}")\n\n# ------------------------------------------------------------------------\n# RESUMEN FINAL\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ VALIDACI√ìN COMPLETADA - RANDOM FOREST")\nprint("="*80)\n\nprint(f"\\nüìä RESULTADOS FINALES:")\nprint(f"   ‚Ä¢ Targets: {\', \'.join(y_train.columns)}")\nprint(f"   ‚Ä¢ RMSE promedio (Train): {avg_rmse_train:.2f}")\nprint(f"   ‚Ä¢ RMSE promedio (Test):  {avg_rmse_test:.2f}")\nprint(f"   ‚Ä¢ R¬≤ promedio (Train):   {avg_r2_train:.4f}")\nprint(f"   ‚Ä¢ R¬≤ promedio (Test):    {avg_r2_test:.4f}")\n\nprint(f"\\nüìÅ ARCHIVOS GENERADOS:")\nprint(f"   ‚Ä¢ {metrics_csv_path.name}")\nprint(f"   ‚Ä¢ {summary_path.name}")\nprint(f"   ‚Ä¢ {metrics_plot_path.name}")\nprint(f"   ‚Ä¢ {predictions_plot_path.name}")\nprint(f"   ‚Ä¢ {residuals_plot_path.name}")\nif timeseries_plot_path is not None:\n    print(f"   ‚Ä¢ {timeseries_plot_path.name}")\n    print(f"   ‚Ä¢ {timeseries_zoom_plot_path.name}")\n\nprint(f"\\nüí° Random Forest captura relaciones no lineales mejor que Ridge.")\nprint(f"\\n‚öôÔ∏è  Normalizaci√≥n: Scalers independientes por target para balancear escalas")\nprint("="*80)',
            '"""\nM√≥dulo para generar gr√°ficas adicionales de time series para modelos ML.\n\nEste m√≥dulo genera:\n1. Time series completas: Real vs Predicho (3 series aleatorias)\n2. Time series con zoom de 50 segundos\n\nAdaptado para trabajar con:\n- Scalers independientes por columna (scalers_X.pkl, scalers_y.pkl)\n- Targets nuevos: M_0, M_1c, M_1s\n\nIMPORTANTE: Este m√≥dulo NO carga predicciones de archivo.\nUsa y_test_pred que ya est√° en memoria del notebook.\n"""\n\ndef generate_timeseries_plots(root_dir, y_test, y_test_pred, output_folder, \n                              model_name=\'Model\', n_series=3, zoom_seconds=50, random_seed=42):\n    """\n    Generar gr√°ficas de time series (Real vs Predicho) con zoom.\n    \n    Este c√≥digo replica EXACTAMENTE el flujo de la celda 81 del notebook.\n    \n    Args:\n        root_dir (Path): Directorio ra√≠z del proyecto\n        y_test (pd.DataFrame): Valores reales de test (ya en memoria)\n        y_test_pred (pd.DataFrame): Predicciones del modelo (ya en memoria)\n        output_folder (Path): Carpeta donde guardar las gr√°ficas\n        model_name (str): Nombre del modelo para t√≠tulos\n        n_series (int): N√∫mero de series aleatorias a graficar (default: 3)\n        zoom_seconds (int): Duraci√≥n de la ventana de zoom (default: 50)\n        random_seed (int): Semilla para selecci√≥n aleatoria de series (default: 42)\n        \n    Returns:\n        dict: Rutas de las gr√°ficas generadas\n        \n    Example:\n        >>> from aditionalPlots import generate_timeseries_plots\n        >>> \n        >>> plots = generate_timeseries_plots(\n        ...     root_dir=root_dir,\n        ...     y_test=y_test,                    # ‚Üê Variable en memoria\n        ...     y_test_pred=y_test_pred,          # ‚Üê Variable en memoria\n        ...     output_folder=training_folder,\n        ...     model_name=\'Random Forest\'\n        ... )\n    """\n    \n    print("\\n" + "="*80)\n    print(f"üìä {model_name.upper()} - GR√ÅFICAS DE TIME SERIES")\n    print("="*80)\n    \n    root_dir = Path(root_dir)\n    output_folder = Path(output_folder)\n    output_folder.mkdir(parents=True, exist_ok=True)\n    \n    # ========================================================================\n    # PASO 1: PREPARAR DATOS PARA TIME SERIES\n    # ========================================================================\n    print(f"\\n{\'=\'*80}")\n    print("PASO 1: PREPARAR DATOS PARA TIME SERIES")\n    print("="*80)\n    \n    print(f"\\n[1/3] Cargando Time_test desde archivo pkl...")\n    \n    data_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n    Time_test_path = data_ml_folder / \'Time_test.pkl\'\n    \n    if Time_test_path.exists():\n        Time_test = joblib.load(Time_test_path)\n        print(f"   ‚úÖ Time_test cargado: {len(Time_test):,} valores")\n        print(f"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s")\n        print(f"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s")\n    else:\n        print(f"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}")\n        raise FileNotFoundError(f"Archivo requerido no encontrado: {Time_test_path}")\n    \n    print(f"\\n[2/3] Generando series_id_test a partir de Time_test...")\n    \n    # Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\n    series_id_test_values = np.zeros(len(Time_test), dtype=int)\n    current_series = 0\n    \n    Time_test_array = Time_test.values\n    for i in range(1, len(Time_test_array)):\n        if Time_test_array[i] < Time_test_array[i-1]:\n            current_series += 1\n        series_id_test_values[i] = current_series\n    \n    # Convertir a pandas Series con el mismo index que Time_test\n    series_id_test = pd.Series(series_id_test_values, index=Time_test.index, name=\'series_id\')\n    \n    n_test_series = series_id_test.max() + 1\n    \n    print(f"   ‚úÖ Series temporales identificadas en test: {n_test_series}")\n    \n    # Analizar cada serie\n    print(f"\\n   üìä Resumen de series en TEST:")\n    for sid in range(min(5, n_test_series)):\n        mask = series_id_test == sid\n        n_rows = mask.sum()\n        time_min = Time_test.loc[mask].min()\n        time_max = Time_test.loc[mask].max()\n        print(f"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s")\n    \n    if n_test_series > 5:\n        print(f"      ... y {n_test_series - 5} series m√°s")\n    \n    print(f"\\n[3/3] Verificando que los datos coinciden con predicciones...")\n    \n    # Verificar que los √≠ndices coinciden\n    if not all(series_id_test.index == y_test.index):\n        print(f"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...")\n        series_id_test = series_id_test.reindex(y_test.index)\n        Time_test = Time_test.reindex(y_test.index)\n    \n    print(f"   ‚úÖ √çndices verificados:")\n    print(f"      ‚Ä¢ y_test: {y_test.shape[0]:,} filas")\n    print(f"      ‚Ä¢ y_test_pred: {y_test_pred.shape[0]:,} filas")\n    print(f"      ‚Ä¢ series_id_test: {len(series_id_test):,} valores")\n    print(f"      ‚Ä¢ Time_test: {len(Time_test):,} valores")\n    \n    # ========================================================================\n    # PASO 2: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS\n    # ========================================================================\n    print(f"\\n{\'=\'*80}")\n    print("PASO 2: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS")\n    print("="*80)\n    \n    print(f"\\n[1/1] Creando gr√°ficas de time series (Real vs Predicho)...")\n    \n    # Obtener series √∫nicas del conjunto de test\n    unique_test_series = series_id_test.unique()\n    \n    # Seleccionar series aleatorias\n    np.random.seed(random_seed)  # Usar semilla configurable\n    selected_series = np.random.choice(unique_test_series, \n                                      size=min(n_series, len(unique_test_series)), \n                                      replace=False)\n    \n    print(f"   ‚Ä¢ Total series disponibles: {len(unique_test_series)}")\n    print(f"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}")\n    \n    # Detectar n√∫mero de targets autom√°ticamente\n    n_targets = len(y_test.columns)\n    \n    # Crear figura con n_series filas x n_targets columnas\n    fig, axes = plt.subplots(n_series, n_targets, figsize=(6*n_targets, 4.5*n_series))\n    \n    # Asegurar que axes sea 2D\n    if n_series == 1 and n_targets == 1:\n        axes = np.array([[axes]])\n    elif n_series == 1:\n        axes = axes.reshape(1, -1)\n    elif n_targets == 1:\n        axes = axes.reshape(-1, 1)\n    \n    for plot_idx, series_num in enumerate(selected_series):\n        # Filtrar datos de esta serie\n        series_mask = series_id_test == series_num\n        series_indices = series_mask[series_mask].index\n        \n        # Obtener tiempo\n        time_series = Time_test.loc[series_indices]\n        \n        # Para cada target, graficar en subplot separado\n        for target_idx, col in enumerate(y_test.columns):\n            ax = axes[plot_idx, target_idx]\n            \n            # Valores reales\n            y_real = y_test.loc[series_indices, col]\n            # Valores predichos\n            y_pred = y_test_pred.loc[series_indices, col]\n            \n            # Calcular m√©tricas para esta serie y target\n            r2_series = r2_score(y_real, y_pred)\n            rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n            \n            # Graficar\n            ax.plot(time_series, y_real, label=\'Real\', \n                    linewidth=2, alpha=0.8, color=\'blue\')\n            ax.plot(time_series, y_pred, label=\'Predicho\', \n                    linestyle=\'--\', linewidth=2, alpha=0.8, color=\'red\')\n            \n            # Configurar subplot\n            ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n            ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n            ax.set_title(f\'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}\', \n                         fontsize=11, fontweight=\'bold\')\n            ax.legend(loc=\'best\', fontsize=9)\n            ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    timeseries_plot_path = output_folder / \'timeseries_comparison.png\'\n    plt.savefig(timeseries_plot_path, dpi=300, bbox_inches=\'tight\')\n    plt.show()\n    \n    print(f"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}")\n    \n    # ========================================================================\n    # PASO 3: GR√ÅFICAS DE TIME SERIES - ZOOM\n    # ========================================================================\n    print(f"\\n{\'=\'*80}")\n    print(f"PASO 3: GR√ÅFICAS DE TIME SERIES - ZOOM {zoom_seconds} SEGUNDOS")\n    print("="*80)\n    \n    print(f"\\n[1/1] Creando gr√°ficas de time series con zoom ({zoom_seconds}s)...")\n    \n    # Crear figura con n_series filas x n_targets columnas\n    fig, axes = plt.subplots(n_series, n_targets, figsize=(6*n_targets, 4.5*n_series))\n    \n    # Asegurar que axes sea 2D\n    if n_series == 1 and n_targets == 1:\n        axes = np.array([[axes]])\n    elif n_series == 1:\n        axes = axes.reshape(1, -1)\n    elif n_targets == 1:\n        axes = axes.reshape(-1, 1)\n    \n    for plot_idx, series_num in enumerate(selected_series):\n        # Filtrar datos de esta serie\n        series_mask = series_id_test == series_num\n        series_indices = series_mask[series_mask].index\n        \n        # Obtener tiempo\n        time_series = Time_test.loc[series_indices]\n        \n        # Definir ventana de zoom desde el inicio\n        time_min = time_series.min()\n        time_max_zoom = time_min + zoom_seconds\n        \n        # Filtrar por ventana de tiempo\n        zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n        zoom_indices = time_series[zoom_mask].index\n        time_zoom = time_series[zoom_mask]\n        \n        # Para cada target, graficar en subplot separado\n        for target_idx, col in enumerate(y_test.columns):\n            ax = axes[plot_idx, target_idx]\n            \n            # Valores reales y predichos (con zoom)\n            y_real_zoom = y_test.loc[zoom_indices, col]\n            y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n            \n            # Calcular m√©tricas para esta ventana\n            r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n            rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n            \n            # Graficar\n            ax.plot(time_zoom, y_real_zoom, label=\'Real\', \n                    linewidth=2.5, alpha=0.8, color=\'blue\', marker=\'o\', markersize=4)\n            ax.plot(time_zoom, y_pred_zoom, label=\'Predicho\', \n                    linestyle=\'--\', linewidth=2.5, alpha=0.8, color=\'red\', marker=\'x\', markersize=5)\n            \n            # Configurar subplot\n            ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n            ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n            ax.set_title(f\'Serie {series_num} - {col} (Zoom: 0-{zoom_seconds}s)\\n\'\n                        f\'R¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}\', \n                         fontsize=11, fontweight=\'bold\')\n            ax.legend(loc=\'best\', fontsize=9)\n            ax.grid(True, alpha=0.3)\n            \n            # A√±adir texto con informaci√≥n de puntos\n            n_points = len(zoom_indices)\n            ax.text(0.02, 0.02, f\'Puntos: {n_points}\', transform=ax.transAxes, fontsize=9,\n                    verticalalignment=\'bottom\', bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.5))\n    \n    plt.tight_layout()\n    timeseries_zoom_plot_path = output_folder / f\'timeseries_comparison_zoom{zoom_seconds}s.png\'\n    plt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches=\'tight\')\n    plt.show()\n    \n    print(f"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}")\n    \n    # ========================================================================\n    # RESUMEN FINAL\n    # ========================================================================\n    print(f"\\n{\'=\'*80}")\n    print("‚úÖ GR√ÅFICAS DE TIME SERIES COMPLETADAS")\n    print("="*80)\n    \n    print(f"\\nüìä GR√ÅFICAS GENERADAS:")\n    print(f"   ‚Ä¢ {timeseries_plot_path.name}")\n    print(f"   ‚Ä¢ {timeseries_zoom_plot_path.name}")\n    \n    print(f"\\nüí° Targets graficados: {list(y_test.columns)}")\n    print(f"üí° Series aleatorias: {list(selected_series)}")\n    \n    print("="*80)\n    \n    return {\n        \'timeseries_full\': timeseries_plot_path,\n        \'timeseries_zoom\': timeseries_zoom_plot_path\n    }\n\n',
            "# O especificar tu propia semilla\nplots = generate_timeseries_plots(\n    root_dir=root_dir,\n    y_test=y_test,\n    y_test_pred=y_test_pred,\n    output_folder=training_folder,\n    model_name='Random Forest',\n    zoom_seconds=30,\n    random_seed=25  # ‚Üê Semilla personalizada\n)",
        ],
    },
    {
        "title": 'üå≥ MODELO 3: XGBOOST',
        "cells": [
            '# =============================================================================\n# MODELO 3: XGBOOST - ENTRENAMIENTO\n# =============================================================================\nimport time\n\nprint("\\n" + "="*80)\nprint("üå≥ MODELO: XGBOOST - ENTRENAMIENTO")\nprint("="*80)\n\n# Verificar disponibilidad de XGBoost\nif not XGBOOST_AVAILABLE:\n    print("\\n‚ùå ERROR: XGBoost no est√° instalado")\n    print("   Instalar con: pip install xgboost")\n    raise ImportError("XGBoost no disponible")\n\nprint("‚úÖ XGBoost disponible")\n\nimport xgboost as xgb\n\n# ------------------------------------------------------------------------\n# 1) Preparar carpeta y cargar datos\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: PREPARAR CARPETA Y CARGAR DATOS")\nprint("="*80)\n\nprint(f"\\n[1/4] Creando carpeta para XGBoost...")\n\ntraining_folder = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost\'\nos.makedirs(training_folder, exist_ok=True)\n\nprint(f"   ‚úÖ Carpeta: {training_folder}")\n\nprint(f"\\n[2/4] Cargando datos de entrenamiento normalizados...")\n\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\nX_train_norm = joblib.load(data_ml_folder / \'X_train_norm.pkl\')\ny_train_norm = joblib.load(data_ml_folder / \'y_train_norm.pkl\')\n\nprint(f"   ‚úÖ X_train_norm: {X_train_norm.shape}")\nprint(f"   ‚úÖ y_train_norm: {y_train_norm.shape}")\n\nprint(f"\\n[3/4] Verificando datos originales (para m√©tricas posteriores)...")\n\nX_train = joblib.load(data_ml_folder / \'X_train.pkl\')\ny_train = joblib.load(data_ml_folder / \'y_train.pkl\')\n\nprint(f"   ‚úÖ X_train: {X_train.shape}")\nprint(f"   ‚úÖ y_train: {y_train.shape}")\n\nprint(f"\\n[4/4] Verificando n√∫mero de features y targets...")\n\nprint(f"   ‚Ä¢ Features (inputs):  {X_train_norm.shape[1]}")\nprint(f"   ‚Ä¢ Targets (outputs):  {y_train_norm.shape[1]} (ambas palas)")\nprint(f"   ‚Ä¢ Muestras training:  {X_train_norm.shape[0]:,}")\n\n# ------------------------------------------------------------------------\n# 2) Configurar y entrenar modelo XGBoost\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: CONFIGURACI√ìN Y ENTRENAMIENTO")\nprint("="*80)\n\nprint(f"\\n[1/3] Configurando XGBoost con MultiOutputRegressor...")\n\n# PRIMERA PRUEBA - PAR√ÅMETROS POR DEFECTO\n# Configurar XGBoost con par√°metros optimizados\n# xgb_base = xgb.XGBRegressor(\n#     n_estimators=100,          # N√∫mero de √°rboles\n#     max_depth=6,               # Profundidad m√°xima de cada √°rbol\n#     learning_rate=0.1,         # Tasa de aprendizaje\n#     subsample=0.8,             # Fracci√≥n de muestras para cada √°rbol\n#     colsample_bytree=0.8,      # Fracci√≥n de features para cada √°rbol\n#     gamma=0,                   # M√≠nima reducci√≥n de loss para split\n#     min_child_weight=1,        # Peso m√≠nimo en nodo hijo\n#     reg_alpha=0,               # Regularizaci√≥n L1\n#     reg_lambda=1,              # Regularizaci√≥n L2\n#     random_state=42,\n#     n_jobs=-1,                 # Usar todos los cores\n#     verbosity=0                # Sin mensajes de XGBoost\n# )\n\n\n# SEGUNDA PRUEBA - PAR√ÅMETROS OPTIMIZADOS TOP DE MOOMENTO\nxgb_base = xgb.XGBRegressor(\n    n_estimators=2000,          # N√∫mero de √°rboles\n    max_depth=4,               # Profundidad m√°xima de cada √°rbol\n    learning_rate=0.05,         # Tasa de aprendizaje\n    subsample=0.7,             # Fracci√≥n de muestras para cada √°rbol\n    colsample_bytree=0.7,      # Fracci√≥n de features para cada √°rbol\n    gamma=0.5,                   # M√≠nima reducci√≥n de loss para split\n    min_child_weight=12,        # Peso m√≠nimo en nodo hijo\n    reg_alpha=2,               # Regularizaci√≥n L1\n    reg_lambda=10,              # Regularizaci√≥n L2\n    random_state=42,\n    n_jobs=-1,                 # Usar todos los cores\n    verbosity=0                # Sin mensajes de XGBoost\n)\n\n\n\n\n# Usar MultiOutputRegressor para predecir ambas palas\nxgb_model = MultiOutputRegressor(xgb_base)\n\nprint(f"   ‚úÖ Modelo configurado:")\nprint(f"      ‚Ä¢ Tipo: XGBoost (Gradient Boosting)")\nprint(f"      ‚Ä¢ N_estimators: 100")\nprint(f"      ‚Ä¢ Max_depth: 6")\nprint(f"      ‚Ä¢ Learning_rate: 0.1")\nprint(f"      ‚Ä¢ Subsample: 0.8")\nprint(f"      ‚Ä¢ Colsample_bytree: 0.8")\nprint(f"      ‚Ä¢ Outputs: {len(y_train_norm.columns)} (ambas palas)")\n\nprint(f"\\n[2/3] Entrenando modelo con datos normalizados...")\nprint(f"   ‚Ä¢ Datos de entrenamiento: {X_train_norm.shape}")\nprint(f"   ‚Ä¢ Targets de entrenamiento: {y_train_norm.shape}")\n\nstart_time = time.time()\n\n# Entrenar el modelo\nxgb_model.fit(X_train_norm, y_train_norm)\n\ntraining_time = time.time() - start_time\n\nprint(f"   ‚úÖ Modelo entrenado en {training_time:.2f} segundos")\n\nprint(f"\\n[3/3] Guardando modelo...")\n\n# Guardar el modelo entrenado\nmodel_path = training_folder / \'xgboost_model.pkl\'\njoblib.dump(xgb_model, model_path)\n\nprint(f"   ‚úÖ Modelo guardado: {model_path.name}")\n\n# ------------------------------------------------------------------------\n# RESUMEN\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ ENTRENAMIENTO COMPLETADO - XGBOOST")\nprint("="*80)\n\nprint(f"\\nüìä INFORMACI√ìN DEL MODELO:")\nprint(f"   ‚Ä¢ Algoritmo: XGBoost (Extreme Gradient Boosting)")\nprint(f"   ‚Ä¢ Tiempo de entrenamiento: {training_time:.2f} segundos")\nprint(f"   ‚Ä¢ Muestras entrenadas: {X_train_norm.shape[0]:,}")\nprint(f"   ‚Ä¢ Features utilizadas: {X_train_norm.shape[1]}")\nprint(f"   ‚Ä¢ Outputs predichos: {y_train_norm.shape[1]}")\n\nprint(f"\\nüìÅ ARCHIVO GENERADO:")\nprint(f"   ‚Ä¢ {model_path.name}")\n\nprint(f"\\nüí° XGBoost usa gradient boosting optimizado para mejorar iterativamente las predicciones.")\nprint("="*80)',
            '"""\nScript para validar modelo XGBoost entrenado.\n\nEste script:\n1. Carga modelo entrenado y datos de test\n2. Normaliza datos de test con scalers independientes\n3. Realiza predicciones y desnormaliza\n4. Calcula m√©tricas (RMSE, R¬≤)\n5. Genera visualizaciones\n\nAdaptado para trabajar con:\n- Scalers independientes por columna (scalers_X.pkl, scalers_y.pkl)\n- Targets nuevos: M_0, M_1c, M_1s\n\nAutor: Adaptado de validaci√≥n XGBoost\nFecha: Enero 2026\n"""\n\n# =============================================================================\n# MODELO: XGBOOST - VALIDACI√ìN Y VISUALIZACIONES\n# =============================================================================\n\nprint("\\n" + "="*80)\nprint("üéØ MODELO: XGBOOST - VALIDACI√ìN")\nprint("="*80)\n\n# ------------------------------------------------------------------------\n# 1) Cargar modelo entrenado y datos de test\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: CARGAR MODELO Y DATOS DE TEST")\nprint("="*80)\n\ntraining_folder = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost\'\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\nprint(f"\\n[1/5] Cargando modelo entrenado...")\nmodel_path = training_folder / \'xgboost_model.pkl\'\nxgb_model = joblib.load(model_path)\nprint(f"   ‚úÖ Modelo cargado desde: {model_path.name}")\n\nprint(f"\\n[2/5] Cargando datos de test originales...")\nX_test = joblib.load(data_ml_folder / \'X_test.pkl\')\ny_test = joblib.load(data_ml_folder / \'y_test.pkl\')\nprint(f"   ‚úÖ X_test: {X_test.shape}")\nprint(f"   ‚úÖ y_test: {y_test.shape}")\nprint(f"   üí° Targets: {list(y_test.columns)}")\n\nprint(f"\\n[3/5] Cargando datos de train originales (para m√©tricas)...")\nX_train = joblib.load(data_ml_folder / \'X_train.pkl\')\ny_train = joblib.load(data_ml_folder / \'y_train.pkl\')\nX_train_norm = joblib.load(data_ml_folder / \'X_train_norm.pkl\')\nprint(f"   ‚úÖ Datos de train cargados")\n\nprint(f"\\n[4/5] Cargando scalers independientes...")\nscalers_X = joblib.load(root_dir / \'notebook\' / \'01_Models_scaler\' / \'scalers_X.pkl\')\nscalers_y = joblib.load(root_dir / \'notebook\' / \'01_Models_scaler\' / \'scalers_y.pkl\')\nprint(f"   ‚úÖ Scalers cargados:")\nprint(f"      ‚Ä¢ scalers_X: {len(scalers_X)} scalers (uno por feature)")\nprint(f"      ‚Ä¢ scalers_y: {len(scalers_y)} scalers (uno por target)")\n\nprint(f"\\n[5/5] Normalizando datos de test con scalers independientes...")\n# Normalizar columna por columna usando scalers individuales\nX_test_norm = pd.DataFrame(index=X_test.index, columns=X_test.columns)\n\nfor col in X_test.columns:\n    if col in scalers_X:\n        X_test_norm[col] = scalers_X[col].transform(X_test[[col]])\n    else:\n        print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales")\n        X_test_norm[col] = X_test[col]\n\n# Convertir a float32 para consistencia\nX_test_norm = X_test_norm.astype(\'float32\')\n\nprint(f"   ‚úÖ X_test normalizado: {X_test_norm.shape}")\nprint(f"   üí° Normalizaci√≥n columna por columna con scalers independientes")\n\n# ------------------------------------------------------------------------\n# 2) Realizar predicciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: PREDICCIONES")\nprint("="*80)\n\nprint(f"\\n[1/2] Prediciendo sobre conjunto de entrenamiento...")\n\ny_train_pred_norm = xgb_model.predict(X_train_norm)\ny_train_pred_norm = pd.DataFrame(\n    y_train_pred_norm,\n    index=X_train_norm.index,\n    columns=y_train.columns\n)\n\nprint(f"   ‚úÖ Predicciones train (normalizadas): {y_train_pred_norm.shape}")\n\nprint(f"\\n[2/2] Prediciendo sobre conjunto de test...")\n\ny_test_pred_norm = xgb_model.predict(X_test_norm)\ny_test_pred_norm = pd.DataFrame(\n    y_test_pred_norm,\n    index=y_test.index,\n    columns=y_test.columns\n)\n\nprint(f"   ‚úÖ Predicciones test (normalizadas): {y_test_pred_norm.shape}")\n\n# ------------------------------------------------------------------------\n# 3) Desnormalizar predicciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: DESNORMALIZACI√ìN DE PREDICCIONES")\nprint("="*80)\n\nprint(f"\\n[1/2] Desnormalizando predicciones de train...")\n\n# Desnormalizar columna por columna usando scalers independientes\ny_train_pred = pd.DataFrame(index=y_train_pred_norm.index, columns=y_train_pred_norm.columns)\n\nfor col in y_train_pred_norm.columns:\n    if col in scalers_y:\n        y_train_pred[col] = scalers_y[col].inverse_transform(y_train_pred_norm[[col]])\n    else:\n        print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores normalizados")\n        y_train_pred[col] = y_train_pred_norm[col]\n\nprint(f"   ‚úÖ Predicciones train desnormalizadas")\nprint(f"   üí° Desnormalizaci√≥n columna por columna con scalers independientes")\n\nprint(f"\\n[2/2] Desnormalizando predicciones de test...")\n\n# Desnormalizar columna por columna usando scalers independientes\ny_test_pred = pd.DataFrame(index=y_test_pred_norm.index, columns=y_test_pred_norm.columns)\n\nfor col in y_test_pred_norm.columns:\n    if col in scalers_y:\n        y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm[[col]])\n    else:\n        print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores normalizados")\n        y_test_pred[col] = y_test_pred_norm[col]\n\nprint(f"   ‚úÖ Predicciones test desnormalizadas")\n\n# ------------------------------------------------------------------------\n# 4) Calcular m√©tricas\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: C√ÅLCULO DE M√âTRICAS")\nprint("="*80)\n\n# Diccionarios para almacenar m√©tricas\nmetrics_train = {}\nmetrics_test = {}\n\nprint(f"\\n{\'M√âTRICA\':20} {\'CONJUNTO\':10} {\'M_0\':>15} {\'M_1c\':>15} {\'M_1s\':>15}")\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\n\n# Calcular m√©tricas para cada target\nfor i, col in enumerate(y_train.columns):\n    # TRAIN\n    rmse_train = np.sqrt(mean_squared_error(y_train[col], y_train_pred[col]))\n    r2_train = r2_score(y_train[col], y_train_pred[col])\n    \n    # TEST\n    rmse_test = np.sqrt(mean_squared_error(y_test[col], y_test_pred[col]))\n    r2_test = r2_score(y_test[col], y_test_pred[col])\n    \n    # Guardar m√©tricas\n    metrics_train[col] = {\'RMSE\': rmse_train, \'R2\': r2_train}\n    metrics_test[col] = {\'RMSE\': rmse_test, \'R2\': r2_test}\n    \n    # Mostrar\n    if i == 0:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {rmse_train:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {rmse_test:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {r2_train:>15.4f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {r2_test:>15.4f} {\'-\':>15} {\'-\':>15}")\n    elif i == 1:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {rmse_train:>15.2f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {rmse_test:>15.2f} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {r2_train:>15.4f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {r2_test:>15.4f} {\'-\':>15}")\n    else:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {rmse_train:>15.2f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {rmse_test:>15.2f}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {r2_train:>15.4f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {r2_test:>15.4f}")\n\n# Calcular promedios\navg_rmse_train = np.mean([m[\'RMSE\'] for m in metrics_train.values()])\navg_rmse_test = np.mean([m[\'RMSE\'] for m in metrics_test.values()])\navg_r2_train = np.mean([m[\'R2\'] for m in metrics_train.values()])\navg_r2_test = np.mean([m[\'R2\'] for m in metrics_test.values()])\n\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\nprint(f"{\'PROMEDIO RMSE\':20} {\'TRAIN\':10} {avg_rmse_train:>15.2f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_rmse_test:>15.2f}")\nprint(f"{\'PROMEDIO R¬≤\':20} {\'TRAIN\':10} {avg_r2_train:>15.4f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_r2_test:>15.4f}")\n\n# ------------------------------------------------------------------------\n# 5) Visualizaciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 5: GENERACI√ìN DE GR√ÅFICAS")\nprint("="*80)\n\n# 5.1) Gr√°fica de m√©tricas por target\nprint(f"\\n[1/3] Creando gr√°fica de m√©tricas...")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# RMSE\ntargets = list(metrics_train.keys())\nrmse_train_vals = [metrics_train[t][\'RMSE\'] for t in targets]\nrmse_test_vals = [metrics_test[t][\'RMSE\'] for t in targets]\n\nx = np.arange(len(targets))\nwidth = 0.35\n\naxes[0].bar(x - width/2, rmse_train_vals, width, label=\'Train\', alpha=0.8)\naxes[0].bar(x + width/2, rmse_test_vals, width, label=\'Test\', alpha=0.8)\naxes[0].set_xlabel(\'Target\', fontsize=12)\naxes[0].set_ylabel(\'RMSE [kNm]\', fontsize=12)\naxes[0].set_title(\'RMSE por Target - XGBoost\', fontsize=14, fontweight=\'bold\')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(targets, fontsize=11)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\n# R¬≤\nr2_train_vals = [metrics_train[t][\'R2\'] for t in targets]\nr2_test_vals = [metrics_test[t][\'R2\'] for t in targets]\n\naxes[1].bar(x - width/2, r2_train_vals, width, label=\'Train\', alpha=0.8)\naxes[1].bar(x + width/2, r2_test_vals, width, label=\'Test\', alpha=0.8)\naxes[1].set_xlabel(\'Target\', fontsize=12)\naxes[1].set_ylabel(\'R¬≤\', fontsize=12)\naxes[1].set_title(\'R¬≤ por Target - XGBoost\', fontsize=14, fontweight=\'bold\')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(targets, fontsize=11)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim([0, 1])\n\nplt.tight_layout()\nmetrics_plot_path = training_folder / \'metrics_comparison.png\'\nplt.savefig(metrics_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {metrics_plot_path.name}")\n\n# 5.2) Gr√°ficas de predicciones vs reales\nprint(f"\\n[2/3] Creando gr√°ficas de predicciones vs reales...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(y_train.columns):\n    row = idx\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train[col], y_train_pred[col], alpha=0.3, s=1)\n    axes[row, 0].plot([y_train[col].min(), y_train[col].max()], \n                       [y_train[col].min(), y_train[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 0].set_xlabel(\'Real [kNm]\', fontsize=11)\n    axes[row, 0].set_ylabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 0].set_title(f\'{col} - TRAIN (R¬≤={metrics_train[col]["R2"]:.4f})\', \n                          fontsize=12, fontweight=\'bold\')\n    axes[row, 0].legend(fontsize=10)\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test[col], y_test_pred[col], alpha=0.3, s=1, color=\'orange\')\n    axes[row, 1].plot([y_test[col].min(), y_test[col].max()], \n                       [y_test[col].min(), y_test[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 1].set_xlabel(\'Real [kNm]\', fontsize=11)\n    axes[row, 1].set_ylabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 1].set_title(f\'{col} - TEST (R¬≤={metrics_test[col]["R2"]:.4f})\', \n                          fontsize=12, fontweight=\'bold\')\n    axes[row, 1].legend(fontsize=10)\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\npredictions_plot_path = training_folder / \'predictions_vs_real.png\'\nplt.savefig(predictions_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {predictions_plot_path.name}")\n\n# 5.3) Gr√°fica de residuos\nprint(f"\\n[3/3] Creando gr√°fica de residuos...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(y_train.columns):\n    row = idx\n    \n    # Calcular residuos\n    residuals_train = y_train[col] - y_train_pred[col]\n    residuals_test = y_test[col] - y_test_pred[col]\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train_pred[col], residuals_train, alpha=0.3, s=1)\n    axes[row, 0].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 0].set_xlabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 0].set_ylabel(\'Residuo (Real - Predicho) [kNm]\', fontsize=11)\n    axes[row, 0].set_title(f\'{col} - Residuos TRAIN\', fontsize=12, fontweight=\'bold\')\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test_pred[col], residuals_test, alpha=0.3, s=1, color=\'orange\')\n    axes[row, 1].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 1].set_xlabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 1].set_ylabel(\'Residuo (Real - Predicho) [kNm]\', fontsize=11)\n    axes[row, 1].set_title(f\'{col} - Residuos TEST\', fontsize=12, fontweight=\'bold\')\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nresiduals_plot_path = training_folder / \'residuals_analysis.png\'\nplt.savefig(residuals_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {residuals_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 6) Guardar m√©tricas en archivo\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 6: GUARDAR RESULTADOS")\nprint("="*80)\n\n# Crear DataFrame con m√©tricas\nmetrics_df = pd.DataFrame({\n    \'Target\': list(metrics_train.keys()) + [\'PROMEDIO\'],\n    \'RMSE_Train\': list([m[\'RMSE\'] for m in metrics_train.values()]) + [avg_rmse_train],\n    \'RMSE_Test\': list([m[\'RMSE\'] for m in metrics_test.values()]) + [avg_rmse_test],\n    \'R2_Train\': list([m[\'R2\'] for m in metrics_train.values()]) + [avg_r2_train],\n    \'R2_Test\': list([m[\'R2\'] for m in metrics_test.values()]) + [avg_r2_test]\n})\n\nmetrics_csv_path = training_folder / \'metrics_results.csv\'\nmetrics_df.to_csv(metrics_csv_path, index=False)\n\nprint(f"\\n   ‚úÖ M√©tricas guardadas: {metrics_csv_path.name}")\n\n# Crear resumen de validaci√≥n\nsummary = {\n    \'Model\': \'XGBoost\',\n    \'Train_Samples\': len(X_train),\n    \'Test_Samples\': len(X_test),\n    \'Features\': X_train.shape[1],\n    \'Targets\': y_train.shape[1],\n    \'Avg_RMSE_Train\': avg_rmse_train,\n    \'Avg_RMSE_Test\': avg_rmse_test,\n    \'Avg_R2_Train\': avg_r2_train,\n    \'Avg_R2_Test\': avg_r2_test,\n    \'N_Estimators\': 100,\n    \'Max_Depth\': 6,\n    \'Learning_Rate\': 0.1,\n    \'Subsample\': 0.8,\n    \'Colsample_Bytree\': 0.8,\n    \'Tree_Method\': \'hist\'\n}\n\nsummary_df = pd.DataFrame([summary])\nsummary_path = training_folder / \'validation_summary.csv\'\nsummary_df.to_csv(summary_path, index=False)\n\nprint(f"   ‚úÖ Resumen guardado: {summary_path.name}")\n\n# ------------------------------------------------------------------------\n# RESUMEN FINAL\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ VALIDACI√ìN COMPLETADA - XGBOOST")\nprint("="*80)\n\nprint(f"\\nüìä RESULTADOS FINALES:")\nprint(f"   ‚Ä¢ RMSE promedio (Train): {avg_rmse_train:.2f} kNm")\nprint(f"   ‚Ä¢ RMSE promedio (Test):  {avg_rmse_test:.2f} kNm")\nprint(f"   ‚Ä¢ R¬≤ promedio (Train):   {avg_r2_train:.4f}")\nprint(f"   ‚Ä¢ R¬≤ promedio (Test):    {avg_r2_test:.4f}")\n\nprint(f"\\nüìÅ ARCHIVOS GENERADOS:")\nprint(f"   ‚Ä¢ {metrics_csv_path.name}")\nprint(f"   ‚Ä¢ {summary_path.name}")\nprint(f"   ‚Ä¢ {metrics_plot_path.name}")\nprint(f"   ‚Ä¢ {predictions_plot_path.name}")\nprint(f"   ‚Ä¢ {residuals_plot_path.name}")\n\nprint(f"\\nüí° XGBoost con scalers independientes por columna (X e y).")\nprint(f"üí° Targets: {list(y_test.columns)}")\nprint("="*80)',
            '\n# =============================================================================\n# XGBOOST - GR√ÅFICAS ADICIONALES DE TIME SERIES\n# =============================================================================\n\nprint("\\n" + "="*80)\nprint("üìä XGBOOST - GR√ÅFICAS DE TIME SERIES")\nprint("="*80)\n\n# NOTA: Este script requiere que y_test y y_test_pred est√©n en memoria\n# Ejecutar primero XGBOOST_Validation.py\n\n# Verificar que las variables existan\ntry:\n    y_test\n    y_test_pred\n    print(f"\\n‚úÖ Variables encontradas en memoria:")\n    print(f"   ‚Ä¢ y_test: {y_test.shape}")\n    print(f"   ‚Ä¢ y_test_pred: {y_test_pred.shape}")\nexcept NameError:\n    print(f"\\n‚ùå ERROR: y_test y/o y_test_pred no est√°n en memoria")\n    print(f"   Por favor, ejecuta primero XGBOOST_Validation.py")\n    raise\n\n# ------------------------------------------------------------------------\n# 1) Cargar Time_test y generar series_id_test\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: PREPARAR DATOS PARA TIME SERIES")\nprint("="*80)\n\ntraining_folder = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost\'\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\nprint(f"\\n[1/3] Cargando Time_test desde archivo pkl...")\n\nTime_test_path = data_ml_folder / \'Time_test.pkl\'\n\nif Time_test_path.exists():\n    Time_test = joblib.load(Time_test_path)\n    print(f"   ‚úÖ Time_test cargado: {len(Time_test):,} valores")\n    print(f"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s")\n    print(f"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s")\nelse:\n    print(f"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}")\n    raise FileNotFoundError(f"Archivo requerido no encontrado: {Time_test_path}")\n\nprint(f"\\n[2/3] Generando series_id_test a partir de Time_test...")\n\n# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\nseries_id_test_values = np.zeros(len(Time_test), dtype=int)\ncurrent_series = 0\n\nTime_test_array = Time_test.values\nfor i in range(1, len(Time_test_array)):\n    if Time_test_array[i] < Time_test_array[i-1]:\n        current_series += 1\n    series_id_test_values[i] = current_series\n\n# Convertir a pandas Series con el mismo index que Time_test\nseries_id_test = pd.Series(series_id_test_values, index=Time_test.index, name=\'series_id\')\n\nn_test_series = series_id_test.max() + 1\n\nprint(f"   ‚úÖ Series temporales identificadas en test: {n_test_series}")\n\n# Analizar cada serie\nprint(f"\\n   üìä Resumen de series en TEST:")\nfor sid in range(min(5, n_test_series)):\n    mask = series_id_test == sid\n    n_rows = mask.sum()\n    time_min = Time_test.loc[mask].min()\n    time_max = Time_test.loc[mask].max()\n    print(f"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s")\n\nif n_test_series > 5:\n    print(f"      ... y {n_test_series - 5} series m√°s")\n\nprint(f"\\n[3/3] Verificando que los datos coinciden con predicciones...")\n\n# Verificar que los √≠ndices coinciden\nif not all(series_id_test.index == y_test.index):\n    print(f"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...")\n    series_id_test = series_id_test.reindex(y_test.index)\n    Time_test = Time_test.reindex(y_test.index)\n\nprint(f"   ‚úÖ √çndices verificados:")\nprint(f"      ‚Ä¢ y_test: {y_test.shape[0]:,} filas")\nprint(f"      ‚Ä¢ y_test_pred: {y_test_pred.shape[0]:,} filas")\nprint(f"      ‚Ä¢ series_id_test: {len(series_id_test):,} valores")\nprint(f"      ‚Ä¢ Time_test: {len(Time_test):,} valores")\n\n# ------------------------------------------------------------------------\n# 2) Gr√°fica de time series: Real vs Predicho (3 series aleatorias)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas de time series (Real vs Predicho)...")\n\n# Obtener series √∫nicas del conjunto de test\nunique_test_series = series_id_test.unique()\n\n# Seleccionar 3 series aleatorias\nnp.random.seed(42)  # Para reproducibilidad\nselected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n\nprint(f"   ‚Ä¢ Total series disponibles: {len(unique_test_series)}")\nprint(f"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}")\n\n# Detectar n√∫mero de targets autom√°ticamente\nn_targets = len(y_test.columns)\nprint(f"   ‚Ä¢ Targets a graficar: {list(y_test.columns)}")\n\n# Crear figura con 3 filas x n_targets columnas (3 series, 3 targets)\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales\n        y_real = y_test.loc[series_indices, col]\n        # Valores predichos\n        y_pred = y_test_pred.loc[series_indices, col]\n        \n        # Calcular m√©tricas para esta serie y target\n        r2_series = r2_score(y_real, y_pred)\n        rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n        \n        # Graficar\n        ax.plot(time_series, y_real, label=\'Real\', \n                linewidth=2, alpha=0.8, color=\'blue\')\n        ax.plot(time_series, y_pred, label=\'Predicho\', \n                linestyle=\'--\', linewidth=2, alpha=0.8, color=\'red\')\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}\', \n                     fontsize=11, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=9)\n        ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\ntimeseries_plot_path = training_folder / \'timeseries_comparison.png\'\nplt.savefig(timeseries_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 3) Gr√°fica de time series con ZOOM (50 segundos)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: GR√ÅFICAS DE TIME SERIES - ZOOM 50 SEGUNDOS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas de time series con zoom (50s)...")\n\n# Crear figura con 3 filas x n_targets columnas (3 series, 3 targets)\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Definir ventana de 50 segundos desde el inicio\n    time_min = time_series.min()\n    time_max_zoom = time_min + 50\n    \n    # Filtrar por ventana de tiempo\n    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n    zoom_indices = time_series[zoom_mask].index\n    time_zoom = time_series[zoom_mask]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales y predichos (con zoom)\n        y_real_zoom = y_test.loc[zoom_indices, col]\n        y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n        \n        # Calcular m√©tricas para esta ventana\n        r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n        rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n        \n        # Graficar\n        ax.plot(time_zoom, y_real_zoom, label=\'Real\', \n                linewidth=2.5, alpha=0.8, color=\'blue\', marker=\'o\', markersize=4)\n        ax.plot(time_zoom, y_pred_zoom, label=\'Predicho\', \n                linestyle=\'--\', linewidth=2.5, alpha=0.8, color=\'red\', marker=\'x\', markersize=5)\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}\', \n                     fontsize=11, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=9)\n        ax.grid(True, alpha=0.3)\n        \n        # A√±adir texto con informaci√≥n de puntos\n        n_points = len(zoom_indices)\n        ax.text(0.02, 0.02, f\'Puntos: {n_points}\', transform=ax.transAxes, fontsize=9,\n                verticalalignment=\'bottom\', bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.5))\n\nplt.tight_layout()\ntimeseries_zoom_plot_path = training_folder / \'timeseries_comparison_zoom50s.png\'\nplt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# RESUMEN FINAL\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ GR√ÅFICAS DE TIME SERIES COMPLETADAS")\nprint("="*80)\n\nprint(f"\\nüìä GR√ÅFICAS GENERADAS:")\nprint(f"   ‚Ä¢ {timeseries_plot_path.name}")\nprint(f"   ‚Ä¢ {timeseries_zoom_plot_path.name}")\n\nprint(f"\\nüí° Targets graficados: {list(y_test.columns)}")\nprint(f"üí° Series aleatorias: {list(selected_series)}")\n\nprint(f"\\nüí° Ahora tienes todas las gr√°ficas para XGBoost:")\nprint(f"   1. M√©tricas por target (RMSE y R¬≤)")\nprint(f"   2. Predicciones vs Real")\nprint(f"   3. An√°lisis de residuos")\nprint(f"   4. Time series completas (3 series aleatorias √ó {n_targets} targets)")\nprint(f"   5. Time series con zoom de 50s")\n\nprint("="*80)',
        ],
    },
    {
        "title": 'üß† XGBOOST SIN NORMALIZAR',
        "cells": [
            '# =============================================================================\n# MODELO 3b: XGBOOST SIN NORMALIZAR - ENTRENAMIENTO\n# =============================================================================\nimport time\n\nprint("\\n" + "="*80)\nprint("üå≥ MODELO: XGBOOST SIN NORMALIZAR - ENTRENAMIENTO")\nprint("="*80)\n\n# Verificar disponibilidad de XGBoost\nif not XGBOOST_AVAILABLE:\n    print("\\n‚ùå ERROR: XGBoost no est√° instalado")\n    print("   Instalar con: pip install xgboost")\n    raise ImportError("XGBoost no disponible")\n\nprint("‚úÖ XGBoost disponible")\n\nimport xgboost as xgb\n\n# ------------------------------------------------------------------------\n# 1) Preparar carpeta y cargar datos\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: PREPARAR CARPETA Y CARGAR DATOS")\nprint("="*80)\n\nprint(f"\\n[1/3] Creando carpeta para XGBoost sin normalizar...")\n\ntraining_folder = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost_NoNorm\'\nos.makedirs(training_folder, exist_ok=True)\n\nprint(f"   ‚úÖ Carpeta: {training_folder}")\n\nprint(f"\\n[2/3] Cargando datos de entrenamiento SIN normalizar...")\n\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\n# Cargar datos originales (sin normalizar)\nX_train = joblib.load(data_ml_folder / \'X_train.pkl\')\ny_train = joblib.load(data_ml_folder / \'y_train.pkl\')\n\nprint(f"   ‚úÖ X_train (sin normalizar): {X_train.shape}")\nprint(f"   ‚úÖ y_train (sin normalizar): {y_train.shape}")\n\nprint(f"\\n[3/3] Verificando n√∫mero de features y targets...")\n\nprint(f"   ‚Ä¢ Features (inputs):  {X_train.shape[1]}")\nprint(f"   ‚Ä¢ Targets (outputs):  {y_train.shape[1]} componentes Coleman")\nprint(f"   ‚Ä¢ Muestras training:  {X_train.shape[0]:,}")\n\n# Mostrar estad√≠sticas de los datos sin normalizar\nprint(f"\\n   üìä Estad√≠sticas de los datos originales:")\nprint(f"      ‚Ä¢ X_train - Min: {X_train.min().min():.4f}, Max: {X_train.max().max():.4f}")\nprint(f"      ‚Ä¢ y_train - Min: {y_train.min().min():.2f}, Max: {y_train.max().max():.2f}")\n\n# ------------------------------------------------------------------------\n# 2) Configurar y entrenar modelo XGBoost\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: CONFIGURACI√ìN Y ENTRENAMIENTO")\nprint("="*80)\n\nprint(f"\\n[1/3] Configurando XGBoost sin normalizaci√≥n...")\nprint(f"   üí° XGBoost es robusto a diferentes escalas y no requiere normalizaci√≥n")\n\n# Configuraci√≥n optimizada para datos sin normalizar\n# XGBoost maneja bien diferentes escalas gracias a su estructura de √°rboles\nxgb_base = xgb.XGBRegressor(\n    n_estimators=2000,          # N√∫mero de √°rboles\n    max_depth=6,                # Profundidad m√°xima (puede ser mayor sin normalizar)\n    learning_rate=0.05,         # Tasa de aprendizaje\n    subsample=0.8,              # Fracci√≥n de muestras para cada √°rbol\n    colsample_bytree=0.8,       # Fracci√≥n de features para cada √°rbol\n    gamma=0.5,                  # M√≠nima reducci√≥n de loss para split\n    min_child_weight=5,         # Peso m√≠nimo en nodo hijo\n    reg_alpha=1,                # Regularizaci√≥n L1\n    reg_lambda=5,               # Regularizaci√≥n L2\n    random_state=42,\n    n_jobs=-1,                  # Usar todos los cores\n    verbosity=0,                # Sin mensajes de XGBoost\n    tree_method=\'hist\',         # M√©todo de construcci√≥n de √°rboles (m√°s r√°pido)\n    enable_categorical=False    # No usar features categ√≥ricas\n)\n\n\n\n\n# Usar MultiOutputRegressor para predecir m√∫ltiples targets\nxgb_model_nonorm = MultiOutputRegressor(xgb_base)\n\nprint(f"   ‚úÖ Modelo configurado:")\nprint(f"      ‚Ä¢ Tipo: XGBoost (Gradient Boosting)")\nprint(f"      ‚Ä¢ N_estimators: {xgb_base.n_estimators}")\nprint(f"      ‚Ä¢ Max_depth: {xgb_base.max_depth}")\nprint(f"      ‚Ä¢ Learning_rate: {xgb_base.learning_rate}")\nprint(f"      ‚Ä¢ Subsample: {xgb_base.subsample}")\nprint(f"      ‚Ä¢ Colsample_bytree: {xgb_base.colsample_bytree}")\nprint(f"      ‚Ä¢ Reg_alpha (L1): {xgb_base.reg_alpha}")\nprint(f"      ‚Ä¢ Reg_lambda (L2): {xgb_base.reg_lambda}")\nprint(f"      ‚Ä¢ Outputs: {len(y_train.columns)}")\nprint(f"      ‚Ä¢ Datos: SIN NORMALIZAR")\n\nprint(f"\\n[2/3] Entrenando modelo con datos sin normalizar...")\nprint(f"   ‚Ä¢ Datos de entrenamiento: {X_train.shape}")\nprint(f"   ‚Ä¢ Targets de entrenamiento: {y_train.shape}")\n\nstart_time = time.time()\n\n# Entrenar el modelo\nxgb_model_nonorm.fit(X_train, y_train)\n\n\n\ntraining_time = time.time() - start_time\n\nprint(f"   ‚úÖ Modelo entrenado en {training_time:.2f} segundos ({training_time/60:.2f} minutos)")\n\nprint(f"\\n[3/3] Guardando modelo...")\n\n# Guardar el modelo entrenado\nmodel_path = training_folder / \'xgboost_nonorm_model.pkl\'\njoblib.dump(xgb_model_nonorm, model_path)\n\nprint(f"   ‚úÖ Modelo guardado: {model_path.name}")\n\n# ------------------------------------------------------------------------\n# 3) Generar predicciones en training set\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: EVALUAR MODELO EN TRAINING SET")\nprint("="*80)\n\nprint(f"\\n[1/2] Generando predicciones en datos de entrenamiento...")\n\ny_train_pred = xgb_model_nonorm.predict(X_train)\n\n# Convertir a DataFrame\ny_train_pred = pd.DataFrame(\n    y_train_pred,\n    columns=y_train.columns,\n    index=y_train.index\n)\n\nprint(f"   ‚úÖ Predicciones generadas: {y_train_pred.shape}")\n\nprint(f"\\n[2/2] Calculando m√©tricas en training set...")\n\n# Calcular m√©tricas para cada target\nmetrics_train = {}\nfor target in y_train.columns:\n    r2 = r2_score(y_train[target], y_train_pred[target])\n    rmse = np.sqrt(mean_squared_error(y_train[target], y_train_pred[target]))\n    mae = mean_absolute_error(y_train[target], y_train_pred[target])\n    \n    metrics_train[target] = {\n        \'R2\': r2,\n        \'RMSE\': rmse,\n        \'MAE\': mae\n    }\n    \n    print(f"   ‚Ä¢ {target}:")\n    print(f"      - R¬≤ = {r2:.6f}")\n    print(f"      - RMSE = {rmse:.4f} kNm")\n    print(f"      - MAE = {mae:.4f} kNm")\n\n# Calcular m√©tricas promedio\navg_r2 = np.mean([m[\'R2\'] for m in metrics_train.values()])\navg_rmse = np.mean([m[\'RMSE\'] for m in metrics_train.values()])\navg_mae = np.mean([m[\'MAE\'] for m in metrics_train.values()])\n\nprint(f"\\n   üìä M√âTRICAS PROMEDIO (Training):")\nprint(f"      - R¬≤ promedio: {avg_r2:.6f}")\nprint(f"      - RMSE promedio: {avg_rmse:.4f} kNm")\nprint(f"      - MAE promedio: {avg_mae:.4f} kNm")\n\n# ------------------------------------------------------------------------\n# RESUMEN\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ ENTRENAMIENTO COMPLETADO - XGBOOST SIN NORMALIZAR")\nprint("="*80)\n\nprint(f"\\nüìä INFORMACI√ìN DEL MODELO:")\nprint(f"   ‚Ä¢ Algoritmo: XGBoost (Extreme Gradient Boosting)")\nprint(f"   ‚Ä¢ Datos: SIN NORMALIZAR")\nprint(f"   ‚Ä¢ Tiempo de entrenamiento: {training_time:.2f} segundos")\nprint(f"   ‚Ä¢ Muestras entrenadas: {X_train.shape[0]:,}")\nprint(f"   ‚Ä¢ Features utilizadas: {X_train.shape[1]}")\nprint(f"   ‚Ä¢ Outputs predichos: {y_train.shape[1]}")\n\nprint(f"\\nüìÅ ARCHIVOS GENERADOS:")\nprint(f"   ‚Ä¢ {model_path.name}")\n\nprint(f"\\nüí° VENTAJAS DE NO NORMALIZAR CON XGBOOST:")\nprint(f"   ‚Ä¢ Los √°rboles de decisi√≥n son invariantes a escalas")\nprint(f"   ‚Ä¢ No se pierde informaci√≥n sobre magnitudes reales")\nprint(f"   ‚Ä¢ Interpretaci√≥n directa de importancia de features")\nprint(f"   ‚Ä¢ Menor preprocesamiento en producci√≥n")\n\nprint(f"\\n‚ö†Ô∏è  NOTA:")\nprint(f"   ‚Ä¢ Comparar con modelo normalizado para ver cu√°l funciona mejor")\nprint(f"   ‚Ä¢ Evaluar en test set para verificar generalizaci√≥n")\n\nprint("="*80)',
            '"""\nScript para validar modelo XGBoost SIN NORMALIZAR entrenado.\n\nEste script:\n1. Carga modelo entrenado y datos de test SIN NORMALIZAR\n2. Realiza predicciones directamente (sin normalizaci√≥n)\n3. Calcula m√©tricas (RMSE, R¬≤)\n4. Genera visualizaciones\n5. Compara con modelo normalizado\n\nAdaptado para trabajar con:\n- Datos originales sin normalizaci√≥n\n- Targets: M_0, M_1c, M_1s\n\nAutor: Adaptado de validaci√≥n XGBoost\nFecha: Febrero 2026\n"""\n\n# =============================================================================\n# MODELO: XGBOOST SIN NORMALIZAR - VALIDACI√ìN Y VISUALIZACIONES\n# =============================================================================\n\nprint("\\n" + "="*80)\nprint("üéØ MODELO: XGBOOST SIN NORMALIZAR - VALIDACI√ìN")\nprint("="*80)\n\n# ------------------------------------------------------------------------\n# 1) Cargar modelo entrenado y datos de test\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: CARGAR MODELO Y DATOS DE TEST")\nprint("="*80)\n\ntraining_folder_nonorm = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost_NoNorm\'\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\nprint(f"\\n[1/4] Cargando modelo entrenado SIN NORMALIZAR...")\nmodel_path_nonorm = training_folder_nonorm / \'xgboost_nonorm_model.pkl\'\n\nif not model_path_nonorm.exists():\n    print(f"   ‚ùå ERROR: Modelo no encontrado en {model_path_nonorm}")\n    print(f"   üí° Primero debes entrenar el modelo sin normalizar")\n    raise FileNotFoundError(f"No se encontr√≥ el modelo en {model_path_nonorm}")\n\nxgb_model_nonorm = joblib.load(model_path_nonorm)\nprint(f"   ‚úÖ Modelo cargado desde: {model_path_nonorm.name}")\n\nprint(f"\\n[2/4] Cargando datos de test SIN NORMALIZAR...")\nX_test = joblib.load(data_ml_folder / \'X_test.pkl\')\ny_test = joblib.load(data_ml_folder / \'y_test.pkl\')\nprint(f"   ‚úÖ X_test: {X_test.shape}")\nprint(f"   ‚úÖ y_test: {y_test.shape}")\nprint(f"   üí° Targets: {list(y_test.columns)}")\n\nprint(f"\\n[3/4] Cargando datos de train SIN NORMALIZAR (para m√©tricas)...")\nX_train = joblib.load(data_ml_folder / \'X_train.pkl\')\ny_train = joblib.load(data_ml_folder / \'y_train.pkl\')\nprint(f"   ‚úÖ X_train: {X_train.shape}")\nprint(f"   ‚úÖ y_train: {y_train.shape}")\n\nprint(f"\\n[4/4] Verificando escalas de los datos...")\nprint(f"   üìä Estad√≠sticas X_test:")\nprint(f"      ‚Ä¢ Min: {X_test.min().min():.4f}")\nprint(f"      ‚Ä¢ Max: {X_test.max().max():.4f}")\nprint(f"      ‚Ä¢ Mean: {X_test.mean().mean():.4f}")\nprint(f"   üìä Estad√≠sticas y_test:")\nprint(f"      ‚Ä¢ Min: {y_test.min().min():.2f} kNm")\nprint(f"      ‚Ä¢ Max: {y_test.max().max():.2f} kNm")\nprint(f"      ‚Ä¢ Mean: {y_test.mean().mean():.2f} kNm")\n\nprint(f"\\n   üí° Datos en escala original - NO NORMALIZADOS")\n\n# ------------------------------------------------------------------------\n# 2) Realizar predicciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: PREDICCIONES (SIN NORMALIZACI√ìN)")\nprint("="*80)\n\nprint(f"\\n[1/2] Prediciendo sobre conjunto de entrenamiento...")\n\ny_train_pred = xgb_model_nonorm.predict(X_train)\ny_train_pred = pd.DataFrame(\n    y_train_pred,\n    index=X_train.index,\n    columns=y_train.columns\n)\n\nprint(f"   ‚úÖ Predicciones train: {y_train_pred.shape}")\nprint(f"   üí° Predicci√≥n directa en escala original")\n\nprint(f"\\n[2/2] Prediciendo sobre conjunto de test...")\n\ny_test_pred = xgb_model_nonorm.predict(X_test)\ny_test_pred = pd.DataFrame(\n    y_test_pred,\n    index=y_test.index,\n    columns=y_test.columns\n)\n\nprint(f"   ‚úÖ Predicciones test: {y_test_pred.shape}")\nprint(f"   üí° No requiere desnormalizaci√≥n - ya en kNm")\n\n# Verificar rango de predicciones\nprint(f"\\n   üìä Rango de predicciones:")\nfor col in y_test_pred.columns:\n    print(f"      ‚Ä¢ {col}: [{y_test_pred[col].min():.2f}, {y_test_pred[col].max():.2f}] kNm")\n\n# ------------------------------------------------------------------------\n# 3) Calcular m√©tricas\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: C√ÅLCULO DE M√âTRICAS")\nprint("="*80)\n\n# Diccionarios para almacenar m√©tricas\nmetrics_train_nonorm = {}\nmetrics_test_nonorm = {}\n\nprint(f"\\n{\'M√âTRICA\':20} {\'CONJUNTO\':10} {\'M_0\':>15} {\'M_1c\':>15} {\'M_1s\':>15}")\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\n\n# Calcular m√©tricas para cada target\nfor i, col in enumerate(y_train.columns):\n    # TRAIN\n    rmse_train = np.sqrt(mean_squared_error(y_train[col], y_train_pred[col]))\n    r2_train = r2_score(y_train[col], y_train_pred[col])\n    mae_train = mean_absolute_error(y_train[col], y_train_pred[col])\n    \n    # TEST\n    rmse_test = np.sqrt(mean_squared_error(y_test[col], y_test_pred[col]))\n    r2_test = r2_score(y_test[col], y_test_pred[col])\n    mae_test = mean_absolute_error(y_test[col], y_test_pred[col])\n    \n    # Guardar m√©tricas\n    metrics_train_nonorm[col] = {\'RMSE\': rmse_train, \'R2\': r2_train, \'MAE\': mae_train}\n    metrics_test_nonorm[col] = {\'RMSE\': rmse_test, \'R2\': r2_test, \'MAE\': mae_test}\n    \n    # Mostrar\n    if i == 0:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {rmse_train:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {rmse_test:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'MAE\':20} {\'TRAIN\':10} {mae_train:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {mae_test:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {r2_train:>15.4f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {r2_test:>15.4f} {\'-\':>15} {\'-\':>15}")\n    elif i == 1:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {rmse_train:>15.2f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {rmse_test:>15.2f} {\'-\':>15}")\n        print(f"{\'MAE\':20} {\'TRAIN\':10} {\'-\':>15} {mae_train:>15.2f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {mae_test:>15.2f} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {r2_train:>15.4f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {r2_test:>15.4f} {\'-\':>15}")\n    else:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {rmse_train:>15.2f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {rmse_test:>15.2f}")\n        print(f"{\'MAE\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {mae_train:>15.2f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {mae_test:>15.2f}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {r2_train:>15.4f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {r2_test:>15.4f}")\n\n# Calcular promedios\navg_rmse_train = np.mean([m[\'RMSE\'] for m in metrics_train_nonorm.values()])\navg_rmse_test = np.mean([m[\'RMSE\'] for m in metrics_test_nonorm.values()])\navg_mae_train = np.mean([m[\'MAE\'] for m in metrics_train_nonorm.values()])\navg_mae_test = np.mean([m[\'MAE\'] for m in metrics_test_nonorm.values()])\navg_r2_train = np.mean([m[\'R2\'] for m in metrics_train_nonorm.values()])\navg_r2_test = np.mean([m[\'R2\'] for m in metrics_test_nonorm.values()])\n\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\nprint(f"{\'PROMEDIO RMSE\':20} {\'TRAIN\':10} {avg_rmse_train:>15.2f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_rmse_test:>15.2f}")\nprint(f"{\'PROMEDIO MAE\':20} {\'TRAIN\':10} {avg_mae_train:>15.2f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_mae_test:>15.2f}")\nprint(f"{\'PROMEDIO R¬≤\':20} {\'TRAIN\':10} {avg_r2_train:>15.4f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_r2_test:>15.4f}")\n\n# ------------------------------------------------------------------------\n# 4) Comparaci√≥n con modelo normalizado (si existe)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: COMPARACI√ìN CON MODELO NORMALIZADO")\nprint("="*80)\n\ntraining_folder_norm = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost\'\nmetrics_norm_path = training_folder_norm / \'metrics_results.csv\'\n\nif metrics_norm_path.exists():\n    print(f"\\n[1/1] Comparando con modelo normalizado...")\n    \n    metrics_norm_df = pd.read_csv(metrics_norm_path)\n    \n    # Extraer m√©tricas del modelo normalizado\n    norm_rmse_test = metrics_norm_df[metrics_norm_df[\'Target\'] == \'PROMEDIO\'][\'RMSE_Test\'].values[0] if \'PROMEDIO\' in metrics_norm_df[\'Target\'].values else metrics_norm_df[\'RMSE_Test\'].mean()\n    norm_r2_test = metrics_norm_df[metrics_norm_df[\'Target\'] == \'PROMEDIO\'][\'R2_Test\'].values[0] if \'PROMEDIO\' in metrics_norm_df[\'Target\'].values else metrics_norm_df[\'R2_Test\'].mean()\n    \n    print(f"\\n   üìä COMPARACI√ìN EN TEST SET:")\n    print(f"   {\'MODELO\':25} {\'RMSE [kNm]\':>15} {\'R¬≤\':>15} {\'DIFERENCIA RMSE\':>20}")\n    print(f"   {\'‚îÄ\'*25} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*20}")\n    print(f"   {\'XGBoost NORMALIZADO\':25} {norm_rmse_test:>15.2f} {norm_r2_test:>15.4f} {\'‚îÄ\':>20}")\n    print(f"   {\'XGBoost SIN NORMALIZAR\':25} {avg_rmse_test:>15.2f} {avg_r2_test:>15.4f} {avg_rmse_test - norm_rmse_test:>+20.2f}")\n    \n    # Determinar cu√°l es mejor\n    if avg_r2_test > norm_r2_test:\n        print(f"\\n   ‚úÖ Modelo SIN NORMALIZAR es MEJOR (R¬≤ m√°s alto)")\n        improvement = ((avg_r2_test - norm_r2_test) / norm_r2_test) * 100\n        print(f"   üí° Mejora en R¬≤: {improvement:+.2f}%")\n    elif avg_r2_test < norm_r2_test:\n        print(f"\\n   ‚ö†Ô∏è  Modelo NORMALIZADO es MEJOR (R¬≤ m√°s alto)")\n        difference = ((norm_r2_test - avg_r2_test) / norm_r2_test) * 100\n        print(f"   üí° Diferencia en R¬≤: {difference:.2f}%")\n    else:\n        print(f"\\n   ‚öñÔ∏è  Ambos modelos tienen rendimiento similar")\nelse:\n    print(f"\\n   ‚ö†Ô∏è  No se encontraron m√©tricas del modelo normalizado")\n    print(f"   üí° Entrena y valida el modelo normalizado para comparar")\n\n# ------------------------------------------------------------------------\n# 5) Visualizaciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 5: GENERACI√ìN DE GR√ÅFICAS")\nprint("="*80)\n\n# 5.1) Gr√°fica de m√©tricas por target\nprint(f"\\n[1/3] Creando gr√°fica de m√©tricas...")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# RMSE\ntargets = list(metrics_train_nonorm.keys())\nrmse_train_vals = [metrics_train_nonorm[t][\'RMSE\'] for t in targets]\nrmse_test_vals = [metrics_test_nonorm[t][\'RMSE\'] for t in targets]\n\nx = np.arange(len(targets))\nwidth = 0.35\n\naxes[0].bar(x - width/2, rmse_train_vals, width, label=\'Train\', alpha=0.8, color=\'#2ecc71\')\naxes[0].bar(x + width/2, rmse_test_vals, width, label=\'Test\', alpha=0.8, color=\'#e74c3c\')\naxes[0].set_xlabel(\'Target\', fontsize=12)\naxes[0].set_ylabel(\'RMSE [kNm]\', fontsize=12)\naxes[0].set_title(\'RMSE por Target - XGBoost (Sin Normalizar)\', fontsize=14, fontweight=\'bold\')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(targets, fontsize=11)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\n# R¬≤\nr2_train_vals = [metrics_train_nonorm[t][\'R2\'] for t in targets]\nr2_test_vals = [metrics_test_nonorm[t][\'R2\'] for t in targets]\n\naxes[1].bar(x - width/2, r2_train_vals, width, label=\'Train\', alpha=0.8, color=\'#2ecc71\')\naxes[1].bar(x + width/2, r2_test_vals, width, label=\'Test\', alpha=0.8, color=\'#e74c3c\')\naxes[1].set_xlabel(\'Target\', fontsize=12)\naxes[1].set_ylabel(\'R¬≤\', fontsize=12)\naxes[1].set_title(\'R¬≤ por Target - XGBoost (Sin Normalizar)\', fontsize=14, fontweight=\'bold\')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(targets, fontsize=11)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim([0, 1])\n\nplt.tight_layout()\nmetrics_plot_path = training_folder_nonorm / \'metrics_comparison_nonorm.png\'\nplt.savefig(metrics_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {metrics_plot_path.name}")\n\n# 5.2) Gr√°ficas de predicciones vs reales\nprint(f"\\n[2/3] Creando gr√°ficas de predicciones vs reales...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(y_train.columns):\n    row = idx\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train[col], y_train_pred[col], alpha=0.3, s=1, color=\'#2ecc71\')\n    axes[row, 0].plot([y_train[col].min(), y_train[col].max()], \n                       [y_train[col].min(), y_train[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 0].set_xlabel(\'Real [kNm]\', fontsize=11)\n    axes[row, 0].set_ylabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 0].set_title(f\'{col} - TRAIN (R¬≤={metrics_train_nonorm[col]["R2"]:.4f})\', \n                          fontsize=12, fontweight=\'bold\')\n    axes[row, 0].legend(fontsize=10)\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test[col], y_test_pred[col], alpha=0.3, s=1, color=\'#e74c3c\')\n    axes[row, 1].plot([y_test[col].min(), y_test[col].max()], \n                       [y_test[col].min(), y_test[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 1].set_xlabel(\'Real [kNm]\', fontsize=11)\n    axes[row, 1].set_ylabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 1].set_title(f\'{col} - TEST (R¬≤={metrics_test_nonorm[col]["R2"]:.4f})\', \n                          fontsize=12, fontweight=\'bold\')\n    axes[row, 1].legend(fontsize=10)\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.suptitle(\'Predicciones vs Real - XGBoost Sin Normalizar\', fontsize=16, fontweight=\'bold\', y=1.0)\nplt.tight_layout()\npredictions_plot_path = training_folder_nonorm / \'predictions_vs_real_nonorm.png\'\nplt.savefig(predictions_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {predictions_plot_path.name}")\n\n# 5.3) Gr√°fica de residuos\nprint(f"\\n[3/3] Creando gr√°fica de residuos...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(y_train.columns):\n    row = idx\n    \n    # Calcular residuos\n    residuals_train = y_train[col] - y_train_pred[col]\n    residuals_test = y_test[col] - y_test_pred[col]\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train_pred[col], residuals_train, alpha=0.3, s=1, color=\'#2ecc71\')\n    axes[row, 0].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 0].set_xlabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 0].set_ylabel(\'Residuo (Real - Predicho) [kNm]\', fontsize=11)\n    axes[row, 0].set_title(f\'{col} - Residuos TRAIN (œÉ={residuals_train.std():.2f})\', \n                          fontsize=12, fontweight=\'bold\')\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test_pred[col], residuals_test, alpha=0.3, s=1, color=\'#e74c3c\')\n    axes[row, 1].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 1].set_xlabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 1].set_ylabel(\'Residuo (Real - Predicho) [kNm]\', fontsize=11)\n    axes[row, 1].set_title(f\'{col} - Residuos TEST (œÉ={residuals_test.std():.2f})\', \n                          fontsize=12, fontweight=\'bold\')\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.suptitle(\'An√°lisis de Residuos - XGBoost Sin Normalizar\', fontsize=16, fontweight=\'bold\', y=1.0)\nplt.tight_layout()\nresiduals_plot_path = training_folder_nonorm / \'residuals_analysis_nonorm.png\'\nplt.savefig(residuals_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {residuals_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 6) Guardar m√©tricas en archivo\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 6: GUARDAR RESULTADOS")\nprint("="*80)\n\n# Crear DataFrame con m√©tricas\nmetrics_df = pd.DataFrame({\n    \'Target\': list(metrics_train_nonorm.keys()) + [\'PROMEDIO\'],\n    \'RMSE_Train\': list([m[\'RMSE\'] for m in metrics_train_nonorm.values()]) + [avg_rmse_train],\n    \'RMSE_Test\': list([m[\'RMSE\'] for m in metrics_test_nonorm.values()]) + [avg_rmse_test],\n    \'MAE_Train\': list([m[\'MAE\'] for m in metrics_train_nonorm.values()]) + [avg_mae_train],\n    \'MAE_Test\': list([m[\'MAE\'] for m in metrics_test_nonorm.values()]) + [avg_mae_test],\n    \'R2_Train\': list([m[\'R2\'] for m in metrics_train_nonorm.values()]) + [avg_r2_train],\n    \'R2_Test\': list([m[\'R2\'] for m in metrics_test_nonorm.values()]) + [avg_r2_test]\n})\n\nmetrics_csv_path = training_folder_nonorm / \'metrics_results_nonorm.csv\'\nmetrics_df.to_csv(metrics_csv_path, index=False)\n\nprint(f"\\n   ‚úÖ M√©tricas guardadas: {metrics_csv_path.name}")\n\n# Crear resumen de validaci√≥n\nsummary = {\n    \'Model\': \'XGBoost_NoNorm\',\n    \'Normalized\': False,\n    \'Train_Samples\': len(X_train),\n    \'Test_Samples\': len(X_test),\n    \'Features\': X_train.shape[1],\n    \'Targets\': y_train.shape[1],\n    \'Avg_RMSE_Train\': avg_rmse_train,\n    \'Avg_RMSE_Test\': avg_rmse_test,\n    \'Avg_MAE_Train\': avg_mae_train,\n    \'Avg_MAE_Test\': avg_mae_test,\n    \'Avg_R2_Train\': avg_r2_train,\n    \'Avg_R2_Test\': avg_r2_test,\n    \'N_Estimators\': 2000,\n    \'Max_Depth\': 6,\n    \'Learning_Rate\': 0.05,\n    \'Subsample\': 0.8,\n    \'Colsample_Bytree\': 0.8,\n    \'Reg_Alpha\': 1,\n    \'Reg_Lambda\': 5\n}\n\nsummary_df = pd.DataFrame([summary])\nsummary_path = training_folder_nonorm / \'validation_summary_nonorm.csv\'\nsummary_df.to_csv(summary_path, index=False)\n\nprint(f"   ‚úÖ Resumen guardado: {summary_path.name}")\n\n# ------------------------------------------------------------------------\n# RESUMEN FINAL\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ VALIDACI√ìN COMPLETADA - XGBOOST SIN NORMALIZAR")\nprint("="*80)\n\nprint(f"\\nüìä RESULTADOS FINALES:")\nprint(f"   ‚Ä¢ RMSE promedio (Train): {avg_rmse_train:.2f} kNm")\nprint(f"   ‚Ä¢ RMSE promedio (Test):  {avg_rmse_test:.2f} kNm")\nprint(f"   ‚Ä¢ MAE promedio (Train):  {avg_mae_train:.2f} kNm")\nprint(f"   ‚Ä¢ MAE promedio (Test):   {avg_mae_test:.2f} kNm")\nprint(f"   ‚Ä¢ R¬≤ promedio (Train):   {avg_r2_train:.4f}")\nprint(f"   ‚Ä¢ R¬≤ promedio (Test):    {avg_r2_test:.4f}")\n\nprint(f"\\nüìÅ ARCHIVOS GENERADOS:")\nprint(f"   ‚Ä¢ {metrics_csv_path.name}")\nprint(f"   ‚Ä¢ {summary_path.name}")\nprint(f"   ‚Ä¢ {metrics_plot_path.name}")\nprint(f"   ‚Ä¢ {predictions_plot_path.name}")\nprint(f"   ‚Ä¢ {residuals_plot_path.name}")\n\nprint(f"\\nüí° VENTAJAS DE NO NORMALIZAR:")\nprint(f"   ‚Ä¢ Predicciones directamente en kNm (sin desnormalizaci√≥n)")\nprint(f"   ‚Ä¢ M√°s r√°pido en producci√≥n")\nprint(f"   ‚Ä¢ Importancia de features m√°s interpretable")\nprint(f"   ‚Ä¢ XGBoost es robusto a diferentes escalas")\n\nprint(f"\\nüí° Targets: {list(y_test.columns)}")\nprint(f"üí° Datos SIN NORMALIZAR - Escala original")\nprint("="*80)',
            '# =============================================================================\n# XGBOOST SIN NORMALIZAR - GR√ÅFICAS ADICIONALES DE TIME SERIES\n# =============================================================================\n\nprint("\\n" + "="*80)\nprint("üìä XGBOOST SIN NORMALIZAR - GR√ÅFICAS DE TIME SERIES")\nprint("="*80)\n\n# NOTA: Este script requiere que y_test y y_test_pred est√©n en memoria\n# Ejecutar primero la validaci√≥n de XGBoost sin normalizar\n\n# Verificar que las variables existan\ntry:\n    y_test\n    y_test_pred\n    print(f"\\n‚úÖ Variables encontradas en memoria:")\n    print(f"   ‚Ä¢ y_test: {y_test.shape}")\n    print(f"   ‚Ä¢ y_test_pred: {y_test_pred.shape}")\n    print(f"   üí° Datos en escala original (sin normalizar)")\nexcept NameError:\n    print(f"\\n‚ùå ERROR: y_test y/o y_test_pred no est√°n en memoria")\n    print(f"   Por favor, ejecuta primero la validaci√≥n de XGBoost sin normalizar")\n    raise\n\n# ------------------------------------------------------------------------\n# 1) Cargar Time_test y generar series_id_test\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: PREPARAR DATOS PARA TIME SERIES")\nprint("="*80)\n\ntraining_folder_nonorm = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost_NoNorm\'\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\nprint(f"\\n[1/3] Cargando Time_test desde archivo pkl...")\n\nTime_test_path = data_ml_folder / \'Time_test.pkl\'\n\nif Time_test_path.exists():\n    Time_test = joblib.load(Time_test_path)\n    print(f"   ‚úÖ Time_test cargado: {len(Time_test):,} valores")\n    print(f"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s")\n    print(f"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s")\nelse:\n    print(f"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}")\n    raise FileNotFoundError(f"Archivo requerido no encontrado: {Time_test_path}")\n\nprint(f"\\n[2/3] Generando series_id_test a partir de Time_test...")\n\n# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\nseries_id_test_values = np.zeros(len(Time_test), dtype=int)\ncurrent_series = 0\n\nTime_test_array = Time_test.values\nfor i in range(1, len(Time_test_array)):\n    if Time_test_array[i] < Time_test_array[i-1]:\n        current_series += 1\n    series_id_test_values[i] = current_series\n\n# Convertir a pandas Series con el mismo index que Time_test\nseries_id_test = pd.Series(series_id_test_values, index=Time_test.index, name=\'series_id\')\n\nn_test_series = series_id_test.max() + 1\n\nprint(f"   ‚úÖ Series temporales identificadas en test: {n_test_series}")\n\n# Analizar cada serie\nprint(f"\\n   üìä Resumen de series en TEST:")\nfor sid in range(min(5, n_test_series)):\n    mask = series_id_test == sid\n    n_rows = mask.sum()\n    time_min = Time_test.loc[mask].min()\n    time_max = Time_test.loc[mask].max()\n    print(f"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s")\n\nif n_test_series > 5:\n    print(f"      ... y {n_test_series - 5} series m√°s")\n\nprint(f"\\n[3/3] Verificando que los datos coinciden con predicciones...")\n\n# Verificar que los √≠ndices coinciden\nif not all(series_id_test.index == y_test.index):\n    print(f"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...")\n    series_id_test = series_id_test.reindex(y_test.index)\n    Time_test = Time_test.reindex(y_test.index)\n\nprint(f"   ‚úÖ √çndices verificados:")\nprint(f"      ‚Ä¢ y_test: {y_test.shape[0]:,} filas")\nprint(f"      ‚Ä¢ y_test_pred: {y_test_pred.shape[0]:,} filas")\nprint(f"      ‚Ä¢ series_id_test: {len(series_id_test):,} valores")\nprint(f"      ‚Ä¢ Time_test: {len(Time_test):,} valores")\n\n# ------------------------------------------------------------------------\n# 2) Gr√°fica de time series: Real vs Predicho (3 series aleatorias)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas de time series (Real vs Predicho)...")\n\n# Obtener series √∫nicas del conjunto de test\nunique_test_series = series_id_test.unique()\n\n# Seleccionar 3 series aleatorias\nnp.random.seed(42)  # Para reproducibilidad\nselected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n\nprint(f"   ‚Ä¢ Total series disponibles: {len(unique_test_series)}")\nprint(f"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}")\n\n# Detectar n√∫mero de targets autom√°ticamente\nn_targets = len(y_test.columns)\nprint(f"   ‚Ä¢ Targets a graficar: {list(y_test.columns)}")\n\n# Crear figura con 3 filas x n_targets columnas (3 series, n targets)\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\n# Colores para modelo sin normalizar\ncolor_real = \'#2ecc71\'      # Verde\ncolor_pred = \'#e74c3c\'      # Rojo\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales\n        y_real = y_test.loc[series_indices, col]\n        # Valores predichos\n        y_pred = y_test_pred.loc[series_indices, col]\n        \n        # Calcular m√©tricas para esta serie y target\n        r2_series = r2_score(y_real, y_pred)\n        rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n        \n        # Graficar\n        ax.plot(time_series, y_real, label=\'Real\', \n                linewidth=2, alpha=0.8, color=color_real)\n        ax.plot(time_series, y_pred, label=\'Predicho (No Norm)\', \n                linestyle=\'--\', linewidth=2, alpha=0.8, color=color_pred)\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col} (Sin Normalizar)\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}\', \n                     fontsize=11, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=9)\n        ax.grid(True, alpha=0.3)\n\nplt.suptitle(\'XGBoost Sin Normalizar - Time Series Completas\', \n             fontsize=16, fontweight=\'bold\', y=1.0)\nplt.tight_layout()\ntimeseries_plot_path = training_folder_nonorm / \'timeseries_comparison_nonorm.png\'\nplt.savefig(timeseries_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 3) Gr√°fica de time series con ZOOM (50 segundos)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: GR√ÅFICAS DE TIME SERIES - ZOOM 50 SEGUNDOS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas de time series con zoom (50s)...")\n\n# Crear figura con 3 filas x n_targets columnas (3 series, n targets)\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Definir ventana de 50 segundos desde el inicio\n    time_min = time_series.min()\n    time_max_zoom = time_min + 50\n    \n    # Filtrar por ventana de tiempo\n    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n    zoom_indices = time_series[zoom_mask].index\n    time_zoom = time_series[zoom_mask]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales y predichos (con zoom)\n        y_real_zoom = y_test.loc[zoom_indices, col]\n        y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n        \n        # Calcular m√©tricas para esta ventana\n        r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n        rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n        \n        # Graficar\n        ax.plot(time_zoom, y_real_zoom, label=\'Real\', \n                linewidth=2.5, alpha=0.8, color=color_real, marker=\'o\', markersize=4)\n        ax.plot(time_zoom, y_pred_zoom, label=\'Predicho (No Norm)\', \n                linestyle=\'--\', linewidth=2.5, alpha=0.8, color=color_pred, marker=\'x\', markersize=5)\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col} (Sin Norm, Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}\', \n                     fontsize=11, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=9)\n        ax.grid(True, alpha=0.3)\n        \n        # A√±adir texto con informaci√≥n de puntos\n        n_points = len(zoom_indices)\n        ax.text(0.02, 0.02, f\'Puntos: {n_points}\', transform=ax.transAxes, fontsize=9,\n                verticalalignment=\'bottom\', bbox=dict(boxstyle=\'round\', facecolor=\'lightgreen\', alpha=0.5))\n\nplt.suptitle(\'XGBoost Sin Normalizar - Time Series Zoom 50s\', \n             fontsize=16, fontweight=\'bold\', y=1.0)\nplt.tight_layout()\ntimeseries_zoom_plot_path = training_folder_nonorm / \'timeseries_comparison_zoom50s_nonorm.png\'\nplt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 4) Gr√°fica comparativa (Real vs Predicho con m√©tricas detalladas)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: GR√ÅFICAS COMPARATIVAS CON M√âTRICAS DETALLADAS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas con estad√≠sticas por serie...")\n\n# Crear una tabla de m√©tricas por serie\nmetrics_by_series = []\n\nfor series_num in selected_series:\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    for col in y_test.columns:\n        y_real = y_test.loc[series_indices, col]\n        y_pred = y_test_pred.loc[series_indices, col]\n        \n        r2 = r2_score(y_real, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_real, y_pred))\n        mae = mean_absolute_error(y_real, y_pred)\n        \n        metrics_by_series.append({\n            \'Serie\': series_num,\n            \'Target\': col,\n            \'R2\': r2,\n            \'RMSE\': rmse,\n            \'MAE\': mae,\n            \'N_points\': len(y_real)\n        })\n\nmetrics_series_df = pd.DataFrame(metrics_by_series)\n\n# Guardar m√©tricas por serie\nmetrics_series_path = training_folder_nonorm / \'timeseries_metrics_nonorm.csv\'\nmetrics_series_df.to_csv(metrics_series_path, index=False)\n\nprint(f"   ‚úÖ M√©tricas por serie guardadas: {metrics_series_path.name}")\n\n# Mostrar tabla\nprint(f"\\n   üìä M√©tricas por Serie y Target:")\nprint(f"   {\'Serie\':<8} {\'Target\':<8} {\'R¬≤\':>10} {\'RMSE\':>10} {\'MAE\':>10} {\'N_points\':>10}")\nprint(f"   {\'-\'*8} {\'-\'*8} {\'-\'*10} {\'-\'*10} {\'-\'*10} {\'-\'*10}")\n\nfor _, row in metrics_series_df.iterrows():\n    print(f"   {row[\'Serie\']:<8} {row[\'Target\']:<8} {row[\'R2\']:>10.4f} "\n          f"{row[\'RMSE\']:>10.2f} {row[\'MAE\']:>10.2f} {row[\'N_points\']:>10.0f}")\n\n# Calcular promedios\navg_r2 = metrics_series_df[\'R2\'].mean()\navg_rmse = metrics_series_df[\'RMSE\'].mean()\navg_mae = metrics_series_df[\'MAE\'].mean()\n\nprint(f"   {\'-\'*8} {\'-\'*8} {\'-\'*10} {\'-\'*10} {\'-\'*10} {\'-\'*10}")\nprint(f"   {\'PROMEDIO\':<8} {\'\':<8} {avg_r2:>10.4f} {avg_rmse:>10.2f} {avg_mae:>10.2f}")\n\n# ------------------------------------------------------------------------\n# RESUMEN FINAL\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ GR√ÅFICAS DE TIME SERIES COMPLETADAS - XGBOOST SIN NORMALIZAR")\nprint("="*80)\n\nprint(f"\\nüìä GR√ÅFICAS GENERADAS:")\nprint(f"   ‚Ä¢ {timeseries_plot_path.name}")\nprint(f"   ‚Ä¢ {timeseries_zoom_plot_path.name}")\nprint(f"   ‚Ä¢ {metrics_series_path.name}")\n\nprint(f"\\nüìä M√âTRICAS PROMEDIO DE LAS SERIES GRAFICADAS:")\nprint(f"   ‚Ä¢ R¬≤ promedio: {avg_r2:.4f}")\nprint(f"   ‚Ä¢ RMSE promedio: {avg_rmse:.2f} kNm")\nprint(f"   ‚Ä¢ MAE promedio: {avg_mae:.2f} kNm")\n\nprint(f"\\nüí° Targets graficados: {list(y_test.columns)}")\nprint(f"üí° Series aleatorias: {list(selected_series)}")\n\nprint(f"\\nüí° Caracter√≠sticas del modelo SIN NORMALIZAR:")\nprint(f"   ‚Ä¢ Predicciones directas en kNm")\nprint(f"   ‚Ä¢ No requiere normalizaci√≥n/desnormalizaci√≥n")\nprint(f"   ‚Ä¢ M√°s r√°pido en producci√≥n")\nprint(f"   ‚Ä¢ Valores interpretables directamente")\n\nprint(f"\\nüí° Ahora tienes todas las gr√°ficas para XGBoost sin normalizar:")\nprint(f"   1. M√©tricas por target (RMSE y R¬≤)")\nprint(f"   2. Predicciones vs Real")\nprint(f"   3. An√°lisis de residuos")\nprint(f"   4. Time series completas (3 series aleatorias √ó {n_targets} targets)")\nprint(f"   5. Time series con zoom de 50s")\nprint(f"   6. M√©tricas detalladas por serie")\n\nprint("="*80)',
        ],
    },
    {
        "title": 'üéØ MODELO 3c: XGBOOST SIN NORMALIZAR - UN MODELO POR TARGET CON EARLY STOPPING',
        "cells": [
            '# =============================================================================\n# MODELO 3c: XGBOOST SIN NORMALIZAR - UN MODELO POR TARGET CON EARLY STOPPING\n# =============================================================================\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nprint("\\n" + "="*80)\nprint("üå≥ MODELO: XGBOOST SIN NORMALIZAR - UN MODELO POR TARGET")\nprint("="*80)\n\n# ------------------------------------------------------------------------\n# 1) Preparar carpeta y cargar datos\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: PREPARAR CARPETA Y CARGAR/DIVIDIR DATOS")\nprint("="*80)\n\nprint(f"\\n[1/4] Creando carpeta para modelos individuales...")\n\ntraining_folder_individual = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost_NoNorm_Individual\'\nos.makedirs(training_folder_individual, exist_ok=True)\n\nprint(f"   ‚úÖ Carpeta: {training_folder_individual}")\n\nprint(f"\\n[2/4] Cargando datos de entrenamiento SIN normalizar...")\n\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\n# Cargar datos originales (sin normalizar)\n# NOTA: X_train y y_train ya est√°n separados del test set\nX_train_full = joblib.load(data_ml_folder / \'X_train.pkl\')\ny_train_full = joblib.load(data_ml_folder / \'y_train.pkl\')\n\nprint(f"   ‚úÖ X_train (sin normalizar): {X_train_full.shape}")\nprint(f"   ‚úÖ y_train (sin normalizar): {y_train_full.shape}")\nprint(f"   üí° Estos datos ya est√°n separados del test set")\n\nprint(f"\\n[3/4] Dividiendo training en train/validation para early stopping...")\nprint(f"   üí° Necesitamos un conjunto de validaci√≥n para monitorizar early stopping")\n\n# Dividir datos de training en train (80%) y validation (20%)\nX_tr, X_va, y_tr, y_va = train_test_split(\n    X_train_full, y_train_full, \n    test_size=0.2, \n    random_state=42,\n    shuffle=True\n)\n\nprint(f"   ‚úÖ X_train_full ‚Üí X_tr: {X_tr.shape}, X_va: {X_va.shape}")\nprint(f"   ‚úÖ y_train_full ‚Üí y_tr: {y_tr.shape}, y_va: {y_va.shape}")\n\nprint(f"\\n[4/4] Verificando distribuci√≥n de datos...")\n\nprint(f"   ‚Ä¢ Train (para entrenar): {X_tr.shape[0]:,} muestras ({X_tr.shape[0]/X_train_full.shape[0]*100:.1f}%)")\nprint(f"   ‚Ä¢ Validation (para early stopping): {X_va.shape[0]:,} muestras ({X_va.shape[0]/X_train_full.shape[0]*100:.1f}%)")\nprint(f"   ‚Ä¢ Test (separado previamente): se evaluar√° despu√©s")\n\n# ------------------------------------------------------------------------\n# 2) Configurar par√°metros comunes y entrenar modelo por target\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: ENTRENAMIENTO DE MODELOS INDIVIDUALES")\nprint("="*80)\n\nprint(f"\\n[1/3] Configurando par√°metros comunes de XGBoost...")\n\n# Par√°metros comunes para todos los modelos\n# NOTA: early_stopping_rounds y eval_metric van en el constructor en versiones recientes\nxgb_common = {\n    \'n_estimators\': 2000,           # M√°ximo de √°rboles (se detendr√° antes con early stopping)\n    \'max_depth\': 6,                 # Profundidad m√°xima\n    \'learning_rate\': 0.05,          # Tasa de aprendizaje\n    \'subsample\': 0.8,               # Fracci√≥n de muestras para cada √°rbol\n    \'colsample_bytree\': 0.8,        # Fracci√≥n de features para cada √°rbol\n    \'gamma\': 0.5,                   # M√≠nima reducci√≥n de loss para split\n    \'min_child_weight\': 5,          # Peso m√≠nimo en nodo hijo\n    \'reg_alpha\': 1,                 # Regularizaci√≥n L1\n    \'reg_lambda\': 5,                # Regularizaci√≥n L2\n    \'random_state\': 42,\n    \'n_jobs\': -1,                   # Usar todos los cores\n    \'tree_method\': \'hist\',          # M√©todo de construcci√≥n de √°rboles (m√°s r√°pido)\n    \'enable_categorical\': False,    # No usar features categ√≥ricas\n    \'early_stopping_rounds\': 300,   # Detenerse si no mejora en 300 iteraciones\n    \'eval_metric\': \'rmse\'           # M√©trica de parada\n}\n\nprint(f"   ‚úÖ Par√°metros comunes configurados")\nprint(f"      ‚Ä¢ Max depth: {xgb_common[\'max_depth\']}")\nprint(f"      ‚Ä¢ Learning rate: {xgb_common[\'learning_rate\']}")\nprint(f"      ‚Ä¢ Early stopping rounds: {xgb_common[\'early_stopping_rounds\']}")\nprint(f"      ‚Ä¢ Eval metric: {xgb_common[\'eval_metric\']}")\n\nprint(f"\\n[2/3] Entrenando un modelo por cada target...")\n\ntargets = y_train_full.columns.tolist()  # [\'M_0\', \'M_1c\', \'M_1s\']\n\nmodels = {}\nbest_iters = {}\nmetrics_val = {}\ntraining_times = {}\n\nfor i, target in enumerate(targets, 1):\n    print(f"\\n   [{i}/{len(targets)}] Target: {target}")\n    print(f"      {\'‚îÄ\'*60}")\n    \n    start_time = time.time()\n    \n    # Crear modelo individual\n    model = xgb.XGBRegressor(**xgb_common)\n    \n    # Entrenar con early stopping\n    # eval_set se pasa en fit(), pero eval_metric y early_stopping_rounds est√°n en el constructor\n    model.fit(\n        X_tr, y_tr[target],\n        eval_set=[(X_va, y_va[target])],\n        verbose=False                   # Sin output durante training\n    )\n    \n    training_time = time.time() - start_time\n    \n    # Guardar modelo y metadata\n    models[target] = model\n    best_iters[target] = getattr(model, \'best_iteration\', None)\n    training_times[target] = training_time\n    \n    # Evaluar en validation\n    pred_va = model.predict(X_va)\n    \n    metrics_val[target] = {\n        \'R2\': r2_score(y_va[target], pred_va),\n        \'RMSE\': np.sqrt(mean_squared_error(y_va[target], pred_va)),\n        \'MAE\': mean_absolute_error(y_va[target], pred_va),\n        \'best_iter\': best_iters[target]\n    }\n    \n    print(f"      ‚Ä¢ Tiempo: {training_time:.2f}s")\n    print(f"      ‚Ä¢ Best iteration: {metrics_val[target][\'best_iter\']}")\n    print(f"      ‚Ä¢ R¬≤ (val): {metrics_val[target][\'R2\']:.6f}")\n    print(f"      ‚Ä¢ RMSE (val): {metrics_val[target][\'RMSE\']:.4f} kNm")\n    print(f"      ‚Ä¢ MAE (val): {metrics_val[target][\'MAE\']:.4f} kNm")\n\ntotal_training_time = sum(training_times.values())\nprint(f"\\n   ‚úÖ Todos los modelos entrenados en {total_training_time:.2f}s")\n\nprint(f"\\n[3/3] Guardando modelos individuales...")\n\n# Guardar cada modelo\nfor target in targets:\n    model_path = training_folder_individual / f\'xgboost_nonorm_{target}.pkl\'\n    joblib.dump(models[target], model_path)\n    print(f"   ‚úÖ {model_path.name}")\n\n# Guardar diccionario con todos los modelos\nmodels_dict_path = training_folder_individual / \'xgboost_nonorm_models_dict.pkl\'\njoblib.dump(models, models_dict_path)\nprint(f"   ‚úÖ {models_dict_path.name} (diccionario completo)")\n\n# Guardar metadata\nmetadata = {\n    \'targets\': targets,\n    \'best_iters\': best_iters,\n    \'metrics_val\': metrics_val,\n    \'training_times\': training_times,\n    \'xgb_params\': xgb_common\n}\nmetadata_path = training_folder_individual / \'training_metadata.pkl\'\njoblib.dump(metadata, metadata_path)\nprint(f"   ‚úÖ {metadata_path.name}")\n\n# ------------------------------------------------------------------------\n# 3) Evaluar en conjunto completo de training (X_train_full)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: EVALUAR MODELOS EN TRAINING COMPLETO")\nprint("="*80)\n\nprint(f"\\n[1/2] Generando predicciones en training completo...")\nprint(f"   üí° Evaluando en los {X_train_full.shape[0]:,} datos originales de training")\n\n# Predecir con cada modelo en todo el training set original\ny_train_pred_individual = pd.DataFrame(index=y_train_full.index, columns=targets)\n\nfor target in targets:\n    y_train_pred_individual[target] = models[target].predict(X_train_full)\n\nprint(f"   ‚úÖ Predicciones generadas: {y_train_pred_individual.shape}")\n\nprint(f"\\n[2/2] Calculando m√©tricas en training completo...")\n\n# Calcular m√©tricas para cada target\nmetrics_train = {}\nfor target in targets:\n    r2 = r2_score(y_train_full[target], y_train_pred_individual[target])\n    rmse = np.sqrt(mean_squared_error(y_train_full[target], y_train_pred_individual[target]))\n    mae = mean_absolute_error(y_train_full[target], y_train_pred_individual[target])\n    \n    metrics_train[target] = {\n        \'R2\': r2,\n        \'RMSE\': rmse,\n        \'MAE\': mae\n    }\n    \n    print(f"   ‚Ä¢ {target}:")\n    print(f"      - R¬≤ = {r2:.6f}")\n    print(f"      - RMSE = {rmse:.4f} kNm")\n    print(f"      - MAE = {mae:.4f} kNm")\n\n# Calcular m√©tricas promedio\navg_r2_train = np.mean([m[\'R2\'] for m in metrics_train.values()])\navg_rmse_train = np.mean([m[\'RMSE\'] for m in metrics_train.values()])\navg_mae_train = np.mean([m[\'MAE\'] for m in metrics_train.values()])\n\nprint(f"\\n   üìä M√âTRICAS PROMEDIO (Training completo):")\nprint(f"      - R¬≤ promedio: {avg_r2_train:.6f}")\nprint(f"      - RMSE promedio: {avg_rmse_train:.4f} kNm")\nprint(f"      - MAE promedio: {avg_mae_train:.4f} kNm")\n\n# ------------------------------------------------------------------------\n# 4) Comparaci√≥n: Validation vs Training\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: COMPARACI√ìN VALIDATION VS TRAINING COMPLETO")\nprint("="*80)\n\nprint(f"\\nüìä RESUMEN POR TARGET:")\nprint(f"   {\'Target\':<10} {\'Best Iter\':>12} {\'R¬≤ (Val)\':>12} {\'R¬≤ (Train)\':>12} {\'RMSE (Val)\':>13} {\'RMSE (Train)\':>13}")\nprint(f"   {\'-\'*10} {\'-\'*12} {\'-\'*12} {\'-\'*12} {\'-\'*13} {\'-\'*13}")\n\nfor target in targets:\n    print(f"   {target:<10} {best_iters[target]:>12} "\n          f"{metrics_val[target][\'R2\']:>12.6f} {metrics_train[target][\'R2\']:>12.6f} "\n          f"{metrics_val[target][\'RMSE\']:>13.4f} {metrics_train[target][\'RMSE\']:>13.4f}")\n\navg_r2_val = np.mean([metrics_val[t][\'R2\'] for t in targets])\navg_rmse_val = np.mean([metrics_val[t][\'RMSE\'] for t in targets])\n\nprint(f"   {\'-\'*10} {\'-\'*12} {\'-\'*12} {\'-\'*12} {\'-\'*13} {\'-\'*13}")\nprint(f"   {\'PROMEDIO\':<10} {\'-\':>12} "\n      f"{avg_r2_val:>12.6f} {avg_r2_train:>12.6f} "\n      f"{avg_rmse_val:>13.4f} {avg_rmse_train:>13.4f}")\n\n# ------------------------------------------------------------------------\n# RESUMEN\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ ENTRENAMIENTO COMPLETADO - MODELOS INDIVIDUALES CON EARLY STOPPING")\nprint("="*80)\n\nprint(f"\\nüìä INFORMACI√ìN DE LOS MODELOS:")\nprint(f"   ‚Ä¢ Algoritmo: XGBoost (un modelo por target)")\nprint(f"   ‚Ä¢ Targets: {len(targets)} ({\', \'.join(targets)})")\nprint(f"   ‚Ä¢ Datos: SIN NORMALIZAR")\nprint(f"   ‚Ä¢ Early stopping: 300 rounds (RMSE en validation)")\nprint(f"   ‚Ä¢ Tiempo total: {total_training_time:.2f}s")\nprint(f"   ‚Ä¢ Muestras train: {X_tr.shape[0]:,} (80% de training)")\nprint(f"   ‚Ä¢ Muestras validation: {X_va.shape[0]:,} (20% de training)")\nprint(f"   ‚Ä¢ Features: {X_train_full.shape[1]}")\n\nprint(f"\\nüìÅ ARCHIVOS GENERADOS:")\nfor target in targets:\n    print(f"   ‚Ä¢ xgboost_nonorm_{target}.pkl")\nprint(f"   ‚Ä¢ xgboost_nonorm_models_dict.pkl")\nprint(f"   ‚Ä¢ training_metadata.pkl")\n\nprint(f"\\nüí° VENTAJAS DE MODELOS INDIVIDUALES CON EARLY STOPPING:")\nprint(f"   ‚Ä¢ Cada target tiene su n√∫mero √≥ptimo de iteraciones")\nprint(f"   ‚Ä¢ Early stopping previene overfitting autom√°ticamente")\nprint(f"   ‚Ä¢ Mayor flexibilidad para ajustar hiperpar√°metros por target")\nprint(f"   ‚Ä¢ Mejor control sobre la convergencia de cada modelo")\n\nprint(f"\\n‚ö†Ô∏è  NOTA:")\nprint(f"   ‚Ä¢ Split interno train/val solo para early stopping")\nprint(f"   ‚Ä¢ M√©tricas en training completo eval√∫an sobre todos los datos de training")\nprint(f"   ‚Ä¢ Test set (separado previamente) se evaluar√° en siguiente paso")\n\nprint("="*80)',
            '"""\nScript para validar modelos XGBoost SIN NORMALIZAR - UN MODELO POR TARGET.\n\nEste script:\n1. Carga modelos individuales entrenados con early stopping\n2. Realiza predicciones en test set\n3. Calcula m√©tricas por target\n4. Genera visualizaciones\n5. Compara con modelo MultiOutput\n\nAdaptado para trabajar con:\n- Modelos individuales por target (M_0, M_1c, M_1s)\n- Early stopping activado\n- Datos sin normalizaci√≥n\n\nAutor: Adaptado de validaci√≥n XGBoost\nFecha: Febrero 2026\n"""\n\n# =============================================================================\n# MODELO: XGBOOST SIN NORMALIZAR (INDIVIDUAL) - VALIDACI√ìN Y VISUALIZACIONES\n# =============================================================================\n\nprint("\\n" + "="*80)\nprint("üéØ MODELO: XGBOOST INDIVIDUAL SIN NORMALIZAR - VALIDACI√ìN")\nprint("="*80)\n\n# ------------------------------------------------------------------------\n# 1) Cargar modelos entrenados y datos de test\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: CARGAR MODELOS Y DATOS DE TEST")\nprint("="*80)\n\ntraining_folder_individual = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost_NoNorm_Individual\'\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\nprint(f"\\n[1/5] Cargando modelos individuales...")\n\n# Cargar diccionario con todos los modelos\nmodels_dict_path = training_folder_individual / \'xgboost_nonorm_models_dict.pkl\'\n\nif not models_dict_path.exists():\n    print(f"   ‚ùå ERROR: Modelos no encontrados en {models_dict_path}")\n    print(f"   üí° Primero debes entrenar los modelos individuales")\n    raise FileNotFoundError(f"No se encontraron los modelos en {models_dict_path}")\n\nmodels_individual = joblib.load(models_dict_path)\nprint(f"   ‚úÖ Modelos cargados: {list(models_individual.keys())}")\n\n# Cargar metadata\nmetadata_path = training_folder_individual / \'training_metadata.pkl\'\nif metadata_path.exists():\n    metadata = joblib.load(metadata_path)\n    print(f"   ‚úÖ Metadata cargada")\n    print(f"      ‚Ä¢ Best iterations: {metadata[\'best_iters\']}")\nelse:\n    print(f"   ‚ö†Ô∏è  Metadata no encontrada")\n    metadata = None\n\nprint(f"\\n[2/5] Cargando datos de test SIN NORMALIZAR...")\nX_test = joblib.load(data_ml_folder / \'X_test.pkl\')\ny_test = joblib.load(data_ml_folder / \'y_test.pkl\')\nprint(f"   ‚úÖ X_test: {X_test.shape}")\nprint(f"   ‚úÖ y_test: {y_test.shape}")\nprint(f"   üí° Targets: {list(y_test.columns)}")\n\nprint(f"\\n[3/5] Cargando datos de train SIN NORMALIZAR (para m√©tricas)...")\nX_train_full = joblib.load(data_ml_folder / \'X_train.pkl\')\ny_train_full = joblib.load(data_ml_folder / \'y_train.pkl\')\nprint(f"   ‚úÖ X_train: {X_train_full.shape}")\nprint(f"   ‚úÖ y_train: {y_train_full.shape}")\n\nprint(f"\\n[4/5] Verificando escalas de los datos...")\nprint(f"   üìä Estad√≠sticas X_test:")\nprint(f"      ‚Ä¢ Min: {X_test.min().min():.4f}")\nprint(f"      ‚Ä¢ Max: {X_test.max().max():.4f}")\nprint(f"      ‚Ä¢ Mean: {X_test.mean().mean():.4f}")\nprint(f"   üìä Estad√≠sticas y_test:")\nprint(f"      ‚Ä¢ Min: {y_test.min().min():.2f} kNm")\nprint(f"      ‚Ä¢ Max: {y_test.max().max():.2f} kNm")\nprint(f"      ‚Ä¢ Mean: {y_test.mean().mean():.2f} kNm")\n\nprint(f"\\n[5/5] Informaci√≥n de los modelos individuales...")\ntargets = list(models_individual.keys())\nprint(f"   ‚Ä¢ N√∫mero de modelos: {len(models_individual)}")\nprint(f"   ‚Ä¢ Targets: {targets}")\nif metadata:\n    for target in targets:\n        print(f"      - {target}: best_iter={metadata[\'best_iters\'][target]}")\n\nprint(f"\\n   üí° Datos en escala original - NO NORMALIZADOS")\n\n# ------------------------------------------------------------------------\n# 2) Realizar predicciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: PREDICCIONES CON MODELOS INDIVIDUALES")\nprint("="*80)\n\nprint(f"\\n[1/2] Prediciendo sobre conjunto de entrenamiento...")\n\n# Predecir con cada modelo individual\ny_train_pred_individual = pd.DataFrame(index=y_train_full.index, columns=targets)\n\nfor target in targets:\n    y_train_pred_individual[target] = models_individual[target].predict(X_train_full)\n\nprint(f"   ‚úÖ Predicciones train: {y_train_pred_individual.shape}")\nprint(f"   üí° Cada target predicho por su modelo individual")\n\nprint(f"\\n[2/2] Prediciendo sobre conjunto de test...")\n\n# Predecir con cada modelo individual\ny_test_pred_individual = pd.DataFrame(index=y_test.index, columns=targets)\n\nfor target in targets:\n    y_test_pred_individual[target] = models_individual[target].predict(X_test)\n\nprint(f"   ‚úÖ Predicciones test: {y_test_pred_individual.shape}")\nprint(f"   üí° No requiere desnormalizaci√≥n - ya en kNm")\n\n# Verificar rango de predicciones\nprint(f"\\n   üìä Rango de predicciones:")\nfor col in y_test_pred_individual.columns:\n    print(f"      ‚Ä¢ {col}: [{y_test_pred_individual[col].min():.2f}, {y_test_pred_individual[col].max():.2f}] kNm")\n\n# ------------------------------------------------------------------------\n# 3) Calcular m√©tricas\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: C√ÅLCULO DE M√âTRICAS")\nprint("="*80)\n\n# Diccionarios para almacenar m√©tricas\nmetrics_train_individual = {}\nmetrics_test_individual = {}\n\nprint(f"\\n{\'M√âTRICA\':20} {\'CONJUNTO\':10} {\'M_0\':>15} {\'M_1c\':>15} {\'M_1s\':>15}")\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\n\n# Calcular m√©tricas para cada target\nfor i, col in enumerate(targets):\n    # TRAIN\n    rmse_train = np.sqrt(mean_squared_error(y_train_full[col], y_train_pred_individual[col]))\n    r2_train = r2_score(y_train_full[col], y_train_pred_individual[col])\n    mae_train = mean_absolute_error(y_train_full[col], y_train_pred_individual[col])\n    \n    # TEST\n    rmse_test = np.sqrt(mean_squared_error(y_test[col], y_test_pred_individual[col]))\n    r2_test = r2_score(y_test[col], y_test_pred_individual[col])\n    mae_test = mean_absolute_error(y_test[col], y_test_pred_individual[col])\n    \n    # Guardar m√©tricas\n    metrics_train_individual[col] = {\'RMSE\': rmse_train, \'R2\': r2_train, \'MAE\': mae_train}\n    metrics_test_individual[col] = {\'RMSE\': rmse_test, \'R2\': r2_test, \'MAE\': mae_test}\n    \n    # Mostrar\n    if i == 0:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {rmse_train:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {rmse_test:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'MAE\':20} {\'TRAIN\':10} {mae_train:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {mae_test:>15.2f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {r2_train:>15.4f} {\'-\':>15} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {r2_test:>15.4f} {\'-\':>15} {\'-\':>15}")\n    elif i == 1:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {rmse_train:>15.2f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {rmse_test:>15.2f} {\'-\':>15}")\n        print(f"{\'MAE\':20} {\'TRAIN\':10} {\'-\':>15} {mae_train:>15.2f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {mae_test:>15.2f} {\'-\':>15}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {r2_train:>15.4f} {\'-\':>15}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {r2_test:>15.4f} {\'-\':>15}")\n    else:\n        print(f"{\'RMSE\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {rmse_train:>15.2f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {rmse_test:>15.2f}")\n        print(f"{\'MAE\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {mae_train:>15.2f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {mae_test:>15.2f}")\n        print(f"{\'R¬≤\':20} {\'TRAIN\':10} {\'-\':>15} {\'-\':>15} {r2_train:>15.4f}")\n        print(f"{\'\':20} {\'TEST\':10} {\'-\':>15} {\'-\':>15} {r2_test:>15.4f}")\n\n# Calcular promedios\navg_rmse_train = np.mean([m[\'RMSE\'] for m in metrics_train_individual.values()])\navg_rmse_test = np.mean([m[\'RMSE\'] for m in metrics_test_individual.values()])\navg_mae_train = np.mean([m[\'MAE\'] for m in metrics_train_individual.values()])\navg_mae_test = np.mean([m[\'MAE\'] for m in metrics_test_individual.values()])\navg_r2_train = np.mean([m[\'R2\'] for m in metrics_train_individual.values()])\navg_r2_test = np.mean([m[\'R2\'] for m in metrics_test_individual.values()])\n\nprint(f"{\'‚îÄ\'*20} {\'‚îÄ\'*10} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*15}")\nprint(f"{\'PROMEDIO RMSE\':20} {\'TRAIN\':10} {avg_rmse_train:>15.2f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_rmse_test:>15.2f}")\nprint(f"{\'PROMEDIO MAE\':20} {\'TRAIN\':10} {avg_mae_train:>15.2f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_mae_test:>15.2f}")\nprint(f"{\'PROMEDIO R¬≤\':20} {\'TRAIN\':10} {avg_r2_train:>15.4f}")\nprint(f"{\'\':20} {\'TEST\':10} {avg_r2_test:>15.4f}")\n\n# ------------------------------------------------------------------------\n# 4) Comparaci√≥n con modelo MultiOutput (si existe)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: COMPARACI√ìN CON MODELO MULTIOUTPUT")\nprint("="*80)\n\ntraining_folder_multioutput = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost_NoNorm\'\nmetrics_multioutput_path = training_folder_multioutput / \'metrics_results_nonorm.csv\'\n\nif metrics_multioutput_path.exists():\n    print(f"\\n[1/1] Comparando con modelo MultiOutput...")\n    \n    metrics_multi_df = pd.read_csv(metrics_multioutput_path)\n    \n    # Extraer m√©tricas del modelo MultiOutput\n    multi_rmse_test = metrics_multi_df[metrics_multi_df[\'Target\'] == \'PROMEDIO\'][\'RMSE_Test\'].values[0] if \'PROMEDIO\' in metrics_multi_df[\'Target\'].values else metrics_multi_df[\'RMSE_Test\'].mean()\n    multi_r2_test = metrics_multi_df[metrics_multi_df[\'Target\'] == \'PROMEDIO\'][\'R2_Test\'].values[0] if \'PROMEDIO\' in metrics_multi_df[\'Target\'].values else metrics_multi_df[\'R2_Test\'].mean()\n    \n    print(f"\\n   üìä COMPARACI√ìN EN TEST SET:")\n    print(f"   {\'MODELO\':30} {\'RMSE [kNm]\':>15} {\'R¬≤\':>15} {\'DIFERENCIA RMSE\':>20}")\n    print(f"   {\'‚îÄ\'*30} {\'‚îÄ\'*15} {\'‚îÄ\'*15} {\'‚îÄ\'*20}")\n    print(f"   {\'XGBoost MultiOutput\':30} {multi_rmse_test:>15.2f} {multi_r2_test:>15.4f} {\'‚îÄ\':>20}")\n    print(f"   {\'XGBoost Individual + Early\':30} {avg_rmse_test:>15.2f} {avg_r2_test:>15.4f} {avg_rmse_test - multi_rmse_test:>+20.2f}")\n    \n    # Determinar cu√°l es mejor\n    if avg_r2_test > multi_r2_test:\n        print(f"\\n   ‚úÖ Modelos INDIVIDUALES son MEJORES (R¬≤ m√°s alto)")\n        improvement = ((avg_r2_test - multi_r2_test) / multi_r2_test) * 100\n        print(f"   üí° Mejora en R¬≤: {improvement:+.2f}%")\n    elif avg_r2_test < multi_r2_test:\n        print(f"\\n   ‚ö†Ô∏è  Modelo MULTIOUTPUT es MEJOR (R¬≤ m√°s alto)")\n        difference = ((multi_r2_test - avg_r2_test) / multi_r2_test) * 100\n        print(f"   üí° Diferencia en R¬≤: {difference:.2f}%")\n    else:\n        print(f"\\n   ‚öñÔ∏è  Ambos enfoques tienen rendimiento similar")\n    \n    # Comparaci√≥n detallada por target\n    print(f"\\n   üìä COMPARACI√ìN DETALLADA POR TARGET:")\n    print(f"   {\'Target\':10} {\'MultiOut R¬≤\':>15} {\'Individual R¬≤\':>15} {\'Diferencia\':>15}")\n    print(f"   {\'-\'*10} {\'-\'*15} {\'-\'*15} {\'-\'*15}")\n    for target in targets:\n        if target in metrics_multi_df[\'Target\'].values:\n            multi_r2 = metrics_multi_df[metrics_multi_df[\'Target\'] == target][\'R2_Test\'].values[0]\n            indiv_r2 = metrics_test_individual[target][\'R2\']\n            diff = indiv_r2 - multi_r2\n            print(f"   {target:10} {multi_r2:>15.4f} {indiv_r2:>15.4f} {diff:>+15.4f}")\nelse:\n    print(f"\\n   ‚ö†Ô∏è  No se encontraron m√©tricas del modelo MultiOutput")\n    print(f"   üí° Entrena y valida el modelo MultiOutput para comparar")\n\n# ------------------------------------------------------------------------\n# 5) Visualizaciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 5: GENERACI√ìN DE GR√ÅFICAS")\nprint("="*80)\n\n# 5.1) Gr√°fica de m√©tricas por target\nprint(f"\\n[1/3] Creando gr√°fica de m√©tricas...")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# RMSE\nrmse_train_vals = [metrics_train_individual[t][\'RMSE\'] for t in targets]\nrmse_test_vals = [metrics_test_individual[t][\'RMSE\'] for t in targets]\n\nx = np.arange(len(targets))\nwidth = 0.35\n\naxes[0].bar(x - width/2, rmse_train_vals, width, label=\'Train\', alpha=0.8, color=\'#2ecc71\')\naxes[0].bar(x + width/2, rmse_test_vals, width, label=\'Test\', alpha=0.8, color=\'#e74c3c\')\naxes[0].set_xlabel(\'Target\', fontsize=12)\naxes[0].set_ylabel(\'RMSE [kNm]\', fontsize=12)\naxes[0].set_title(\'RMSE por Target - XGBoost Individual\', fontsize=14, fontweight=\'bold\')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(targets, fontsize=11)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\n# R¬≤\nr2_train_vals = [metrics_train_individual[t][\'R2\'] for t in targets]\nr2_test_vals = [metrics_test_individual[t][\'R2\'] for t in targets]\n\naxes[1].bar(x - width/2, r2_train_vals, width, label=\'Train\', alpha=0.8, color=\'#2ecc71\')\naxes[1].bar(x + width/2, r2_test_vals, width, label=\'Test\', alpha=0.8, color=\'#e74c3c\')\naxes[1].set_xlabel(\'Target\', fontsize=12)\naxes[1].set_ylabel(\'R¬≤\', fontsize=12)\naxes[1].set_title(\'R¬≤ por Target - XGBoost Individual\', fontsize=14, fontweight=\'bold\')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(targets, fontsize=11)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim([0, 1])\n\nplt.tight_layout()\nmetrics_plot_path = training_folder_individual / \'metrics_comparison_individual.png\'\nplt.savefig(metrics_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {metrics_plot_path.name}")\n\n# 5.2) Gr√°ficas de predicciones vs reales\nprint(f"\\n[2/3] Creando gr√°ficas de predicciones vs reales...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(targets):\n    row = idx\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train_full[col], y_train_pred_individual[col], alpha=0.3, s=1, color=\'#2ecc71\')\n    axes[row, 0].plot([y_train_full[col].min(), y_train_full[col].max()], \n                       [y_train_full[col].min(), y_train_full[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 0].set_xlabel(\'Real [kNm]\', fontsize=11)\n    axes[row, 0].set_ylabel(\'Predicho [kNm]\', fontsize=11)\n    title_train = f\'{col} - TRAIN (R¬≤={metrics_train_individual[col]["R2"]:.4f})\'\n    if metadata:\n        title_train += f\'\\nBest iter: {metadata["best_iters"][col]}\'\n    axes[row, 0].set_title(title_train, fontsize=12, fontweight=\'bold\')\n    axes[row, 0].legend(fontsize=10)\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test[col], y_test_pred_individual[col], alpha=0.3, s=1, color=\'#e74c3c\')\n    axes[row, 1].plot([y_test[col].min(), y_test[col].max()], \n                       [y_test[col].min(), y_test[col].max()], \n                       \'r--\', lw=2, label=\'Perfect prediction\')\n    axes[row, 1].set_xlabel(\'Real [kNm]\', fontsize=11)\n    axes[row, 1].set_ylabel(\'Predicho [kNm]\', fontsize=11)\n    title_test = f\'{col} - TEST (R¬≤={metrics_test_individual[col]["R2"]:.4f})\'\n    if metadata:\n        title_test += f\'\\nBest iter: {metadata["best_iters"][col]}\'\n    axes[row, 1].set_title(title_test, fontsize=12, fontweight=\'bold\')\n    axes[row, 1].legend(fontsize=10)\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.suptitle(\'Predicciones vs Real - XGBoost Individual + Early Stopping\', fontsize=16, fontweight=\'bold\', y=1.0)\nplt.tight_layout()\npredictions_plot_path = training_folder_individual / \'predictions_vs_real_individual.png\'\nplt.savefig(predictions_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {predictions_plot_path.name}")\n\n# 5.3) Gr√°fica de residuos\nprint(f"\\n[3/3] Creando gr√°fica de residuos...")\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 16))\n\nfor idx, col in enumerate(targets):\n    row = idx\n    \n    # Calcular residuos\n    residuals_train = y_train_full[col] - y_train_pred_individual[col]\n    residuals_test = y_test[col] - y_test_pred_individual[col]\n    \n    # TRAIN\n    axes[row, 0].scatter(y_train_pred_individual[col], residuals_train, alpha=0.3, s=1, color=\'#2ecc71\')\n    axes[row, 0].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 0].set_xlabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 0].set_ylabel(\'Residuo (Real - Predicho) [kNm]\', fontsize=11)\n    axes[row, 0].set_title(f\'{col} - Residuos TRAIN (œÉ={residuals_train.std():.2f})\', \n                          fontsize=12, fontweight=\'bold\')\n    axes[row, 0].grid(True, alpha=0.3)\n    \n    # TEST\n    axes[row, 1].scatter(y_test_pred_individual[col], residuals_test, alpha=0.3, s=1, color=\'#e74c3c\')\n    axes[row, 1].axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n    axes[row, 1].set_xlabel(\'Predicho [kNm]\', fontsize=11)\n    axes[row, 1].set_ylabel(\'Residuo (Real - Predicho) [kNm]\', fontsize=11)\n    axes[row, 1].set_title(f\'{col} - Residuos TEST (œÉ={residuals_test.std():.2f})\', \n                          fontsize=12, fontweight=\'bold\')\n    axes[row, 1].grid(True, alpha=0.3)\n\nplt.suptitle(\'An√°lisis de Residuos - XGBoost Individual + Early Stopping\', fontsize=16, fontweight=\'bold\', y=1.0)\nplt.tight_layout()\nresiduals_plot_path = training_folder_individual / \'residuals_analysis_individual.png\'\nplt.savefig(residuals_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {residuals_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 6) Guardar m√©tricas en archivo\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 6: GUARDAR RESULTADOS")\nprint("="*80)\n\n# Crear DataFrame con m√©tricas\nmetrics_df = pd.DataFrame({\n    \'Target\': list(metrics_train_individual.keys()) + [\'PROMEDIO\'],\n    \'RMSE_Train\': list([m[\'RMSE\'] for m in metrics_train_individual.values()]) + [avg_rmse_train],\n    \'RMSE_Test\': list([m[\'RMSE\'] for m in metrics_test_individual.values()]) + [avg_rmse_test],\n    \'MAE_Train\': list([m[\'MAE\'] for m in metrics_train_individual.values()]) + [avg_mae_train],\n    \'MAE_Test\': list([m[\'MAE\'] for m in metrics_test_individual.values()]) + [avg_mae_test],\n    \'R2_Train\': list([m[\'R2\'] for m in metrics_train_individual.values()]) + [avg_r2_train],\n    \'R2_Test\': list([m[\'R2\'] for m in metrics_test_individual.values()]) + [avg_r2_test]\n})\n\n# A√±adir best iterations si hay metadata\nif metadata:\n    metrics_df[\'Best_Iteration\'] = list(metadata[\'best_iters\'].values()) + [\'-\']\n\nmetrics_csv_path = training_folder_individual / \'metrics_results_individual.csv\'\nmetrics_df.to_csv(metrics_csv_path, index=False)\n\nprint(f"\\n   ‚úÖ M√©tricas guardadas: {metrics_csv_path.name}")\n\n# Crear resumen de validaci√≥n\nsummary = {\n    \'Model\': \'XGBoost_Individual\',\n    \'Normalized\': False,\n    \'Early_Stopping\': True,\n    \'Early_Stopping_Rounds\': 300,\n    \'Train_Samples\': len(X_train_full),\n    \'Test_Samples\': len(X_test),\n    \'Features\': X_train_full.shape[1],\n    \'Targets\': len(targets),\n    \'Avg_RMSE_Train\': avg_rmse_train,\n    \'Avg_RMSE_Test\': avg_rmse_test,\n    \'Avg_MAE_Train\': avg_mae_train,\n    \'Avg_MAE_Test\': avg_mae_test,\n    \'Avg_R2_Train\': avg_r2_train,\n    \'Avg_R2_Test\': avg_r2_test\n}\n\nif metadata:\n    summary.update({\n        \'Max_Depth\': metadata[\'xgb_params\'][\'max_depth\'],\n        \'Learning_Rate\': metadata[\'xgb_params\'][\'learning_rate\'],\n        \'Subsample\': metadata[\'xgb_params\'][\'subsample\'],\n        \'Colsample_Bytree\': metadata[\'xgb_params\'][\'colsample_bytree\'],\n        \'Reg_Alpha\': metadata[\'xgb_params\'][\'reg_alpha\'],\n        \'Reg_Lambda\': metadata[\'xgb_params\'][\'reg_lambda\']\n    })\n\nsummary_df = pd.DataFrame([summary])\nsummary_path = training_folder_individual / \'validation_summary_individual.csv\'\nsummary_df.to_csv(summary_path, index=False)\n\nprint(f"   ‚úÖ Resumen guardado: {summary_path.name}")\n\n# ------------------------------------------------------------------------\n# RESUMEN FINAL\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ VALIDACI√ìN COMPLETADA - XGBOOST INDIVIDUAL CON EARLY STOPPING")\nprint("="*80)\n\nprint(f"\\nüìä RESULTADOS FINALES:")\nprint(f"   ‚Ä¢ RMSE promedio (Train): {avg_rmse_train:.2f} kNm")\nprint(f"   ‚Ä¢ RMSE promedio (Test):  {avg_rmse_test:.2f} kNm")\nprint(f"   ‚Ä¢ MAE promedio (Train):  {avg_mae_train:.2f} kNm")\nprint(f"   ‚Ä¢ MAE promedio (Test):   {avg_mae_test:.2f} kNm")\nprint(f"   ‚Ä¢ R¬≤ promedio (Train):   {avg_r2_train:.4f}")\nprint(f"   ‚Ä¢ R¬≤ promedio (Test):    {avg_r2_test:.4f}")\n\nif metadata:\n    print(f"\\nüìä BEST ITERATIONS POR TARGET:")\n    for target, best_iter in metadata[\'best_iters\'].items():\n        print(f"   ‚Ä¢ {target}: {best_iter} iteraciones")\n\nprint(f"\\nüìÅ ARCHIVOS GENERADOS:")\nprint(f"   ‚Ä¢ {metrics_csv_path.name}")\nprint(f"   ‚Ä¢ {summary_path.name}")\nprint(f"   ‚Ä¢ {metrics_plot_path.name}")\nprint(f"   ‚Ä¢ {predictions_plot_path.name}")\nprint(f"   ‚Ä¢ {residuals_plot_path.name}")\n\nprint(f"\\nüí° VENTAJAS DE MODELOS INDIVIDUALES + EARLY STOPPING:")\nprint(f"   ‚Ä¢ Cada target optimizado independientemente")\nprint(f"   ‚Ä¢ Early stopping previene overfitting autom√°ticamente")\nprint(f"   ‚Ä¢ N√∫mero √≥ptimo de iteraciones por target")\nprint(f"   ‚Ä¢ Predicciones directamente en kNm (sin desnormalizaci√≥n)")\n\nprint(f"\\nüí° Targets: {targets}")\nprint(f"üí° Datos SIN NORMALIZAR - Escala original")\nprint("="*80)',
            '# =============================================================================\n# XGBOOST INDIVIDUAL - GR√ÅFICAS ADICIONALES DE TIME SERIES\n# =============================================================================\n\nprint("\\n" + "="*80)\nprint("üìä XGBOOST INDIVIDUAL - GR√ÅFICAS DE TIME SERIES")\nprint("="*80)\n\n# NOTA: Este script requiere que y_test y y_test_pred_individual est√©n en memoria\n# Ejecutar primero la validaci√≥n de XGBoost Individual\n\n# Verificar que las variables existan\ntry:\n    y_test\n    y_test_pred_individual\n    print(f"\\n‚úÖ Variables encontradas en memoria:")\n    print(f"   ‚Ä¢ y_test: {y_test.shape}")\n    print(f"   ‚Ä¢ y_test_pred_individual: {y_test_pred_individual.shape}")\n    print(f"   üí° Datos en escala original (sin normalizar)")\n    print(f"   üí° Predicciones con modelos individuales + early stopping")\nexcept NameError:\n    print(f"\\n‚ùå ERROR: y_test y/o y_test_pred_individual no est√°n en memoria")\n    print(f"   Por favor, ejecuta primero la validaci√≥n de XGBoost Individual")\n    raise\n\n# ------------------------------------------------------------------------\n# 1) Cargar Time_test y generar series_id_test\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: PREPARAR DATOS PARA TIME SERIES")\nprint("="*80)\n\ntraining_folder_individual = root_dir / \'notebook\' / \'03_ML_traditional_models\' / \'XGBoost_NoNorm_Individual\'\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\n\nprint(f"\\n[1/3] Cargando Time_test desde archivo pkl...")\n\nTime_test_path = data_ml_folder / \'Time_test.pkl\'\n\nif Time_test_path.exists():\n    Time_test = joblib.load(Time_test_path)\n    print(f"   ‚úÖ Time_test cargado: {len(Time_test):,} valores")\n    print(f"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s")\n    print(f"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s")\nelse:\n    print(f"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}")\n    raise FileNotFoundError(f"Archivo requerido no encontrado: {Time_test_path}")\n\nprint(f"\\n[2/3] Generando series_id_test a partir de Time_test...")\n\n# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\nseries_id_test_values = np.zeros(len(Time_test), dtype=int)\ncurrent_series = 0\n\nTime_test_array = Time_test.values\nfor i in range(1, len(Time_test_array)):\n    if Time_test_array[i] < Time_test_array[i-1]:\n        current_series += 1\n    series_id_test_values[i] = current_series\n\n# Convertir a pandas Series con el mismo index que Time_test\nseries_id_test = pd.Series(series_id_test_values, index=Time_test.index, name=\'series_id\')\n\nn_test_series = series_id_test.max() + 1\n\nprint(f"   ‚úÖ Series temporales identificadas en test: {n_test_series}")\n\n# Analizar cada serie\nprint(f"\\n   üìä Resumen de series en TEST:")\nfor sid in range(min(5, n_test_series)):\n    mask = series_id_test == sid\n    n_rows = mask.sum()\n    time_min = Time_test.loc[mask].min()\n    time_max = Time_test.loc[mask].max()\n    print(f"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s")\n\nif n_test_series > 5:\n    print(f"      ... y {n_test_series - 5} series m√°s")\n\nprint(f"\\n[3/3] Verificando que los datos coinciden con predicciones...")\n\n# Verificar que los √≠ndices coinciden\nif not all(series_id_test.index == y_test.index):\n    print(f"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...")\n    series_id_test = series_id_test.reindex(y_test.index)\n    Time_test = Time_test.reindex(y_test.index)\n\nprint(f"   ‚úÖ √çndices verificados:")\nprint(f"      ‚Ä¢ y_test: {y_test.shape[0]:,} filas")\nprint(f"      ‚Ä¢ y_test_pred_individual: {y_test_pred_individual.shape[0]:,} filas")\nprint(f"      ‚Ä¢ series_id_test: {len(series_id_test):,} valores")\nprint(f"      ‚Ä¢ Time_test: {len(Time_test):,} valores")\n\n# ------------------------------------------------------------------------\n# 2) Gr√°fica de time series: Real vs Predicho (3 series aleatorias)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas de time series (Real vs Predicho)...")\n\n# Obtener series √∫nicas del conjunto de test\nunique_test_series = series_id_test.unique()\n\n# Seleccionar 3 series aleatorias\nnp.random.seed(42)  # Para reproducibilidad\nselected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n\nprint(f"   ‚Ä¢ Total series disponibles: {len(unique_test_series)}")\nprint(f"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}")\n\n# Detectar n√∫mero de targets autom√°ticamente\nn_targets = len(y_test.columns)\nprint(f"   ‚Ä¢ Targets a graficar: {list(y_test.columns)}")\n\n# Cargar metadata para mostrar best iterations\nmetadata_path = training_folder_individual / \'training_metadata.pkl\'\nif metadata_path.exists():\n    metadata = joblib.load(metadata_path)\n    print(f"   ‚Ä¢ Best iterations: {metadata[\'best_iters\']}")\nelse:\n    metadata = None\n    print(f"   ‚ö†Ô∏è  Metadata no encontrada")\n\n# Crear figura con 3 filas x n_targets columnas (3 series, n targets)\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\n# Colores para modelo individual\ncolor_real = \'#2ecc71\'      # Verde\ncolor_pred = \'#e74c3c\'      # Rojo\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales\n        y_real = y_test.loc[series_indices, col]\n        # Valores predichos\n        y_pred = y_test_pred_individual.loc[series_indices, col]\n        \n        # Calcular m√©tricas para esta serie y target\n        r2_series = r2_score(y_real, y_pred)\n        rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n        \n        # Graficar\n        ax.plot(time_series, y_real, label=\'Real\', \n                linewidth=2, alpha=0.8, color=color_real)\n        ax.plot(time_series, y_pred, label=\'Predicho (Individual)\', \n                linestyle=\'--\', linewidth=2, alpha=0.8, color=color_pred)\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        \n        # T√≠tulo con best iteration si est√° disponible\n        title = f\'Serie {series_num} - {col} (Individual)\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}\'\n        if metadata:\n            title += f\'\\nBest iter: {metadata["best_iters"][col]}\'\n        ax.set_title(title, fontsize=11, fontweight=\'bold\')\n        \n        ax.legend(loc=\'best\', fontsize=9)\n        ax.grid(True, alpha=0.3)\n\nplt.suptitle(\'XGBoost Individual + Early Stopping - Time Series Completas\', \n             fontsize=16, fontweight=\'bold\', y=1.0)\nplt.tight_layout()\ntimeseries_plot_path = training_folder_individual / \'timeseries_comparison_individual.png\'\nplt.savefig(timeseries_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 3) Gr√°fica de time series con ZOOM (50 segundos)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: GR√ÅFICAS DE TIME SERIES - ZOOM 50 SEGUNDOS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas de time series con zoom (50s)...")\n\n# Crear figura con 3 filas x n_targets columnas (3 series, n targets)\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Definir ventana de 50 segundos desde el inicio\n    time_min = time_series.min()\n    time_max_zoom = time_min + 50\n    \n    # Filtrar por ventana de tiempo\n    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n    zoom_indices = time_series[zoom_mask].index\n    time_zoom = time_series[zoom_mask]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales y predichos (con zoom)\n        y_real_zoom = y_test.loc[zoom_indices, col]\n        y_pred_zoom = y_test_pred_individual.loc[zoom_indices, col]\n        \n        # Calcular m√©tricas para esta ventana\n        r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n        rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n        \n        # Graficar\n        ax.plot(time_zoom, y_real_zoom, label=\'Real\', \n                linewidth=2.5, alpha=0.8, color=color_real, marker=\'o\', markersize=4)\n        ax.plot(time_zoom, y_pred_zoom, label=\'Predicho (Individual)\', \n                linestyle=\'--\', linewidth=2.5, alpha=0.8, color=color_pred, marker=\'x\', markersize=5)\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        \n        # T√≠tulo con best iteration si est√° disponible\n        title = f\'Serie {series_num} - {col} (Individual, Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}\'\n        if metadata:\n            title += f\'\\nBest iter: {metadata["best_iters"][col]}\'\n        ax.set_title(title, fontsize=11, fontweight=\'bold\')\n        \n        ax.legend(loc=\'best\', fontsize=9)\n        ax.grid(True, alpha=0.3)\n        \n        # A√±adir texto con informaci√≥n de puntos\n        n_points = len(zoom_indices)\n        ax.text(0.02, 0.02, f\'Puntos: {n_points}\', transform=ax.transAxes, fontsize=9,\n                verticalalignment=\'bottom\', bbox=dict(boxstyle=\'round\', facecolor=\'lightgreen\', alpha=0.5))\n\nplt.suptitle(\'XGBoost Individual + Early Stopping - Time Series Zoom 50s\', \n             fontsize=16, fontweight=\'bold\', y=1.0)\nplt.tight_layout()\ntimeseries_zoom_plot_path = training_folder_individual / \'timeseries_comparison_zoom50s_individual.png\'\nplt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 4) Gr√°fica comparativa (Real vs Predicho con m√©tricas detalladas)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: GR√ÅFICAS COMPARATIVAS CON M√âTRICAS DETALLADAS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas con estad√≠sticas por serie...")\n\n# Crear una tabla de m√©tricas por serie\nmetrics_by_series = []\n\nfor series_num in selected_series:\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    for col in y_test.columns:\n        y_real = y_test.loc[series_indices, col]\n        y_pred = y_test_pred_individual.loc[series_indices, col]\n        \n        r2 = r2_score(y_real, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_real, y_pred))\n        mae = mean_absolute_error(y_real, y_pred)\n        \n        metric_dict = {\n            \'Serie\': series_num,\n            \'Target\': col,\n            \'R2\': r2,\n            \'RMSE\': rmse,\n            \'MAE\': mae,\n            \'N_points\': len(y_real)\n        }\n        \n        # A√±adir best iteration si est√° disponible\n        if metadata:\n            metric_dict[\'Best_Iteration\'] = metadata[\'best_iters\'][col]\n        \n        metrics_by_series.append(metric_dict)\n\nmetrics_series_df = pd.DataFrame(metrics_by_series)\n\n# Guardar m√©tricas por serie\nmetrics_series_path = training_folder_individual / \'timeseries_metrics_individual.csv\'\nmetrics_series_df.to_csv(metrics_series_path, index=False)\n\nprint(f"   ‚úÖ M√©tricas por serie guardadas: {metrics_series_path.name}")\n\n# Mostrar tabla\nprint(f"\\n   üìä M√©tricas por Serie y Target:")\nif metadata:\n    print(f"   {\'Serie\':<8} {\'Target\':<8} {\'R¬≤\':>10} {\'RMSE\':>10} {\'MAE\':>10} {\'N_points\':>10} {\'Best_Iter\':>10}")\n    print(f"   {\'-\'*8} {\'-\'*8} {\'-\'*10} {\'-\'*10} {\'-\'*10} {\'-\'*10} {\'-\'*10}")\n    \n    for _, row in metrics_series_df.iterrows():\n        print(f"   {row[\'Serie\']:<8} {row[\'Target\']:<8} {row[\'R2\']:>10.4f} "\n              f"{row[\'RMSE\']:>10.2f} {row[\'MAE\']:>10.2f} {row[\'N_points\']:>10.0f} "\n              f"{row[\'Best_Iteration\']:>10.0f}")\nelse:\n    print(f"   {\'Serie\':<8} {\'Target\':<8} {\'R¬≤\':>10} {\'RMSE\':>10} {\'MAE\':>10} {\'N_points\':>10}")\n    print(f"   {\'-\'*8} {\'-\'*8} {\'-\'*10} {\'-\'*10} {\'-\'*10} {\'-\'*10}")\n    \n    for _, row in metrics_series_df.iterrows():\n        print(f"   {row[\'Serie\']:<8} {row[\'Target\']:<8} {row[\'R2\']:>10.4f} "\n              f"{row[\'RMSE\']:>10.2f} {row[\'MAE\']:>10.2f} {row[\'N_points\']:>10.0f}")\n\n# Calcular promedios\navg_r2 = metrics_series_df[\'R2\'].mean()\navg_rmse = metrics_series_df[\'RMSE\'].mean()\navg_mae = metrics_series_df[\'MAE\'].mean()\n\nif metadata:\n    print(f"   {\'-\'*8} {\'-\'*8} {\'-\'*10} {\'-\'*10} {\'-\'*10} {\'-\'*10} {\'-\'*10}")\n    print(f"   {\'PROMEDIO\':<8} {\'\':<8} {avg_r2:>10.4f} {avg_rmse:>10.2f} {avg_mae:>10.2f}")\nelse:\n    print(f"   {\'-\'*8} {\'-\'*8} {\'-\'*10} {\'-\'*10} {\'-\'*10} {\'-\'*10}")\n    print(f"   {\'PROMEDIO\':<8} {\'\':<8} {avg_r2:>10.4f} {avg_rmse:>10.2f} {avg_mae:>10.2f}")\n\n# ------------------------------------------------------------------------\n# RESUMEN FINAL\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ GR√ÅFICAS DE TIME SERIES COMPLETADAS - XGBOOST INDIVIDUAL")\nprint("="*80)\n\nprint(f"\\nüìä GR√ÅFICAS GENERADAS:")\nprint(f"   ‚Ä¢ {timeseries_plot_path.name}")\nprint(f"   ‚Ä¢ {timeseries_zoom_plot_path.name}")\nprint(f"   ‚Ä¢ {metrics_series_path.name}")\n\nprint(f"\\nüìä M√âTRICAS PROMEDIO DE LAS SERIES GRAFICADAS:")\nprint(f"   ‚Ä¢ R¬≤ promedio: {avg_r2:.4f}")\nprint(f"   ‚Ä¢ RMSE promedio: {avg_rmse:.2f} kNm")\nprint(f"   ‚Ä¢ MAE promedio: {avg_mae:.2f} kNm")\n\nif metadata:\n    print(f"\\nüìä BEST ITERATIONS POR TARGET:")\n    for target, best_iter in metadata[\'best_iters\'].items():\n        print(f"   ‚Ä¢ {target}: {best_iter} iteraciones")\n\nprint(f"\\nüí° Targets graficados: {list(y_test.columns)}")\nprint(f"üí° Series aleatorias: {list(selected_series)}")\n\nprint(f"\\nüí° Caracter√≠sticas de los MODELOS INDIVIDUALES:")\nprint(f"   ‚Ä¢ Un modelo XGBoost por cada target")\nprint(f"   ‚Ä¢ Early stopping activado (300 rounds)")\nprint(f"   ‚Ä¢ Cada target con su n√∫mero √≥ptimo de iteraciones")\nprint(f"   ‚Ä¢ Predicciones directas en kNm")\nprint(f"   ‚Ä¢ No requiere normalizaci√≥n/desnormalizaci√≥n")\n\nprint(f"\\nüí° Ahora tienes todas las gr√°ficas para XGBoost Individual:")\nprint(f"   1. M√©tricas por target (RMSE y R¬≤)")\nprint(f"   2. Predicciones vs Real")\nprint(f"   3. An√°lisis de residuos")\nprint(f"   4. Time series completas (3 series aleatorias √ó {n_targets} targets)")\nprint(f"   5. Time series con zoom de 50s")\nprint(f"   6. M√©tricas detalladas por serie (con best iterations)")\n\nprint("="*80)',
        ],
    },
    {
        "title": 'üß† MULTI-LAYER PERCEPTRON (MLP)',
        "cells": [
            '"""\n============================================================================\nPASO 1: ENTRENAMIENTO DE MODELO MLP (MULTI-LAYER PERCEPTRON)\n============================================================================\n\nEste script entrena un modelo de Red Neuronal MLP para predecir cargas en \npalas de aerogenerador.\n\nAutor: [Tu nombre]\nFecha: 2026-01-08\n============================================================================\n"""\n\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport time\nfrom datetime import datetime\n\n# Deep Learning\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# M√©tricas\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Configuraci√≥n de warnings\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\nprint("="*80)\nprint("ü§ñ PASO 1: ENTRENAMIENTO DE MODELO MLP")\nprint("="*80)\nprint(f"üìÖ Fecha: {datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')}")\nprint("="*80)\n\n# ============================================================================\n# 1. CONFIGURACI√ìN DE RUTAS\n# ============================================================================\nprint("\\n[1/7] Configurando rutas...")\n\n# Directorio ra√≠z del proyecto\ndata_ml_folder = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\nmodels_folder = root_dir / \'notebook\' / \'03_ML_traditional_models\'\n\n# Crear carpeta de modelos si no existe\nmodels_folder.mkdir(parents=True, exist_ok=True)\n\nprint(f"   ‚úÖ Carpeta de datos: {data_ml_folder}")\nprint(f"   ‚úÖ Carpeta de modelos: {models_folder}")\n\n# Verificar que existen los datos\nif not data_ml_folder.exists():\n    raise FileNotFoundError(f"No se encuentra la carpeta: {data_ml_folder}")\n\n# ============================================================================\n# 2. CARGAR DATOS\n# ============================================================================\nprint("\\n[2/7] Cargando datos de entrenamiento...")\n\ntry:\n    # Cargar datos normalizados (MLP funciona mejor con datos normalizados)\n    X_train_norm = joblib.load(data_ml_folder / \'X_train_norm.pkl\')\n    y_train_norm = joblib.load(data_ml_folder / \'y_train_norm.pkl\')\n    \n    print(f"   ‚úÖ X_train_norm cargado: {X_train_norm.shape}")\n    print(f"   ‚úÖ y_train_norm cargado: {y_train_norm.shape}")\n    \n    # Info de los datos\n    n_samples, n_features = X_train_norm.shape\n    n_outputs = y_train_norm.shape[1]\n    \n    print(f"\\n   üìä Informaci√≥n de los datos:")\n    print(f"      ‚Ä¢ Muestras de entrenamiento: {n_samples:,}")\n    print(f"      ‚Ä¢ Features: {n_features}")\n    print(f"      ‚Ä¢ Outputs: {n_outputs}")\n    print(f"      ‚Ä¢ Tama√±o en memoria: {X_train_norm.memory_usage(deep=True).sum() / (1024**2):.1f} MB")\n    \nexcept Exception as e:\n    print(f"   ‚ùå Error cargando datos: {e}")\n    sys.exit(1)\n\n# ============================================================================\n# 3. CONFIGURACI√ìN DEL MODELO MLP\n# ============================================================================\nprint("\\n[3/7] Configurando modelo MLP...")\n\n# Par√°metros del modelo\n# mlp_params = {\n#     # Arquitectura de la red\n#     \'hidden_layer_sizes\': (256, 128, 64),  # 3 capas ocultas con 256, 128 y 64 neuronas\n#     \n#     # Funci√≥n de activaci√≥n\n#     \'activation\': \'relu\',  # ReLU es est√°ndar para regresi√≥n\n#     \n#     # Solver (optimizador)\n#     \'solver\': \'adam\',  # Adam es eficiente y robusto\n#     \n#     # Par√°metros de regularizaci√≥n\n#     \'alpha\': 0.0001,  # L2 penalty (regularizaci√≥n)\n#     \n#     # Par√°metros de entrenamiento\n#     \'batch_size\': 256,  # Tama√±o de batch para mini-batch gradient descent\n#     \'learning_rate\': \'adaptive\',  # Ajusta learning rate autom√°ticamente\n#     \'learning_rate_init\': 0.001,  # Learning rate inicial\n#     \'max_iter\': 500,  # M√°ximo de √©pocas\n#     \n#     # Early stopping\n#     \'early_stopping\': True,  # Detener si no hay mejora\n#     \'validation_fraction\': 0.1,  # 10% para validaci√≥n interna\n#     \'n_iter_no_change\': 20,  # Parar despu√©s de 20 √©pocas sin mejora\n#     \'tol\': 1e-4,  # Tolerancia para la mejora\n#     \n#     # Otros\n#     \'random_state\': 42,\n#     \'verbose\': True,  # Mostrar progreso\n#     \'warm_start\': False,  # No usar warm start\n# }\n\nmlp_params = {\n    # Arquitectura de la red\n    \'hidden_layer_sizes\': (128, 64),  # 2 capas ocultas con 256, 128 y 64 neuronas\n    \n    # Funci√≥n de activaci√≥n\n    \'activation\': \'relu\',  # ReLU es est√°ndar para regresi√≥n\n    \n    # Solver (optimizador)\n    \'solver\': \'adam\',  # Adam es eficiente y robusto\n    \n    # Par√°metros de regularizaci√≥n\n    \'alpha\': 0.01,  # L2 penalty (regularizaci√≥n)\n    \n    # Par√°metros de entrenamiento\n    \'batch_size\': 512,  # Tama√±o de batch para mini-batch gradient descent\n    \'learning_rate\': \'constant\',  # Ajusta learning rate autom√°ticamente\n    \'learning_rate_init\': 5e-4,  # Learning rate inicial\n    \'max_iter\': 2000,  # M√°ximo de √©pocas\n    \n    # Early stopping\n    \'early_stopping\': True,  # Detener si no hay mejora\n    \'validation_fraction\': 0.1,  # 10% para validaci√≥n interna\n    \'n_iter_no_change\': 20,  # Parar despu√©s de 20 √©pocas sin mejora\n    \'tol\': 1e-2,  # Tolerancia para la mejora\n    \n    # Otros\n    \'random_state\': 42,\n    \'verbose\': True,  # Mostrar progreso\n    \'warm_start\': False,  # No usar warm start\n}\n\nprint(f"   üìã Configuraci√≥n del modelo:")\nprint(f"      ‚Ä¢ Arquitectura: {mlp_params[\'hidden_layer_sizes\']}")\nprint(f"      ‚Ä¢ Activaci√≥n: {mlp_params[\'activation\']}")\nprint(f"      ‚Ä¢ Solver: {mlp_params[\'solver\']}")\nprint(f"      ‚Ä¢ Learning rate: {mlp_params[\'learning_rate_init\']}")\nprint(f"      ‚Ä¢ Batch size: {mlp_params[\'batch_size\']}")\nprint(f"      ‚Ä¢ Max √©pocas: {mlp_params[\'max_iter\']}")\nprint(f"      ‚Ä¢ Early stopping: {mlp_params[\'early_stopping\']}")\nprint(f"      ‚Ä¢ Regularizaci√≥n (alpha): {mlp_params[\'alpha\']}")\n\n# Crear modelo\nmlp_model = MLPRegressor(**mlp_params)\n\nprint(f"\\n   ‚úÖ Modelo MLP creado")\nprint(f"      Total de par√°metros estimados: ~{sum(mlp_params[\'hidden_layer_sizes\']) * n_features + sum(mlp_params[\'hidden_layer_sizes\']) * n_outputs:,}")\n\n# ============================================================================\n# 4. ENTRENAR MODELO\n# ============================================================================\nprint("\\n[4/7] Entrenando modelo MLP...")\nprint("   ‚è≥ Esto puede tardar varios minutos...")\nprint("   " + "-"*76)\n\nstart_time = time.time()\n\ntry:\n    # Entrenar el modelo\n    mlp_model.fit(X_train_norm, y_train_norm)\n    \n    training_time = time.time() - start_time\n    \n    print("   " + "-"*76)\n    print(f"\\n   ‚úÖ Entrenamiento completado!")\n    print(f"      ‚Ä¢ Tiempo de entrenamiento: {training_time/60:.2f} minutos ({training_time:.1f} segundos)")\n    print(f"      ‚Ä¢ √âpocas ejecutadas: {mlp_model.n_iter_}")\n    print(f"      ‚Ä¢ Loss final: {mlp_model.loss_:.6f}")\n    \n    if mlp_model.n_iter_ < mlp_params[\'max_iter\']:\n        print(f"      ‚Ä¢ Early stopping activado ‚úì")\n    \nexcept Exception as e:\n    print(f"\\n   ‚ùå Error durante el entrenamiento: {e}")\n    sys.exit(1)\n\n# ============================================================================\n# 5. EVALUACI√ìN EN TRAIN\n# ============================================================================\nprint("\\n[5/7] Evaluando modelo en conjunto de entrenamiento...")\n\ntry:\n    # Predicciones en train\n    y_train_pred = mlp_model.predict(X_train_norm)\n    \n    # Calcular m√©tricas para cada output\n    target_names = y_train_norm.columns if isinstance(y_train_norm, pd.DataFrame) else [f\'Output_{i}\' for i in range(n_outputs)]\n    \n    print(f"\\n   üìä M√©tricas de entrenamiento:")\n    print(f"      {\'Target\':<30} {\'MAE\':>12} {\'RMSE\':>12} {\'R¬≤\':>10}")\n    print(f"      {\'-\'*30} {\'-\'*12} {\'-\'*12} {\'-\'*10}")\n    \n    metrics_train = {}\n    \n    for i, target_name in enumerate(target_names):\n        if isinstance(y_train_norm, pd.DataFrame):\n            y_true = y_train_norm.iloc[:, i]\n            y_pred = y_train_pred[:, i]\n        else:\n            y_true = y_train_norm[:, i]\n            y_pred = y_train_pred[:, i]\n        \n        mae = mean_absolute_error(y_true, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n        r2 = r2_score(y_true, y_pred)\n        \n        metrics_train[target_name] = {\'MAE\': mae, \'RMSE\': rmse, \'R2\': r2}\n        \n        print(f"      {target_name:<30} {mae:12.6f} {rmse:12.6f} {r2:10.6f}")\n    \n    # M√©trica global\n    mae_global = mean_absolute_error(y_train_norm, y_train_pred)\n    rmse_global = np.sqrt(mean_squared_error(y_train_norm, y_train_pred))\n    r2_global = r2_score(y_train_norm, y_train_pred)\n    \n    print(f"      {\'-\'*30} {\'-\'*12} {\'-\'*12} {\'-\'*10}")\n    print(f"      {\'PROMEDIO GLOBAL\':<30} {mae_global:12.6f} {rmse_global:12.6f} {r2_global:10.6f}")\n    \nexcept Exception as e:\n    print(f"   ‚ùå Error calculando m√©tricas: {e}")\n    metrics_train = {}\n\n# ============================================================================\n# 6. GUARDAR MODELO\n# ============================================================================\nprint("\\n[6/7] Guardando modelo entrenado...")\n\ntry:\n    # Nombre del archivo del modelo\n    model_filename = models_folder / \'mlp_model.pkl\'\n    \n    # Guardar modelo\n    joblib.dump(mlp_model, model_filename)\n    \n    # Tama√±o del archivo\n    model_size = model_filename.stat().st_size / (1024**2)\n    \n    print(f"   ‚úÖ Modelo guardado exitosamente")\n    print(f"      ‚Ä¢ Archivo: {model_filename}")\n    print(f"      ‚Ä¢ Tama√±o: {model_size:.2f} MB")\n    \nexcept Exception as e:\n    print(f"   ‚ùå Error guardando modelo: {e}")\n    sys.exit(1)\n\n# ============================================================================\n# 7. GUARDAR INFORMACI√ìN DEL ENTRENAMIENTO\n# ============================================================================\nprint("\\n[7/7] Guardando informaci√≥n del entrenamiento...")\n\ntry:\n    # Crear diccionario con informaci√≥n del entrenamiento\n    training_info = {\n        \'timestamp\': datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n        \'model_type\': \'MLPRegressor\',\n        \'sklearn_version\': __import__(\'sklearn\').__version__,\n        \n        # Datos\n        \'n_samples\': n_samples,\n        \'n_features\': n_features,\n        \'n_outputs\': n_outputs,\n        \n        # Par√°metros del modelo\n        \'model_params\': mlp_params,\n        \n        # Resultados del entrenamiento\n        \'training_time_seconds\': training_time,\n        \'n_iterations\': mlp_model.n_iter_,\n        \'final_loss\': mlp_model.loss_,\n        \n        # M√©tricas\n        \'train_metrics\': metrics_train,\n        \'train_mae_global\': mae_global,\n        \'train_rmse_global\': rmse_global,\n        \'train_r2_global\': r2_global,\n    }\n    \n    # Guardar informaci√≥n\n    info_filename = models_folder / \'mlp_training_info.pkl\'\n    joblib.dump(training_info, info_filename)\n    \n    print(f"   ‚úÖ Informaci√≥n guardada en: {info_filename}")\n    \n    # Tambi√©n guardar como JSON legible\n    import json\n    \n    # Convertir a formato serializable\n    training_info_json = {\n        \'timestamp\': training_info[\'timestamp\'],\n        \'model_type\': training_info[\'model_type\'],\n        \'sklearn_version\': training_info[\'sklearn_version\'],\n        \'n_samples\': int(training_info[\'n_samples\']),\n        \'n_features\': int(training_info[\'n_features\']),\n        \'n_outputs\': int(training_info[\'n_outputs\']),\n        \'training_time_minutes\': round(training_info[\'training_time_seconds\'] / 60, 2),\n        \'n_iterations\': int(training_info[\'n_iterations\']),\n        \'final_loss\': float(training_info[\'final_loss\']),\n        \'train_mae_global\': float(training_info[\'train_mae_global\']),\n        \'train_rmse_global\': float(training_info[\'train_rmse_global\']),\n        \'train_r2_global\': float(training_info[\'train_r2_global\']),\n    }\n    \n    info_json_filename = models_folder / \'mlp_training_info.json\'\n    with open(info_json_filename, \'w\') as f:\n        json.dump(training_info_json, f, indent=4)\n    \n    print(f"   ‚úÖ Resumen guardado en: {info_json_filename}")\n    \nexcept Exception as e:\n    print(f"   ‚ö†Ô∏è  Advertencia: No se pudo guardar la informaci√≥n completa: {e}")\n\n# ============================================================================\n# RESUMEN FINAL\n# ============================================================================\nprint("\\n" + "="*80)\nprint("‚úÖ ENTRENAMIENTO COMPLETADO EXITOSAMENTE")\nprint("="*80)\n\nprint(f"\\nüìä RESUMEN:")\nprint(f"   ‚Ä¢ Modelo: MLPRegressor")\nprint(f"   ‚Ä¢ Arquitectura: {mlp_params[\'hidden_layer_sizes\']}")\nprint(f"   ‚Ä¢ Datos entrenamiento: {n_samples:,} muestras")\nprint(f"   ‚Ä¢ Features: {n_features}")\nprint(f"   ‚Ä¢ Tiempo de entrenamiento: {training_time/60:.2f} minutos")\nprint(f"   ‚Ä¢ √âpocas: {mlp_model.n_iter_}/{mlp_params[\'max_iter\']}")\nprint(f"   ‚Ä¢ Loss final: {mlp_model.loss_:.6f}")\n\nprint(f"\\nüìà M√âTRICAS DE ENTRENAMIENTO:")\nprint(f"   ‚Ä¢ MAE:  {mae_global:.6f}")\nprint(f"   ‚Ä¢ RMSE: {rmse_global:.6f}")\nprint(f"   ‚Ä¢ R¬≤:   {r2_global:.6f}")\n\nprint(f"\\nüíæ ARCHIVOS GENERADOS:")\nprint(f"   ‚Ä¢ {model_filename}")\nprint(f"   ‚Ä¢ {info_filename}")\nprint(f"   ‚Ä¢ {info_json_filename}")\n\nprint(f"\\nüéØ SIGUIENTE PASO:")\nprint(f"   Ejecutar: 02_MLP_validation.py")\nprint(f"   Para evaluar el modelo en el conjunto de test")\n\nprint("\\n" + "="*80)\n',
            '"""\n============================================================================\nPASO 2: VALIDACI√ìN DEL MODELO MLP\n============================================================================\n\nEste script eval√∫a el rendimiento del modelo MLP en el conjunto de test\ny genera gr√°ficos de an√°lisis.\n\nIMPORTANTE: Adaptado para scalers independientes por columna\n- scalers_X.pkl: Diccionario con un scaler por feature\n- scalers_y.pkl: Diccionario con un scaler por target\n\nAutor: Adaptado para scalers independientes\nFecha: Enero 2026\n============================================================================\n"""\n\n# Configuraci√≥n de matplotlib\nplt.rcParams[\'figure.dpi\'] = 100\nplt.rcParams[\'savefig.dpi\'] = 300\nplt.rcParams[\'font.size\'] = 10\nsns.set_style("whitegrid")\n\n# =============================================================================\n# CONFIGURACI√ìN INICIAL\n# =============================================================================\n\n# Directorio ra√≠z del proyecto (ajustar seg√∫n necesidad)\nroot_dir = Path(r"C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub")\n\nprint("="*80)\nprint("üìä PASO 2: VALIDACI√ìN DEL MODELO MLP")\nprint("="*80)\nprint(f"üìÖ Fecha: {datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')}")\nprint("="*80)\n\n# ============================================================================\n# 1. CONFIGURACI√ìN DE RUTAS\n# ============================================================================\nprint("\\n[1/8] Configurando rutas...")\n\n# Directorios\ndata_ml_folder = root_dir / "notebook" / "02_Data_ML_traditional"\nmodels_folder = root_dir / "notebook" / "03_ML_traditional_models"\nscaler_folder = root_dir / "notebook" / "01_Models_scaler"\nmlp_folder = root_dir / "notebook" / "03_ML_traditional_models" / "MLP"\n\n# Crear carpeta espec√≠fica para MLP si no existe\nmlp_folder.mkdir(parents=True, exist_ok=True)\n\nprint(f"   ‚úÖ Carpeta de datos: {data_ml_folder}")\nprint(f"   ‚úÖ Carpeta de scalers: {scaler_folder}")\nprint(f"   ‚úÖ Carpeta de modelos: {models_folder}")\nprint(f"   ‚úÖ Carpeta MLP: {mlp_folder}")\n\n# ============================================================================\n# 2. CARGAR MODELO\n# ============================================================================\nprint("\\n[2/8] Cargando modelo MLP entrenado...")\n\ntry:\n    model_path = mlp_folder / \'mlp_model.pkl\'\n    mlp_model = joblib.load(model_path)\n    \n    print(f"   ‚úÖ Modelo cargado desde: {model_path}")\n    print(f"   üìä Arquitectura: {mlp_model.hidden_layer_sizes}")\n    print(f"   üìä √âpocas entrenadas: {mlp_model.n_iter_}")\n    \nexcept Exception as e:\n    print(f"   ‚ùå Error cargando modelo: {e}")\n    sys.exit(1)\n\n# ============================================================================\n# 3. CARGAR DATOS DE TEST Y SCALERS\n# ============================================================================\nprint("\\n[3/8] Cargando datos de test y scalers independientes...")\n\ntry:\n    # Cargar datos de test originales\n    X_test = joblib.load(data_ml_folder / \'X_test.pkl\')\n    y_test = joblib.load(data_ml_folder / \'y_test.pkl\')\n    \n    print(f"   ‚úÖ X_test cargado: {X_test.shape}")\n    print(f"   ‚úÖ y_test cargado: {y_test.shape}")\n    print(f"   üí° Targets: {list(y_test.columns)}")\n    \n    # Cargar scalers independientes (diccionarios)\n    scalers_X = joblib.load(scaler_folder / \'scalers_X.pkl\')\n    scalers_y = joblib.load(scaler_folder / \'scalers_y.pkl\')\n    \n    print(f"   ‚úÖ Scalers independientes cargados:")\n    print(f"      ‚Ä¢ scalers_X: {len(scalers_X)} scalers (uno por feature)")\n    print(f"      ‚Ä¢ scalers_y: {len(scalers_y)} scalers (uno por target)")\n    \n    # Normalizar X_test columna por columna usando scalers independientes\n    X_test_norm = pd.DataFrame(index=X_test.index, columns=X_test.columns)\n    \n    for col in X_test.columns:\n        if col in scalers_X:\n            X_test_norm[col] = scalers_X[col].transform(X_test[[col]])\n        else:\n            print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales")\n            X_test_norm[col] = X_test[col]\n    \n    X_test_norm = X_test_norm.astype(\'float32\')\n    \n    # Normalizar y_test columna por columna usando scalers independientes\n    y_test_norm = pd.DataFrame(index=y_test.index, columns=y_test.columns)\n    \n    for col in y_test.columns:\n        if col in scalers_y:\n            y_test_norm[col] = scalers_y[col].transform(y_test[[col]])\n        else:\n            print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales")\n            y_test_norm[col] = y_test[col]\n    \n    print(f"   ‚úÖ Datos de test normalizados columna por columna")\n    \n    n_samples = len(X_test)\n    target_names = list(y_test.columns)\n    \nexcept Exception as e:\n    print(f"   ‚ùå Error cargando datos: {e}")\n    sys.exit(1)\n\n# ============================================================================\n# 4. PREDICCIONES EN TEST\n# ============================================================================\nprint("\\n[4/8] Generando predicciones en test...")\n\ntry:\n    # Predicciones normalizadas\n    y_test_pred_norm = mlp_model.predict(X_test_norm)\n    \n    # Convertir a DataFrame\n    y_test_pred_norm_df = pd.DataFrame(\n        y_test_pred_norm,\n        columns=y_test_norm.columns,\n        index=y_test_norm.index\n    )\n    \n    print(f"   ‚úÖ Predicciones generadas (normalizadas): {y_test_pred_norm.shape}")\n    \n    # Desnormalizar predicciones columna por columna usando scalers independientes\n    y_test_pred = pd.DataFrame(index=y_test_pred_norm_df.index, columns=y_test_pred_norm_df.columns)\n    \n    for col in y_test_pred_norm_df.columns:\n        if col in scalers_y:\n            y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm_df[[col]])\n        else:\n            print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores normalizados")\n            y_test_pred[col] = y_test_pred_norm_df[col]\n    \n    print(f"   ‚úÖ Predicciones desnormalizadas columna por columna")\n    \nexcept Exception as e:\n    print(f"   ‚ùå Error generando predicciones: {e}")\n    sys.exit(1)\n\n# ============================================================================\n# 5. CALCULAR M√âTRICAS EN TEST\n# ============================================================================\nprint("\\n[5/8] Calculando m√©tricas en test...")\n\ntry:\n    print(f"\\n   üìä M√©tricas de TEST (datos normalizados):")\n    print(f"      {\'Target\':<30} {\'MAE\':>12} {\'RMSE\':>12} {\'R¬≤\':>10}")\n    print(f"      {\'-\'*30} {\'-\'*12} {\'-\'*12} {\'-\'*10}")\n    \n    metrics_test = {}\n    \n    for i, target_name in enumerate(target_names):\n        y_true = y_test_norm.iloc[:, i]\n        y_pred = y_test_pred_norm[:, i]\n        \n        mae = mean_absolute_error(y_true, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n        r2 = r2_score(y_true, y_pred)\n        \n        metrics_test[target_name] = {\n            \'MAE\': mae,\n            \'RMSE\': rmse,\n            \'R2\': r2\n        }\n        \n        print(f"      {target_name:<30} {mae:12.6f} {rmse:12.6f} {r2:10.6f}")\n    \n    # M√©trica global\n    mae_global = mean_absolute_error(y_test_norm, y_test_pred_norm)\n    rmse_global = np.sqrt(mean_squared_error(y_test_norm, y_test_pred_norm))\n    r2_global = r2_score(y_test_norm, y_test_pred_norm)\n    \n    print(f"      {\'-\'*30} {\'-\'*12} {\'-\'*12} {\'-\'*10}")\n    print(f"      {\'PROMEDIO GLOBAL\':<30} {mae_global:12.6f} {rmse_global:12.6f} {r2_global:10.6f}")\n    \n    # Guardar m√©tricas en CSV\n    metrics_df = pd.DataFrame(metrics_test).T\n    metrics_csv_path = mlp_folder / \'validation_summary.csv\'\n    metrics_df.to_csv(metrics_csv_path)\n    print(f"\\n   üíæ M√©tricas guardadas en: {metrics_csv_path}")\n    \nexcept Exception as e:\n    print(f"   ‚ùå Error calculando m√©tricas: {e}")\n    metrics_test = {}\n\n# ============================================================================\n# 6. GR√ÅFICO 1: PREDICCIONES VS REALES (SCATTER)\n# ============================================================================\nprint("\\n[6/8] Generando gr√°fico de predicciones vs reales...")\n\ntry:\n    # Detectar n√∫mero de targets autom√°ticamente\n    n_targets = len(target_names)\n    \n    # Crear grid flexible: 1 fila si ‚â§3 targets, 2 filas si >3\n    if n_targets <= 3:\n        n_rows, n_cols = 1, n_targets\n        figsize = (6*n_targets, 6)\n    else:\n        n_rows = 2\n        n_cols = int(np.ceil(n_targets / 2))\n        figsize = (6*n_cols, 12)\n    \n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    \n    # Asegurar que axes sea siempre iterable\n    if n_targets == 1:\n        axes = [axes]\n    else:\n        axes = axes.flat\n    \n    for i, (ax, target_name) in enumerate(zip(axes, target_names)):\n        y_true = y_test_norm.iloc[:, i].values\n        y_pred = y_test_pred_norm[:, i]\n        \n        # Scatter plot\n        ax.scatter(y_true, y_pred, alpha=0.3, s=1, label=\'Predicciones\')\n        \n        # L√≠nea perfecta (y=x)\n        min_val = min(y_true.min(), y_pred.min())\n        max_val = max(y_true.max(), y_pred.max())\n        ax.plot([min_val, max_val], [min_val, max_val], \'r--\', lw=2, label=\'Predicci√≥n perfecta\')\n        \n        # M√©tricas en el gr√°fico\n        mae = metrics_test[target_name][\'MAE\']\n        rmse = metrics_test[target_name][\'RMSE\']\n        r2 = metrics_test[target_name][\'R2\']\n        \n        textstr = f\'MAE = {mae:.4f}\\nRMSE = {rmse:.4f}\\nR¬≤ = {r2:.4f}\'\n        props = dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.8)\n        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n                verticalalignment=\'top\', bbox=props)\n        \n        ax.set_xlabel(\'Valores Reales (Normalizados)\', fontsize=11, fontweight=\'bold\')\n        ax.set_ylabel(\'Predicciones (Normalizadas)\', fontsize=11, fontweight=\'bold\')\n        ax.set_title(f\'{target_name}\\nMLP - Test Set\', fontsize=12, fontweight=\'bold\')\n        ax.legend(loc=\'lower right\')\n        ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    scatter_path = mlp_folder / \'predictions_vs_real.png\'\n    plt.savefig(scatter_path, dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    \n    print(f"   ‚úÖ Gr√°fica guardada: {scatter_path}")\n    \nexcept Exception as e:\n    print(f"   ‚ö†Ô∏è  Error generando gr√°fico de scatter: {e}")\n\n# ============================================================================\n# 7. GR√ÅFICO 2: AN√ÅLISIS DE RESIDUOS\n# ============================================================================\nprint("\\n[7/8] Generando an√°lisis de residuos...")\n\ntry:\n    # Grid para residuos: n_targets filas √ó 2 columnas\n    fig, axes = plt.subplots(n_targets, 2, figsize=(16, 5*n_targets))\n    \n    # Asegurar que axes sea 2D\n    if n_targets == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i, target_name in enumerate(target_names):\n        y_true = y_test_norm.iloc[:, i].values\n        y_pred = y_test_pred_norm[:, i]\n        \n        residuals = y_true - y_pred\n        \n        # Subplot 1: Residuos vs Predicciones\n        ax = axes[i, 0]\n        ax.scatter(y_pred, residuals, alpha=0.3, s=1, color=\'steelblue\')\n        ax.axhline(y=0, color=\'r\', linestyle=\'--\', lw=2)\n        ax.set_xlabel(\'Predicciones\', fontsize=11, fontweight=\'bold\')\n        ax.set_ylabel(\'Residuos\', fontsize=11, fontweight=\'bold\')\n        ax.set_title(f\'{target_name}\\nResiduos vs Predicciones\', fontsize=12, fontweight=\'bold\')\n        ax.grid(True, alpha=0.3)\n        \n        # Subplot 2: Histograma de residuos\n        ax = axes[i, 1]\n        ax.hist(residuals, bins=100, color=\'steelblue\', alpha=0.7, edgecolor=\'black\')\n        ax.axvline(x=0, color=\'r\', linestyle=\'--\', lw=2)\n        ax.set_xlabel(\'Residuos\', fontsize=11, fontweight=\'bold\')\n        ax.set_ylabel(\'Frecuencia\', fontsize=11, fontweight=\'bold\')\n        ax.set_title(f\'{target_name}\\nDistribuci√≥n de Residuos\', fontsize=12, fontweight=\'bold\')\n        ax.grid(True, alpha=0.3)\n        \n        # Estad√≠sticas de residuos\n        mean_res = np.mean(residuals)\n        std_res = np.std(residuals)\n        textstr = f\'Media = {mean_res:.6f}\\nStd = {std_res:.6f}\'\n        props = dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.8)\n        ax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n                verticalalignment=\'top\', bbox=props)\n    \n    plt.tight_layout()\n    \n    residuals_path = mlp_folder / \'residuals_analysis.png\'\n    plt.savefig(residuals_path, dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    \n    print(f"   ‚úÖ Gr√°fica guardada: {residuals_path}")\n    \nexcept Exception as e:\n    print(f"   ‚ö†Ô∏è  Error generando an√°lisis de residuos: {e}")\n\n# ============================================================================\n# 8. TABLA COMPARATIVA DE M√âTRICAS\n# ============================================================================\nprint("\\n[8/8] Generando tabla comparativa de m√©tricas...")\n\ntry:\n    # Cargar informaci√≥n de entrenamiento\n    training_info_path = mlp_folder / \'mlp_training_info.pkl\'\n    if training_info_path.exists():\n        training_info = joblib.load(training_info_path)\n        metrics_train = training_info.get(\'train_metrics\', {})\n        \n        # Crear tabla comparativa\n        comparison_data = []\n        \n        for target_name in target_names:\n            if target_name in metrics_train and target_name in metrics_test:\n                comparison_data.append({\n                    \'Target\': target_name,\n                    \'MAE_Train\': metrics_train[target_name][\'MAE\'],\n                    \'MAE_Test\': metrics_test[target_name][\'MAE\'],\n                    \'RMSE_Train\': metrics_train[target_name][\'RMSE\'],\n                    \'RMSE_Test\': metrics_test[target_name][\'RMSE\'],\n                    \'R2_Train\': metrics_train[target_name][\'R2\'],\n                    \'R2_Test\': metrics_test[target_name][\'R2\'],\n                })\n        \n        comparison_df = pd.DataFrame(comparison_data)\n        \n        # Guardar tabla\n        comparison_path = mlp_folder / \'metrics_comparison.csv\'\n        comparison_df.to_csv(comparison_path, index=False)\n        \n        print(f"\\n   üìä Comparaci√≥n Train vs Test:")\n        print(comparison_df.to_string(index=False))\n        print(f"\\n   üíæ Tabla guardada en: {comparison_path}")\n        \n        # Gr√°fico de comparaci√≥n\n        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n        metrics_to_plot = [\'MAE\', \'RMSE\', \'R2\']\n        \n        for ax, metric in zip(axes, metrics_to_plot):\n            x = np.arange(len(target_names))\n            width = 0.35\n            \n            train_vals = [comparison_data[i][f\'{metric}_Train\'] for i in range(len(target_names))]\n            test_vals = [comparison_data[i][f\'{metric}_Test\'] for i in range(len(target_names))]\n            \n            ax.bar(x - width/2, train_vals, width, label=\'Train\', color=\'steelblue\', alpha=0.8)\n            ax.bar(x + width/2, test_vals, width, label=\'Test\', color=\'orange\', alpha=0.8)\n            \n            ax.set_xlabel(\'Target\', fontsize=11, fontweight=\'bold\')\n            ax.set_ylabel(metric, fontsize=11, fontweight=\'bold\')\n            ax.set_title(f\'{metric}: Train vs Test\', fontsize=12, fontweight=\'bold\')\n            ax.set_xticks(x)\n            ax.set_xticklabels(target_names, rotation=45, ha=\'right\')\n            ax.legend()\n            ax.grid(True, alpha=0.3, axis=\'y\')\n        \n        plt.tight_layout()\n        \n        comparison_plot_path = mlp_folder / \'metrics_comparison.png\'\n        plt.savefig(comparison_plot_path, dpi=300, bbox_inches=\'tight\')\n        plt.close()\n        \n        print(f"   ‚úÖ Gr√°fica guardada: {comparison_plot_path}")\n        \n    else:\n        print(f"   ‚ö†Ô∏è  No se encontr√≥ informaci√≥n de entrenamiento")\n        \nexcept Exception as e:\n    print(f"   ‚ö†Ô∏è  Error generando comparaci√≥n: {e}")\n\n# ============================================================================\n# RESUMEN FINAL\n# ============================================================================\nprint("\\n" + "="*80)\nprint("‚úÖ VALIDACI√ìN COMPLETADA EXITOSAMENTE")\nprint("="*80)\n\nprint(f"\\nüìä RESUMEN DE RESULTADOS:")\nprint(f"   ‚Ä¢ Muestras de test: {n_samples:,}")\nprint(f"   ‚Ä¢ Targets: {target_names}")\nprint(f"   ‚Ä¢ MAE Global:  {mae_global:.6f}")\nprint(f"   ‚Ä¢ RMSE Global: {rmse_global:.6f}")\nprint(f"   ‚Ä¢ R¬≤ Global:   {r2_global:.6f}")\n\nprint(f"\\nüíæ ARCHIVOS GENERADOS EN: {mlp_folder}")\nprint(f"   ‚Ä¢ validation_summary.csv")\nprint(f"   ‚Ä¢ predictions_vs_real.png")\nprint(f"   ‚Ä¢ residuals_analysis.png")\nprint(f"   ‚Ä¢ metrics_comparison.csv")\nprint(f"   ‚Ä¢ metrics_comparison.png")\n\nprint(f"\\nüéØ SIGUIENTE PASO:")\nprint(f"   Ejecutar: MLP_Validation_timeseries.py")\nprint(f"   Para an√°lisis de series temporales")\n\nprint(f"\\nüí° Scalers independientes utilizados correctamente")\nprint(f"   ‚Ä¢ X: Normalizaci√≥n columna por columna ({len(scalers_X)} scalers)")\nprint(f"   ‚Ä¢ y: Normalizaci√≥n columna por columna ({len(scalers_y)} scalers)")\n\nprint("\\n" + "="*80)',
            '"""\n============================================================================\nPASO 3: AN√ÅLISIS DE SERIES TEMPORALES - MODELO MLP\n============================================================================\n\nEste script genera gr√°ficos de series temporales comparando las predicciones\ndel modelo MLP con los valores reales en el conjunto de test.\n\nIMPORTANTE: Adaptado para scalers independientes por columna\n- scalers_X.pkl: Diccionario con un scaler por feature\n- scalers_y.pkl: Diccionario con un scaler por target\n\nAutor: Adaptado para scalers independientes\nFecha: Enero 2026\n============================================================================\n"""\n\n# Configuraci√≥n de matplotlib\nplt.rcParams[\'figure.dpi\'] = 100\nplt.rcParams[\'savefig.dpi\'] = 300\nplt.rcParams[\'font.size\'] = 10\nsns.set_style("whitegrid")\n\n# =============================================================================\n# CONFIGURACI√ìN INICIAL\n# =============================================================================\n\n# Directorio ra√≠z del proyecto (ajustar seg√∫n necesidad)\nroot_dir = Path(r"C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub")\n\nprint("\\n" + "="*80)\nprint("üìä MLP - GR√ÅFICAS DE TIME SERIES")\nprint("="*80)\n\n# ------------------------------------------------------------------------\n# 1) Cargar Time_test y generar series_id_test\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: PREPARAR DATOS PARA TIME SERIES")\nprint("="*80)\n\n# Directorios\ndata_ml_folder = root_dir / "notebook" / "02_Data_ML_traditional"\nmodels_folder = root_dir / "notebook" / "03_ML_traditional_models"\nscaler_folder = root_dir / "notebook" / "01_Models_scaler"\nmlp_folder = models_folder / "MLP"\n\nprint(f"\\n[1/3] Cargando Time_test desde archivo pkl...")\n\nTime_test_path = data_ml_folder / \'Time_test.pkl\'\n\nif Time_test_path.exists():\n    Time_test = joblib.load(Time_test_path)\n    print(f"   ‚úÖ Time_test cargado: {len(Time_test):,} valores")\n    print(f"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s")\n    print(f"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s")\nelse:\n    print(f"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}")\n    raise FileNotFoundError(f"Archivo requerido no encontrado: {Time_test_path}")\n\nprint(f"\\n[2/3] Generando series_id_test a partir de Time_test...")\n\n# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\nseries_id_test_values = np.zeros(len(Time_test), dtype=int)\ncurrent_series = 0\n\nTime_test_array = Time_test.values\nfor i in range(1, len(Time_test_array)):\n    if Time_test_array[i] < Time_test_array[i-1]:\n        current_series += 1\n    series_id_test_values[i] = current_series\n\n# Convertir a pandas Series con el mismo index que Time_test\nseries_id_test = pd.Series(series_id_test_values, index=Time_test.index, name=\'series_id\')\n\nn_test_series = series_id_test.max() + 1\n\nprint(f"   ‚úÖ Series temporales identificadas en test: {n_test_series}")\n\n# Analizar cada serie\nprint(f"\\n   üìä Resumen de series en TEST:")\nfor sid in range(min(5, n_test_series)):\n    mask = series_id_test == sid\n    n_rows = mask.sum()\n    time_min = Time_test.loc[mask].min()\n    time_max = Time_test.loc[mask].max()\n    print(f"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s")\n\nif n_test_series > 5:\n    print(f"      ... y {n_test_series - 5} series m√°s")\n\n# ------------------------------------------------------------------------\n# 2) Cargar modelo, datos y generar predicciones\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: CARGAR MODELO Y GENERAR PREDICCIONES")\nprint("="*80)\n\nprint(f"\\n[1/6] Cargando modelo entrenado...")\nmodel_path = mlp_folder / \'mlp_model.pkl\'\nmlp_model = joblib.load(model_path)\nprint(f"   ‚úÖ Modelo cargado desde: {model_path.name}")\n\nprint(f"\\n[2/6] Cargando datos de test originales...")\nX_test = joblib.load(data_ml_folder / \'X_test.pkl\')\ny_test = joblib.load(data_ml_folder / \'y_test.pkl\')\nprint(f"   ‚úÖ X_test: {X_test.shape}")\nprint(f"   ‚úÖ y_test: {y_test.shape}")\nprint(f"   üí° Targets: {list(y_test.columns)}")\n\nprint(f"\\n[3/6] Cargando scalers independientes...")\nscalers_X = joblib.load(scaler_folder / \'scalers_X.pkl\')\nscalers_y = joblib.load(scaler_folder / \'scalers_y.pkl\')\nprint(f"   ‚úÖ Scalers independientes cargados:")\nprint(f"      ‚Ä¢ scalers_X: {len(scalers_X)} scalers (uno por feature)")\nprint(f"      ‚Ä¢ scalers_y: {len(scalers_y)} scalers (uno por target)")\n\nprint(f"\\n[4/6] Normalizando datos de test con scalers independientes...")\n# Normalizar columna por columna\nX_test_norm = pd.DataFrame(index=X_test.index, columns=X_test.columns)\n\nfor col in X_test.columns:\n    if col in scalers_X:\n        X_test_norm[col] = scalers_X[col].transform(X_test[[col]])\n    else:\n        print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales")\n        X_test_norm[col] = X_test[col]\n\nX_test_norm = X_test_norm.astype(\'float32\')\nprint(f"   ‚úÖ X_test normalizado columna por columna: {X_test_norm.shape}")\n\nprint(f"\\n[5/6] Generando predicciones...")\ny_test_pred_norm = mlp_model.predict(X_test_norm)\ny_test_pred_norm = pd.DataFrame(\n    y_test_pred_norm,\n    index=y_test.index,\n    columns=y_test.columns\n)\nprint(f"   ‚úÖ Predicciones generadas (normalizadas): {y_test_pred_norm.shape}")\n\nprint(f"\\n[6/6] Desnormalizando predicciones con scalers independientes...")\n# Desnormalizar columna por columna\ny_test_pred = pd.DataFrame(index=y_test_pred_norm.index, columns=y_test_pred_norm.columns)\n\nfor col in y_test_pred_norm.columns:\n    if col in scalers_y:\n        y_test_pred[col] = scalers_y[col].inverse_transform(y_test_pred_norm[[col]])\n    else:\n        print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores normalizados")\n        y_test_pred[col] = y_test_pred_norm[col]\n\nprint(f"   ‚úÖ Predicciones desnormalizadas columna por columna: {y_test_pred.shape}")\n\nprint(f"\\n[3/3] Verificando que los datos coinciden con predicciones...")\n\n# Verificar que los √≠ndices coinciden\nif not all(series_id_test.index == y_test.index):\n    print(f"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...")\n    series_id_test = series_id_test.reindex(y_test.index)\n    Time_test = Time_test.reindex(y_test.index)\n\nprint(f"   ‚úÖ √çndices verificados:")\nprint(f"      ‚Ä¢ y_test: {y_test.shape[0]:,} filas")\nprint(f"      ‚Ä¢ y_test_pred: {y_test_pred.shape[0]:,} filas")\nprint(f"      ‚Ä¢ series_id_test: {len(series_id_test):,} valores")\nprint(f"      ‚Ä¢ Time_test: {len(Time_test):,} valores")\n\n\n# ------------------------------------------------------------------------\n# 3) Gr√°fica de time series: Real vs Predicho (3 series aleatorias)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: GR√ÅFICAS DE TIME SERIES - SERIES COMPLETAS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas de time series (Real vs Predicho)...")\n\n# Obtener series √∫nicas del conjunto de test\nunique_test_series = series_id_test.unique()\n\n# Seleccionar 3 series aleatorias\nnp.random.seed(42)  # Para reproducibilidad\nselected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n\nprint(f"   ‚Ä¢ Total series disponibles: {len(unique_test_series)}")\nprint(f"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}")\n\n# Detectar n√∫mero de targets autom√°ticamente\nn_targets = len(y_test.columns)\nprint(f"   ‚Ä¢ Targets a graficar: {list(y_test.columns)}")\n\n# Crear figura con 3 filas x n_targets columnas (3 series, 3 targets)\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales\n        y_real = y_test.loc[series_indices, col]\n        # Valores predichos\n        y_pred = y_test_pred.loc[series_indices, col]\n        \n        # Calcular m√©tricas para esta serie y target\n        r2_series = r2_score(y_real, y_pred)\n        rmse_series = np.sqrt(mean_squared_error(y_real, y_pred))\n        \n        # Graficar\n        ax.plot(time_series, y_real, label=\'Real\', \n                linewidth=2, alpha=0.8, color=\'blue\')\n        ax.plot(time_series, y_pred, label=\'Predicho\', \n                linestyle=\'--\', linewidth=2, alpha=0.8, color=\'red\')\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}\', \n                     fontsize=11, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=9)\n        ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\ntimeseries_plot_path = mlp_folder / \'timeseries_comparison.png\'\nplt.savefig(timeseries_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# 4) Gr√°fica de time series con ZOOM (50 segundos)\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: GR√ÅFICAS DE TIME SERIES - ZOOM 50 SEGUNDOS")\nprint("="*80)\n\nprint(f"\\n[1/1] Creando gr√°ficas de time series con zoom (50s)...")\n\n# Crear figura con 3 filas x n_targets columnas (3 series, 3 targets)\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Definir ventana de 50 segundos desde el inicio\n    time_min = time_series.min()\n    time_max_zoom = time_min + 50\n    \n    # Filtrar por ventana de tiempo\n    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n    zoom_indices = time_series[zoom_mask].index\n    time_zoom = time_series[zoom_mask]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(y_test.columns):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales y predichos (con zoom)\n        y_real_zoom = y_test.loc[zoom_indices, col]\n        y_pred_zoom = y_test_pred.loc[zoom_indices, col]\n        \n        # Calcular m√©tricas para esta ventana\n        r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n        rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n        \n        # Graficar\n        ax.plot(time_zoom, y_real_zoom, label=\'Real\', \n                linewidth=2.5, alpha=0.8, color=\'blue\', marker=\'o\', markersize=4)\n        ax.plot(time_zoom, y_pred_zoom, label=\'Predicho\', \n                linestyle=\'--\', linewidth=2.5, alpha=0.8, color=\'red\', marker=\'x\', markersize=5)\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}\', \n                     fontsize=11, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=9)\n        ax.grid(True, alpha=0.3)\n        \n        # A√±adir texto con informaci√≥n de puntos\n        n_points = len(zoom_indices)\n        ax.text(0.02, 0.02, f\'Puntos: {n_points}\', transform=ax.transAxes, fontsize=9,\n                verticalalignment=\'bottom\', bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.5))\n\nplt.tight_layout()\ntimeseries_zoom_plot_path = mlp_folder / \'timeseries_comparison_zoom50s.png\'\nplt.savefig(timeseries_zoom_plot_path, dpi=300, bbox_inches=\'tight\')\nplt.show()\n\nprint(f"   ‚úÖ Gr√°fica guardada: {timeseries_zoom_plot_path.name}")\n\n# ------------------------------------------------------------------------\n# RESUMEN FINAL\n# ------------------------------------------------------------------------\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ GR√ÅFICAS DE TIME SERIES COMPLETADAS")\nprint("="*80)\n\nprint(f"\\nüìä GR√ÅFICAS GENERADAS:")\nprint(f"   ‚Ä¢ {timeseries_plot_path.name}")\nprint(f"   ‚Ä¢ {timeseries_zoom_plot_path.name}")\n\nprint(f"\\nüí° Targets graficados: {list(y_test.columns)}")\nprint(f"üí° Series aleatorias: {list(selected_series)}")\n\nprint(f"\\nüí° Ahora tienes todas las gr√°ficas para MLP:")\nprint(f"   1. M√©tricas por target (RMSE y R¬≤)")\nprint(f"   2. Predicciones vs Real")\nprint(f"   3. An√°lisis de residuos")\nprint(f"   4. Time series completas (3 series aleatorias √ó {n_targets} targets)")\nprint(f"   5. Time series con zoom de 50s")\n\nprint(f"\\nüí° Scalers independientes utilizados correctamente")\nprint(f"   ‚Ä¢ X: Normalizaci√≥n columna por columna ({len(scalers_X)} scalers)")\nprint(f"   ‚Ä¢ y: Desnormalizaci√≥n columna por columna ({len(scalers_y)} scalers)")\n\nprint("="*80)\n',
        ],
    },
    {
        "title": 'üîÑ ESTRUCTURA DEL PASO 5 (DIVIDIDO EN SUB-PASOS)',
        "cells": [
            '# ================================================================================\n# üìä COMPARACI√ìN COMPLETA DE MODELOS - VALIDACI√ìN EN TEST\n# ================================================================================\n"""\nCompara el rendimiento de todos los modelos entrenados:\n- Linear Ridge\n- Random Forest  \n- XGBoost\n- MLP\n\nGenera visualizaciones comparativas y guarda resultados en carpeta com√∫n.\n\nIMPORTANTE: Adaptado para scalers independientes por columna\n- scalers_X.pkl: Diccionario con un scaler por feature\n- scalers_y.pkl: Diccionario con un scaler por target\n"""\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport joblib\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\n# ================================================================================\n# CONFIGURACI√ìN\n# ================================================================================\nprint("=" * 80)\nprint("üìä COMPARACI√ìN DE TODOS LOS MODELOS - VALIDACI√ìN EN TEST")\nprint("=" * 80)\nprint(f"üìÖ Fecha: {datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')}")\nprint("=" * 80)\nprint()\n\n# Configuraci√≥n de matplotlib\nplt.rcParams[\'figure.dpi\'] = 100\nplt.rcParams[\'savefig.dpi\'] = 300\nplt.rcParams[\'font.size\'] = 10\nsns.set_style("whitegrid")\n\n# Rutas base\nroot_dir = Path(r"C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub")\nmodels_dir = root_dir / \'notebook\' / \'03_ML_traditional_models\'\ndata_dir = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\nscaler_folder = root_dir / \'notebook\' / \'01_Models_scaler\'\noutput_dir = root_dir / \'notebook\' / \'04_ML_all_models\'\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Definir modelos a comparar con sus archivos correspondientes\nmodel_info = {\n    \'Linear_Ridge\': {\n        \'folder\': \'Linear_Ridge\',\n        \'file\': \'ridge_model.pkl\',\n        \'display_name\': \'Linear Ridge\',\n        \'requires_normalization\': True\n    },\n    \'Random_Forest\': {\n        \'folder\': \'Random_Forest\',\n        \'file\': \'random_forest_model.pkl\',\n        \'display_name\': \'Random Forest\',\n        \'requires_normalization\': True\n    },\n    \'XGBoost\': {\n        \'folder\': \'XGBoost\',\n        \'file\': \'xgboost_model.pkl\',\n        \'display_name\': \'XGBoost\',\n        \'requires_normalization\': True\n    },\n    \'MLP\': {\n        \'folder\': \'MLP\',\n        \'file\': \'mlp_model.pkl\',\n        \'display_name\': \'MLP\',\n        \'requires_normalization\': True\n    },\n    \'XGBoost_NoNorm\': {\n        \'folder\': \'XGBoost_NoNorm\',\n        \'file\': \'xgboost_nonorm_model.pkl\',\n        \'display_name\': \'XGBoost (No Norm)\',\n        \'requires_normalization\': False\n    }\n}\n\n# ================================================================================\n# PASO 1: DETECTAR MODELOS DISPONIBLES\n# ================================================================================\nprint("=" * 80)\nprint("PASO 1: DETECTAR MODELOS DISPONIBLES")\nprint("=" * 80)\nprint()\n\n# [1/3] Detectar carpetas disponibles\nprint(f"[1/3] Detectando carpetas de modelos en: {models_dir}")\navailable_folders = [f.name for f in models_dir.iterdir() if f.is_dir()]\nprint(f"   ‚úÖ Carpetas detectadas: {len(available_folders)}")\nfor folder in available_folders:\n    print(f"      ‚Ä¢ {folder}")\nprint()\n\n# [2/3] Verificar qu√© modelos est√°n disponibles\nprint("[2/3] Mapeando nombres de carpetas a archivos de modelos...")\navailable_models = {}\nfor model_key, info in model_info.items():\n    model_path = models_dir / info[\'folder\'] / info[\'file\']\n    if model_path.exists():\n        available_models[model_key] = info\n        print(f"   ‚úÖ {info[\'folder\']}: {info[\'file\']}")\n    else:\n        print(f"   ‚ö†Ô∏è  {info[\'folder\']}: {info[\'file\']} NO encontrado")\nprint()\n\n# [3/3] Resumen\nprint("[3/3] Resumen de modelos disponibles:")\nprint(f"   ‚Ä¢ Total modelos a comparar: {len(available_models)}")\nprint(f"   ‚Ä¢ Carpeta de resultados: {output_dir}")\nprint()\n\nif len(available_models) == 0:\n    print("‚ùå ERROR: No se encontr√≥ ning√∫n modelo para comparar")\n    raise FileNotFoundError("No hay modelos disponibles")\n\n# ================================================================================\n# PASO 2: CARGAR DATOS DE TEST ORIGINALES Y SCALERS\n# ================================================================================\nprint("=" * 80)\nprint("PASO 2: CARGAR DATOS DE TEST ORIGINALES Y SCALERS")\nprint("=" * 80)\nprint()\n\n# [1/4] Cargar datos de test ORIGINALES (sin normalizar)\nprint("[1/4] Cargando datos de test originales (sin normalizar)...")\nX_test = joblib.load(data_dir / \'X_test.pkl\')\ny_test = joblib.load(data_dir / \'y_test.pkl\')\nprint(f"   ‚úÖ X_test: {X_test.shape}")\nprint(f"   ‚úÖ y_test: {y_test.shape}")\n\n# Obtener nombres de targets\ntargets = y_test.columns.tolist()\nprint(f"   üí° Targets: {targets}")\nprint()\n\n# [2/4] Cargar scalers independientes\nprint("[2/4] Cargando scalers independientes...")\nscalers_X = joblib.load(scaler_folder / \'scalers_X.pkl\')\nscalers_y = joblib.load(scaler_folder / \'scalers_y.pkl\')\nprint(f"   ‚úÖ Scalers independientes cargados:")\nprint(f"      ‚Ä¢ scalers_X: {len(scalers_X)} scalers (uno por feature)")\nprint(f"      ‚Ä¢ scalers_y: {len(scalers_y)} scalers (uno por target)")\nprint()\n\n# [3/4] Normalizar X_test con scalers independientes\nprint("[3/4] Normalizando X_test con scalers independientes...")\nX_test_norm = pd.DataFrame(index=X_test.index, columns=X_test.columns)\n\nfor col in X_test.columns:\n    if col in scalers_X:\n        X_test_norm[col] = scalers_X[col].transform(X_test[[col]])\n    else:\n        print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales")\n        X_test_norm[col] = X_test[col]\n\nX_test_norm = X_test_norm.astype(\'float32\')\nprint(f"   ‚úÖ X_test normalizado columna por columna: {X_test_norm.shape}")\nprint()\n\n# [4/4] Verificar scalers disponibles para targets\nprint("[4/4] Verificando scalers disponibles para targets...")\nmissing_scalers = [t for t in targets if t not in scalers_y]\nif missing_scalers:\n    print(f"   ‚ö†Ô∏è  WARNING: Faltan scalers para: {missing_scalers}")\nelse:\n    print(f"   ‚úÖ Todos los targets tienen scaler: {list(scalers_y.keys())}")\nprint()\n\n# ================================================================================\n# PASO 3: CARGAR MODELOS Y HACER PREDICCIONES\n# ================================================================================\nprint("=" * 80)\nprint("PASO 3: CARGAR MODELOS Y HACER PREDICCIONES")\nprint("=" * 80)\nprint()\n\n# Diccionario para almacenar predicciones (desnormalizadas) y m√©tricas\nall_predictions = {}\nall_metrics = {}\n\nfor model_key, info in available_models.items():\n    print(f"üîÑ Procesando modelo: {info[\'display_name\']}")\n    print("-" * 80)\n    \n    # [1/5] Cargar modelo\n    print("   [1/5] Cargando modelo...")\n    model_path = models_dir / info[\'folder\'] / info[\'file\']\n    model = joblib.load(model_path)\n    print(f"      ‚úÖ Modelo cargado desde: {model_path.name}")\n    \n    # [2/5] Determinar si requiere normalizaci√≥n\n    requires_norm = info[\'requires_normalization\']\n    print(f"   [2/5] Verificando necesidad de normalizaci√≥n...")\n    if requires_norm:\n        print(f"      üí° Modelo CON normalizaci√≥n - usando X_test_norm")\n        X_to_use = X_test_norm\n    else:\n        print(f"      üí° Modelo SIN normalizaci√≥n - usando X_test original")\n        X_to_use = X_test\n    \n    # [3/5] Hacer predicciones\n    print(f"   [3/5] Generando predicciones...")\n    y_pred_raw = model.predict(X_to_use)\n    \n    # Convertir a DataFrame si es necesario\n    if not isinstance(y_pred_raw, pd.DataFrame):\n        y_pred_raw = pd.DataFrame(y_pred_raw, columns=targets, index=X_test.index)\n    \n    print(f"      ‚úÖ Predicciones generadas: {y_pred_raw.shape}")\n    \n    # [4/5] Desnormalizar predicciones si es necesario\n    print(f"   [4/5] Procesando predicciones...")\n    if requires_norm:\n        # Desnormalizar columna por columna\n        print(f"      üîÑ Desnormalizando predicciones con scalers independientes...")\n        y_pred = pd.DataFrame(index=y_pred_raw.index, columns=targets)\n        \n        for target in targets:\n            if target in scalers_y:\n                y_pred[target] = scalers_y[target].inverse_transform(y_pred_raw[[target]])\n            else:\n                print(f"      ‚ö†Ô∏è  WARNING: No hay scaler para {target}, usando valores normalizados")\n                y_pred[target] = y_pred_raw[target]\n        \n        print(f"      ‚úÖ Predicciones desnormalizadas columna por columna: {y_pred.shape}")\n    else:\n        # Sin normalizaci√≥n, usar predicciones directamente\n        y_pred = y_pred_raw\n        print(f"      ‚úÖ Predicciones ya en escala original (sin normalizaci√≥n)")\n    \n    # [5/5] Verificar rangos de valores\n    print("   [5/5] Verificando rangos de valores...")\n    for target in targets:\n        print(f"      ‚Ä¢ {target}:")\n        print(f"         Real:      min={y_test[target].min():8.2f}, max={y_test[target].max():8.2f}, mean={y_test[target].mean():8.2f}")\n        print(f"         Predicho:  min={y_pred[target].min():8.2f}, max={y_pred[target].max():8.2f}, mean={y_pred[target].mean():8.2f}")\n    \n    # Guardar predicciones\n    all_predictions[model_key] = y_pred\n    print(f"   ‚úÖ {info[\'display_name\']} completado")\n    print()\n\nprint(f"‚úÖ Predicciones completadas para {len(all_predictions)} modelos")\nprint()\n\n# ================================================================================\n# PASO 4: CALCULAR M√âTRICAS\n# ================================================================================\nprint("=" * 80)\nprint("PASO 4: CALCULAR M√âTRICAS")\nprint("=" * 80)\nprint()\n\nfor model_key, y_pred in all_predictions.items():\n    print(f"üìä Calculando m√©tricas para: {available_models[model_key][\'display_name\']}")\n    \n    metrics = {}\n    for target in targets:\n        mse = mean_squared_error(y_test[target], y_pred[target])\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(y_test[target], y_pred[target])\n        r2 = r2_score(y_test[target], y_pred[target])\n        \n        metrics[target] = {\n            \'MSE\': mse,\n            \'RMSE\': rmse,\n            \'MAE\': mae,\n            \'R2\': r2\n        }\n        \n        print(f"   ‚Ä¢ {target}: R¬≤ = {r2:.4f}, RMSE = {rmse:.2f}, MAE = {mae:.2f}")\n    \n    all_metrics[model_key] = metrics\n    print()\n\n# ================================================================================\n# PASO 5: CREAR TABLA COMPARATIVA DE M√âTRICAS\n# ================================================================================\nprint("=" * 80)\nprint("PASO 5: CREAR TABLA COMPARATIVA")\nprint("=" * 80)\nprint()\n\n# Crear DataFrame con todas las m√©tricas\ncomparison_data = []\nfor model_key, metrics in all_metrics.items():\n    for target, target_metrics in metrics.items():\n        row = {\n            \'Model\': available_models[model_key][\'display_name\'],\n            \'Target\': target,\n            \'R2\': target_metrics[\'R2\'],\n            \'RMSE\': target_metrics[\'RMSE\'],\n            \'MAE\': target_metrics[\'MAE\'],\n            \'MSE\': target_metrics[\'MSE\']\n        }\n        comparison_data.append(row)\n\ndf_comparison = pd.DataFrame(comparison_data)\n\n# Guardar tabla\ntable_path = output_dir / \'model_comparison_metrics.csv\'\ndf_comparison.to_csv(table_path, index=False)\nprint(f"‚úÖ Tabla de m√©tricas guardada: {table_path.name}")\nprint()\n\n# Mostrar tabla resumida\nprint("üìã RESUMEN DE M√âTRICAS (R¬≤ Score):")\nprint("-" * 80)\npivot_r2 = df_comparison.pivot(index=\'Model\', columns=\'Target\', values=\'R2\')\nprint(pivot_r2.to_string())\nprint()\n\nprint("üìã RESUMEN DE M√âTRICAS (RMSE):")\nprint("-" * 80)\npivot_rmse = df_comparison.pivot(index=\'Model\', columns=\'Target\', values=\'RMSE\')\nprint(pivot_rmse.to_string())\nprint()\n\n# ================================================================================\n# PASO 6: VISUALIZACIONES COMPARATIVAS\n# ================================================================================\nprint("=" * 80)\nprint("PASO 6: GENERAR VISUALIZACIONES")\nprint("=" * 80)\nprint()\n\n# [1/5] Gr√°fico de barras comparativo de R¬≤\nprint("[1/5] Creando gr√°fico de barras de R¬≤...")\nfig, axes = plt.subplots(1, len(targets), figsize=(6*len(targets), 5))\nif len(targets) == 1:\n    axes = [axes]\n\nfor idx, target in enumerate(targets):\n    ax = axes[idx]\n    \n    # Filtrar datos para este target\n    target_data = df_comparison[df_comparison[\'Target\'] == target].sort_values(\'R2\', ascending=False)\n    \n    # Crear barras\n    colors = [\'#3498db\', \'#e74c3c\', \'#2ecc71\', \'#f39c12\'][:len(target_data)]\n    bars = ax.bar(target_data[\'Model\'], target_data[\'R2\'], color=colors, alpha=0.8, edgecolor=\'black\')\n    \n    # A√±adir valores en las barras\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f\'{height:.4f}\',\n                ha=\'center\', va=\'bottom\', fontsize=10, fontweight=\'bold\')\n    \n    ax.set_title(f\'R¬≤ Score - {target}\', fontsize=14, fontweight=\'bold\')\n    ax.set_ylabel(\'R¬≤ Score\', fontsize=12)\n    ax.set_ylim([0, 1])\n    ax.grid(axis=\'y\', alpha=0.3)\n    ax.tick_params(axis=\'x\', rotation=45)\n\nplt.tight_layout()\nplt.savefig(output_dir / \'comparison_r2_scores.png\', dpi=300, bbox_inches=\'tight\')\nplt.close()\nprint("   ‚úÖ Guardado: comparison_r2_scores.png")\n\n# [2/5] Scatter plots para cada target\nprint("[2/5] Creando scatter plots por target...")\nfor target in targets:\n    n_models = len(available_models)\n    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n    if n_models == 1:\n        axes = [axes]\n    \n    for idx, (model_key, y_pred) in enumerate(all_predictions.items()):\n        ax = axes[idx]\n        \n        # Scatter plot\n        ax.scatter(y_test[target], y_pred[target], alpha=0.3, s=1, color=\'steelblue\')\n        \n        # L√≠nea diagonal\n        min_val = min(y_test[target].min(), y_pred[target].min())\n        max_val = max(y_test[target].max(), y_pred[target].max())\n        ax.plot([min_val, max_val], [min_val, max_val], \'r--\', lw=2, label=\'Ideal\')\n        \n        # M√©tricas\n        r2 = all_metrics[model_key][target][\'R2\']\n        rmse = all_metrics[model_key][target][\'RMSE\']\n        \n        ax.set_title(f"{available_models[model_key][\'display_name\']}\\nR¬≤={r2:.4f}, RMSE={rmse:.2f}", \n                     fontsize=12, fontweight=\'bold\')\n        ax.set_xlabel(\'Real [kNm]\', fontsize=11)\n        ax.set_ylabel(\'Predicho [kNm]\', fontsize=11)\n        ax.legend(loc=\'upper left\', fontsize=9)\n        ax.grid(alpha=0.3)\n    \n    plt.suptitle(f\'Predicciones vs Real - {target}\', fontsize=14, fontweight=\'bold\', y=1.02)\n    plt.tight_layout()\n    plt.savefig(output_dir / f\'comparison_scatter_{target}.png\', dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    print(f"   ‚úÖ Guardado: comparison_scatter_{target}.png")\n\n# [3/5] Histogramas de residuos\nprint("[3/5] Creando histogramas de residuos...")\nfor target in targets:\n    n_models = len(available_models)\n    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n    if n_models == 1:\n        axes = [axes]\n    \n    for idx, (model_key, y_pred) in enumerate(all_predictions.items()):\n        ax = axes[idx]\n        \n        # Calcular residuos\n        residuals = y_test[target] - y_pred[target]\n        \n        # Histograma\n        ax.hist(residuals, bins=50, alpha=0.7, color=\'steelblue\', edgecolor=\'black\')\n        \n        # L√≠nea vertical en 0\n        ax.axvline(0, color=\'red\', linestyle=\'--\', linewidth=2, label=\'Residuo=0\')\n        \n        # Estad√≠sticas\n        mean_res = residuals.mean()\n        std_res = residuals.std()\n        \n        ax.set_title(f"{available_models[model_key][\'display_name\']}\\nŒº={mean_res:.2f}, œÉ={std_res:.2f}", \n                     fontsize=12, fontweight=\'bold\')\n        ax.set_xlabel(\'Residuos (Real - Predicho) [kNm]\', fontsize=11)\n        ax.set_ylabel(\'Frecuencia\', fontsize=11)\n        ax.legend(loc=\'upper right\', fontsize=9)\n        ax.grid(alpha=0.3)\n    \n    plt.suptitle(f\'Distribuci√≥n de Residuos - {target}\', fontsize=14, fontweight=\'bold\', y=1.02)\n    plt.tight_layout()\n    plt.savefig(output_dir / f\'comparison_residuals_{target}.png\', dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    print(f"   ‚úÖ Guardado: comparison_residuals_{target}.png")\n\n# [4/5] Boxplot de residuos absolutos\nprint("[4/5] Creando boxplots de residuos...")\nfig, axes = plt.subplots(1, len(targets), figsize=(6*len(targets), 6))\nif len(targets) == 1:\n    axes = [axes]\n\nfor idx, target in enumerate(targets):\n    ax = axes[idx]\n    \n    # Preparar datos para boxplot\n    residuals_data = []\n    labels = []\n    for model_key, y_pred in all_predictions.items():\n        residuals = np.abs(y_test[target] - y_pred[target])\n        residuals_data.append(residuals)\n        labels.append(available_models[model_key][\'display_name\'])\n    \n    # Boxplot\n    bp = ax.boxplot(residuals_data, labels=labels, patch_artist=True)\n    \n    # Colores\n    colors = [\'#3498db\', \'#e74c3c\', \'#2ecc71\', \'#f39c12\'][:len(residuals_data)]\n    for patch, color in zip(bp[\'boxes\'], colors):\n        patch.set_facecolor(color)\n        patch.set_alpha(0.7)\n    \n    ax.set_title(f\'Residuos Absolutos - {target}\', fontsize=14, fontweight=\'bold\')\n    ax.set_ylabel(\'|Residuo| [kNm]\', fontsize=12)\n    ax.grid(axis=\'y\', alpha=0.3)\n    ax.tick_params(axis=\'x\', rotation=45)\n\nplt.tight_layout()\nplt.savefig(output_dir / \'comparison_residuals_boxplot.png\', dpi=300, bbox_inches=\'tight\')\nplt.close()\nprint("   ‚úÖ Guardado: comparison_residuals_boxplot.png")\n\n# [5/5] Heatmap de m√©tricas\nprint("[5/5] Creando heatmap de m√©tricas...")\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\nmetrics_list = [\'R2\', \'RMSE\', \'MAE\', \'MSE\']\nfor idx, metric in enumerate(metrics_list):\n    ax = axes[idx]\n    \n    # Crear matriz para heatmap\n    pivot_metric = df_comparison.pivot(index=\'Model\', columns=\'Target\', values=metric)\n    \n    # Heatmap\n    if metric == \'R2\':\n        sns.heatmap(pivot_metric, annot=True, fmt=\'.4f\', cmap=\'RdYlGn\', \n                    vmin=0, vmax=1, ax=ax, cbar_kws={\'label\': metric}, linewidths=0.5)\n    else:\n        sns.heatmap(pivot_metric, annot=True, fmt=\'.2f\', cmap=\'RdYlGn_r\', \n                    ax=ax, cbar_kws={\'label\': metric}, linewidths=0.5)\n    \n    ax.set_title(f\'{metric} por Modelo y Target\', fontsize=14, fontweight=\'bold\')\n    ax.set_xlabel(\'Target\', fontsize=11)\n    ax.set_ylabel(\'Modelo\', fontsize=11)\n\nplt.suptitle(\'Comparaci√≥n de M√©tricas - Todos los Modelos\', fontsize=16, fontweight=\'bold\', y=1.02)\nplt.tight_layout()\nplt.savefig(output_dir / \'comparison_metrics_heatmap.png\', dpi=300, bbox_inches=\'tight\')\nplt.close()\nprint("   ‚úÖ Guardado: comparison_metrics_heatmap.png")\n\nprint()\n\n# ================================================================================\n# RESUMEN FINAL\n# ================================================================================\nprint("=" * 80)\nprint("‚úÖ COMPARACI√ìN COMPLETADA")\nprint("=" * 80)\nprint()\nprint(f"üìÅ Archivos generados en: {output_dir}")\nprint()\nprint("üìÑ Archivos creados:")\nprint("   ‚Ä¢ model_comparison_metrics.csv")\nprint("   ‚Ä¢ comparison_r2_scores.png")\nfor target in targets:\n    print(f"   ‚Ä¢ comparison_scatter_{target}.png")\n    print(f"   ‚Ä¢ comparison_residuals_{target}.png")\nprint("   ‚Ä¢ comparison_residuals_boxplot.png")\nprint("   ‚Ä¢ comparison_metrics_heatmap.png")\nprint()\nprint("üéØ Mejor modelo por target (R¬≤ Score):")\nfor target in targets:\n    best_model = df_comparison[df_comparison[\'Target\'] == target].nlargest(1, \'R2\')\n    print(f"   ‚Ä¢ {target}: {best_model[\'Model\'].values[0]} (R¬≤ = {best_model[\'R2\'].values[0]:.4f})")\nprint()\nprint("üí° Scalers independientes utilizados correctamente:")\nprint(f"   ‚Ä¢ X: Normalizaci√≥n columna por columna ({len(scalers_X)} scalers)")\nprint(f"   ‚Ä¢ y: Desnormalizaci√≥n columna por columna ({len(scalers_y)} scalers)")\nprint()\nprint("=" * 80)',
            '# ============================================================================\n# PASO 5.2: COMPARACI√ìN DE TODOS LOS MODELOS - SERIES TEMPORALES\n# ============================================================================\n"""\nEste script compara el comportamiento temporal de TODOS los modelos de ML:\n- Ridge\n- Random Forest\n- XGBoost\n- MLP\n\nGenera gr√°ficos de series temporales mostrando:\n- Series completas: Real vs Predicciones de cada modelo\n- Zoom 50 segundos: Detalle de los primeros 50 segundos\n- Gr√°ficos individuales por modelo\n- Gr√°ficos conjuntos con todos los modelos superpuestos\n\nIMPORTANTE: Adaptado para scalers independientes por columna\n- scalers_X.pkl: Diccionario con un scaler por feature\n- scalers_y.pkl: Diccionario con un scaler por target\n\nAutor: Adaptado para scalers independientes\nFecha: Enero 2026\n============================================================================\n"""\n\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# Configuraci√≥n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\n# Configuraci√≥n de matplotlib\nplt.rcParams[\'figure.dpi\'] = 100\nplt.rcParams[\'savefig.dpi\'] = 300\nplt.rcParams[\'font.size\'] = 10\nsns.set_style("whitegrid")\n\nprint("\\n" + "="*80)\nprint("üìä COMPARACI√ìN DE TODOS LOS MODELOS - SERIES TEMPORALES")\nprint("="*80)\nprint(f"üìÖ Fecha: {datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')}")\nprint("="*80)\n\n# ============================================================================\n# 1. CONFIGURACI√ìN Y DETECCI√ìN DE MODELOS\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("PASO 1: CONFIGURACI√ìN Y DETECCI√ìN DE MODELOS")\nprint("="*80)\n\n# Directorio ra√≠z del proyecto\nroot_dir = Path(r"C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub")\nmodels_folder = root_dir / "notebook" / "03_ML_traditional_models"\ndata_ml_folder = root_dir / "notebook" / "02_Data_ML_traditional"\nscaler_folder = root_dir / "notebook" / "01_Models_scaler"\ncomparison_folder = root_dir / "notebook" / "04_ML_all_models"\n\n# Crear carpeta de salida si no existe\ncomparison_folder.mkdir(parents=True, exist_ok=True)\n\nprint(f"\\n[1/2] Detectando modelos disponibles...")\n\n# Mapeo de carpetas a archivos de modelo con flag de normalizaci√≥n\nmodel_mapping = {\n    \'Linear_Ridge\': {\n        \'file\': \'ridge_model.pkl\',\n        \'requires_normalization\': True\n    },\n    \'Random_Forest\': {\n        \'file\': \'random_forest_model.pkl\',\n        \'requires_normalization\': True\n    },\n    \'XGBoost\': {\n        \'file\': \'xgboost_model.pkl\',\n        \'requires_normalization\': True\n    },\n    \'MLP\': {\n        \'file\': \'mlp_model.pkl\',\n        \'requires_normalization\': True\n    },\n    \'XGBoost_NoNorm\': {\n        \'file\': \'xgboost_nonorm_model.pkl\',\n        \'requires_normalization\': False\n    }\n}\n\n# Detectar qu√© modelos existen\navailable_models = {}\nfor folder_name, model_info in model_mapping.items():\n    model_path = models_folder / folder_name / model_info[\'file\']\n    if model_path.exists():\n        available_models[folder_name] = {\n            \'path\': model_path,\n            \'requires_normalization\': model_info[\'requires_normalization\']\n        }\n        norm_status = "CON normalizaci√≥n" if model_info[\'requires_normalization\'] else "SIN normalizaci√≥n"\n        print(f"   ‚úÖ {folder_name}: {model_info[\'file\']} ({norm_status})")\n    else:\n        print(f"   ‚ö†Ô∏è  {folder_name}: {model_info[\'file\']} NO encontrado")\n\nif len(available_models) == 0:\n    print(f"\\n   ‚ùå ERROR: No se encontraron modelos entrenados")\n    sys.exit(1)\n\nprint(f"\\n   üí° Total modelos detectados: {len(available_models)}")\n\nprint(f"\\n[2/2] Definiendo colores √∫nicos para cada modelo...")\n\n# Colores consistentes (iguales que en validaci√≥n)\nmodel_colors = {\n    \'Linear_Ridge\': \'#3498db\',      # Azul brillante\n    \'Random_Forest\': "#28df10",     # Verde esmeralda\n    \'XGBoost\': \'#eef11a\',           # Amarillo\n    \'MLP\': \'#e74c3c\',               # Rojo\n    \'XGBoost_NoNorm\': \'#2ecc71\'     # Verde (sin normalizaci√≥n)\n}\n\nfor model_name in available_models.keys():\n    print(f"   ‚Ä¢ {model_name}: {model_colors[model_name]}")\n\n# ============================================================================\n# 2. CARGAR TIME_TEST Y GENERAR SERIES_ID_TEST\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("PASO 2: PREPARAR DATOS PARA TIME SERIES")\nprint("="*80)\n\nprint(f"\\n[1/4] Cargando Time_test desde archivo pkl...")\n\nTime_test_path = data_ml_folder / \'Time_test.pkl\'\n\nif Time_test_path.exists():\n    Time_test = joblib.load(Time_test_path)\n    print(f"   ‚úÖ Time_test cargado: {len(Time_test):,} valores")\n    print(f"   ‚Ä¢ Tiempo m√≠nimo: {Time_test.min():.2f}s")\n    print(f"   ‚Ä¢ Tiempo m√°ximo: {Time_test.max():.2f}s")\nelse:\n    print(f"   ‚ùå ERROR: No se encontr√≥ {Time_test_path}")\n    raise FileNotFoundError(f"Archivo requerido no encontrado: {Time_test_path}")\n\nprint(f"\\n[2/4] Generando series_id_test a partir de Time_test...")\n\n# Detectar inicio de cada serie (cuando Time reinicia, es decir, disminuye)\nseries_id_test_values = np.zeros(len(Time_test), dtype=int)\ncurrent_series = 0\n\nTime_test_array = Time_test.values\nfor i in range(1, len(Time_test_array)):\n    if Time_test_array[i] < Time_test_array[i-1]:\n        current_series += 1\n    series_id_test_values[i] = current_series\n\n# Convertir a pandas Series con el mismo index que Time_test\nseries_id_test = pd.Series(series_id_test_values, index=Time_test.index, name=\'series_id\')\n\nn_test_series = series_id_test.max() + 1\n\nprint(f"   ‚úÖ Series temporales identificadas en test: {n_test_series}")\n\n# Analizar cada serie\nprint(f"\\n   üìä Resumen de series en TEST:")\nfor sid in range(min(5, n_test_series)):\n    mask = series_id_test == sid\n    n_rows = mask.sum()\n    time_min = Time_test.loc[mask].min()\n    time_max = Time_test.loc[mask].max()\n    print(f"      Serie {sid:3d}: {n_rows:6,} filas | Time: {time_min:6.1f}s ‚Üí {time_max:6.1f}s")\n\nif n_test_series > 5:\n    print(f"      ... y {n_test_series - 5} series m√°s")\n\nprint(f"\\n[3/4] Cargando datos de test originales (sin normalizar)...")\n\nX_test = joblib.load(data_ml_folder / \'X_test.pkl\')\ny_test = joblib.load(data_ml_folder / \'y_test.pkl\')\nprint(f"   ‚úÖ X_test: {X_test.shape}")\nprint(f"   ‚úÖ y_test: {y_test.shape}")\n\ntarget_names = y_test.columns.tolist()\nprint(f"   üí° Targets: {target_names}")\n\nprint(f"\\n[4/4] Verificando √≠ndices...")\n\n# Verificar que los √≠ndices coinciden\nif not all(series_id_test.index == y_test.index):\n    print(f"   ‚ö†Ô∏è  Ajustando √≠ndices para que coincidan...")\n    series_id_test = series_id_test.reindex(y_test.index)\n    Time_test = Time_test.reindex(y_test.index)\n\nprint(f"   ‚úÖ √çndices verificados:")\nprint(f"      ‚Ä¢ y_test: {y_test.shape[0]:,} filas")\nprint(f"      ‚Ä¢ series_id_test: {len(series_id_test):,} valores")\nprint(f"      ‚Ä¢ Time_test: {len(Time_test):,} valores")\n\n# ============================================================================\n# 3. CARGAR SCALERS Y NORMALIZAR X_TEST\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("PASO 3: CARGAR SCALERS Y NORMALIZAR DATOS")\nprint("="*80)\n\nprint(f"\\n[1/3] Cargando scalers independientes...")\nscalers_X = joblib.load(scaler_folder / \'scalers_X.pkl\')\nscalers_y = joblib.load(scaler_folder / \'scalers_y.pkl\')\nprint(f"   ‚úÖ Scalers independientes cargados:")\nprint(f"      ‚Ä¢ scalers_X: {len(scalers_X)} scalers (uno por feature)")\nprint(f"      ‚Ä¢ scalers_y: {len(scalers_y)} scalers (uno por target)")\n\nprint(f"\\n[2/3] Normalizando X_test con scalers independientes...")\nX_test_norm = pd.DataFrame(index=X_test.index, columns=X_test.columns)\n\nfor col in X_test.columns:\n    if col in scalers_X:\n        X_test_norm[col] = scalers_X[col].transform(X_test[[col]])\n    else:\n        print(f"   ‚ö†Ô∏è  Columna {col} no tiene scaler, usando valores originales")\n        X_test_norm[col] = X_test[col]\n\nX_test_norm = X_test_norm.astype(\'float32\')\nprint(f"   ‚úÖ X_test normalizado columna por columna: {X_test_norm.shape}")\n\nprint(f"\\n[3/3] Verificando scalers disponibles para targets...")\nmissing_scalers = [t for t in target_names if t not in scalers_y]\nif missing_scalers:\n    print(f"   ‚ö†Ô∏è  WARNING: Faltan scalers para: {missing_scalers}")\nelse:\n    print(f"   ‚úÖ Todos los targets tienen scaler: {list(scalers_y.keys())}")\n\n# ============================================================================\n# 4. GENERAR PREDICCIONES PARA TODOS LOS MODELOS\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("PASO 4: CARGAR MODELOS Y GENERAR PREDICCIONES")\nprint("="*80)\n\n# Diccionario para almacenar predicciones (desnormalizadas)\npredictions = {}\n\nfor idx, (model_name, model_info) in enumerate(available_models.items(), 1):\n    print(f"\\n   [{idx}/{len(available_models)}] Procesando modelo: {model_name}")\n    \n    try:\n        # [1/4] Cargar modelo\n        print(f"      [1/4] Cargando modelo...")\n        model = joblib.load(model_info[\'path\'])\n        print(f"         ‚úÖ Modelo cargado desde: {model_info[\'path\'].name}")\n        \n        # [2/4] Determinar si requiere normalizaci√≥n\n        requires_norm = model_info[\'requires_normalization\']\n        print(f"      [2/4] Verificando necesidad de normalizaci√≥n...")\n        if requires_norm:\n            print(f"         üí° Modelo CON normalizaci√≥n - usando X_test_norm")\n            X_to_use = X_test_norm\n        else:\n            print(f"         üí° Modelo SIN normalizaci√≥n - usando X_test original")\n            X_to_use = X_test\n        \n        # [3/4] Generar predicciones\n        print(f"      [3/4] Generando predicciones...")\n        y_test_pred_raw = model.predict(X_to_use)\n        \n        # Convertir a DataFrame si es necesario\n        if not isinstance(y_test_pred_raw, pd.DataFrame):\n            y_test_pred_raw = pd.DataFrame(\n                y_test_pred_raw,\n                index=y_test.index,\n                columns=y_test.columns\n            )\n        \n        print(f"         ‚úÖ Predicciones generadas: {y_test_pred_raw.shape}")\n        \n        # [4/4] Desnormalizar predicciones si es necesario\n        print(f"      [4/4] Procesando predicciones...")\n        if requires_norm:\n            # Desnormalizar columna por columna\n            print(f"         üîÑ Desnormalizando predicciones con scalers independientes...")\n            y_test_pred = pd.DataFrame(index=y_test_pred_raw.index, columns=y_test_pred_raw.columns)\n            \n            for target in target_names:\n                if target in scalers_y:\n                    y_test_pred[target] = scalers_y[target].inverse_transform(y_test_pred_raw[[target]])\n                else:\n                    print(f"         ‚ö†Ô∏è  WARNING: No hay scaler para {target}, usando valores normalizados")\n                    y_test_pred[target] = y_test_pred_raw[target]\n            \n            print(f"         ‚úÖ Predicciones desnormalizadas columna por columna: {y_test_pred.shape}")\n        else:\n            # Sin normalizaci√≥n, usar predicciones directamente\n            y_test_pred = y_test_pred_raw\n            print(f"         ‚úÖ Predicciones ya en escala original (sin normalizaci√≥n)")\n        \n        # Verificar rangos\n        print(f"      üí° Verificando rangos de valores:")\n        for target in target_names:\n            print(f"         ‚Ä¢ {target}: min={y_test_pred[target].min():.2f}, max={y_test_pred[target].max():.2f}, mean={y_test_pred[target].mean():.2f}")\n        \n        predictions[model_name] = y_test_pred\n        print(f"      ‚úÖ {model_name} completado")\n        \n    except Exception as e:\n        print(f"      ‚ùå Error procesando {model_name}: {e}")\n        continue\n\nprint(f"\\n‚úÖ Predicciones completadas para {len(predictions)} modelos")\n\n# ============================================================================\n# 5. SELECCIONAR SERIES PARA VISUALIZAR\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("PASO 5: SELECCIONAR SERIES PARA VISUALIZAR")\nprint("="*80)\n\n# Obtener series √∫nicas del conjunto de test\nunique_test_series = series_id_test.unique()\n\n# Seleccionar 3 series aleatorias\nnp.random.seed(42)  # Para reproducibilidad\nselected_series = np.random.choice(unique_test_series, size=min(3, len(unique_test_series)), replace=False)\n\nprint(f"\\n   ‚Ä¢ Total series disponibles: {len(unique_test_series)}")\nprint(f"   ‚Ä¢ Series seleccionadas para graficar: {selected_series}")\n\n# Detectar n√∫mero de targets autom√°ticamente\nn_targets = len(target_names)\nprint(f"   ‚Ä¢ N√∫mero de targets: {n_targets}")\n\n# ============================================================================\n# 6. GR√ÅFICOS INDIVIDUALES: SERIES COMPLETAS POR MODELO\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("PASO 6: GR√ÅFICOS INDIVIDUALES - SERIES COMPLETAS")\nprint("="*80)\n\nfor model_name, y_pred in predictions.items():\n    print(f"\\n   [{model_name}] Generando gr√°fico de series completas...")\n    \n    # Crear figura con 3 filas x n_targets columnas\n    fig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n    \n    # Asegurar que axes sea 2D\n    if n_targets == 1:\n        axes = axes.reshape(-1, 1)\n    \n    for plot_idx, series_num in enumerate(selected_series):\n        # Filtrar datos de esta serie\n        series_mask = series_id_test == series_num\n        series_indices = series_mask[series_mask].index\n        \n        # Obtener tiempo\n        time_series = Time_test.loc[series_indices]\n        \n        # Para cada target, graficar en subplot separado\n        for target_idx, col in enumerate(target_names):\n            ax = axes[plot_idx, target_idx]\n            \n            # Valores reales y predichos\n            y_real = y_test.loc[series_indices, col]\n            y_pred_col = y_pred.loc[series_indices, col]\n            \n            # Calcular m√©tricas para esta serie y target\n            r2_series = r2_score(y_real, y_pred_col)\n            rmse_series = np.sqrt(mean_squared_error(y_real, y_pred_col))\n            \n            # Graficar\n            ax.plot(time_series, y_real, label=\'Real\', \n                    linewidth=2, alpha=0.8, color=\'blue\')\n            ax.plot(time_series, y_pred_col, label=f\'Predicho ({model_name})\', \n                    linestyle=\'--\', linewidth=2, alpha=0.8, color=model_colors[model_name])\n            \n            # Configurar subplot\n            ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n            ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n            ax.set_title(f\'Serie {series_num} - {col}\\nR¬≤={r2_series:.4f}, RMSE={rmse_series:.2f}\', \n                         fontsize=11, fontweight=\'bold\')\n            ax.legend(loc=\'best\', fontsize=9)\n            ax.grid(True, alpha=0.3)\n    \n    # T√≠tulo general\n    fig.suptitle(f\'TIME SERIES COMPLETAS - {model_name.upper()}\', \n                 fontsize=14, fontweight=\'bold\', y=1.0)\n    \n    plt.tight_layout()\n    \n    # Guardar\n    timeseries_path = comparison_folder / f\'timeseries_{model_name.lower()}.png\'\n    plt.savefig(timeseries_path, dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    \n    print(f"      ‚úÖ Guardado: {timeseries_path.name}")\n\n# ============================================================================\n# 7. GR√ÅFICOS INDIVIDUALES: ZOOM 50 SEGUNDOS POR MODELO\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("PASO 7: GR√ÅFICOS INDIVIDUALES - ZOOM 50 SEGUNDOS")\nprint("="*80)\n\nfor model_name, y_pred in predictions.items():\n    print(f"\\n   [{model_name}] Generando gr√°fico con zoom...")\n    \n    # Crear figura con 3 filas x n_targets columnas\n    fig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n    \n    # Asegurar que axes sea 2D\n    if n_targets == 1:\n        axes = axes.reshape(-1, 1)\n    \n    for plot_idx, series_num in enumerate(selected_series):\n        # Filtrar datos de esta serie\n        series_mask = series_id_test == series_num\n        series_indices = series_mask[series_mask].index\n        \n        # Obtener tiempo\n        time_series = Time_test.loc[series_indices]\n        \n        # Definir ventana de 50 segundos desde el inicio\n        time_min = time_series.min()\n        time_max_zoom = time_min + 50\n        \n        # Filtrar por ventana de tiempo\n        zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n        zoom_indices = time_series[zoom_mask].index\n        time_zoom = time_series[zoom_mask]\n        \n        # Para cada target, graficar en subplot separado\n        for target_idx, col in enumerate(target_names):\n            ax = axes[plot_idx, target_idx]\n            \n            # Valores reales y predichos (con zoom)\n            y_real_zoom = y_test.loc[zoom_indices, col]\n            y_pred_zoom = y_pred.loc[zoom_indices, col]\n            \n            # Calcular m√©tricas para esta ventana\n            r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n            rmse_zoom = np.sqrt(mean_squared_error(y_real_zoom, y_pred_zoom))\n            \n            # Graficar\n            ax.plot(time_zoom, y_real_zoom, label=\'Real\', \n                    linewidth=2.5, alpha=0.8, color=\'blue\', marker=\'o\', markersize=4)\n            ax.plot(time_zoom, y_pred_zoom, label=f\'Predicho ({model_name})\', \n                    linestyle=\'--\', linewidth=2.5, alpha=0.8, color=model_colors[model_name], \n                    marker=\'x\', markersize=5)\n            \n            # Configurar subplot\n            ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n            ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n            ax.set_title(f\'Serie {series_num} - {col} (Zoom: 0-50s)\\nR¬≤={r2_zoom:.4f}, RMSE={rmse_zoom:.2f}\', \n                         fontsize=11, fontweight=\'bold\')\n            ax.legend(loc=\'best\', fontsize=9)\n            ax.grid(True, alpha=0.3)\n            \n            # A√±adir texto con informaci√≥n de puntos\n            n_points = len(zoom_indices)\n            ax.text(0.02, 0.02, f\'Puntos: {n_points}\', transform=ax.transAxes, fontsize=9,\n                    verticalalignment=\'bottom\', bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.5))\n    \n    # T√≠tulo general\n    fig.suptitle(f\'TIME SERIES ZOOM 50s - {model_name.upper()}\', \n                 fontsize=14, fontweight=\'bold\', y=1.0)\n    \n    plt.tight_layout()\n    \n    # Guardar\n    timeseries_zoom_path = comparison_folder / f\'timeseries_zoom50s_{model_name.lower()}.png\'\n    plt.savefig(timeseries_zoom_path, dpi=300, bbox_inches=\'tight\')\n    plt.close()\n    \n    print(f"      ‚úÖ Guardado: {timeseries_zoom_path.name}")\n\n# ============================================================================\n# 8. GR√ÅFICO CONJUNTO: SERIES COMPLETAS - TODOS LOS MODELOS\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("PASO 8: GR√ÅFICO CONJUNTO - SERIES COMPLETAS CON TODOS LOS MODELOS")\nprint("="*80)\n\nprint(f"\\n   Creando gr√°fico con todos los modelos superpuestos...")\n\n# Crear figura con 3 filas x n_targets columnas\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(target_names):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales (solo una vez)\n        y_real = y_test.loc[series_indices, col]\n        ax.plot(time_series, y_real, label=\'Real\', \n                linewidth=2.5, alpha=0.9, color=\'black\', zorder=10)\n        \n        # Graficar predicciones de cada modelo\n        for model_name, y_pred in predictions.items():\n            y_pred_col = y_pred.loc[series_indices, col]\n            \n            # Calcular R¬≤ para la leyenda\n            r2_series = r2_score(y_real, y_pred_col)\n            \n            ax.plot(time_series, y_pred_col, \n                    label=f\'{model_name} (R¬≤={r2_series:.3f})\', \n                    linestyle=\'--\', linewidth=2, alpha=0.7, \n                    color=model_colors[model_name])\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col}\', \n                     fontsize=11, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=8, framealpha=0.9)\n        ax.grid(True, alpha=0.3)\n\n# T√≠tulo general\nfig.suptitle(\'TIME SERIES COMPLETAS - COMPARACI√ìN DE TODOS LOS MODELOS\', \n             fontsize=15, fontweight=\'bold\', y=1.0)\n\nplt.tight_layout()\n\n# Guardar\ntimeseries_all_path = comparison_folder / \'timeseries_all_models_combined.png\'\nplt.savefig(timeseries_all_path, dpi=300, bbox_inches=\'tight\')\nplt.close()\n\nprint(f"   ‚úÖ Guardado: {timeseries_all_path.name}")\n\n# ============================================================================\n# 9. GR√ÅFICO CONJUNTO: ZOOM 50s - TODOS LOS MODELOS\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("PASO 9: GR√ÅFICO CONJUNTO - ZOOM 50s CON TODOS LOS MODELOS")\nprint("="*80)\n\nprint(f"\\n   Creando gr√°fico con zoom y todos los modelos superpuestos...")\n\n# Crear figura con 3 filas x n_targets columnas\nfig, axes = plt.subplots(3, n_targets, figsize=(6*n_targets, 13.5))\n\n# Asegurar que axes sea 2D\nif n_targets == 1:\n    axes = axes.reshape(-1, 1)\n\nfor plot_idx, series_num in enumerate(selected_series):\n    # Filtrar datos de esta serie\n    series_mask = series_id_test == series_num\n    series_indices = series_mask[series_mask].index\n    \n    # Obtener tiempo\n    time_series = Time_test.loc[series_indices]\n    \n    # Definir ventana de 50 segundos desde el inicio\n    time_min = time_series.min()\n    time_max_zoom = time_min + 50\n    \n    # Filtrar por ventana de tiempo\n    zoom_mask = (time_series >= time_min) & (time_series <= time_max_zoom)\n    zoom_indices = time_series[zoom_mask].index\n    time_zoom = time_series[zoom_mask]\n    \n    # Para cada target, graficar en subplot separado\n    for target_idx, col in enumerate(target_names):\n        ax = axes[plot_idx, target_idx]\n        \n        # Valores reales (solo una vez)\n        y_real_zoom = y_test.loc[zoom_indices, col]\n        ax.plot(time_zoom, y_real_zoom, label=\'Real\', \n                linewidth=2.5, alpha=0.9, color=\'black\', \n                marker=\'o\', markersize=3, zorder=10)\n        \n        # Graficar predicciones de cada modelo\n        for model_name, y_pred in predictions.items():\n            y_pred_zoom = y_pred.loc[zoom_indices, col]\n            \n            # Calcular R¬≤ para la leyenda\n            r2_zoom = r2_score(y_real_zoom, y_pred_zoom)\n            \n            ax.plot(time_zoom, y_pred_zoom, \n                    label=f\'{model_name} (R¬≤={r2_zoom:.3f})\', \n                    linestyle=\'--\', linewidth=2.5, alpha=0.7, \n                    color=model_colors[model_name],\n                    marker=\'x\', markersize=4)\n        \n        # Configurar subplot\n        ax.set_xlabel(\'Tiempo [s]\', fontsize=10)\n        ax.set_ylabel(\'Momento [kNm]\', fontsize=10)\n        ax.set_title(f\'Serie {series_num} - {col} (Zoom: 0-50s)\', \n                     fontsize=11, fontweight=\'bold\')\n        ax.legend(loc=\'best\', fontsize=8, framealpha=0.9)\n        ax.grid(True, alpha=0.3)\n        \n        # A√±adir texto con informaci√≥n de puntos\n        n_points = len(zoom_indices)\n        ax.text(0.02, 0.02, f\'Puntos: {n_points}\', transform=ax.transAxes, fontsize=9,\n                verticalalignment=\'bottom\', bbox=dict(boxstyle=\'round\', facecolor=\'wheat\', alpha=0.5))\n\n# T√≠tulo general\nfig.suptitle(\'TIME SERIES ZOOM 50s - COMPARACI√ìN DE TODOS LOS MODELOS\', \n             fontsize=15, fontweight=\'bold\', y=1.0)\n\nplt.tight_layout()\n\n# Guardar\ntimeseries_zoom_all_path = comparison_folder / \'timeseries_zoom50s_all_models_combined.png\'\nplt.savefig(timeseries_zoom_all_path, dpi=300, bbox_inches=\'tight\')\nplt.close()\n\nprint(f"   ‚úÖ Guardado: {timeseries_zoom_all_path.name}")\n\n# ============================================================================\n# RESUMEN FINAL\n# ============================================================================\nprint(f"\\n{\'=\'*80}")\nprint("‚úÖ COMPARACI√ìN DE SERIES TEMPORALES COMPLETADA")\nprint("="*80)\n\nprint(f"\\nüìä MODELOS COMPARADOS: {len(predictions)}")\nfor model_name in predictions.keys():\n    print(f"   ‚Ä¢ {model_name}")\n\nprint(f"\\nüíæ GR√ÅFICOS GENERADOS EN: {comparison_folder}")\nprint(f"   ‚Ä¢ Time series individuales (completas): {len(predictions)} archivos")\nprint(f"   ‚Ä¢ Time series individuales (zoom 50s): {len(predictions)} archivos")\nprint(f"   ‚Ä¢ Time series conjunto (completas): 1 archivo")\nprint(f"   ‚Ä¢ Time series conjunto (zoom 50s): 1 archivo")\nprint(f"   ‚Ä¢ TOTAL: {len(predictions) * 2 + 2} gr√°ficos")\n\nprint(f"\\nüí° Scalers independientes utilizados correctamente:")\nprint(f"   ‚Ä¢ X: Normalizaci√≥n columna por columna ({len(scalers_X)} scalers)")\nprint(f"   ‚Ä¢ y: Desnormalizaci√≥n columna por columna ({len(scalers_y)} scalers)")\n\nprint(f"\\nüéØ SIGUIENTE PASO:")\nprint(f"   Paso 5.3: Comparaci√≥n con baseline 1P para todos los modelos")\n\nprint("\\n" + "="*80)',
        ],
    },
    {
        "title": 'ANALISIS IMPORTANCIA DE FEATURES EN LOS MODELOS',
        "cells": [
            '# ================================================================================\n# üìä AN√ÅLISIS DE IMPORTANCIA DE FEATURES - SIN REENTRENAMIENTO\n# ================================================================================\n"""\nExtrae la importancia de features de modelos ya entrenados.\n\nM√âTODOS DISPONIBLES:\n1. Tree-based models (XGBoost, Random Forest): feature_importances_\n2. Linear models (Ridge): coeficientes absolutos\n3. MLP: Permutation Importance (requiere datos de test)\n\nIMPORTANTE:\n- Los datos normalizados NO est√°n guardados\n- Se generan usando scalers_X.pkl y scalers_y.pkl (DICCIONARIOS con scaler por columna)\n- XGBoost_NoNorm NO usa normalizaci√≥n\n"""\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport joblib\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\nprint("=" * 80)\nprint("üìä AN√ÅLISIS DE IMPORTANCIA DE FEATURES")\nprint("=" * 80)\nprint("üîç M√©todo: Extracci√≥n desde modelos guardados (SIN reentrenamiento)")\nprint("=" * 80)\n\n# ================================================================================\n# CONFIGURACI√ìN\n# ================================================================================\n\n# Rutas\nroot_dir = Path(r"C:\\\\Users\\\\Bladedgreen\\\\Desktop\\\\_GitHub")\nmodels_dir = root_dir / \'notebook\' / \'03_ML_traditional_models\'\ndata_dir = root_dir / \'notebook\' / \'02_Data_ML_traditional\'\nscaler_folder = root_dir / \'notebook\' / \'01_Models_scaler\'\noutput_dir = root_dir / \'notebook\' / \'06_Feature_Importance\'\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Modelos disponibles\nmodel_info = {\n    \'Linear_Ridge\': {\n        \'folder\': \'Linear_Ridge\',\n        \'file\': \'ridge_model.pkl\',\n        \'display_name\': \'Linear Ridge\',\n        \'requires_normalization\': True,\n        \'method\': \'coefficients\'\n    },\n    \'Random_Forest\': {\n        \'folder\': \'Random_Forest\',\n        \'file\': \'random_forest_model.pkl\',\n        \'display_name\': \'Random Forest\',\n        \'requires_normalization\': True,\n        \'method\': \'feature_importances\'\n    },\n    \'XGBoost\': {\n        \'folder\': \'XGBoost\',\n        \'file\': \'xgboost_model.pkl\',\n        \'display_name\': \'XGBoost\',\n        \'requires_normalization\': True,\n        \'method\': \'feature_importances\'\n    },\n    \'XGBoost_NoNorm\': {\n        \'folder\': \'XGBoost_NoNorm\',\n        \'file\': \'xgboost_nonorm_model.pkl\',\n        \'display_name\': \'XGBoost (No Norm)\',\n        \'requires_normalization\': False,\n        \'method\': \'feature_importances\'\n    },\n    \'MLP\': {\n        \'folder\': \'MLP\',\n        \'file\': \'mlp_model.pkl\',\n        \'display_name\': \'MLP\',\n        \'requires_normalization\': True,\n        \'method\': \'permutation\'\n    }\n}\n\n# Targets\ntarget_cols = [\'M_0\', \'M_1c\', \'M_1s\']\n\n# ================================================================================\n# PASO 1: CARGAR DATOS DE TEST (SIN NORMALIZAR)\n# ================================================================================\nprint("\\n[1/6] Cargando datos de test (sin normalizar)...")\n\nX_test = joblib.load(data_dir / \'X_test.pkl\')\ny_test = joblib.load(data_dir / \'y_test.pkl\')\n\nprint(f"   ‚úÖ X_test: {X_test.shape}")\nprint(f"   ‚úÖ y_test: {y_test.shape}")\nprint(f"   Features: {list(X_test.columns)[:5]}... (total: {len(X_test.columns)})")\n\n# ================================================================================\n# PASO 2: CARGAR SCALERS INDIVIDUALES Y GENERAR DATOS NORMALIZADOS\n# ================================================================================\nprint("\\n[2/6] Cargando scalers individuales y generando datos normalizados...")\n\n# Cargar diccionarios de scalers (UN SCALER POR COLUMNA)\nscalers_X_path = scaler_folder / \'scalers_X.pkl\'\nscalers_y_path = scaler_folder / \'scalers_y.pkl\'\n\nif scalers_X_path.exists() and scalers_y_path.exists():\n    scalers_X = joblib.load(scalers_X_path)\n    scalers_y = joblib.load(scalers_y_path)\n    \n    print(f"   ‚úÖ scalers_X cargado: {len(scalers_X)} escaladores (uno por feature)")\n    print(f"   ‚úÖ scalers_y cargado: {len(scalers_y)} escaladores (uno por target)")\n    \n    # Generar X_test_norm aplicando scaler individual a cada columna\n    print(f"\\n   Generando X_test_norm (columna por columna)...")\n    X_test_norm = X_test.copy()\n    for col in X_test.columns:\n        if col in scalers_X:\n            # Aplicar scaler individual: transform requiere 2D, luego aplanar con ravel()\n            X_test_norm[col] = scalers_X[col].transform(X_test[[col]]).ravel()\n        else:\n            print(f"      ‚ö†Ô∏è  Scaler no encontrado para: {col}")\n    \n    print(f"   ‚úÖ X_test normalizado: {X_test_norm.shape}")\n    \n    # Generar y_test_norm aplicando scaler individual a cada columna\n    print(f"\\n   Generando y_test_norm (columna por columna)...")\n    y_test_norm = y_test.copy()\n    for col in y_test.columns:\n        if col in scalers_y:\n            y_test_norm[col] = scalers_y[col].transform(y_test[[col]]).ravel()\n        else:\n            print(f"      ‚ö†Ô∏è  Scaler no encontrado para: {col}")\n    \n    print(f"   ‚úÖ y_test normalizado: {y_test_norm.shape}")\n    \n    scalers_available = True\nelse:\n    print(f"   ‚ö†Ô∏è  Scalers NO encontrados")\n    print(f"      Buscados en: {scaler_folder}")\n    print(f"      - scalers_X.pkl")\n    print(f"      - scalers_y.pkl")\n    print(f"      Solo se analizar√°n modelos que NO requieren normalizaci√≥n")\n    scalers_available = False\n    X_test_norm = None\n    y_test_norm = None\n\n# ================================================================================\n# PASO 3: CARGAR MODELOS Y EXTRAER IMPORTANCIAS\n# ================================================================================\nprint("\\n[3/6] Cargando modelos y extrayendo importancias...")\n\nall_importances = {}  # {model_name: {target: importance_dict}}\n\nfor model_name, info in model_info.items():\n    model_path = models_dir / info[\'folder\'] / info[\'file\']\n    \n    # Verificar que el modelo existe\n    if not model_path.exists():\n        print(f"\\n   ‚ö†Ô∏è  {info[\'display_name\']}: Modelo no encontrado, SKIP")\n        continue\n    \n    # Verificar que tenemos los datos necesarios\n    if info[\'requires_normalization\'] and not scalers_available:\n        print(f"\\n   ‚ö†Ô∏è  {info[\'display_name\']}: Requiere normalizaci√≥n pero scalers no disponibles, SKIP")\n        continue\n    \n    print(f"\\n   üîç {info[\'display_name\']}:")\n    print(f"      M√©todo: {info[\'method\']}")\n    print(f"      Normalizaci√≥n: {\'S√≠\' if info[\'requires_normalization\'] else \'No\'}")\n    \n    # Cargar modelo\n    model = joblib.load(model_path)\n    \n    # Seleccionar datos apropiados\n    if info[\'requires_normalization\']:\n        X_data = X_test_norm\n        y_data = y_test_norm\n        print(f"      Usando datos normalizados")\n    else:\n        X_data = X_test\n        y_data = y_test\n        print(f"      Usando datos sin normalizar")\n    \n    # Inicializar diccionario para este modelo\n    all_importances[model_name] = {}\n    \n    # ============================================================================\n    # EXTRACCI√ìN DE IMPORTANCIA SEG√öN M√âTODO\n    # ============================================================================\n    \n    if info[\'method\'] == \'coefficients\':\n        # ========================================================================\n        # LINEAR MODELS: Coeficientes absolutos\n        # ========================================================================\n        print(f"      Extrayendo coeficientes...")\n        \n        # IMPORTANTE: Ridge tambi√©n est√° envuelto en MultiOutputRegressor\n        if hasattr(model, \'estimators_\'):\n            # MultiOutputRegressor - acceder a cada estimador individual\n            for i, target in enumerate(target_cols):\n                # Obtener coeficientes del estimador individual\n                coefs = np.abs(model.estimators_[i].coef_)  # Valor absoluto\n                \n                # Crear diccionario feature: importance\n                importance_dict = dict(zip(X_test.columns, coefs))\n                all_importances[model_name][target] = importance_dict\n                \n                top_feature = max(importance_dict, key=importance_dict.get)\n                print(f"         ‚Ä¢ {target}: Top = {top_feature[:30]:30s} ({importance_dict[top_feature]:.6f})")\n        else:\n            # Modelo directo (no MultiOutput)\n            for i, target in enumerate(target_cols):\n                coefs = np.abs(model.coef_[i])\n                importance_dict = dict(zip(X_test.columns, coefs))\n                all_importances[model_name][target] = importance_dict\n                \n                top_feature = max(importance_dict, key=importance_dict.get)\n                print(f"         ‚Ä¢ {target}: Top = {top_feature[:30]:30s} ({importance_dict[top_feature]:.6f})")\n    \n    elif info[\'method\'] == \'feature_importances\':\n        # ========================================================================\n        # TREE-BASED MODELS: feature_importances_\n        # ========================================================================\n        print(f"      Extrayendo feature_importances_...")\n        \n        # Para modelos MultiOutput, acceder a cada estimador\n        if hasattr(model, \'estimators_\'):\n            # MultiOutputRegressor (Random Forest)\n            for i, target in enumerate(target_cols):\n                importances = model.estimators_[i].feature_importances_\n                importance_dict = dict(zip(X_test.columns, importances))\n                all_importances[model_name][target] = importance_dict\n                \n                top_feature = max(importance_dict, key=importance_dict.get)\n                print(f"         ‚Ä¢ {target}: Top = {top_feature[:30]:30s} ({importance_dict[top_feature]:.6f})")\n        else:\n            # Modelo √∫nico con m√∫ltiples outputs (XGBoost directo)\n            importances = model.feature_importances_\n            \n            # Para XGBoost con m√∫ltiples outputs, la importancia es global\n            for target in target_cols:\n                importance_dict = dict(zip(X_test.columns, importances))\n                all_importances[model_name][target] = importance_dict\n            \n            top_feature = max(importance_dict, key=importance_dict.get)\n            print(f"         ‚Ä¢ Importancia global: Top = {top_feature[:30]:30s} ({importance_dict[top_feature]:.6f})")\n    \n    elif info[\'method\'] == \'permutation\':\n        # ========================================================================\n        # MLP: Permutation Importance (m√°s lento pero m√°s preciso)\n        # ========================================================================\n        print(f"      Calculando permutation importance (puede tardar 1-2 min)...")\n        \n        for i, target in enumerate(target_cols):\n            print(f"         Procesando {target}...", end=\' \')\n            \n            # Funci√≥n de scoring para un solo target\n            def target_scorer(model, X, y):\n                y_pred = model.predict(X)\n                return -mean_squared_error(y[:, i], y_pred[:, i])\n            \n            # Calcular permutation importance\n            result = permutation_importance(\n                model, X_data, y_data.values,\n                n_repeats=5,  # Reducido para velocidad\n                random_state=42,\n                scoring=target_scorer,\n                n_jobs=-1\n            )\n            \n            importance_dict = dict(zip(X_test.columns, result.importances_mean))\n            all_importances[model_name][target] = importance_dict\n            \n            top_feature = max(importance_dict, key=importance_dict.get)\n            print(f"Top = {top_feature[:30]:30s} ({importance_dict[top_feature]:.6f})")\n\nprint(f"\\n   ‚úÖ Importancias extra√≠das para {len(all_importances)} modelos")\n\n# ================================================================================\n# PASO 4: CREAR DATAFRAMES DE IMPORTANCIA\n# ================================================================================\nprint("\\n[4/6] Creando DataFrames de importancia...")\n\n# Para cada target, crear DataFrame con todas las importancias\nimportance_dfs = {}\n\nfor target in target_cols:\n    # Crear DataFrame: filas = features, columnas = modelos\n    data = {}\n    \n    for model_name in all_importances.keys():\n        if target in all_importances[model_name]:\n            data[model_info[model_name][\'display_name\']] = all_importances[model_name][target]\n    \n    if len(data) == 0:\n        print(f"   ‚ö†Ô∏è  {target}: Sin datos de importancia")\n        continue\n    \n    df = pd.DataFrame(data)\n    df.index.name = \'Feature\'\n    \n    # Ordenar por importancia promedio\n    df[\'Mean_Importance\'] = df.mean(axis=1)\n    df = df.sort_values(\'Mean_Importance\', ascending=False)\n    \n    importance_dfs[target] = df\n    \n    print(f"   ‚úÖ {target}: {df.shape[0]} features √ó {df.shape[1]-1} modelos")\n\n# ================================================================================\n# PASO 5: VISUALIZACIONES\n# ================================================================================\nprint("\\n[5/6] Generando visualizaciones...")\n\n# Configuraci√≥n de colores\ncolors = {\n    \'Linear Ridge\': \'#3498db\',\n    \'Random Forest\': \'#e74c3c\',\n    \'XGBoost\': \'#2ecc71\',\n    \'XGBoost (No Norm)\': \'#f39c12\',\n    \'MLP\': \'#9b59b6\'\n}\n\n# 5.1: Top 20 features por target (comparaci√≥n de modelos)\nprint("\\n   Generando heatmaps por target...")\nfor target in target_cols:\n    if target not in importance_dfs:\n        continue\n    \n    df = importance_dfs[target].drop(\'Mean_Importance\', axis=1).head(20)\n    \n    fig, ax = plt.subplots(figsize=(14, 8))\n    \n    # Heatmap\n    sns.heatmap(\n        df.T,  # Transponer: modelos en filas, features en columnas\n        annot=False,\n        cmap=\'YlOrRd\',\n        cbar_kws={\'label\': \'Importancia\'},\n        ax=ax,\n        linewidths=0.5\n    )\n    \n    ax.set_title(f\'Top 20 Features - {target}\', fontsize=16, fontweight=\'bold\', pad=20)\n    ax.set_xlabel(\'Features\', fontsize=12, fontweight=\'bold\')\n    ax.set_ylabel(\'Modelos\', fontsize=12, fontweight=\'bold\')\n    \n    plt.xticks(rotation=90, ha=\'right\')\n    plt.tight_layout()\n    \n    # Guardar\n    output_file = output_dir / f\'feature_importance_heatmap_{target}.png\'\n    plt.savefig(output_file, dpi=300, bbox_inches=\'tight\')\n    print(f"      ‚úÖ {output_file.name}")\n    plt.close()\n\n# 5.2: Barplot - Top 15 features por modelo\nprint("\\n   Generando barplots por modelo...")\nfor model_name, info in model_info.items():\n    if model_name not in all_importances:\n        continue\n    \n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n    fig.suptitle(f\'Top 15 Features - {info["display_name"]}\', \n                 fontsize=16, fontweight=\'bold\')\n    \n    for i, target in enumerate(target_cols):\n        if target not in all_importances[model_name]:\n            continue\n        \n        # Obtener top 15\n        imp_dict = all_importances[model_name][target]\n        top_15 = dict(sorted(imp_dict.items(), key=lambda x: x[1], reverse=True)[:15])\n        \n        # Barplot\n        features = list(top_15.keys())\n        values = list(top_15.values())\n        \n        color = colors.get(info[\'display_name\'], \'steelblue\')\n        axes[i].barh(features, values, color=color, alpha=0.7)\n        axes[i].set_xlabel(\'Importancia\', fontsize=11, fontweight=\'bold\')\n        axes[i].set_title(f\'{target}\', fontsize=13, fontweight=\'bold\')\n        axes[i].invert_yaxis()\n        axes[i].grid(axis=\'x\', alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # Guardar\n    output_file = output_dir / f\'feature_importance_barplot_{model_name}.png\'\n    plt.savefig(output_file, dpi=300, bbox_inches=\'tight\')\n    print(f"      ‚úÖ {output_file.name}")\n    plt.close()\n\n# 5.3: Comparaci√≥n consolidada - Media de todos los modelos\nprint("\\n   Generando visualizaci√≥n consolidada...")\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nfig.suptitle(\'Top 20 Features (Media de todos los modelos)\', \n             fontsize=16, fontweight=\'bold\')\n\nfor i, target in enumerate(target_cols):\n    if target not in importance_dfs:\n        continue\n    \n    df = importance_dfs[target]\n    top_20 = df.head(20)[\'Mean_Importance\']\n    \n    axes[i].barh(top_20.index, top_20.values, color=\'darkorange\', alpha=0.7)\n    axes[i].set_xlabel(\'Importancia Promedio\', fontsize=11, fontweight=\'bold\')\n    axes[i].set_title(f\'{target}\', fontsize=13, fontweight=\'bold\')\n    axes[i].invert_yaxis()\n    axes[i].grid(axis=\'x\', alpha=0.3)\n\nplt.tight_layout()\n\noutput_file = output_dir / \'feature_importance_consolidated.png\'\nplt.savefig(output_file, dpi=300, bbox_inches=\'tight\')\nprint(f"      ‚úÖ {output_file.name}")\nplt.close()\n\n# ================================================================================\n# PASO 6: GUARDAR RESULTADOS EN CSV\n# ================================================================================\nprint("\\n[6/6] Guardando resultados en CSV...")\n\nfor target in target_cols:\n    if target not in importance_dfs:\n        continue\n    \n    output_csv = output_dir / f\'feature_importance_{target}.csv\'\n    importance_dfs[target].to_csv(output_csv)\n    print(f"   ‚úÖ {output_csv.name}")\n\n# Resumen consolidado (top 50 global)\nif len(importance_dfs) > 0:\n    all_features_mean = {}\n    for target in target_cols:\n        if target not in importance_dfs:\n            continue\n        for feature in importance_dfs[target].index:\n            if feature not in all_features_mean:\n                all_features_mean[feature] = []\n            all_features_mean[feature].append(importance_dfs[target].loc[feature, \'Mean_Importance\'])\n\n    # Calcular media global\n    global_importance = {k: np.mean(v) for k, v in all_features_mean.items()}\n    global_df = pd.DataFrame({\n        \'Feature\': list(global_importance.keys()),\n        \'Global_Importance\': list(global_importance.values())\n    }).sort_values(\'Global_Importance\', ascending=False)\n\n    output_csv = output_dir / \'feature_importance_global.csv\'\n    global_df.to_csv(output_csv, index=False)\n    print(f"   ‚úÖ {output_csv.name}")\n\n# ================================================================================\n# RESUMEN FINAL\n# ================================================================================\nprint("\\n" + "=" * 80)\nprint("üìä RESUMEN DE IMPORTANCIA DE FEATURES")\nprint("=" * 80)\n\nif len(all_importances) > 0:\n    print(f"\\n‚úÖ Modelos analizados: {len(all_importances)}")\n    for model_name in all_importances.keys():\n        print(f"   ‚Ä¢ {model_info[model_name][\'display_name\']}")\n    \n    print("\\nüèÜ TOP 10 FEATURES GLOBALES (media de todos los modelos y targets):")\n    for i, row in global_df.head(10).iterrows():\n        print(f"   {i+1:2d}. {row[\'Feature\']:40s} ‚Üí {row[\'Global_Importance\']:.6f}")\n    \n    print("\\nüìä TOP 5 FEATURES POR TARGET:")\n    for target in target_cols:\n        if target not in importance_dfs:\n            continue\n        print(f"\\n   {target}:")\n        top_5 = importance_dfs[target].head(5)\n        for j, (feature, row) in enumerate(top_5.iterrows(), 1):\n            print(f"      {j}. {feature:40s} (importancia: {row[\'Mean_Importance\']:.6f})")\nelse:\n    print("\\n‚ö†Ô∏è  No se pudieron extraer importancias de ning√∫n modelo")\n    print("   Verifica que:")\n    print("   - Los modelos .pkl existan en las carpetas correctas")\n    print("   - Los scalers est√©n disponibles (scalers_X.pkl, scalers_y.pkl)")\n\nprint("\\n" + "=" * 80)\nprint(f"‚úÖ Resultados guardados en: {output_dir}")\nprint("=" * 80)',
        ],
    },
]

def run_section(title: str, section_runner: callable, global_ns: dict) -> None:
    print(f"\n====================\nSECCI√ìN: {title}\n====================")
    if not ask_yes_no(f"¬øEjecutar esta secci√≥n? -> {title}", default=True):
        print("Secci√≥n omitida por usuario.")
        return
    section_runner(global_ns)


def main() -> int:
    print("Ejecuci√≥n interactiva del workflow completo")
    print("Notebook fuente: ML_workflow_newTargets_PRETIME_HubLoads.ipynb")
    global_ns = {"__name__": "__main__"}

    section_functions = build_section_functions(SECTIONS)
    print(f"Secciones detectadas: {len(section_functions)}")

    for title, section_runner in section_functions:
        run_section(title, section_runner, global_ns)

    print("\nWorkflow finalizado.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
