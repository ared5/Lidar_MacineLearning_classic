{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b9e387",
   "metadata": {},
   "source": [
    "# üöÄ Machine Learning Workflow - Modelos Avanzados (LSTM, TCN, etc.)\n",
    "\n",
    "Notebook para implementar y evaluar modelos avanzados de series temporales para predicci√≥n de cargas en palas de turbinas e√≥licas.\n",
    "\n",
    "## üìã Objetivo\n",
    "\n",
    "Desarrollar modelos de **Deep Learning y t√©cnicas avanzadas** para mejorar:\n",
    "- **Reducir Phase Lag** (< 200ms objetivo)\n",
    "- **Mejorar NSE** (> 0.85 objetivo)\n",
    "- **Reducir Peak Error** (cr√≠tico para cargas)\n",
    "- **Mejor captura de din√°mica temporal**\n",
    "\n",
    "**Targets**: \n",
    "- Blade root 1 My\n",
    "- Blade root 2 My\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è **PLAN DE IMPLEMENTACI√ìN**\n",
    "\n",
    "### **FASE 1: FEATURES MEJORADAS** ‚ö° (R√°pido, alto impacto)\n",
    "1. ‚úÖ **STEP 1**: Configuraci√≥n inicial + imports\n",
    "2. ‚úÖ **STEP 2**: Cargar datos existentes\n",
    "3. üÜï **STEP 3**: Crear features temporales mejoradas:\n",
    "   - Lags VLOS reducidos: [0.5, 1, 2, 3, 5] segundos (vs [5-25]s actual)\n",
    "   - Derivadas: d(VLOS)/dt (velocidad de cambio)\n",
    "   - Aceleraciones: d¬≤(VLOS)/dt¬≤\n",
    "   - Rolling statistics: std, mean ventanas temporales\n",
    "4. üÜï **STEP 4**: Baseline - Re-entrenar XGBoost con features mejoradas\n",
    "   - Objetivo: Validar si features mejoran performance\n",
    "\n",
    "---\n",
    "\n",
    "### **FASE 2: LSTM/GRU** üß† (M√°ximo impacto esperado)\n",
    "5. üÜï **STEP 5**: Preparar datos en formato secuencial (ventanas temporales)\n",
    "   - Window size: 50 puntos (5 segundos @ 10 Hz)\n",
    "   - Sliding window approach\n",
    "6. üÜï **STEP 6**: Implementar arquitectura LSTM simple:\n",
    "   ```\n",
    "   Input(window_size, n_features)\n",
    "   ‚Üí LSTM(128 units, return_sequences=True)\n",
    "   ‚Üí Dropout(0.2)\n",
    "   ‚Üí LSTM(64 units)\n",
    "   ‚Üí Dropout(0.2)\n",
    "   ‚Üí Dense(2)  # 2 outputs (ambas palas)\n",
    "   ```\n",
    "7. üÜï **STEP 7**: Entrenar LSTM con callbacks:\n",
    "   - EarlyStopping\n",
    "   - ModelCheckpoint\n",
    "   - ReduceLROnPlateau\n",
    "8. üÜï **STEP 8**: Evaluar LSTM vs baseline\n",
    "\n",
    "---\n",
    "\n",
    "### **FASE 3: TEMPORAL CONVOLUTIONAL NETWORK (TCN)** ‚ö° (Baja latencia)\n",
    "9. üÜï **STEP 9**: Implementar TCN:\n",
    "   - Convoluciones causales (no mira al futuro)\n",
    "   - Dilated convolutions para receptive field grande\n",
    "   - Residual connections\n",
    "10. üÜï **STEP 10**: Entrenar y evaluar TCN\n",
    "\n",
    "---\n",
    "\n",
    "### **FASE 4: GRU (Variante LSTM m√°s r√°pida)** üèÉ\n",
    "11. üÜï **STEP 11**: Implementar GRU (similar LSTM pero m√°s eficiente)\n",
    "12. üÜï **STEP 12**: Comparaci√≥n LSTM vs GRU\n",
    "\n",
    "---\n",
    "\n",
    "### **FASE 5: ENSEMBLE & H√çBRIDOS** üé≠\n",
    "13. üÜï **STEP 13**: Ensemble weighted:\n",
    "    - Combinar mejores modelos con pesos optimizados\n",
    "14. üÜï **STEP 14**: Kalman Filter post-procesamiento:\n",
    "    - Filtrar predicciones con Kalman para reducir ruido\n",
    "\n",
    "---\n",
    "\n",
    "### **FASE 6: EVALUACI√ìN COMPLETA** üìä\n",
    "15. üÜï **STEP 15**: Comparaci√≥n exhaustiva:\n",
    "    - R¬≤, RMSE, MAE, NSE, MAPE\n",
    "    - Phase lag analysis\n",
    "    - FFT comparison\n",
    "    - Peak error\n",
    "16. üÜï **STEP 16**: Ranking y recomendaci√≥n final\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Estructura de Carpetas\n",
    "\n",
    "```\n",
    "notebook/\n",
    "‚îú‚îÄ ML_workflow.ipynb              # Original (modelos tradicionales)\n",
    "‚îú‚îÄ ML_workflow_newModels.ipynb    # Este notebook (modelos avanzados)\n",
    "‚îú‚îÄ 01_Models_scaler/              # Scalers (reusamos)\n",
    "‚îú‚îÄ 02_Models_newGeneration/       # NUEVOS MODELOS\n",
    "‚îÇ  ‚îú‚îÄ lstm_v1/\n",
    "‚îÇ  ‚îú‚îÄ gru_v1/\n",
    "‚îÇ  ‚îú‚îÄ tcn_v1/\n",
    "‚îÇ  ‚îú‚îÄ ensemble/\n",
    "‚îÇ  ‚îî‚îÄ xgboost_improved/\n",
    "‚îî‚îÄ 03_Results_newModels/          # Resultados comparativos\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivos Cuantitativos\n",
    "\n",
    "| M√©trica | Baseline Actual | Objetivo LSTM | Objetivo TCN |\n",
    "|---------|-----------------|---------------|-------------|\n",
    "| **NSE** | 0.68-0.72 | > 0.85 | > 0.83 |\n",
    "| **Phase Lag** | 700-800ms | < 200ms | < 100ms |\n",
    "| **Peak Error** | 2.5M kNm | < 500k kNm | < 500k kNm |\n",
    "| **MAE** | 730-830k kNm | < 400k kNm | < 400k kNm |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f4df2",
   "metadata": {},
   "source": [
    "## üîß STEP 1: Configuraci√≥n Inicial del Entorno\n",
    "\n",
    "### üì¶ Librer√≠as:\n",
    "\n",
    "1. **Librer√≠as est√°ndar**: os, sys, pathlib\n",
    "2. **An√°lisis de datos**: pandas, numpy\n",
    "3. **Visualizaci√≥n**: matplotlib, seaborn\n",
    "4. **ML Tradicional**: scikit-learn (baseline)\n",
    "5. **Deep Learning**: TensorFlow/Keras (LSTM, GRU, TCN)\n",
    "6. **Utilidades**: joblib, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89dccc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as est√°ndar importadas\n",
      "‚úÖ Librer√≠as de an√°lisis de datos importadas\n",
      "‚úÖ Librer√≠as de visualizaci√≥n importadas\n",
      "‚úÖ XGBoost importado\n",
      "‚úÖ TensorFlow importado - Usando CPU\n",
      "   Versi√≥n TensorFlow: 2.20.0\n",
      "‚úÖ tqdm importado (progress bars)\n",
      "\n",
      "================================================================================\n",
      "üéâ CONFIGURACI√ìN COMPLETADA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 1.1: Imports est√°ndar\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as est√°ndar importadas\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.2: An√°lisis de datos\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ Librer√≠as de an√°lisis de datos importadas\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.3: Visualizaci√≥n\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Librer√≠as de visualizaci√≥n importadas\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.4: Machine Learning tradicional (para baseline)\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"‚úÖ XGBoost importado\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost no disponible\")\n",
    "    xgb = None\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.5: Deep Learning (TensorFlow/Keras)\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models, callbacks\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import (\n",
    "        LSTM, GRU, Dense, Dropout, Input, \n",
    "        Conv1D, MaxPooling1D, Flatten,\n",
    "        BatchNormalization, Activation\n",
    "    )\n",
    "    \n",
    "    # Configurar para usar GPU si est√° disponible\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"‚úÖ TensorFlow importado - GPU disponible: {len(gpus)} GPU(s)\")\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        print(\"‚úÖ TensorFlow importado - Usando CPU\")\n",
    "    \n",
    "    print(f\"   Versi√≥n TensorFlow: {tf.__version__}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå TensorFlow no disponible - Instalar con: pip install tensorflow\")\n",
    "    tf = None\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.6: Utilidades\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"‚úÖ tqdm importado (progress bars)\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è tqdm no disponible\")\n",
    "    tqdm = lambda x: x  # Dummy function\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1.7: Configuraci√≥n pandas\n",
    "# ============================================================================\n",
    "\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ CONFIGURACI√ìN COMPLETADA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835d970",
   "metadata": {},
   "source": [
    "## üìÇ STEP 2: Importar y Procesar Datos desde Simulaciones Bladed\n",
    "\n",
    "Este paso replica exactamente la funcionalidad de `sim2csv.py` para:\n",
    "1. **Leer simulaciones Bladed** (archivos `.$TE`)\n",
    "2. **Extraer se√±ales** configurables (VLOS, Blade My, Rotor speed, etc.)\n",
    "3. **Feature Engineering**:\n",
    "   - Crear lags de VLOS (5-25 segundos)\n",
    "   - Crear componentes sin/cos del azimuth\n",
    "4. **Exportar a CSV** con todas las features\n",
    "\n",
    "### üìã Configuraci√≥n:\n",
    "- **Input**: Simulaciones Bladed en `U:\\Studies\\437_lidar_VLOS_IPC\\Outputs\\OPT3\\3600s`\n",
    "- **Output**: CSVs en carpeta del notebook con features engineered\n",
    "- **Features**: 181 features (VLOS lags + sin/cos + targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95a92d",
   "metadata": {},
   "source": [
    "### 2.1: Configuraci√≥n de Paths y Variables\n",
    "\n",
    "**Modificar estas variables seg√∫n tus necesidades:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACI√ìN - MODIFICAR SEG√öN TUS NECESIDADES\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Ruta de postprocessbladed\n",
    "POSTPROCESSBLADED_PATH = r\"C:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\"\n",
    "\n",
    "# 2. Ruta donde est√°n las simulaciones de Bladed (archivos .$TE)\n",
    "LOADPATH = r\"U:\\Studies\\437_lidar_VLOS_IPC\\Outputs\\OPT3\\3600s\"\n",
    "\n",
    "# 3. Lista de nombres de archivos de simulaci√≥n a procesar\n",
    "FILE_NAMES =[\n",
    "    \"0001_DLC1.2_030_000_000_01\",\n",
    "    \"0002_DLC1.2_030_000_000_02\",\n",
    "    \"0003_DLC1.2_030_000_000_03\",\n",
    "    \"0004_DLC1.2_030_000_000_04\",\n",
    "    \"0005_DLC1.2_030_000_000_05\",\n",
    "    \"0006_DLC1.2_030_000_000_06\",\n",
    "    \"0007_DLC1.2_050_000_000_01\",\n",
    "    \"0008_DLC1.2_050_000_000_02\",\n",
    "    \"0009_DLC1.2_050_000_000_03\",\n",
    "    \"0010_DLC1.2_050_000_000_04\",\n",
    "    \"0011_DLC1.2_050_000_000_05\",\n",
    "    \"0012_DLC1.2_050_000_000_06\",\n",
    "    \"0013_DLC1.2_070_000_000_01\",\n",
    "    \"0014_DLC1.2_070_000_000_02\",\n",
    "    \"0015_DLC1.2_070_000_000_03\",\n",
    "    \"0016_DLC1.2_070_000_000_04\",\n",
    "    \"0017_DLC1.2_070_000_000_05\",\n",
    "    \"0018_DLC1.2_070_000_000_06\",\n",
    "    \"0019_DLC1.2_090_000_000_01\",\n",
    "    \"0020_DLC1.2_090_000_000_02\",\n",
    "    \"0021_DLC1.2_090_000_000_03\",\n",
    "    \"0022_DLC1.2_090_000_000_04\",\n",
    "    \"0023_DLC1.2_090_000_000_05\",\n",
    "    \"0024_DLC1.2_090_000_000_06\",\n",
    "    \"0025_DLC1.2_110_000_000_01\",\n",
    "    \"0026_DLC1.2_110_000_000_02\",\n",
    "    \"0027_DLC1.2_110_000_000_03\",\n",
    "    \"0028_DLC1.2_110_000_000_04\",\n",
    "    \"0029_DLC1.2_110_000_000_05\",\n",
    "    \"0030_DLC1.2_110_000_000_06\",\n",
    "    \"0031_DLC1.2_130_000_000_01\",\n",
    "    \"0032_DLC1.2_130_000_000_02\",\n",
    "    \"0033_DLC1.2_130_000_000_03\",\n",
    "    \"0034_DLC1.2_130_000_000_04\",\n",
    "    \"0035_DLC1.2_130_000_000_05\",\n",
    "    \"0036_DLC1.2_130_000_000_06\",\n",
    "    \"0037_DLC1.2_150_000_000_01\",\n",
    "    \"0038_DLC1.2_150_000_000_02\",\n",
    "    \"0039_DLC1.2_150_000_000_03\",\n",
    "    \"0040_DLC1.2_150_000_000_04\",\n",
    "    \"0041_DLC1.2_150_000_000_05\",\n",
    "    \"0042_DLC1.2_150_000_000_06\",\n",
    "    \"0043_DLC1.2_170_000_000_01\",\n",
    "    \"0044_DLC1.2_170_000_000_02\",\n",
    "    \"0045_DLC1.2_170_000_000_03\",\n",
    "    \"0046_DLC1.2_170_000_000_04\",\n",
    "    \"0047_DLC1.2_170_000_000_05\",\n",
    "    \"0048_DLC1.2_170_000_000_06\",\n",
    "    \"0049_DLC1.2_190_000_000_01\",\n",
    "    \"0050_DLC1.2_190_000_000_02\",\n",
    "    \"0051_DLC1.2_190_000_000_03\",\n",
    "    \"0052_DLC1.2_190_000_000_04\",\n",
    "    \"0053_DLC1.2_190_000_000_05\",\n",
    "    \"0054_DLC1.2_190_000_000_06\",\n",
    "    \"0055_DLC1.2_210_000_000_01\",\n",
    "    \"0056_DLC1.2_210_000_000_02\",\n",
    "    \"0057_DLC1.2_210_000_000_03\",\n",
    "    \"0058_DLC1.2_210_000_000_04\",\n",
    "    \"0059_DLC1.2_210_000_000_05\",\n",
    "    \"0060_DLC1.2_210_000_000_06\",\n",
    "    \"0061_DLC1.2_230_000_000_01\",\n",
    "    \"0062_DLC1.2_230_000_000_02\",\n",
    "    \"0063_DLC1.2_230_000_000_03\",\n",
    "    \"0064_DLC1.2_230_000_000_04\",\n",
    "    \"0065_DLC1.2_230_000_000_05\",\n",
    "    \"0066_DLC1.2_230_000_000_06\",\n",
    "    \"0067_DLC1.2_250_000_000_01\",\n",
    "    \"0068_DLC1.2_250_000_000_02\",\n",
    "    \"0069_DLC1.2_250_000_000_03\",\n",
    "    \"0070_DLC1.2_250_000_000_04\",\n",
    "    \"0071_DLC1.2_250_000_000_05\",\n",
    "    \"0072_DLC1.2_250_000_000_06\",\n",
    "]\n",
    "                                            \n",
    "\n",
    "# 4. Ruta donde guardar los CSV resultantes (carpeta actual del notebook)\n",
    "RESULTSPATH = r\"C:\\Users\\aitorredondoruiz\\Desktop\\2B_energy\\__Git\\Lidar_My_validation_VLOS\\data_train_NEW_ML\"\n",
    "\n",
    "# 5. Opciones de procesamiento\n",
    "ADD_UNITS = False  # A√±adir fila de unidades en CSV\n",
    "\n",
    "# 6. Feature Engineering - Lags de VLOS\n",
    "CREATE_LAGS = False  # Crear lags de VLOS\n",
    "LAG_SECONDS = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
    "\n",
    "# 7. Feature Engineering - Componentes trigonom√©tricas\n",
    "CREATE_AZIMUTH_COMPONENTS = True  # Crear sin/cos del azimuth\n",
    "\n",
    "# 8. Posiciones radiales para se√±ales Aero (si se usan)\n",
    "AERO_POSITIONS = [0.0, 6.0, 18.0, 30.0, 46.0, 59.0, 68.25]\n",
    "\n",
    "# 9. Diccionarios de variables a extraer (organizadas por se√±al de Bladed)\n",
    "VAR_DICTS = {\n",
    "    \"Hub_rotating\": {\n",
    "        \"Hub_rotating\": [\n",
    "            \"Blade root 1 My\",  # Momento flector pala 1 (TARGET)\n",
    "            \"Blade root 2 My\"   # Momento flector pala 2 (TARGET)\n",
    "        ]\n",
    "    },\n",
    "    \"Pitch_actuator\": {\n",
    "        \"Pitch_actuator\": [\n",
    "            \"Blade 1 pitch angle\",  # √Ångulo de pala 1\n",
    "            \"Blade 2 pitch angle\",  # √Ångulo de pala 2\n",
    "        ]\n",
    "    },\n",
    "    \"Drive_train\": {\n",
    "        \"Drive_train\": [\"Rotor azimuth angle\"]  # √Ångulo azimutal del rotor\n",
    "    },\n",
    "    \"Summary\": {\n",
    "        \"Summary\": [\"Rotor speed\"]  # Velocidad del rotor\n",
    "    },\n",
    "    \"External_controller\": {\n",
    "        \"External_controller\": [\n",
    "            \"LAC_VLOS_BEAM0_RANGE5\",\n",
    "            \"LAC_VLOS_BEAM1_RANGE5\",\n",
    "            \"LAC_VLOS_BEAM2_RANGE5\",\n",
    "            \"LAC_VLOS_BEAM3_RANGE5\",\n",
    "            \"LAC_VLOS_BEAM4_RANGE5\",\n",
    "            \"LAC_VLOS_BEAM5_RANGE5\",\n",
    "            \"LAC_VLOS_BEAM6_RANGE5\",\n",
    "            \"LAC_VLOS_BEAM7_RANGE5\",\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚öôÔ∏è  CONFIGURACI√ìN CARGADA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìÅ Path postprocessbladed: {POSTPROCESSBLADED_PATH}\")\n",
    "print(f\"üìÅ Path simulaciones: {LOADPATH}\")\n",
    "print(f\"üìÅ Path resultados: {RESULTSPATH}\")\n",
    "print(f\"\\nüìä Archivos a procesar: {len(FILE_NAMES)}\")\n",
    "for fname in FILE_NAMES:\n",
    "    print(f\"   ‚Ä¢ {fname}\")\n",
    "print(f\"\\nüîß Feature Engineering:\")\n",
    "print(f\"   ‚Ä¢ Lags VLOS: {CREATE_LAGS} ({len(LAG_SECONDS)} lags: {LAG_SECONDS[0]}-{LAG_SECONDS[-1]}s)\")\n",
    "print(f\"   ‚Ä¢ Sin/Cos Azimuth: {CREATE_AZIMUTH_COMPONENTS}\")\n",
    "print(f\"\\n‚úÖ Configuraci√≥n lista\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b95e3",
   "metadata": {},
   "source": [
    "### 2.2: Importar postprocessbladed y Definir Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac464b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Importar postprocessbladed\n",
    "# ============================================================================\n",
    "\n",
    "# A√±adir path al sys.path\n",
    "if POSTPROCESSBLADED_PATH not in sys.path:\n",
    "    sys.path.insert(0, POSTPROCESSBLADED_PATH)\n",
    "\n",
    "# Importar postprocessbladed (esto tambi√©n importa numpy y pandas)\n",
    "import postprocessbladed.postprocessbladed as pp\n",
    "\n",
    "print(\"‚úÖ postprocessbladed importado correctamente\")\n",
    "print(f\"   Versi√≥n numpy: {np.__version__}\")\n",
    "print(f\"   Versi√≥n pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8116a0",
   "metadata": {},
   "source": [
    "### 2.3: Funciones de Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vlos_lags(df, lag_seconds_list):\n",
    "    \"\"\"\n",
    "    Crea features de lag para las variables de velocidad del viento (VLOS).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con los datos\n",
    "        lag_seconds_list: Lista de lags en segundos a crear\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con las nuevas columnas de lag a√±adidas\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Feature Engineering] Creando lags de VLOS...\")\n",
    "    \n",
    "    # Identificar columnas de velocidad del viento (VLOS)\n",
    "    vlos_columns = [col for col in df.columns if 'LAC_VLOS' in col and '_lag' not in col]\n",
    "    \n",
    "    print(f\"   Variables VLOS encontradas: {len(vlos_columns)}\")\n",
    "    \n",
    "    # Calcular tiempo de muestreo (dt)\n",
    "    if 'Time' in df.columns and len(df) > 1:\n",
    "        dt = df['Time'].iloc[1] - df['Time'].iloc[0]\n",
    "        print(f\"   Tiempo de muestreo: {dt:.4f} segundos\")\n",
    "    else:\n",
    "        dt = 0.05  # Default 20Hz\n",
    "        print(f\"   Tiempo de muestreo por defecto: {dt} segundos\")\n",
    "    \n",
    "    # Crear lags para cada variable VLOS\n",
    "    print(f\"   Creando {len(lag_seconds_list)} lags para {len(vlos_columns)} variables...\")\n",
    "    \n",
    "    total_created = 0\n",
    "    for vlos_col in vlos_columns:\n",
    "        for lag_sec in lag_seconds_list:\n",
    "            # Calcular n√∫mero de muestras para el lag\n",
    "            lag_samples = int(round(lag_sec / dt))\n",
    "            \n",
    "            # Crear nombre de la nueva columna\n",
    "            new_col_name = f\"{vlos_col}_lag{lag_sec}s\"\n",
    "            \n",
    "            # Crear la columna con shift\n",
    "            df[new_col_name] = df[vlos_col].shift(lag_samples)\n",
    "            \n",
    "            total_created += 1\n",
    "    \n",
    "    print(f\"   ‚úÖ {total_created} features de lag creadas\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_azimuth_components(df):\n",
    "    \"\"\"\n",
    "    Crea componentes seno y coseno del √°ngulo de azimuth del rotor.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con los datos\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con las nuevas columnas sin/cos a√±adidas\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Feature Engineering] Creando componentes trigonom√©tricas del azimuth...\")\n",
    "    \n",
    "    azimuth_col = 'Rotor azimuth angle'\n",
    "    \n",
    "    if azimuth_col not in df.columns:\n",
    "        print(f\"   ‚ö†Ô∏è Columna '{azimuth_col}' no encontrada, saltando...\")\n",
    "        return df\n",
    "    \n",
    "    # Verificar rango de valores para determinar unidades\n",
    "    max_val = df[azimuth_col].max()\n",
    "    \n",
    "    if max_val > 6.5:  # Si es > 2*pi, probablemente en grados\n",
    "        print(f\"   Rango detectado: 0-{max_val:.1f}¬∞ (grados)\")\n",
    "        # Convertir de grados a radianes\n",
    "        azimuth_rad = np.deg2rad(df[azimuth_col])\n",
    "    else:\n",
    "        print(f\"   Rango detectado: 0-{max_val:.1f} (radianes)\")\n",
    "        azimuth_rad = df[azimuth_col]\n",
    "    \n",
    "    # Crear componentes\n",
    "    df['sin_rotor_azimuth'] = np.sin(azimuth_rad)\n",
    "    df['cos_rotor_azimuth'] = np.cos(azimuth_rad)\n",
    "    \n",
    "    print(f\"   ‚úÖ Creadas: sin_rotor_azimuth, cos_rotor_azimuth\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_nan_rows(df):\n",
    "    \"\"\"\n",
    "    Elimina las filas que contienen valores NaN.\n",
    "    Esto ocurre al inicio debido a los lags.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con los datos\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame sin filas con NaN\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Limpieza] Eliminando filas con NaN...\")\n",
    "    \n",
    "    original_rows = len(df)\n",
    "    df_clean = df.dropna()\n",
    "    removed_rows = original_rows - len(df_clean)\n",
    "    \n",
    "    print(f\"   Filas originales: {original_rows:,}\")\n",
    "    print(f\"   Filas eliminadas: {removed_rows:,}\")\n",
    "    print(f\"   Filas finales: {len(df_clean):,}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funciones de feature engineering definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868df6d",
   "metadata": {},
   "source": [
    "### 2.4: Procesar Simulaciones y Generar CSVs\n",
    "\n",
    "**Este proceso:**\n",
    "1. Lee archivos binarios Bladed (.$TE)\n",
    "2. Extrae se√±ales configuradas\n",
    "3. Aplica feature engineering\n",
    "4. Guarda CSV con todas las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8667a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PROCESAMIENTO PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöÄ INICIO DEL PROCESAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Procesar cada archivo de simulaci√≥n\n",
    "for file_name in FILE_NAMES:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìÑ Procesando: {file_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Leer se√±ales de Bladed\n",
    "    print(\"\\n[1/5] Leyendo se√±ales de Bladed...\")\n",
    "    \n",
    "    # Construir path completo\n",
    "    full_path = os.path.join(LOADPATH, file_name)\n",
    "    \n",
    "    # Extraer se√±ales usando postprocessbladed\n",
    "    timeseries = pp.extract_csv_from_binary(\n",
    "        [full_path],\n",
    "        VAR_DICTS,\n",
    "        add_units=ADD_UNITS\n",
    "    )\n",
    "    \n",
    "    # 2. Convertir a DataFrame\n",
    "    print(\"\\n[2/5] Convirtiendo a DataFrame...\")\n",
    "    df = pd.DataFrame(timeseries)\n",
    "    print(f\"   Shape inicial: {df.shape}\")\n",
    "    print(f\"   Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # 3. Feature Engineering - Lags\n",
    "    if CREATE_LAGS:\n",
    "        df = create_vlos_lags(df, LAG_SECONDS)\n",
    "        print(f\"   Shape despu√©s de lags: {df.shape}\")\n",
    "    \n",
    "    # 4. Feature Engineering - Azimuth\n",
    "    if CREATE_AZIMUTH_COMPONENTS:\n",
    "        df = create_azimuth_components(df)\n",
    "        print(f\"   Shape despu√©s de azimuth: {df.shape}\")\n",
    "    \n",
    "    # 5. Eliminar NaN\n",
    "    print(\"\\n[3/5] Limpiando datos...\")\n",
    "    df_clean = remove_nan_rows(df)\n",
    "    \n",
    "    # 6. Guardar CSV\n",
    "    print(\"\\n[4/5] Guardando CSV...\")\n",
    "    output_filename = f\"{file_name}.csv\"\n",
    "    output_path = RESULTSPATH / output_filename\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"   ‚úÖ CSV guardado: {output_path}\")\n",
    "    print(f\"   ‚úÖ Shape final: {df_clean.shape}\")\n",
    "    print(f\"   ‚úÖ Columnas totales: {len(df_clean.columns)}\")\n",
    "    \n",
    "    # 7. Resumen\n",
    "    print(\"\\n[5/5] Resumen:\")\n",
    "    print(f\"   ‚Ä¢ Filas: {len(df_clean):,}\")\n",
    "    print(f\"   ‚Ä¢ Columnas: {len(df_clean.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Memoria: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"   ‚Ä¢ Features creadas:\")\n",
    "    if CREATE_LAGS:\n",
    "        lag_cols = [c for c in df_clean.columns if '_lag' in c]\n",
    "        print(f\"      - Lags VLOS: {len(lag_cols)}\")\n",
    "    if CREATE_AZIMUTH_COMPONENTS:\n",
    "        print(f\"      - Sin/Cos azimuth: 2\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Archivo {file_name} procesado exitosamente!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ PROCESAMIENTO COMPLETADO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Total archivos procesados: {len(FILE_NAMES)}\")\n",
    "print(f\"üìÅ Ubicaci√≥n: {RESULTSPATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
